{{AAD_navbar}}
== Market Analysis and Portfolio Optimization (a.k.a Stocks) ==
'''Meeting Times:'''
* Meeting 1: Mondays from 5:50-6:30 (after VIP team meeting)
* Meeting 2: Thursdays from 5:30-6:15
* <s>EMADE group: Fridays from 4-4:45</s>

'''Important Links:'''
* [https://docs.google.com/presentation/d/1Fm_pXaKLDFHDEsk-T1rXW1p2UGS8ax2Nvd8k8TIstqs/edit?usp=sharing Midterm Presentation]
* [https://docs.google.com/presentation/d/1arplCjluOGjVm58LiMHV2zVwXl0GCvCsvgb2Ou7nSN8/edit#slide=id.gadbc172287_0_95 Final Presentation]
* [https://github.gatech.edu/rbhatnager3/emade/tree/stocks-base EMADE fork]

=== Week of November 23 ===
''Subteam Meeting (Monday):''
* Abhiram made normalization primitives but there are still some issues we are trying to work through
* According to Dr. Zutty, our genetic labeling approach is an “oracle” that may not translate to real applications very well, and the reason we’re seeing good results is because are trees are optimized on this oracle over this time period.
** To test if this is an issue, Dr. Zutty suggested that we take some data points, don’t use them for the genetic labeling notebook or for emade, and then our best emade individual over that time period to see how our results translate

''Subteam Meeting (Saturday):''
* Review results of recent emade run
* Began preparation for presentations

=== Week of November 16 ===
[[files/Best Individual Evaluation.jpg|700x700px|thumb]]''Subteam Meeting (Monday):''
* Discussed results of trying to figure out the formula used by the paper to calculate the trend score
* Results of Genetic Labelling look promising, discussed diverging our research from what the paper shows
** Will still compare our method with that of the paper in terms of the prices data and Technical Indicators
* Contacted the researchers behind the paper (No response)
* Tasks:
** Finish writing TI primitives ASAP as we plan to run EMADE with Technical Indicator Primitives

''Subteam Meeting (Thursday):''
* Ran EMADE using S&P 500 data with pre-calculated Technical Indicators:
** Found best individual produced 6% profit in 60 testing window
* Made Normalization primitives for dataset normalization ([https://github.gatech.edu/rbhatnager3/emade/tree/668e327aebfd5f9fba4ff584547f68c24a1fd15f Commit])
* Issues and Errors that need to be looked at
* Plan to run EMADE with new primitives this week/over break

=== Week of November 9 ===
''Subteam Meeting (Monday):''
* We are going to start putting a couple more minds towards the sanity checks to try and get some new ideas and hopefully line all of our data up with the paper
* We are also going to continue to develop the genetic labeling notebook, and we may also explore talking an unsupervised learning approach

''Subteam Meeting (Thursday):''
* Rishi updated primitives and verified our TI values are the same as those listed in the paper ([https://github.gatech.edu/rbhatnager3/emade/commit/bb78b1877bafd40227bdb3dae162ffab7fb3a6c8 link to commit])
** Abhiram also fixed the William's R% indicator [https://github.gatech.edu/rbhatnager3/emade/commit/bccf4f5b1eab56e60fce8159ad4a52f300dc57ee here]
* We still can’t figure out calculation for trend signal (but we have gotten everything else to line up with the paper)
* Max tried making an LSTM, but it hasn't performed too well so far
* Abhiram made updates to the [https://colab.research.google.com/drive/1lptoCb1uDJEbklWqPxuZQjh22XZIuHwj?usp=sharing genetic labeling notebook]:
** Normalizing data over a 200 day window
** Adding more TIs as input to model (no real reason to specific TIs, just added popular ones)
** Has trained a basic ML model using the labels, and it has performed pretty well
** We'll see how our results change (or if they change) once we try doing this in emade

=== Week of November 2 ===
''Subteam Meeting (Monday):''
* Dr. Zutty suggested we perform a "sanity check" on our data to see if that helps us understand the anomalies in our results.
** Kartik and Max tested our trading signal formula and had a disparity with the paper's examples. This will need to be looked into further
** Rishi is performing a check on our TI functions (work in progress)
* Plans moving forward:
** Implement more TIs in EMADE
** Resolve issues in dataset/results

''Subteam Meeting (Thursday):''
* Abhiram and Max have been working on a [https://github.gatech.edu/rbhatnager3/emade/blob/stocks-base/testCode-stocks/genetic_labeling.ipynb genetic labeling notebook] that they're using to create a list of labeled data (of correct trading decisions with according to an omniscient agent) that we can use to train emade on.
** Will be used to perform supervised learning 
** We also improved upon the notebook. Ex: the notebook had previously optimized on funds remaining, but this caused the model to just sell all of its stocks at the end of the time period (which is obviously not what it should do). This probably wouldn't be a problem if the model were deployed, because there would not be a defined end bound to the time period on which the model operates (it would just continually operate), but in this case there is (the end bound of the paper's time period). Regardless, we changed the optimization to net worth, which yielded similar profit percentage while still making reasonable trade decisions towards the end of time period.
* Update on sanity checks:
** Kartik and Max tested our trading signal formula and had a disparity with the paper's examples. This will need to be looked into further
** Rishi is performing a check on our TI functions (work in progress)


=== Week of October 26 ===
''Subteam Meeting (Monday):''
* Abhiram and Rishi introduced the first-semesters to the subteam
* We continued to explore the anomalies in our data. We are confused about the trade suggestions and the relevance of the trade signal. We'll continue to look into possible explanations, and whether we should adjust the strategy of our model
''Subteam Meeting (Thursday):''
* Tanishq [https://github.gatech.edu/rbhatnager3/emade/commit/61032bac75d2d21e548a8c2eee1d57402e7bff56 added the TI methods as EMADE primitives]
* We're continuing to look into our recent results (hoping to come up with solution/possible actions by end of next Monday's meeting)
* Some of us are beginning to explore new technical indicators we may use


=== Week of October 19 ===
''Subteam Meeting (Monday):''
* Midterm Presentation
''Subteam Meeting (Thursday):''
* Before the Midterm presentation, we found the truth data itself produced a negative profit percentage
* Data visualization of the SPY dataset found that our moving average values from the AlphaVantage API were inconsistent with the price values
** First used GOOGL stock data since it was much richer with many peaks and troughs
** Found that we are not using adjusted price data, so switching to take make the moving average look more accurate
* Found that our profit percentage calculation was reversed, resulting in the function buying at high and selling at low
** Reversing improved the profit percentage from -11% on test data to 130%
* Finished writing new functions to evaluate Technical Indicators
** Need to add them as primitives in EMADE
* Delegating work for the first semester students to do
** first semesters will help us write new primitives to calculate technical indicators and modify hyperparameters on technical indicators
*** ex. Moving Average primitive will take a parameter of period of time or decay weighting for exponential moving average

=== Week of October 12 ===
''Subteam Meeting (Monday):''
* Discussed tasking slides for the presentation
** Template has already been made, we just need to make graphs and add images to explain our findings.
* Complete another run of EMADE using our new dataset
''Subteam Meeting (Thursday):''
* Completed the remainder of the presentation and practice individually
* Max and Karthik developed a function to calculate the profit percentage of an individual on test data

=== Week of October 5 ===
''Subteam Meeting (Monday):''
* Spoke with Dr. Zutty on how to go about our research as a whole
** Instead of using EMADE as a means of finding optimal parameters on learners, use EMADE to evolve the means of calculating Technical Indicators
** Dedicated a few people to research methods of implementing TIs as STREAM_TO_FEATURES primitives
* Insisted on using our prepared dataset to generate results for our midterm presentation
''Subteam Meeting (Thursday):''
* Ran 52 generations of EMADE with S&P 500 dataset and found a minimum standard error of 10.8387 for a few models (27.2492 was best of seed individuals)
** Have not been able to replicate this improvement in subsequent runs of EMADE
* Discussed list of priorities for the new couple weeks
*# Analysis of big_colab_run and at_test database
*# Finish Midterm Presentation with results
*# Writing STREAM_TO_FEATURES primitives
*# New Run with evolution primitive changes (already made changes just need to run
*# Writing fitness function to calculate profit instead of MSE
*# Implementing Keras in Stocks branch
*# Research implementing CEFLANN in EMADE
* Improved learner modification primitives to ensure non-negative values and float values in 0-1 range. Also temporarily removed the selection of Ensemble primitives as 'SINGLE' was the only one that worked for regression.
* Went over the structure of the midterm presentation and dedicated a few people to begin preparing it.

=== Week of September 28 ===
''Subteam Meeting (Monday):''
* David and Tanishq conducted research on the CEFLANN architecture so we can better gauge how feasible it would be to implement into emade (it seems doable)
* Max and Kartik were able to run an emade regression problem on colab

''Subteam Meeting (Thursday):''
* Abhiram added regression functionality to our fork of emade
* Rishi is nearly finished getting usable S&P 500 data from the AlphaVantage API. We will conduct a colab run of EMADE with this data once it's ready to go.
** Update (Monday, 10/5): Rishi finished the script and created a csv with the data needed for a run
=== Week of September 21 ===
''Subteam Meeting (Monday):''
* Discussed findings from research over the weekend
* Colab is properly set up for use with a 20 GB AWS server we created
* Tasks before the Thursday Meeting:
** Everyone: Run EMADE through Colab and check results in MySQL Workbench
** Look into using multiple Colab instances for a master-worker setup through Colab
** Run our regression dataset in EMADE

''Subteam Meeting (Thursday):''
* Max and Karthik ran EMADE with our initial Stocks dataset and found that it takes several generations to even generate valid individuals
** Maybe because there are only 5-6 regression-based primitives in EMADE - Look into making more
* Looked into seeding EMADE with a valid individual to kickstart evolution
* Tasks before next meeting:
** Research how a CEFLANN architecture can be implemented in EMADE and how it is different than the NNs that NLP has been doing
** Find more regression primitives to use in EMADE

=== Week of September 14 ===
''Subteam Meeting (Monday):''
* Decided on [https://doi.org/10.1016/j.jfds.2016.03.002 a paper] to focus on for the next couple weeks
** Will try to mirror the study's methodology using EMADE, and see if we can outperform the paper's results using automated algorithm design
* Current plan is to use Google Colab for our runs of EMADE
* For next meeting: read paper linked above

''Subteam Meeting (Thursday):''
* Discussed [https://doi.org/10.1016/j.jfds.2016.03.002 the paper] read in-depth by each member
* Consensus was that paper was interesting and worth replicating in EMADE
* Tasks for next meeting:
** Everyone: Run EMADE on Titanic dataset using subteam fork
** Varies by person: start figuring out how to run EMADE using custom datasets (i.e. not already in the repo) and on Colab, start implementing basic ideas of the paper
=== Week of September 7 ===
''Research Group Meeting:''
* Members: Rishi Bhatnager, Abhiram Tirumala, Kinnera Banda, David Neil Daniell, Anshul Agrawal, Tanishq Sandhu, Joseph Dzaluk
* Of the papers discussed (each member discussed around 3-4 papers they looked at), we decided which papers would be most relevant for our purposes (a list of around 5 papers)
* We will narrow down to 1-2 papers on our Monday meeting (so we can have the whole subteam together)
* Assignment for next week will be to start reading the paper(s) we decide to focus on
* [https://docs.google.com/document/d/1sG8iOknJ61Sry5qHomU9_ACwK7osv4rF-4h15YL7xtw/edit?usp=sharing Meeting Notes]
** Bolded links are for the papers we found most relevant

''EMADE Group Meeting:''
* Members: Abhiram Tirumala, Max Kazman, Kartik Sarangmath, Rishi Bhatnager
* Discussion on how to move forward using EMADE with the papers presented by the Research Group
** Debated the pros and cons of recursive multi-step forecasts and one-step forecasts
** Began brainstorming how EMADE's architecture could be used to handle complex time-series stock data
* For next week: Read [https://doi.org/10.1016/j.jfds.2016.03.002 this paper] and start thinking/implementing this algorithm presented in that paper in EMADE

=== Week of August 31 ===
''Subteam Meeting:''
* Decided to suspend the Technical Indicator development group because the goals aligned very closely to the Research Group
* Decided the subteam could follow two approaches to AAD applications
** Developing our own model from scratch by evolving technical indicators and ensemble learners
** Applying a published ML model in EMADE and evolving it to improve it
* [https://docs.google.com/document/d/1uFtppXUzxEgP6zGU6rvu_qE8gQIYt18wnvSuxvzhX98/edit?usp=sharing Meeting Notes Google Doc]
''Research Group Meeting:''
* Members: Rishi Bhatnager, Abhiram Tirumala, Kinnera Banda, David Neil Daniell, Joseph Dzaluk, Tanishq Sandhu
* Presented notes on different types of technical indicators and which ones are more useful for ML applications
* Each member should research on some published research on ML in finance to come up with ideas on how we should implement in EMADE
** 3-4 papers each, read about 2 of them
* General Meeting time of 5:30 on Thursdays
* [https://docs.google.com/document/d/1NSX0v9jswtGxGBmf0XUtxwnAriEzNzWODmC7aI3iykA/edit?usp=sharing Meeting Notes Google Doc]
''EMADE Group Meeting:''
* Members: Rishi Bhatnager, Max Kazman, Kartik Sarangmath
* In-depth review of potential designs for the model which will be used to suggest trades
* For next week: conduct test run of EMADE and familiarize ourselves with running EMADE on custom datasets (i.e. datasets not already in EMADE's templates)
* [https://docs.google.com/document/d/1ITdSDvSdcw_UCRLP3xziLyaUqQGAphQp77K1RtKmiDo/edit?usp=sharing Meeting Notes]

=== Week of August 24 ===
''Subteam Meeting:''
* Met with team to discuss Sub-Team objectives, scope, and responsibilities
* Goals:
** Predicting stock prices after some interval
** Know which features we will use and how we will use them
** Reach Goal: decide buy/sell
* Splitting into 3 groups/tasks for the first half of the semester:
*# Research ML used in trading. (Group A)
*## Rishi, Joseph, Kinnera, David, Tanishq, Anshul
*# EMADE integration with pre-developed Technical Indicators (Group B)
*## Abhiram, Kartik, Max
*# Developing Technical Indicators in EMADE (Group C) (Low Priority until research is substantial)

* Assigned Sub-Team Co-Leads: Rishi Bhatnager and Abhiram Tirumala
* Team will regularly meet from 5:50pm-6:30pm on Mondays and group meeting times are TBD

''Group A (Research) Meeting:''
* Met to discuss which stock(s) we would focus on trading and what metrics, indicators, etc. our model would use to predict a security's price after some interval of time
* Decided to focus on an S&P index fund ETF to simplify our task in the beginning
** Index fund will limit volatility and doesn't rely on news sentiment as heavily as individual stocks do
* Will focus on trend, momentum, volume indicators
* Tentative design for model: regression model that will try to predict a security's price after some interval, and a secondary the security should be held or bought/sold (and in what quantity) based on the predicted price

''Group B (EMADE) Meeting:''
* Met to discuss steps moving forward for using the outputs of technical indicators in EMADE
* Looked through ways to get stock data and technical indicators
* Need to figure out how to solve regression problems in EMADE
* Created [https://github.gatech.edu/rbhatnager3/emade fork] of emade repo

== EZCGP ==

===== Weekly Meeting Time: Thursday, 5:00-6:00 pm EST =====

=== Notes: ===

=== Week of August 17 ===
* Had first meeting of the semester, group now down to 4 people.
* Decided to have weekly meeting Thursdays at 5 PM.
* Splitting into 2 subgroups, currently 2 people per group, 
** Research and new features sub-team: focuses on researching ideas to implement into ezCGP 
** Code maintenance: works to maintain the current code base and establish baseline performance

=== Week of August 24 ===
* Had second meeting of the semester.
* Split into 2 sub meetings
** Research/New Features Sub-Team (Ford and Bojun)
*** Abstract -> Results -> Methodology -> Full Read -> Resources
*** Each member is finding 15-20 papers each by next meeting, and then be cut down to around 3-5 to work on actual implementation.
** Code Maintenance and baseline group (Henry, Daniel, and Hemang)
*** The second group will work on getting the current code base evolving consistently and well to establish a benchmark for how the current framework runs.
*** Baseline Goals:
**** Produce a baseline of Y# trials for Z# of generations a population size of __ and a mutation rate of ___
*** Their timeline will likely be: 
**** Make sure all the primitives from last semester are translated over and are working cleanly
**** Get it evolving on one gpu
**** Get it evolving on multiple gpus.
**** Get baseline scores

=== Week of August 31 ===
* Met on Monday Aug. 31 and Thursday Sept. 3
** Research Team
*** Finalized Timeline for Paper Hunt
**** Get long list of papers by September 10th
**** Shorten list of papers to 3-5 by September 17th and present why they were chosen to group
*** Discussed papers we have already found
** Implementation Team
*** Identified which primitves we want to transfer
*** Tested run on [https://vip.gatech.edu/wiki/images/thumb/3/34/Test_run.jpg/482px-Test_run.jpg multi-gaussian test problems]
**** Make sure added primitves working in sample problem

=== Week of September 7th ===
* Met on Thursday Sept. 10th
** Research Team
*** Discussed papers we had found
**** In  depth discussion on one paper each
*** Get papers distilled down to 3-5 by Sep 17 meeting
** Implementation Teams
*** Working on migrating old primitives to new ''2020F-BaseCodeDevelopment branch''
**** Convultion and dense layer primitives
*** Created [https://github.com/ezCGP/ezCGP/wiki/Github-Flow:-Committing-Code guidelines] for the team's code development workflow

=== Week of September 14th ===
* Met on Monday Sept. 14th and Thursday Sept. 17th
** Research Team
*** Discussed papers we had found
**** Briefly described each of the 5 papers we had decided on to the rest of the team
***** Link to papers: https://drive.google.com/drive/folders/1vR4CSVpKVIZpGTjfyv1O_9NcFC9-XakX?usp=sharing
** Implementation Team
*** [https://github.com/ezCGP/ezCGP/pull/87 PR] made for porting convolution primitives into new stable ''2020F-BaseCodeDevelopment branch''
*** Issues assigned to members and are in progress

=== Week of September 21st ===
* Meeting on Monday Sept. 21 Thursday Sept. 24
* Will discuss new timelines and actions in breakout after main meeting
** Research Team
*** Met and Rodd went over new code base so we can transition into development team for new features
*** Work on finding seeding high preforming individuals into the new framework
**** Learn new framework along with Tensorflow 2
** Implementation Team
*** Working on merging [https://github.com/ezCGP/ezCGP/pull/87 PR] for convolution2D primitive and added Conv2D transpose primitives
*** Testing loading sample data into ezCGP ([https://github.gatech.edu/emade/ezCGP/blob/master/data/data_tools/data_loader.py data_loader.py])

=== Week of September 28th ===
* Meeting on Monday Sept. 28 Thursday Oct. 1
* Meeting after main meeting to discuss tasking
* Thursday meeting consisted of going over the Augmentor pipeline 
** Research Team
*** Get the conda environment working on PACE again
*** Running into issue where it says 'Disk Quote exceeded'
** Implementation Team
*** Finishing debugging the evaluation of randomly generated networks but with a small dataset and small training step (~30 images, batches of 5, 2 epochs).
**** An issue came up where the transfer learning methods we are implementing are not implemented efficiently (see issue #103), so we're hoping to fix that before running on pace.

=== Week of October 12th ===
* Meeting on Monday Oct. 12 Thursday Oct. 15
* Meeting after main meeting to discuss tasking
** Made general outline for what we wanted presentation to look like
* Thursday meeting was for going over and finalizing presentation
** Research Team
*** Working on making presentation for midterms presentations
** Implementation Team
*** Made PR for changing structure in which transfer learning is used as a primitive
*** Need to talk more about how it will actually be structured though

=== Week of October 19th ===
* Monday presentation day
* Presentation Link:
** https://docs.google.com/presentation/d/1mDjWHefrsxjaRfNSOVlTNI4f-g0Tl0ANK-P-5Z-d6bw/edit?usp=sharing

=== Week of October 26th ===
* Meeting on Monday Oct. 26 Thursday Oct. 29
* Meeting after main meeting to discuss tasking
** Research Team
*** Review code and begin implementing research paper
*** 2 Week timeline
** Implementation Team
*** Updated branch to the newest feature branch
*** Still working on fixing issues for baselines.


=== Week of November 2nd ===
* Meeting on Monday Nov. 2 Thursday Nov. 5
* 2 sub meetings after main VIP meeting
** Rodd doing code review and Q/A with new students
** Returning members going to talk about whats new since last meeting.
* Research Team
** Had code review with Rodd, working on implementation
* Implementation Team
** Still working on fixing issues for baselines.

=== Week of November 2nd ===
* Meeting on Monday Nov. 2, Thursday Nov. 5, Saturday Nov. 7
* 2 sub meetings after main VIP meeting
** Rodd doing code review and Q/A with new students
** Returning members going to talk about whats new since last meeting.
* Research Team
** Had code review with Rodd, working on implementation
* Implementation Team
** Still working on fixing issues for baselines.

=== Week of November 9th ===
* Meeting on Monday Nov. 9, Thursday Nov. 12
* Updates:
** Had meeting on Saturday to get PACE working
*** Fixed issues, everyone has PACE environment setup, should have some baseline runs done by next meeeting
** Updated and fixed cv2 methods for our preprocessing block 
*** Certain methods were failing on 3 channel images

=== Week of November 16th ===
* Meeting on Monday Nov. 16, Thursday Nov. 19
* Updates:
** Final Presentation Timeline
*** Thursday 19th, build out a skeleton/outline
*** Over the weekend, fill in our respective slides
*** Monday 23rd have  a rough draft ready and finish filling in slides
*** Monday 30th go through final draft and make sure we are ready
*** Wed Dec 2nd present
** Baseline Runs
*** Seeding Runs from previous runs with a 10hr runtime on PACE
*** Need to make sure we have enough RAM and good GPU/CPU to run on

=== Week of November 23rd ===
* Meeting on Monday Nov. 23, Thursday Nov. 26
* Updates:
** Final Presentation Timeline
*** Thursday 19th, built out a skeleton/outline
*** Over the weekend, filled in our respective slides
*** Monday 23rd have  a rough draft ready and finish filling in slides
*** Monday 30th go through final draft and make sure we are ready
*** Wed Dec 2nd present
** Baseline Runs
*** Boosted to 128 GB of RAM and running with 2 GPU's
*** Found issue with evolution being greedy
**** We would go from 20 individuals to 400+ individuals each generation
**** We allowed each block to produce children through mating + mutating

=== Week of November 30th ===
* Meeting on Monday Nov. 30, Wed. Dec 2 Final Presentation
* Updates:
** Final Presentation Timeline
*** Thursday 19th, built out a skeleton/outline
*** Over the weekend, filled in our respective slides
*** Monday 23rd have  a rough draft ready and finish filling in slides
*** Monday 30th go through final draft and make sure we are ready
*** Wed Dec 2nd present
** Baseline Runs
*** Working on finalizing results for presentation
* Final Presentation Link:
** https://docs.google.com/presentation/d/1cbx_daOsFvMZIgBQvVnmiJBmmm-Lvha61Mfe68Ej7c8/edit?usp=sharing

== NLP ==

===== Weekly Meeting Time: Friday, 4:30-5:30 pm EST =====

=== Notes: ===
Organizing Google doc: https://docs.google.com/document/d/1jrezh0mv2DKAzgtlhbHza7O9h7FKutCCPlP5enfTmP4/edit?usp=sharing

=== Week of November 23 - November 30 ===
* More or less focusing on the final presentation and getting results
* Presentation Link: https://docs.google.com/presentation/d/1Z8PjzaP_LjhyycpGSeXX7rIHdVYkxHzqg8vaKVEYhdU/edit#slide=id.gade55d12ba_0_17
* Some struggles with PACE still in place - > conda env issues and out of memory issues.
* We may push novelty change results and amazon dataset runs for future work but will touch upon them in final presentation.

=== Week of November 16- November 23 ===
* Our focus upto the final presentation is going to be on making sure we have trial runs on both datasets, and be able to get results.
** Runs split between ICEHAMMER and PACE
** Targeting to complete before final presentation: 
*** 2 baseline runs for both toxicity and chest xray datasets each, 
*** 2 runs with neural networks + all primitives enabled
*** 2 runs with all mating and mutation functions activated in addition to ours
*** 2 runs with larger queue size and population
* Going to analyze the outcome of the chest xray runs, calculate the AUROC, and compare it with that of the LEAF paper.
* We should be hitting a feature freeze, debugging and fixing known issues aside (BERT layer is close to being complete)
* We ideally want to complete trial runs with Novelty Change enabled as well as on the Amazon product review dataset.
* Jon wrote great documentation on some of the CV Primitives in EMADE while looking at further features to add in. Link: https://www.notion.so/Computer-Vision-Primitives-6f160347b15c4f3e8c0ccac10b9bc749
* Tasking is mostly based on completing runs and reporting results for now. Repo for organizing results: https://github.gatech.edu/pagarwal80/EMADEResults

=== Week of November 9- November 16 ===
* We're still working on some of the coding portions to get in the features that we have been talking about. No major blockers so far.
* Rest of the team is now going to be putting in efforts to get in runs via ICEHAMMER, PACE, Colab to get a good number of runs for results.
* We are still figuring out the issue with the GPU
* We're going to try out runs on the Amazon product review dataset as well
* Maxim documentation on PACE: https://www.notion.so/Configuring-EMADE-on-PACE-60aedf065abc445096617c3cec875a11

=== Week of November 2  - November 9 ===
* In terms of items in progress, most are still in progress as last week. Major updates:
** Bounding boxes doesn't seem like it will work for the chest x-rays dataset, mostly because there are only 8 classes that have been defined in the training set with bounding boxes, but there are 15 classes to predict. Trying with only those 8 classes also didn't give good results. Next steps are to just simply try VGGNet as a pretrained model.
* Seems like we are not utilizing the GPU with each process during a run. The GPU is being used when trying the standalone tree evaluator or when attempted on a local machine, but not with Icehammer's GPUs
* We are going to be using the Amazon product review for sentiment analysis dataset on Kaggle (https://www.kaggle.com/bittlingmayer/amazonreviews) to explore a novel problem, with Steven and Christopher experimenting with the dataset and seeing what we can do with it.
* Max was interested in getting experiments set up on PACE-ICE and was able to run EMADE there. He ran into the problem of getting only individuals with no fitness, which we are looking into.

=== Week of October 26 - November 2 ===
* Monday meeting was more or less acquainting the first semesters with our goals and tasks
* Friday meeting was a continuation of this introduction- first semesters were given tasking:
** Max is interested in working on the HPC side of things and is going to try and make sure we can get experiments reliably running on PACE-ICE
** Steven and Christopher are going to be working on exploring a new dataset/novel problem to run EMADE on and see how the nn's do in comparison to a good handtrained baseline model.
** Jon is going to work with the CV subteam and help out with their experiments
* We're still working on the same areas in terms of coding out the features, and have a soft deadline of two weeks from now to be able to run experiments and get results.
** CV team focusing on using YOLO architecture with bounding boxes on the chest xray dataset, and additionally adding primitives for edge detection, etc.
** Adaptive mutation is targeting to be done by next week.
** Getting BERT in is still a work in progress. Cameron found some interesting papers about optimal subarchitecture extraction from BERT that we will look at as well.

=== Week of October 19-October 26 ===
* Monday was presentation - We missed some questions on account of overrunning the presentation. We can discuss them during the general meeting.
* Friday Meeting: Mostly focussed on defining plans for first semesters once they are allocated teams
* We should have a run on the chest-xray dataset using YOLO this week.
* For novelty, we have two main options: 
** getting better results than the CoDEEPNEAT paper does on the same datasets. For this, each member is exploring different avenues as brought up before. We don't have the attention layers, adaptive mutation, word embeddings working yet, but we are working on them.
** exploring a new problem that hasn't been focussed on before and showing how EMADE gets great results on it. We spent some time brainstorming a significant problem to this effect.
* Would like some guidance or assistance in figuring out how to implement coevolution (may need some help from the modularity team for this)
* As mentioned in the presentation, there may be value of adding edge detection primitives and thresholding primitives for CV based tasks. Are there any other avenues we should explore in that area?

=== Week of October 12-October 19 ===
* Mostly focused on the presentation as well as figuring out what we need to prioritize based on the paper deadline.
* Presentation Link: https://docs.google.com/presentation/d/1Z8PjzaP_LjhyycpGSeXX7rIHdVYkxHzqg8vaKVEYhdU/edit?usp=sharing

=== Week of October 6 - October 12 ===
* Stats notebook (inserting images is annoying in wiki)
** https://colab.research.google.com/drive/17vC-WvvFMBhG3y8yROE8qSIwhjMEWrdM?usp=sharing
* Discussion on what is "novel": should we expand and try to show that we consistently get comparable to SOTA results on different problems or focus on beating current SOTA for the problems in the reference paper?
** May want to try news classification, CIFAR-10 among others
* Script for parsing out files and analyzing whether certain layer types are present or not
* Adaptive mutation, Attention, shufflelayers are works in progress.
* Question: should we consider looking at defining training time as an objective to minimize?

=== Week of September 28-October 5 ===
*EMADE results with just the mating function added (one point crossover). Removed different optimizers and used only Adam:
**1st run - Best individual score 0.035619017731052915 after 182 generations. Individual: NNLearner(ARG0, OutputLayer(ARG0, GRULayer(64, defaultActivation, 10, falseBool, falseBool, EmbeddingLayer(trueBool, ARG0, fasttextWeights, InputLayer(ARG0)))), 100, AdamOptimizer)
**2nd run - Best individual score 0.03649772477640045 after 332 generations. Individual: NNLearner(ARG0, OutputLayer(ARG0, LSTMLayer(100, tanhActivation, 9, trueBool, falseBool, EmbeddingLayer(1, ARG0, gloveWeights, InputLayer(ARG0)))), 0, AdamOptimizer)
*This is about the same, so not very conclusive. However an issue that we noticed was that there were too many inf individuals in later generations. Can't tell why that is.
**Started 2 runs with larger pool and queue sizes (titanic numbers)
***Run 1:
***Pareto Individual 0 after gen 14 is NNLearner(ARG0, OutputLayer(ARG0, GRULayer(100, seluActivation, 8, falseBool, trueBool, EmbeddingLayer(6, ARG0, gloveTwitterWeights, InputLayer(ARG0)))), 88, AdamOptimizer)(0.035838694492389744, 1490701.0) Age 1.0  Pareto Individual 1 after gen 14 is NNLearner(ARG0, OutputLayer(ARG0, EmbeddingLayer(100, ARG0, glorotNormalWeights, InputLayer(ARG0))), 95, AdamaxOptimizer)(0.043307704377844036, 800001.0) Age 1.0  Pareto Individual 2 after gen 14 is NNLearner(ARG0, OutputLayer(ARG0, EmbeddingLayer(10, ARG0, randomUniformWeights, InputLayer(ARG0))), 100, AdamOptimizer)(0.043841205083947954, 80001.0) Age 1.0  Pareto Individual 3 after gen 14 is NNLearner(ARG0, OutputLayer(ARG0, EmbeddingLayer(5, ARG0, glorotUniformWeights, InputLayer(ARG0))), 96, NadamOptimizer)(0.043966734661854745, 40001.0) Age 1.0  Pareto Individual 4 after gen 14 is NNLearner(ARG0, OutputLayer(ARG0, EmbeddingLayer(1, ARG0, randomUniformWeights, InputLayer(ARG0))), falseBool, AdamOptimizer)(0.04399811705633139, 8001.0) Age 1.0
***Run 2:
***Pareto Individual 0 after gen 31 is NNLearner(ARG0, OutputLayer(ARG0, EmbeddingLayer(3, ARG0, glorotUniformWeights, InputLayer(ARG0))), 100, NadamOptimizer)(0.04349599874470422, 24001.0) Age 1.0  Pareto Individual 1 after gen 31 is NNLearner(ARG0, OutputLayer(ARG0, EmbeddingLayer(2, ARG0, glorotNormalWeights, InputLayer(ARG0))), 100, NadamOptimizer)(0.044876824101678925, 16001.0) Age 1.0
*Dr. Rohling suggested two different approaches once we get to know what the problematic individuals tend to be
**This entails finding the problematic individuals.
**First approach - abort those individuals so that they're not evaluated in the first place
**Second approach - Genomic healing - develop a set if rules according to which we can change the representation of the fatal instances.
*Current approaches we may focus on: 
**Testing the different mating functions (two point crossover), 
**Using our old primitives,
**adaptive mutation implementation (simple thresholding to begin with), 
**further word embeddings, 
**shuffle layers mutation functions
*Asking about PACE-ICE issues, and alternatives
*Possibility of implementing the "codominance" as mentioned in our reference paper

=== Week of September 21-28 ===
* Ran EMADE with the new mating function added for layers. Results from two separate runs:
** 1st run - Best individual had score 0.0358 after 94 generations. Individual: NNLearner(ARG0, OutputLayer(ARG0, GlobalMaxPoolingLayer1D(GRULayer(50, tanhActivation, 32, trueBool, trueBool, EmbeddingLayer(98, ARG0, fasttextWeights, InputLayer(ARG0))))), 0, NadamOptimizer)(0.03565040012552956, 2205701.0)
** 2nd run - Best individual had score 0.03759 after 65 generations. Individual: NNLearner(ARG0, OutputLayer(ARG0, OutputLayer(ARG0, GRULayer(8, softmaxActivation, 5, trueBool, trueBool, EmbeddingLayer(50, ARG0, fasttextWeights, InputLayer(ARG0))))), 100, NadamOptimizer)
* Observations: Mating function did not push the score further, remained about the same.
* Further steps discussed:
** Try a run with the mating function and exclude the optimizers added, and then vice versa to isolate and see if there is one feature that doesn't improve our performance.
** Adaptive mutation is something we can try to incorporate but we need to figure out a good way to implement it
** Using pretrained word embeddings like BERT, ELMo
** The idea of adverserial regularization was brought up, is this something we can possibly try?
** Is there scope of adding a batch normalization layer and will that help?
** Using the primitives developed by the team in previous semesters
* Some notes:
** Can we get a handtrained model using Concatenate layers, since there was apparently a submission on Kaggle using this that did well on the problem

=== Week of September 14-21 ===
* PACE is set up and is working for CV subteam
* Optimizers have been added and pushed.
* Priority for getting better results is the mating function: based off of one point crossover, later steps include adaptive mutation and pretrained embeddings.
* Reviewed paper on adaptive mutation: https://link.springer.com/article/10.1007/s005000000042
** Summary: Not exactly usable in the same format since they are simply using a threshold (average fitness score for individuals) that is compared to an individuals fitness. This is used to modify the MSB/LSB of the "chromosome".  Could be a primitive version of what we use, but not a final version.
** Further readings needed. Potential paper: https://www.researchgate.net/publication/220117106_A_Genetic_Algorithm_with_Adaptive_Mutations_and_Family_Competition_for_Training_Neural_Networks
* Results with activation functions and pretrained embeddings mutation functions:
** Best individual after 100 generations: 0.03555 score, individual: NNLearner(ARG0, OutputLayer(ARG0, GRULayer(55, sigmoidActivation, 10, falseBool, trueBool, EmbeddingLayer(94, ARG0, gloveTwitterWeights, InputLayer(ARG0)))), 100)
** Best individual after 70 generations: 0.03558 score, individual:NNLearner(ARG0, OutputLayer(ARG0, GRULayer(93, defaultActivation, 150, falseBool, falseBool, EmbeddingLayer(1, ARG0, gloveTwitterWeights, InputLayer(ARG0)))), 95)
** The validation accuracy remained the same: 0.04027.

=== Week of September 7 -14 ===
* No Monday Meeting (Labor Day)
* Optimizers have been added to neural_networks.py as params. This allows optimizers to be selected without needing to specify. 
** Need push access to branch
* PACE should be set up for CV team and able to run experiments. Documentation to set up EMADE on PACE will be here soon.
* Decided to do weekly paper R&R as an auxiliary task to augment knowledge and serve as concepts for further goals.
* Goals and Tasks:
** Identifying what individuals get the 0.034 and 0.033 score and seeing if there are any individuals in our search space that can match this.
** Experiment with evolving activations, initializers, optimizers by adding a specific mutation function for each
** Adding mating functions for the layerlists
** Trying to implement adaptive mutation
** Running chest x-ray on EMADE on PACE

=== Week of August 31-September 7 ===
* New Github branch: https://github.gatech.edu/emade/emade/tree/nn-vip
* Multiple different activation functions have been added to the neural_networks.py file as params. This allows layers to run with different activation functions rather than a specified default.
* CV subteam is setting up PACE-ICE and trying to run experiments for preliminary tests
* Looking at output file from a run on toxicity data, ConcatenateLayers was not getting good results when added to the Layer List for an individual, ran experiments to see if this was a bug or just that it was not good for the dataset. Seems to be the latter for now.
* Looking into the concept of adaptive mutation to change the probability of mutation in order to converge faster. Reference paper: https://www.semanticscholar.org/paper/An-Adaptive-Mutation-Scheme-in-Genetic-Algorithms-Sariel/c3a1b67350ea09f042210c17433c74e36df4aee5?p2df

=== Week of August 24-31 ===
* Discussed the split of teams
* Talked about the tasks and what needs to be set up
* '''Short Term Goals:'''
** Set up Google Colab for short EMADE runs to experiment and test
** Explore out files from recent EMADE runs on toxicity dataset to see what model structures are on pareto front and are there any types that don't feature and why
** Adding hyperparameters and optimizers to be evolved
** Explore new mutation functions that can be added
** Explore crossing over of layer lists as a mating function
** Explore the chest x-ray dataset and create potential models as a baseline reference for beginning of semester.
** Define requirement for functions that need to be added to EMADE to facilitate CV tasks
** Explore adding new word embedding system like ELMO or BERT
* Potential Issues:
** Running experiments - PACE causing TF2.0 issues - need to look into
** Git commits to nn branch without corresponding Jira number.

=== Week of August 17-24 ===
* Met with team to go over team goals for the semester, as well as responsibilities
* Went over developments on the nn branch made by students working at GTRI over summer
* '''Semester Goals (Long Term):'''
** Continue working on expanding the Neural Network integration into EMADE
** Run experiments on datasets to obtain results and submit paper to GECCO
* '''Week Goals:'''
** Split into 2 subteams - one to work on CV based tasks and working with chest x-rays dataset and the other to work on the implementation of functions in EMADE
** Splitting of people and discussion of tasks
** Exploring out files from recent runs of EMADE on toxicity dataset

== Modularity ==

=== '''Week of August 24-31''' ===
* Met with team to discuss team goals for the semester, new research areas, and responsibilities
** ''Semester Goals:''
*** Have more trials runs and have more analysis
**** If we get good results/significance, make a more formal writeup
*** Look into more literature to determine how we can improve our current infrastructure
*** Run our changes on a different dataset, possibly images not just titanic so the problem isn't as trivial
*** Look into limiting our changes to only modify EmadeDataPairs
**** Hopefully will create more impactful changes 
** ''Week Tasking''
*** Try to get GCP working so we can run our experiments there
**** Currently working with Jason on this task
*** Everyone should try to run our fork to get more local runs
**** Found some bugs from last semester that need to be fixed
*** We're doing a literature review this week
**** Everyone will bring in a paper and a summary on our Friday meeting
**** We'll discuss the results of the paper and how we can possibly integrate it into our changes
**** Possible places to look, previous GECCO papers and with a focus on novelty and modifying the evolutionary process
* Team will regularly meet from 4pm-5pm EST on Fridays

=== Week of August 31 - '''September 7''' ===
* ''Previous Tasking Updates''
** Had a literature review
*** Discussed and summarized the papers we found
*** In particular we focused on potential other modularity/abstraction structures and ways to improve our current structure
**** Current ideas include ways to introduce novelty to improve performance, a randomness reduction method when selecting our reusable blocks, etc.
**** There was an interesting paper on abstracting layers in HyperNEAT if the NLP team ever wants to collaborate
** GCP is up and running
** Runs on our local machines are taking too long/resource intensive
*** We have some runs from our locals, but our focus should be on moving to cloud services
* ''Current Tasking''
** Currently running experiment trials for Selection Method from previous semester
*** Hoping to get 10-15 trials to do analysis on
** Debugging Differential Fitness so we can do runs on that after Selection Method
** Get our EMADE fork running on PACE and Colab since GCP is expiring soon

=== '''Week of September 7 - 14''' ===
* ''Previous Tasking Updates''
** Currently have 10+ trials for Selection Method
*** Performing analysis on results
*** [[files/SelMetAUCUpdate.png]]
*** [[files/SelMethP-valueUp.png]]
** Want to start running experiments on Differential Fitness
** Progress being made on PACE and Colab
*** Need a few things clarified
* ''Current Tasking''
** Get EMADE working on PACE and Colab by the end of the week
** Start looking into ways to improve our selection method for individuals and ARLs
*** Improve our tournament select modification from last semester (~1-2 weeks)
**** Potentially conflicting crowding distance and ARLs i.e. may be working against each other
**** After gaining more insights from analysis, find a way to make ARL individuals more prevalent
*** Limiting ARLs to only return an EmadeDataPair (~1-2 weeks)
**** Will hopefully act on more useful primitives that actually modify the data
** Continue reading papers

=== '''Week of September 14 - 21''' ===
* ''Week Tasking''
* Need to debug one of our branches
** Running into "Cannot connect to database during query" MySQL error
** Not sure if it's a local database issue or a code issue
* Running into issues with PACE
** inputSchema.xsd file is running into issues
** Comparing with the nn branch
* Colab
** MySQL database website isn't working well (?)
** Opting to try to connect to local ip
* EmadeDataPair constraint
** Hopefully can run quick runs to see results


=== '''Week of September 21 - 28''' ===
* ''Week Tasking''
* Colab
** Is up and running
** https://colab.research.google.com/drive/1i_niAH2dxqdsdA-SMU3tYXCOW0DMRUXK?usp=sharing
** Database currently hosted on AWS free tier
** Planning to run new selection changes this week
* Running into issues with PACE
** When importing sep there's an issue with cpython and linux?
* EmadeDataPair constraint
** New ARLs seem to be more "useful" at first glance
** However, currently exploring a bug with how the functions are compiled, ARL string is not being properly read
* MNIST Dataset
** Hoping to run our changes on a new dataset to see if there are any improvements or if we can leverage more primitives
** Planning to prepare the dataset in the coming weeks
** http://yann.lecun.com/exdb/mnist/

=== '''Week of September 28 - Oct 5''' ===
* ''Week Tasking''
* Colab
** Running experiments on Kevin's new selection method
** Running experiments/debugging Differential Fitness
* EmadeDataPair constraint
** New ARLs seem to be more "useful" at first glance
** Due to previous design choices from ~2 semesters ago, Arg0 cannot be part of an ARL
** Unfortunately this is preventing these useful ARLs from being created
** Currently working on debugging/refactoring
** May introduce "partial" ARLs
* MNIST Dataset
** Currently exploring dataset and writing cleaning script

=== '''Week of Oct 5 - Oct 12''' ===
* ''Week Tasking''
* EmadeDataPair issue has been fixed
** Can now properly create ARLs that have ARGs
* Experiment Focused Week before presentations
** Merge in DataPair fix
** Run experiments on new DataPair Constraint
** Run experiments on new selection method
** Run experiments on Differential Fitness

* MNIST Dataset
** Currently writing cleaning script
** Need to ask about preferences on how to store the data

=== '''Week of Oct 12 - Oct 19''' ===
* ''Week Tasking''
* Prepare for presentations and first semesters materials
* Continue making runs to prepare for analysis
* EmadeDataPair/Argument Issues
** Still some edge cases that need to be fixed
** On hold till after presentations
* DataPair Constraint
** Get more runs, only managed to get 1 before presentations
* New selection method
** Getting more runs
* Differential Fitness
** Somewhat on hold, current focus is on current work from this semester

* MNIST Dataset
** Currently on hold until after presentations
** Runs are currently being prioritized
* Set up some time to practice presentations with advisors over the weekend
** Implemented their feedback

=== '''Week of Oct 19 - Oct 26''' ===
* ''Week Tasking''
* Light Week
* Continue making runs when there is time
* Prepare some materials for the new first semester students
** Try to reuse some materials from previous semesters
** Noted from current and past students: Needs better documentation
*** Will make this a goal in this coming semester

=== '''Week of Oct 26 - Nov 2''' ===
* ''Week Tasking''
* First Semesters
** Gave lectures on EMADE and ARLs/ADFs
** Tasked with cloning our fork and running a seeded local run
** Will hopefully start helping with other runs on colab this week
* Selection Method
** [[files/AltSelectionAUC.png]]
** [[files/AltSelectionPVal.png]]
** Currently seeing really good results
** This change was still fairly simple, so we want to look into some more complex selection modifications
** Looking into some literature to reference
** Maybe set up some time this week to discuss?
* DataPair Limit
** Continue doing runs
* Arguments Refactor/Edge cases
** Currently in progress

* MNIST Dataset
** Setting up a work session this week to hopefully finish progress
** Where should we store this data?

=== Week of Nov 2 - Nov 9 ===
* First Semesters
** Gave a run through of Colab
** Now planning on assigning them experiements to run this week
** May give them a statistics/analysis lecture
* DataPair Limit
** Continue doing runs
* Arguments Refactor/Edge cases
** Currently in progress
* MNIST Dataset
** Work session today
** Planning to use the npz format that Ford has been using
** Hopefully will be finished today/this week
* Selection Method
** Interesting and weird result
** At generation 40, learners are never the most frequently ARLs
** Analysis of new selection:
[[files/ADF count for New Selection method.png|none|frame]]
**

=== Week of Nov 9 - Nov 16 ===

* DataPair Limit
** Continue doing runs
* Arguments Refactor/Edge cases
** Currently in progress
* MNIST Dataset
** Testing

=== Week of Nov 16 - Nov 23===

* General Note
** Planning a work session over the weekend to finalize everything before final presentations
** https://lettucemeet.com/l/9xE4A
* '''DataPair Limit'''
** Continue doing runs
** Using Icehammer to speed things up
** Analysis on sample size of 4
** [[files/DataLimitAUCN4Fix.png]]
** [[files/DataPairPValN4.png]]
** No significance yet, but promising results, expecting significance once more samples are collected 
*** May have more impact on a different dataset
** So far seeing more of an impact on later generations compared to other experiments
** ARLs are acting as expected and are more "useful" in our eyes
** Previous ARLs
** [[files/DefaultARL.png]]
** Restricted ARLs
** [[files/DataPairARL.png]]
* '''Differential Fitness'''
** ARLs are a bit more similar to the DataPair limit ARLs in later generations
** [[files/DiffFItnessARL.png]]
*** See FFT and ThresholdToZero
*** Data Pair manipulations
** Analysis on sample size 10
** [[files/DiffFitN10.png]]
** [[files/DiffFitPValN10.png]]
** Shown some significance in generations 15-20
** 3 runs were cut short due to server issues, did some analysis if they were removed to see effects on later generations
** [[files/DiffFitN7.png]]
** [[files/DiffFitPValN7.png]]
** Similar to some of our previous experiments, ARLs had significance in the middle generations but later "converged(?)" in the same place as the baseline
** Need to do some analysis on the composition of the ARLs and compare how similar the final generations are to the baseline
** May also need to do longer runs to see if ARLs perform worse on later generations due to limited search
*** Typical experiments have been stopped at generation 40 as that it typically when the baselines and ARLs align
*** Need to test further to see if the baseline actually outperforms our work
* '''Selection Method'''
** Potential hypothesis for why the previous selection method didn't have any impact is that individuals with ARLs without the Data Pair limit simply didn't have higher fitnesses compared to other individuals so they weren't selected
** Merged in Data Pair changes and running experiment to see if more impactful ARLs will have a greater effect on the selection
** May be interesting to see the differential fitness changes as well
** Kevin can talk about in more detail
* Arguments Refactor/Edge cases
** Testing, will merge before next semester
** Isn't causing too much of a problem from what I can tell, so shouldn't affect the results of the experiments too much
** Any fixes should be beneficial though
** May rerun some experiments once this is fixed
* '''MNIST Dataset'''
** Can be loaded into EMADE but individuals are having issues evaluating
** error(**********Found array with 0 feature(s) […])
** Jacob can go into a bit more detail
* '''Final Presentations'''
** Start preparing final presentation
** Can we schedule some time before the final presentations to have a practice run and get some feedback?
* Goals for next semester / Future Ideas
** Integrate all changes together: Differential Fitness, Data Pair restriction, and Selection Method
** Run experiments on MNIST
*** Expecting filters and data pair manipulations to be more impactful on images
** Look into how similar the individuals in the final generation are to each other
*** Want to understand if we are limited the search of EMADE
*** Maybe use something like KNN to measure similarity
** Look into more heuristics building off differential fitness' success
*** NN team has done something comparing an individual's fitness to the average individual's fitness in a generation
*** Possibly implement these heuristics as objectives and see their impacts
*** Maybe start working with other subteams to see how our changes will affect other applications/problems
** If the current selection method experiment still doesn't affect the number of ARLs, think of other ways to help propagate them through the population
*** Possibly introduce a mutation to replace data modifying primitives with an ARL
*** Look into ways to reward novelty to prevent limiting the search
** Create more documentation and prepare a more formal write up
** Refactor code base to be cleaner and more understandable
** Better integrate some visualization and analysis tools
** Look into integrating both ARLs and ADFs
*** Need to compare the effects ADFs have on EMADE as well
*** Our current baselines are without ADFs or ARLs
*** Would be interesting to see the effects of having both in EMADE
*** Would be a more time consuming project
** Pull in CacheV2 / integrate with up to date EMADE
** Look into the effects of ARLs in unseeded runs
*** I'd expect it to reach a lower AUC faster as it should help propagate fit parts of individuals through the population
*** However, our changes don't start affecting the population until at least one valid individual is found so might not have much impact
* Student Feedback
** For all students in the subteam I'd like some feedback for what we can do better and expectations for future semesters
** https://forms.gle/EBNV9ZYzjRUc91oRA
** Completely Anonymous

=== Week of Nov 23 - Nov 30===

* Worked on final presentations
* Had a dry run with Dr. Zutty and Dr. Rohling
* https://docs.google.com/presentation/d/1KU-tlra_DXV93JOS6NjQ5RB24Urj0-M8F99U3YsQQ70/edit?usp=sharing