== Image Processing ==
'''Meeting Times'''

Wednesdays at 5:45p (hybrid style)

[https://bluejeans.com/253303426/9283 Meeting Link]

[https://drive.google.com/drive/folders/1IRk4E5YBhr0adXikULUOvEEea6jgxWMw?usp=sharing F21 Meeting Notes & other data]

=== Week of October 12th - October 17th ===

* Changed inputSchema to be compatible with PACE. Able to run individual EMADE installations with a shared conda environment.
* Baseline runs showing a very limited set of primitives that do not cause trees to error out, means there are bugs we need to fix
** Realistically won't get new runs with bug fixes until after midterm presentation. 
* Implementing new geometric (simulated binary and blended) and semantic crossover methods to test whether we can improve diversity during the evolutionary process. Done with the implementation just testing now.
* NSGA-III - 
* Hyper features have been implemented. Just need to be tested.
* For this week - now that most of the hyper features, selection methods and crossover methods have been implemented we need to just test against our baseline EMADE run and compare results. Other work for this week is just to get started on the mid semester presentation.

=== Week of September 27th - October 3rd ===

* NSGA-III Implementation - Tweak p parameter, test against working datasets
* Mating/Mutation - New method
* Trouble with seeds
* Hyper feature ideas

=== Week of September 20th - 26th ===

* Selection Methods
** NSGA-III Implementation
* Hyper-feature & Primitive Packaging
** Searching for combinations
* Data Prep/PACE-ICE Setup
** TF/Keras API for Image Data Preprocessing to prep dataset (completed augmentation of the dataset for EMADE runs)
** PACE Storage workaround
** Baseline run unsuccessful, retrying with seed file provided by Anish.
* Mating/Mutation
** Update on geometric semantic genetic programming methods.

=== Week of September 13th - 19th ===

This week we outlined our goals and began tasking ourselves. We will be focusing on a multi-label image classification problem using the chest x-ray dataset. This has been used by a group in the past but we are hoping to improve EMADE’s selection methods and mating/mutation process to better handle images with multiple labels. In particular, we are looking into NSGA-III and Lexicase as selection methods, and improving some existing features by looking for synergies and packaging them together so they’re not broken up by the evolution process.

Question: if we’re comparing performance to the CheXNet paper, how freely can we modify the dataset such that the results are still comparable. 


=== Week of September 6th - 12th ===
[https://docs.google.com/document/d/1MQi36xly1XsjBzVa8GxB9-9LCIY_ahbkibp4lFlqkNI/edit Notes]

We had our first sub team meeting! With the creation/branching off of a new subteam, we decided our overall objectives for the semester are to add new primitives to EMADE that can help with image processing tasks. We'd like to explore how we can use autoML for either image classification or object detection problems. The team agreed to find at least one good paper that had data readily available and a technique that wasn't just "here's our fancy neural network". We were wondering about how to make this a little more "emade-able"

== Market Analysis and Portfolio Optimization (a.k.a Stocks) ==
'''Meeting Times:'''
* Meeting 1: Mondays from 5:50-6:30 (after VIP team meeting)
* Meeting 2: Thursdays from 5:30-6:15

'''Important Links:'''
* [https://github.gatech.edu/rbhatnager3/emade/tree/stocks-base EMADE fork]
* [https://www.sciencedirect.com/science/article/pii/S1568494611000937 Basis paper]

=== Week of September 20 ===
* Literature review on following articles:
** https://www.sciencedirect.com/science/article/pii/S092523120900040X (Prediction-based portfolio optimization model using neural networks)
*** Focused on diversifying portfolio
*** Ensuring portfolio has low risk rather than focused on short term time series analysis
** https://ieeexplore.ieee.org/abstract/document/1257413/references#references (Support vector machine with adaptive parameters in financial time series forecasting)
*** SVM in financial time series forecasting
*** Based on the structural risk minimization (SRM) principle which seeks to minimize an upper bound of the generalization error consisting of the sum of the training error and a confidence interval.
*** Five real futures contracts collated from the Chicago Mercantile Market are examined in the experiment. They are the Standard & Poor 500 stock index futures (CME-SP), United Sates 30-year government bond (CBOT-US), Unite States 10-year government bond (CBOT-BO), German 10-year government bond (EUREX-BUND), and French government stock index futures (MATIF-CAC40).
** https://www.researchgate.net/publication/324802031_Algorithmic_Financial_Trading_with_Deep_Convolutional_Neural_Networks_Time_Series_to_Image_Conversion_Approach (Algorithmic Financial Trading with Deep Convolutional Neural Networks: Time Series to Image Conversion Approach)
*** Used 15 technical indicators with different parameters
*** The models generates two dimensional images of the time series for these indicators over a 15 day period
*** These images are categorized as buy, sell, and hold
*** Seems to outperform the buy and hold strategy for stocks

A good resource to learn more about algorithmic trading: https://github.com/stefan-jansen/machine-learning-for-trading


=== Week of September 13 ===

* Determining semester long goal: Stock generalization or fundamental analysis
* Stock generalization:
** Goal: Finding models/individuals that perform well on multiple stocks
** Model/individual performs well on stocks in one sector as well as a different sector
* Fundamental Analysis
** Using company quarterly balance sheets along with technical analysis to predict stock buy/sells
** Difficult to implement since there is less fundamental analysis data and it can be difficult to manipulate

* https://ieeexplore.ieee.org/abstract/document/4598507 (An empirical study of Genetic Programming generated trading rules in computerized stock trading service system)
*** Trades based on rules generated
*** Rules follow a tree based structure
*** Comparison of GP tree based algorithm to MACD indicator and buy/hold strategy
*** Data used was 30 companies from the Dow Jones Industrial Average (DJIA).


=== Week of April 19 ===
''Subteam Meeting (Monday):''
* Sriram implementing Fibonacci Retracement
* David W will experiment with other evaluation functions and how to assess effectiveness of individual TIs
* We discussed increasing the window size: since our current dataset is fairly small, we will increase the window size from 30 to 40 (so we don’t make the number of windows too small), but on our next run when we have a larger dataset we will increase our window size further 
* Ran EMADE with window size of 40 and using the same eval functions as last run and add on CDF: profit percentage, average profit per transaction, variance of profit percentage, and CDF

''Subteam Meeting (Thursday):''
* Image is how the performance of our best individual from the last run compared to the random distribution. What's interesting is how well it performed despite being so simple (only used one TI).
** The individual: Learner(MyBollingerBand(ARG0, 2, 61, falseBool), LearnerType('DECISIONTREE_REGRESSION', None), EnsembleType('SINGLE', None))
[[files/random_experiment.jpg]]
* David W will continue his experiment by looking for a correlation amongst top performing individuals: which TI’s are most prevalent in good performing individuals
* Fibonacci Retracement was added to emade (one of the first leading indicators we've added)
* For next run:
** We're considering using CDF as an objective function without using full profit percentage and without profit percentage variance
** We will add larger datasets
*** Update: we added XLP (consumer staples ETF) for a relatively stable stock and SH (S&P 500 short) for a downward-trending stock that would be extremely difficult to make a profit off of (so we can see how well emade outperforms the random distribution and if maybe it can find the optimal buy points and make profit). For both stocks, we used a train period of 7 years (2010-16) and a test period of 3 years (2017-19).

=== Week of April 12 ===
''Subteam Meeting (Monday):''
* Discussed improvements to our EMADE implementation in preparation for a run next Monday
* Discussed adding another dataset to our folds to evaluated indiviuals' performance on larger datasets
** Would give us a baseline to see how EMADE performs on data that isn't the data from the paper
** Ideas: S%P 500, XLP, ETFs in the range of 2010 to 2019
* Discussed looking for more technical indicators to add
** Abhiram developed visualizations to compare an individual with a Monte-Carlo random distribution
[[files/AAD Stocks random experiment.png|200px|thumb]]

''Subteam Meeting (Thursday):''
* Discussed new technical indicators to be added (Fibonacci, Stochastic RSI, Beta, Aroon, VWMA, VWAP)
* Max and Kartik developed an eval function that computes the normal CDF of the individual's profit compared to its closest random experiment result
** Abhiram reviewed the code to make sure it works
* Discussed running EMADE separately on each stock, thinking that the optimal individual would be different for different stocks
** Abhiram ran an experiment EMADE run for a few generations on just AAPL and VZ data, and found that good individuals correlated to good performance on other stocks
* Devesh is working on a matric to compute the error in an individual's buy-sell decisions to the nearest local min and local maxes
* Plan to do an EMADE run on Monday using these new functions.

=== Week of April 5 ===
''Subteam Meeting (Monday):''
* Continued to onboard first semester students
* Discussed potential applications of the material from the Stats lecture: prevailing idea was to see if we can conduct Welch’s test on indivduals’ profit percentage on various stocks
* Rishi and Abhiram finished the talib methods are completed (but final optimizations and fixing seeds still need to be done before an EMADE run)

''Subteam Meeting (Thursday):''
* We discussed new possibilities for evaluation functions 
** One suggestion (from Devesh) was if we could try to implement an evaluation function that determines how close our buy and sell points are to the nearest local max and min points
* Set up first semesters with colab and our SQL server
* Max’s random experiments found that we didn’t do that much better than random trading besides on AUO
** Potential solution: instead of having a profit percentage evaluation function, we compute the z-score of individuals’ profit percentage so we can normalize it relative to random trading (because a given profit percentage is more impressive on some stocks compared to others, so this will give an unbiased way for emade to compare stocks)
* Began a run of EMADE
=== Week of March 22 ===
''Subteam Meeting (Monday):''
*Onboarding new students:
**Made a presentation that reviews the basics of EMADE, and what exactly the stocks team is doing with regards to coding
**discussed the paper we are using, tasked them with reading the paper and coming on Thursday with questions
*Discussed what Dr. Zutty meant by Monte-Carlo simulations to compare individuals
**Dr. Zutyy came into the meeting to explain what he meant, and the purpose of it
**Kartik and Max will run an experiment with this
''Subteam Meeting (Thursday):''
*Onboarding new students:
**Questions about what PLR is and what Exponential Smoothing is
*Max ran an experiment with this new random methodology, and how some interesting results
* Rishi and Abhiram will prepare the rest of the TA-lib functions for a run possibly next week
=== Week of March 15 ===
''Subteam Meeting (Thursday):''
*Discussed takeaways from our presentation on Monday
**Increasing evolvability of EMADE individuals - reducing places where EMADE can error
*Dr. Zutty mentioned after the presentation to use a Monte Carlo algorithm to compare to our EMADE individuals
**Randomly decide on buy or sell decisions, and compare profit to that of the EMADE individual (should correlate to the stock price trend)
**We decided that this method had too high of a variance, and that we should instead compareit  with a buy-and-hold scenario
*Discussed possibilities for new fitness functions in EMADE
**Number of Transactions is neither something we want to minimize or maximize generally
**Mean Absolute Error is something that doesn't generally correlate well to profit percentage
**Average Profit Per Transaction (Maximize)
**Variance of Profit per Transactions (Minimize)
**(Individual Profit Percentage) - (Profit from Buy-and-hold)
*People were tasked with thinking of other fitness functions to optimize
=== Week of March 8 ===
''Subteam Meeting (Monday):''
* Discussed improvements to our code base and preparation for an EMADE run later this week
* Dicussed the model that is used in the paper, and tried to replicate it in EMADE using our primitives
** Found that the profit calculation was different than what we had been using, as it accounted for transaction fees and taxes(?)
** Decided not to include tax in the calculation because usually tax is calculated at end of fiscal year
* TA-Lib primitives are being developed to replace the ones that we already have - Will still have to make more volume-based and complex primitives as they are not in TA-Lib

''Subteam Meeting (Thursday):''
* New EMADE Run!
** Modified the evolution parameters to prioritize evolving the seeded individuals by crossover and mutation more than generating new ones
** Increased the population size to 1024 to increase the chance of a valid individual being created
** Ran EMADE for about 30 generations (large population size impacted performance) in 4 hours, only 2 valid individuals were made that were not seeded individuals
*** Both of these individuals performed pretty mediocre and were not complex at all
* Looking into another run next week with some new changes:
** decrease population size to same as before (about 60/generation)
** decrease mutation probabilities
** Found that many individuals were erroring because the mode and axis were not set properly, otherwise was a very promising individual
*** Possible to Hard-code the TI primitives to be STREAM_TO_FEATURES no matter what? All primitives will only work in STREAM_TO_FEATURES Mode?
*** This method will make a lot more valid individuals in fewer generations
** Use the New TA-Lib primitives instead
=== Week of March 1 ===
''Subteam Meeting (Monday):''
* Some general confusion as to the purpose of the genetic algorithm in the paper. A couple people will work on trying to figure this out
* ta-lib looks like good replacement for how we can write primitives
* PLR and exponential smoothing should be good to go

''Subteam Meeting (Thursday):''
* Still some confusion on how the paper is finding the optimal threshold using its GA. Our plan is to just figure out the optimal threshold ourselves and use that so we can move on.
* We are going to try and start wrapping up writing TI primitives so we can focus our efforts elsewhere. We will emphasize writing primitives for TIs included in ta-lib, although the library is lacking in certain areas (e.g. volume indicators), so we'll still need to code some ourselves.
* We are planning on doing a run of emade within a week, and so we'll be preparing for that in the coming days.

=== Week of February 22 ===
''Subteam Meeting (Monday):''
* Discussed new improvements to the PLR Code
* New Technical Indicators implemented:
** Abhiram wrote BIAS, DeltaSMA, DeltaBIAS, DeltaMACD, DeltaSTOCH, DeltaWILLR, and DeltaRSI primitives
** Youssef wrote BiasEMA, DeltaEMA, and finished documentation that was not provided for other TI primitives
** Krithik, Joseph, Youssef, and Kinnera will work on making more volume-based TI primitives
* Found that some of our price data were inconsistent with that of the paper, Rishi will look into a different source to get accurate data

''Subteam Meeting (Thursday):''
* Kartik and Abhiram looked into why our trading signals have flat parts between segments
** Main reason for this be because is a trend has an even length, the peak will fall between two adjacent points, and is therefore offset but the current calculation
* Abhiram and David looked into Exponential Smoothing and how it works, implemented Proof of Concept
* Rishi fetched new stock data from AlphaVantage, far more consistent with paper
* Max looked into a Python library that calculates Technical indicators, discussed how we could integrate that to generate many more TIs
* Looked into how the GA threshold optimization works, and will probably use DEAP to try it out
* Goals (optimistically by next week):
** Integrate Exponential Smoothing and Trading Point Decision in EMADE as a fitness function
** Finish PLR code, generate labels for all 6 datasets
** Build GA algorithm to find the optimal threshold value that makes the most profit
** Prepare Datasets for EMADE runs, as well as XML template
=== Week of February 15 ===
''Subteam Meeting (Monday):''
* In terms of dealing with our data, we are planning on creating a Monte Carlo fold per stock so we can most effectively test our pipeline and create a good predictive model for a given stock
* We considered the possibility of adding stream-to-stream primitives, but this isn't a priority at the moment
* Tasks include fixing Abhiram's PLR code to match the paper's results and adding the paper's primitives to EMADE

''Subteam Meeting (Thursday):''
* Continued to discuss the main paper, as well as how we could use a [https://doi.org/10.1109/TSMCC.2008.2007255 related paper] (one common author and a citation of the main paper)
* There was some confusion on the methodology of the papers and how to replicate the PLR code. We will continue to try and make sense of the paper over the weekend, but to ensure we do not fall into the same trap as last semester, Krithik will begin looking around for another paper in case we choose to shift our focus away from this one.
** Update (2/22): Abhiram and Kartik were able to replicate the PLR code of the paper

=== Week of February 8 ===
''Subteam Meeting (Monday):''
* Discussed Weekly Meeting time and checked with Max and Joseph if they were available
* Kinnera found some potential papers that we would look at, most had interesting results, but we wanted to use data from American Markets
* Looked into sources of the paper we used last semester and found some good candidates

''Subteam Meeting (Thursday):''
* Found a good paper to use: https://www.sciencedirect.com/science/article/pii/S1568494611000937
** Combination of different techniques to build a stock prediciton model:
*** Stock Market Data - Rishi, David, Kinnera
**** Uses various stock tickers that have various long-term trends
**** APPL for long-term bullish (primary dataset)
*** PLR (Piecewise Linear Representation) - Abhiram, Max, Karthik
**** a simple algorithm that recursively finds a piecewise linear fit to the raw stock price data
**** Useful to simplify the time series into simple trends
**** Uses a GA procedure to find an optimal threshold that produces the most profit
**** The local mins and maxes of the output piecewise function are converted into buy-sell labels to train a model with
*** Technical Indicator Inputs - Krithik, Joseph, Max, Youssef
**** Use Several technical indicators as inputs to the model
**** Most of these are already developed in EMADE, just the BIAS indicator, and difference in technical indicators between days need to be developed
**** Simple task, should take less than a couple of hours
*** Neural Networks
**** Uses an ANN for regression training, predicts a value between 0 and 1
**** Maybe expand the NN capabilities of EMADE, but MLPRegressor from sklearn should do fine
*** Exponential Smoothing - Max?
**** After output in generated from the neural network, the values are put into another algorithm to turn the continuous value into a buy-sell decision
* Hopefully we can develop all of these component in 1-2 weeks and start EMADE run after
* A lot of code can be reused from last semester, so this would not be starting fresh

=== Week of February 1 ===
''Subteam Meeting (Monday):''
* Goals for this semester:
** Create a model capable of making a profit on test data
** Find and outperform a new research paper 
* We're planning on exploring some changes to our dataset 
** Instead of just using S&P, we my try to include other stocks/ETFs. Options on the table:
*** Blue-chip stocks in various industries
*** Sector ETFs/indices
*** Small cap stocks 
** We might try to go more granular than daily data (hourly or half-hourly). This could help minimize the effects of non-technical factors such as company news, but it'd also make data more volatile. 
*** We could also look for abnormalities in volume data to account for these factors (e.g. technical analysis could not predict the huge spike in the prices of GME or AMC, but maybe we could infer something is going on based on the fact that their volumes also had a massive spike)
** This may change once we find a new research paper (depending on what dataset it uses)
* Some of the tasks being distributed include looking for a new paper and looking into fundamental analysis

''Subteam Meeting (Thursday):''
* Meetings on Thursdays at 5:30 seems to work for everyone
* Slow couple of days, most people are planning on doing their weekly work over the weekend
* Max will look into unsupervised clustering to find out how to treat trends
* Others will continue tasks from earlier in the week, namely looking for a new paper that is more consistent and better aligns with our new goals (or maybe even one of the ones we liked but didn't choose last semester)

=== Week of January 25 ===
''Subteam Meeting (Thursday):''
* Intro meeting to discuss goals for the semester: ideally we would like to be able to make real-time trades (and build a model formidable enough to do so)
** Can use [https://alpaca.markets/algotrading Alpaca], which has a testing environment so we don't need to use real money
* Abhiram told us that over break he fixed our primitives: we had assumed that EMADE would give us all of the data, but Abhiram explained that instead we get a sliding window of the data (a list of lists). There are many different commits, but the updated file is [https://github.gatech.edu/rbhatnager3/emade/blob/stocks-experimental/src/GPFramework/stock_methods.py here].
* We expressed a couple of different ideas on where to go for the semester:
** We seemed to agree that we did not want to follow a paper as rigidly as we did last semester (it didn't go too well then, and now we also have a better idea of what we want to do, what we can do, how to do it, etc.)
** We might find another paper that looks interesting and (loosely) use it for ideas
** We'll definitely continue to add primitives to EMADE
** We'll continue looking for alternatives to the genetic labelling we used last semester
** If people have differing interests we might spilt into fluid groups temporarily 
* Not everyone could make the meeting, so we didn't make any concrete decisions (we'll do that on Monday when everyone should be in the meeting)

== ezCGP ==

=== Week of January 25-February 1 ===
* Met with team to discuss team semester's goals
* ''Semester Goals:''
** Daniel - Continue CIFAR-10 experiments to identify bottlenecks in the system and fix them
*** Get CIFAR working without transfer learning
** Hemang - Implementing new primitives (recurrent neural networks/transformers)

=== Week of February 1-8 ===
* Research some papers
** Research on Transformers and Hyperparameter tuning using genetic evolution
** Genetic Algorithm for optimizing Recurrent Neural Network: http://aqibsaeed.github.io/2017-08-11-genetic-algorithm-for-optimizing-rnn/
** Lightweight GPT implementation: https://github.com/karpathy/minGPT
** Decided to implement lightweight GPT as a primitive for image classification
* Updated Problem file and removed references to transfer learning ([https://github.com/ezCGP/ezCGP/blob/feature/130-update-problem/problems/problem_cifar_no_transfer.py commit])
** Got access to PACE-ICE
** Will test the updated problem file this week
** Will review results in order to better
* Tested minGPT
** Base model can generate CIFAR-10 like images (not classification)
** Trained on Google Colab: 10+ hours of training for decent results
** Training times are likely prohibitive for the use of untrained architectures within genetic programming applications

=== Week of February 8-15 ===
* Got PACE-ICE setup up for our accounts
** Made a shared .conda configuration file 
* Tried to do a run problem file but would die after ~2 hours 
** Likely due to memory issues 
** Resources used:
 Rsrc Used:  cput=01:53:05,vmem=49973120kb,walltime=01:29:21,mem=14667336kb,energy_used=0
** Results after running:
 234/234 - 10s - loss: 0.8955 - precision: 0.7541 - recall: 0.5941 - val_loss: 1.2060- val_precision: 0.6891 - val_recall: 0.5253
* Working with Rodd to remove extra image data from individuals (related to Augmentor pipeline)
** Hopefully with reduce memory usage and make faster
* Replaced normalization primitive with equalize ([https://github.com/ezCGP/ezCGP/pull/132 PR]) since normalization didn't work with pillow image formatting
** Used pillow function to perform equalization
* We want to have better baseline results by the next meeting
** Want to analyze them in order to determine necessary improvements

=== Week of February 15-22 ===
* Worked to run ezCGP without transfer learning
** Had issues with batch size, incompatible shapes
* Ran with transfer learning again 
* Added multi-channel support to normalize/equalize ([https://github.com/ezCGP/ezCGP/pull/132 PR] merged)
* Test with transfer learning for a full run:
 234/234 - 8s - loss: 0.1286 - accuracy: 0.0077 - precision: 0.9644 - recall: 0.9552 - val_loss: 1.4547 - val_accuracy: 0.0261 - val_precision: 0.7209 - val_recall: 0.6990
* Test with no transfer learning
 234/234 - 5s - loss: 0.5966 - accuracy: 2.3433e-05 - precision: 0.9073 - recall: 0.7457 - val_loss: 1.7232 - val_accuracy: 1.0016e-05 - val_precision: 0.5513 - val_recall: 0.4054
* Cherry-picked best results from generation 0 (initPop0-6) 
* Would fail after 1 generation
** Unable to visualize individual because it died before then 
* We will be working to iron out bugs that are inhibiting our testing
** There seem to be some issues with batch_size and selection that we need to investigate further 

=== Week of February 22-March 1 ===
* Decided to benchmark and fine-tune individual evaluation training times
** Run experiments to figure out training time and batch size for each individual
** Seed architectures that are known to perform well
* Resolved Pipeline Bugs:
** Fixed issues with the pipeline not going to next generation 
*** An issue with return types in the non-transfer learn block definition
** Fixed issue with accuracy not being the categorical accuracy 
** The operator was being unnecessarily added to the augmentor pipeline 
*** Was causing runs to be lower as more unnecessary operators were being added
*** Temporary fix for pipeline wrapper ([https://github.com/ezCGP/ezCGP/commit/452d03194f475db1429b632e3b9d2faf6ffbd8ae Commit Link])
* Ran ezCGP without transfer learning on CIFAR 10
* 8 hours of run, 7 generations
 234/234 - 7s - loss: 0.0573 - categorical_accuracy: 0.9811 - precision: 0.9833 - recall: 0.9789 - val_loss: 4.2954 - val_categorical_accuracy: 0.4552 - val_precision: 0.4639 - val_recall: 0.4467
* Perform runs without pretrained imagenet weights on transfer learning
** Essentially using Resnet architecture to see if ezCGP training parameters are sufficient for converging on CIFAR 10 with a fit individual
** Initial Run:
  234/234 - 8s - loss: 0.1108 - accuracy: 3.7828e-04 - precision: 0.9676 - recall: 0.9618 - val_loss: 1.3538 - val_accuracy: 7.3117e-04 - val_precision: 0.7209 - val_recall: 0.6968
* Action Items:
** Visualize individuals and plot Pareto front
** Come up with detailed methods to analyze results and fine-tune training parameters accordingly
** Run with categorical accuracy and bug fix from Thursday 

=== Week of March 1-March 8 ===
* From 8 hours of run, 7 generations managed to get categorical accuracy ~47%
** Visualized individual
** Updated the old script to display block structure ([https://github.com/ezCGP/ezCGP/pull/151 PR])
** Seems to be successfully adding conv2D layers, need to see why not adding dense layers
** Maybe remove some preprocessing
** Seems to be overfitting by looking at previous individuals, thoughts?
* [[files/Visualize Individual.png|center|thumb|1654x1654px]]
* Tried to run some tests on transfer learning but had issues producing individuals 

=== Week of March 8-March 15 ===
* Tested no transfer learning problem file with max and avg pooling layers and accuracy is similar ~ 56 %
** Still a bit of overfitting  
* Working to add dense layers
** Some issues with shape mismatch
** Likely going to add a dense block after conv2D block size 
** Right there isn't good of a way to check shape when adding primitives

* Worked on producing training time benchmarks outside of ezCGP:

* Replicated the structure to train VGG16 with the same parameters as in ezCGP
* Checked convergence with imagenet weights and no pretrained weights on CIFAR10
* Result:
** 20 epochs with given batch size is likely sufficient to reach convergence on this dataset with good accuracy.
** VGG16 seems converge in around 15 epochs, but more complex architectures might need more training.[[files/Graph of pre train weigths.png|center|thumb|868x868px]]

=== Week of March 15-March 22 ===
* Worked on Presentation 
** Completed corresponding slides
** Created plot and visualization
*** Pareto fronts for no transfer learning
** Discuss new member projects 
* Presentation (https://docs.google.com/presentation/d/1fMtCogms23wqFeJDX-Sf56T6UzzbgvGezD7s9RCG6gY/edit?usp=sharing)  

=== Week of March 22-March 29 ===
* Still working on adding max-pooling / dropout layers and dense layer 
* Ran into some issues with the framework not adding max-pooling even though individuals should be able to evolve to have them (aka we add the parameters and the primitive)
* We decided to use some of the same structures for our after-transfer learning block for the dense layer
** Still having some issues with shape.

=== Week of March 29-April 5 ===
* Added maxpooling and drop layers
** Ran for 50 generations with 8 Individuals 
** Got an accuracy of 68.4% (much better than 56% from before)
** Looking at the individuals the diversity was very limtied
*** After a few generations only one individual was present with small mutations
** Example of good individual:
 [[files/Example Indiivdual.png|center|thumb|1654x1654px]]
** Individuals with droppout layer also had really high loss in the beginning so were most likely to be dropped
* Ran again with 20 individuals
** A bit more diversity but still 2-4 individuals solely presented in later generations  
** Accuracy was about 56% (lower than run where we just optimized one individual)
* Introduced stack to new members
** Will have them working on visualization of individuals, mating, and seeding existing architectures 

=== Week of April 5-April 12 ===
* Made small additions for experimenting:
** Added average pooling 
** Hard-coded some dense layers 
** Still similar results 
* Analyzed why individuals usually have 4-5 nodes
** Examining all the individuals before selection seems to have varying sizes 
** Individuals with 6-8 nodes are being generated just not chosen
** Experimented with changing objective score to be loss and accuracy (maybe would help)
* New members are working on visualization and mating
** Mating Team read ([https://link.springer.com/content/pdf/10.1007%2F978-3-319-77553-1_13.pdf A Comparative Study on Crossover in Cartesian Genetic Programming])
** Visualization Team working on:
*** Shown inactive nodes
**** https://drive.google.com/file/d/1lw63Fr-gPE1fFt6OB_oE9BKD6x9qydIy/view?usp=sharing (Can't add image)
*** Adding node number
*** Add parameter names

=== Week of April 12-April 19 ===
* Visualization Team 
** Added parameters 
** https://drive.google.com/file/d/1EulCP3usaOVv_TKthWYu6-vivBZYirGn/view?usp=sharing
* Mating Team
** Finished last week's paper and now working on implementing benchmark problems (symbolic regression)
* Dense Layers
** Added dense layers 
**** https://drive.google.com/file/d/1_VS1VLRB6Hg92iv3W33UxtDBJFJED4eJ/view?usp=sharing (Can't add image)
** Best accuracy is 55%
** Seems to be issues with the GPU building model:
***  Allocator (GPU_0_bfc) ran out of memory trying to allocate 93.97GiB (rounded to 100900274176)
*** Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
** Unable to do a full run
*** Sometimes an individual would die 
*** Had to assign dead individual fitness to get generations to work

=== Week of April 19-April 26 ===
* Research primitives 
** Scraped layers used by common pre-trained networks 
** Pull layer data from files in tensorflow.keras.applications's github page
** Visualization Plot (https://drive.google.com/file/d/1r7tOgLmRnifylaTwBclGOTXpnb81L6df/view?usp=sharing)
* Mating 
** Experimented with one-point crossover mating
** Created several individuals 
*** Example: (1 / (1 + np.power(data1, -4))) + (1 / (1 + np.power(data2, -4))) #Pagie1 equation
*** Individual: https://drive.google.com/file/d/10R8LHvZqb9pcCWlr1DmxYk2Ozo1lInhb/view?usp=sharing
* Dense 
** Think that capping the parameters fix the gpu issues
** Still need to test more
* Presentation 
** Working on slides
** Assigned presentables

=== Week of April 26-April 30 ===
* Slides completed and assigned
* Presentation: https://docs.google.com/presentation/d/1eMU46VktpHKwrQK5wQQ_oSM8ZK6Zzxky1rn5YFm27iw/edit?usp=sharing

== Modularity ==

[[MODULARITY GUIDE|https://github.gatech.edu/emade/emade/wiki/Modularity]]

=== Team ===
'''''Modularity Sub-Team:'''''
* [[Notebook Vincent H Huang|Vincent H Huang]] (vhuang31@gatech)
* [[Notebook Tian Sun|Tian Sun]] (tsun90@gatech.edu)
* [[Notebook Xufei Liu|Xufei Lu]] (xufeiliu2000@gatech)
* [[Notebook Angela Young|Angela Young]] (ayoung97@gatech.edu)
* [[Notebook Bernadette Gabrielle Santiago Bal|Bernadette Bal]] (bgsanbal@gatech.edu)
'''Graduated Students'''
* [[Notebook Gabriel Qi Wang|Gabriel Wang]] (gwang340@gatech.edu)
* [[Notebook Kevin Lin Lu|Kevin Lu]] (klu@gatech.edu)
* [[Notebook Regina Ivanna Gomez Quiroz|Regina Ivanna Gomez Quiroz]] (rquiroz7@gatech.edu)



=== Week of Oct 11-Oct 16 ===

=== Week of Oct 4-Oct 10 ===


=== Week of Sept 27-Oct 3 ===

* Implemented workaround for add_all_subtrees large individuals bug
** Gabe suggested instead of completely ignoring large individuals for ARL consideration or refactoring current framework, to only consider subtrees which take in an EMADE Data pair
** This should be a lot easier to implement than refactoring current architecture; added to the to-do list.
* Fixed bug regarding incorrect arities in contract_arls
** Example output
```
arl4: lambda arl_arg_0,arl_arg_1,arl_arg_2: (EqualizeHist(arl_arg_0,arl_arg_1,arl_arg_2))
Indiv copy:  Learner(EqualizeHist(ARG0, 2, 3), learnerType('RAND_FOREST', {'n_estimators': 100, 'criterion': 0, 'max_depth': 3, 'class_weight': 0}, 'SINGLE', None))
occurrence!  68 ((('EqualizeHist', 2, 3, -1), ('ARG0', 0, 0, 0), ('-6', 0, 0, 1)), 1)
Learner(EqualizeHist(ARG0, 2, 3), learnerType('RAND_FOREST', {'n_estimators': 100, 'criterion': 0, 'max_depth': 3, 'class_weight': 0}, 'SINGLE', None))
individual before removal [('Learner', 2, 0), ('EqualizeHist', 3, 1), ('ARG0', 0, 2), ('-6', 0, 3), ('0', 0, 4), ("learnerType('BOOSTING', {'learning_rate': 0.1, 'n_estimators': 100, 'max_depth': 3}, 'BAGGED', None)", 0, 5)]
Nodes to remove:  [3, 2, 1]
individual after removal [('Learner', 2, 0), ('0', 0, 1), ("learnerType('BOOSTING', {'learning_rate': 0.1, 'n_estimators': 100, 'max_depth': 3}, 'BAGGED', None)", 0, 2)]
arl to insert <deap.gp.Primitive object at 0x7f205264e6d8> original arity 3 new arity 1
len individual after arl insert 4
individual after arl insert [('Learner', 2, 0), ('arl4', 1, 1), ('0', 0, 2), ("learnerType('BOOSTING', {'learning_rate': 0.1, 'n_estimators': 100, 'max_depth': 3}, 'BAGGED', None)", 0, 3)]
```
** Individuals with incorrect arities still appear in the population
** Problem with occurrences code, not properly including the entire ARL
* PACE-ICE still causing issues
** What we thought switching to PACE-ICE could help us with over Google Collab:
*** Faster Runs (ARLs code doesn't benefit from GPUs on PACE-ICE as much as NN/CV subteams do)
*** Longer Runs (Guide notes a 8 hour limit for PACE-ICE, Google Collab has a 12 hour limit)
*** No inactivity clicking script
*** Not terribly difficult to switch to with the new guide (We have now spent 3 weeks trying to get it to work)
** Switching back to Google Collab
** Stocks data has been migrated into our repo and we're now ready to do those runs

=== Week of Sept 20-Sept 26 ===
* Bug causing the crashes has been identified
** Contract ARLs method wasn't properly updating arities of the node(s) surrounding the contracted ARL
*** Example
           (node 0, arity 2)
                 /  \
 (node 1, arity 0) (node 2, arity 0)

             (ARL, arity 2)
                    \
                   (node 2, arity 0)
** Caused a list index out of bounds error whenever an individual containing such an arl was iterated through in mating, mutating, inserting modify learner, finding all subtrees, etc
** Large chunk of code just wrapped in a try except block
* Problem with add_all_subtrees method
** The current ARL creation code stores all possible subtrees in memory and randomly chooses a number of them, weighted based on its "goodness" (fitness of individual the ARL was created from)
** This causes problems with decently sized individuals (eg, length of 82 and depth of 6)
** Python really doesn't like this, grinds to a halt. Could be running out of memory or just taking a really long time to find all subtrees.
** Workaround: Don't consider individuals above a certain size for ARLs
** Future solution: Refactor code to generate ARLs as the subtrees are found
* Mnist team working through getting everyone on PACE-ICE to do runs
** There was some ambiguity in the instructions which caused some confusion

=== Week of Sept 13-Sept 19 ===
'''Extended ARL'''
* Began doing extended ARL runs
** Starting off with max depth 10 trees
** Everything seems to be working, there exist ARLs with depth > 2
** Example ARL
 Learner(arl_arg_0,ModifyLearnerBool(learnerType('BOOSTING', {'learning_rate': 0.1, 'n_estimators': 100, 'max_depth': 3}, 'ADABOOST', {'n_estimators': 50, 'learning_rate': 1.0}),arl_arg_1))
* New goal: Test the significance of the depth of ARLs on the performance of individuals
** Problem 1: It takes a while for individuals with significant depth to appear, and therefore it takes ARLs with significant depth even longer to appear
*** Working on a seeding file which has more complex individuals so larger ARLs can generate more quickly
*** Manually randomly select individuals from runs which look different from the original seeds
*** Potential problem with limiting diversity?
Old Seeds
 Learner(ARG0, learnerType('RAND_FOREST', {'n_estimators': 100, 'criterion':0, 'max_depth': 3, 'class_weight':0}, 'SINGLE', None))
 Learner(ARG0, learnerType('KNN', {'K': 3, 'weights':0}, 'BAGGED', None))
 Learner(ARG0, learnerType('SVM', {'C':1.0, 'kernel':0}, 'SINGLE', None))
 Learner(ARG0, learnerType('DECISION_TREE', {'criterion':0, 'splitter':0}, 'SINGLE', None))
New Seeds (Used in addition to Old Seeds)
 Learner(ARG0, learnerType('BOOSTING', {'learning_rate': 0.1, 'n_estimators': 100, 'max_depth': 3}, 'SINGLE', None))
 Learner(EqualizeHist(ARG0, 2, 3), learnerType('RAND_FOREST', {'n_estimators': 100, 'criterion': 0, 'max_depth': 3, 'class_weight': 0}, 'SINGLE', None))
 Learner(ARG0, learnerType('LIGHTGBM', {'max_depth': -1, 'learning_rate': 0.1, 'boosting_type': 0, 'num_leaves': 31}, 'ADABOOST', {'n_estimators': 50, 'learning_rate': 1.0}))
 Learner(ARG0, learnerType('ARGMAX', {'sampling_rate': 1}, 'BAGGED', None))
 Learner(ARG0, ModifyLearnerFloat(learnerType('ARGMIN', {'sampling_rate': 1}, 'SINGLE', None), 0.01))
 Learner(ARG0, learnerType('ARGMAX', {'sampling_rate': 1}, 'ADABOOST', {'n_estimators': 50, 'learning_rate': 1.0}))
 Learner(ARG0, ModifyLearnerList(ModifyLearnerInt(ModifyLearnerFloat(learnerType('DEPTH_ESTIMATE', {'sampling_rate': 1, 'off_nadir_angle': 20.0}, 'SINGLE', None), 1.0), notEqual(-2.6349412187954435, 0.1), myIntSub(255, -6)), passList(myListAppend([1, 6], [-2, 14]))))
** Problem 2: There are several uncommon bugs which are ending runs prematurely
*** Both have to do with invalid value encountered in double_scalars individuals[j].fitness.values[l]
*** Might have to do with a floating point error causing divide by zero errors
*** Dr. Zutty mentioned that it could be caused by an unregistered primitive, check the primitives.
 /home/vincent/anaconda3/lib/python3.6/site-packages/GPFramework-1.0-py3.6.egg/GPFramework/adfs.py:134: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
 /home/vincent/anaconda3/lib/python3.6/site-packages/deap/tools/emo.py:735: RuntimeWarning: invalid value encountered in double_scalars individuals[j].fitness.values[l]
 Traceback (most recent call last):
 File "src/GPFramework/didLaunch.py", line 126, in main(evolutionParametersDict, objectivesDict, datasetDict, stats_dict, misc_dict, reuse, database_str, num_workers, debug=True)
 File "src/GPFramework/didLaunch.py", line 116, in main database_str=database_str, reuse=reuse, debug=True)
 File "/home/vincent/anaconda3/lib/python3.6/site-packages/GPFramework-1.0-py3.6.egg/GPFramework/EMADE.py", line 802, in master_algorithm count = mutate(offspring, _inst.toolbox.mutateLearner, MUTLPB, needs_pset=True)
 File "/home/vincent/anaconda3/lib/python3.6/site-packages/GPFramework-1.0-py3.6.egg/GPFramework/EMADE.py", line 600, in mutate mutate_function(mutant, inst.pset)
 File "/home/vincent/anaconda3/lib/python3.6/site-packages/GPFramework-1.0-py3.6.egg/GPFramework/emade_operators.py", line 26, in insert_modifyLearner slice = individual.searchSubtree(index)
 File "/home/vincent/anaconda3/lib/python3.6/site-packages/deap/gp.py", line 180, in searchSubtree total += self[end].arity - 1
 IndexError: list index out of range

 /home/vincent/anaconda3/lib/python3.6/site-packages/deap/tools/emo.py:735: RuntimeWarning: invalid value encountered in double_scalars individuals[j].fitness.values[l]
 Traceback (most recent call last):
 File "src/GPFramework/didLaunch.py", line 126, in main(evolutionParametersDict, objectivesDict, datasetDict, stats_dict, misc_dict, reuse, database_str, num_workers, debug=True)
 File "src/GPFramework/didLaunch.py", line 116, in main database_str=database_str, reuse=reuse, debug=True)
 File "/home/vincent/anaconda3/lib/python3.6/site-packages/GPFramework-1.0-py3.6.egg/GPFramework/EMADE.py", line 1097, in master_algorithm new_adfs, updated_individual_indices = _inst.adf_controller.update_representation(parents) # only modifies parent representation
 File "/home/vincent/anaconda3/lib/python3.6/site-packages/GPFramework-1.0-py3.6.egg/GPFramework/adfs.py", line 1160, in update_representation population_info = self._find_arls(population)
 File "/home/vincent/anaconda3/lib/python3.6/site-packages/GPFramework-1.0-py3.6.egg/GPFramework/adfs.py", line 568, in _find_arls self.search_individual(population[individual_num], individual_num, dictionary, self.max_adf_size)
 File "/home/vincent/anaconda3/lib/python3.6/site-packages/GPFramework-1.0-py3.6.egg/GPFramework/adfs.py", line 1219, in search_individual self.generate_child_dict(individual, child_dict, next_dict, 0)
 File "/home/vincent/anaconda3/lib/python3.6/site-packages/GPFramework-1.0-py3.6.egg/GPFramework/adfs.py", line 623, in generate_child_dict child_idx = self.generate_child_dict(individual, child_dict, next_dict, child_idx)
 File "/home/vincent/anaconda3/lib/python3.6/site-packages/GPFramework-1.0-py3.6.egg/GPFramework/adfs.py", line 610, in generate_child_dict num_children_left = individual[node_idx].arity
 IndexError: list index out of range
* Fixed temporary commits from last semester that were causing issues

'''Mnist Runs'''
* Began working on moving away from Google Collab and towards PACE-ICE
** Once finished, Extended ARL runs can also be done on PACE_ICE

=== Week of Sept 6-Sept 12 ===
09/08/21 - Met with team to discuss team goals for the semester, new research areas, and responsibilities
'''Semester Goals:'''
* Explore runs using stock data 
* Explore left off work from last semester (from Spring 2021 final slides)
** New Models
*** Deep Ensembles with a diversity term[4] 
*** A CNN architecture with decaying learning rate
** Selection Method
*** Modifying the evolutionary selection method to help encourage the spread of ARLs throughout the population and complexity. 
** New Dataset Training
*** Look at other datasets to expand ARL training to see which ARLs stored in the database are the most used and why.
*** Practice on more image datasets and multi-class classification datasets.
** Diversity Measures
*** Create some quantifiable way to measure diversity, generalizable for EMADE. May use a diversity measure as a heuristic when finding ARLs.
** ARL Database Storage
*** Improve the way ARLs are stored in the database to keep any information from being lost
** EMADE Integration
*** Integrate ARLs with EMADE’s original architecture and other modularity techniques[2]
** More Runs

09/10/21 - Time Conflict Meeting

'''Regarding MNIST Runs'''
* Could use:
** datasets/../mnist, gen_mnist - for loading data (pickle formatting)
** templates/../mnist, input_mnist - two objectives are 'precision' and 'recall'
''Regarding Semester Goals''
* Dr. Zutty/Gabe mentioned that another area of research could be looking into 'how ARLs are constructed'
** hyperparameters
** amount of ARLs built per generation (currently default is 5)
** ranking criteria for ARLs
* Also possible to look into writing a paper as we look back at our code
** use LaTeX 'ACM SigConf' template

'''Week Tasking'''
* Set up a weekly subteam meeting time 
* Contact previous semester students (Gabe) to continue modularity work
** AWS storage for ARLs
** Codebase ownership
* Continue major areas of work from last semester: 
** '''ARL Depth''': Vincent, Xufei 
** '''MNIST Runs''': Bernadette, Angela, Tian, Xufei (Runs)
*** Try and understand why there were errors with runs from the previous semester
'''Current Meeting Time: 11-12PM @ CoC Lobby'''

== NLP ==

Meetings: 2:00 pm on Wednesdays, Virtual
=== Week of September 27th - October 3rd ===
* This week we began coding our changes to the infrastructure needed to work with QA problems
* We redesigned our flow based off of feedback from Anish
* Karthik provided us with a CSV
* Steven matched the format of the dataset to be similar to Amazon, and implemented a load_qa_data_from_file
* Devan created a presentation to catch up 2 new members
* Kevin worked with the NNLearner to overload it

=== Week of September 20th - 27th ===
* This week we designed the primitives/infrastructure needed to tackle the QA problem with EMADE.
* There are two major issues, which we designed solutions for. The problems are as follows:
** Problem 1: Unlike other datasets, we have two inputs that the model needs to handle separately: the context and the query.
** Solution 1: We create 2 new primitives, the ContextEmbeddingLayer and the QueryEmbeddingLayer. We also create a new type of data pair that we can fetch both the context and query separately in. Therefore, if the passed in data pair is of this new type, we can return the context and query in the ContextEmbeddingLayer and the QueryEmbeddingLayer, respectively.
** Problem 2: The output is determined by calculating the max probabilities of start and end words of the answer (detailed more in Steven's notebook and below). We cannot solely determine the output by calling model.predict(), as the final output should be a list of size 2N with a softmax applied, where N is the number of words in the context.
** Solution 2: With the different type of data pair, we can check in the NNLearner's code for the type via an "if" statement, and determine the output in this way.
** Example from Steven's Notebook on how the output is obtained: "if our context was 'The Titanic sank in 1912' and we had the output vector (0 0 0 1 0 0 0 0 0 1), then our answer would be 'in 1912'".

=== Week of September 13th - 20th ===
* Dataset work
* Decided on objective functions (F1 vs num params)
* Coded up modified F1
* Review of QA systems

=== Week of September 6th - 12th ===
* We had our first sub team meeting of the year
* We've decided on our goal as using EMADE to look for less complex, yet as accurate Neural Architecture for state of the art Question Answering Systems, similar to how BORT was made as a less complex BERT: https://arxiv.org/abs/2010.10499
* We outlined steps for achieving our goal
** Setup everyone on EMADE/PACE
** Work with dataset, make it work on EMADE
** Implement primitives and infrastructure to make Question Answering problems work with EMADE
** Collect and analyze run results

=== Week of April 19th ===

Breakout Meeting
* Dr. Zutty joined at Cameron's request to discuss NNLearners as subtrees
* Cameron pushed update to PACE files and team branch (adjusted seeding file, naming) on git
* Steven found a couple individuals that don't perform well in standalone tree evaluator to test evolution
* Karthik is playing around with FNR and FPR metrics
* Hua had a few runs (both gpu and cpu only) but they yielded poor results
* Nishant, Prahlad, and Harris have ran EMADE and will review results
* Sumit, Anshul, Heidi, and myself are working through PACE
* Discussed high level goals achieved this term:
** Streamlining PACE env
** EMADE producing competitive individuals

Subgroup Meeting
* Cameron W. continuing with many workers implementation
** Many workers successfully spin up but will attach to the same gpu even when specified not to
** Update 4/25: Cameron pushed large update to github, fully implementing many workers
*** Huge performance increase; typical run completes ~1 generation per hour on Amazon dataset, with many workers 15 generations completed in 2.5 hours (~6 gen per hour)
** Run results: 8 hours, 23 generations, best individuals are on par with seeded ones
** Individual with 93.2% accuracy: https://drive.google.com/file/d/1zOHhgbm6-QYRa4SMTrDbp0BiDB0I_iEk/view?usp=sharing
** Still not getting much more depth than seeded individuals
* Sumit having issues with conda environment (specifically with keras and sklearn packages). 
** Is using the same yml file as everyone else so might be a compiler error.
* Steven working on seeding individuals that performed poorly in the standalone tree evaluator
* Karthik cloning new branch and will get FNR/FPR running
* Hua getting competitive results in his PACE runs
* Nishant, Harris, Prahlad, Cameron B., and myself are troubleshooting miscellaneous PACE issues
** End of meeting was used for helping each other out

Final Week Tasks/Dates:
* Working on presentation
* Practice run of presentation - Wednesday @ 6pm
* Code freeze - Wednesday @ 12pm
** A few people still debugging/altering primitives. This is their deadline
* Last set of runs
* Reviewing results and compiling in presentation

=== Week of April 12th ===

Breakout Meeting
* Anshul and Sumit are finishing up their PACE installation
* Cameron B., Harris, Hua, Karthik, Nishantm Prahlad, and myself have PACE set up
* Karthik created shell [https://github.gatech.edu/cwhaley9/PACE-files/blob/master/pace-login.sh file] to automate SSH connection and launch of EMADE
* Cameron resolved GPU issue
** Completed GPU multi-run 
*** Three 8 hour runs where ouput of run seeded the next
*** 5-6 individuals evaluated (best individual had 0.932 accuracy)
* Steven completed two runs (8 hour GPU, 24 hour CPU only)
** Showed pareto front of his completed run (best individual had 0.9296 accuracy)

Subgroup Meeting
* Team is tasked per [https://docs.google.com/document/d/1V-etbhOdzUfgjwMLX7qtFNQEVGNnmrx5GfaQxfeosJ4/edit Gdoc]
* Harris, Karthik, and Nishant will explore PACE this weekend and learn how querying the db works
* Temi got PACE set up
* Hua test run got stuck in gen 1, to troubleshoot
* Cameron working on a pull request to have many workers available in one run, re-use turned on to allow 24 hour runs
** Oddly a non-LSTM individual in a run had an accuracy ~0.9
* Steven expanding on analysis he showed in the breakout meeting
** Will seed run with bad individuals to see if evolution works
* I will port NLP primitives Notion doc to [https://wiki.vip.gatech.edu/mediawiki/index.php/Guide_to_NLP_Primitives wiki]

=== Week of April 5th ===

Breakout Meeting
* Steven wrote script that generates kfold splits on a given dataset
* Meeting was used for helping everyone get set up in PACE

Subgroup Meeting
* Anshul gave Neural Nets 101 [https://docs.google.com/presentation/d/1CB7nFttRU0psaFTDHHWIScy8nFkvT0X5bTc3T_En808/edit#slide=id.gc84dce302c_2_50 presentation] 
* Cameron and Steven helped with PACE 
** Team is mostly set up, new tasks will be distributed
*** Tackling the "evolution problem" (complexity of individuals) - will require most members
*** Fixing pretrained embedding layers
*** Increasing number of workers on pace (if possible)
* Karthik created a quick shell script to automate the logging into pace and running EMADE
* Steven will have long run over the weekend
* Cameron noted issue in PACE which caused gpu runs to fail
** Update: Cameron found solution to issue, resolved by adding the following line to the "launchEMADE_amazon.pbs" file
*** export LD_LIBRARY_PATH="/usr/local/pace-apps/manual/packages/cuda/11.1/lib64"

=== Week of March 29th ===
Breakout Meeting
* New and old members introduced themselves
* Cameron and Anshul offered to give lectures on the basics of EMADE and NNs during Friday's meeting
* Cameron investigating NNLearner not sending individuals to evaluation and not getting print statements (Zutty offered a few places to look)
* Steven revised code, ran EMADE and hit runtime wall, will be designated PACE helper
* All new members and all old members (less Cameron and Steven) are tasked with setting up PACE

Subgroup Meeting
* Most members had issues with accessing wiki (to follow PACE set up guide)
* Cameron gave EMADE 101 "the basics" [https://docs.google.com/presentation/d/1v33k5I9b-_MIR9f3QhO4U81HJaBRwWqt6xzoSecDsoA/edit#slide=id.p presentation]
** Will give EMADE 102 "the NNLearner" presentation in next breakout
** Will create PACE install guide video
*** Update 4/4: PACE install [https://www.youtube.com/watch?v=LashYCCJF3E guide]
* Anshul to give Neural Nets 101 presentation next availability as well
* Cameron figured out why NNLearners were failing
** Amazon dataset is ~20x larger than toxicity dataset, caused PACE to crash (after tokenizing dataset)
** Will try the following revisions:
*** Reducing size of train dataset (simplest)
*** Using scipy sparse matrices
*** Splitting the dataset into folds
*** Varying the MAXLEN parameter used for tokenization & increasing memory in PACE runs
* Steven using standalone tree evaluator, doing a deep-dive in EMADE (researching source code)

=== Week of March 22nd ===
Subgroup Meeting
* Team discussed pivoting based on Dr. Zutty's feedback on midterm presentation
** Team will refocus any efforts that will not help investigation of trivial solutions
*** Any members not focused on investigation will get PACE set up and help with troubleshooting/runs
* Team discussed how best to onboard new members
** All new members will get set up in PACE
* Steven and Cameron troubleshooting trivial results
** Cameron brainstormed areas to investigate to isolate issue
* Sumit wrapped up benchmarking and will get set up in PACE

=== Week of March 15th ===
Breakout Meeting
* Team will prioritize midterm [https://docs.google.com/presentation/d/1bpIN_1nL6PB8fMq1yvEDQnuy_ktcSY87HV2nxNsvmas/edit#slide=id.gc84dce302c_2_50 presentation]
* Cameron and Steven
** Cameron will study which primitives result in individuals with infinity fitness
** Steven will debug PACE instance
* Alex, Anshul, and Sumit
** Alex built NNLearner in EMADE using LSTM model
** Anshul and Sumit will focus on presentation
* I will copy previous term's slide deck as a framework for this term's presentation
Subgroup Meeting
* Primarily discussed presentation, set up additional meeting on Sunday to have a practice run
* Cameron resolved a few issues and started another EMADE run
* Sumit will add FastText functionality to EMADE
Sunday Meeting
* Discussed work new members will do
* Flow and organization of presentation was iteratively improved through discussion
* Practice run of presentation, ended at 16:20
* Minor revisions made post practice run

=== Week of March 8th ===
Breakout Meeting
*Cameron and Steven
**Cameron and Steven's runs had warning GPU was not used
***Tried upgrading cuda library - no effect
****.pbs script needed to be updated to request GPU nodes
****Cameron will update guide to include this step
*Alex, Anshul, and Sumit
**Alex is reviewing other Kaggle models, will try building from scratch
**Anshul is recreating old model from scratch to see if error was specific to notebook
**Sumit is exploring other baseline models
***Most use fasttext embeddings (not yet implemented in EMADE)
*I will compare PyTorch Lightning with PyTorch methods
Subgroup Meeting
*Cameron and Steven
**Cameron's 8 hour run did not get past gen 0
**Steven's Amazon run went 6 hours  (68 generations) before encountering an error in mutation
***Error may be on DEAP's side
**Neither run had particularly strong individuals (need to seed runs)
**Both are running into database issues when running seeding_from_file.py
***MySQL OperationalError 2002
*Alex, Anshul, and Sumit
**Alex will run EMADE model
**Anshul is building LSTM model from scratch outside of EMADE
**'''Sumit ran baseline fasttext model, 91% accuracy'''
*I am comparing the existing PyTorch methods file to PyTorch Lightning
*This week we will focus on the midterm presentation

=== Week of March 1st ===
Breakout Meeting
* Cameron and Steven
** Both instances of PACE set up
*** Cameron will start an Amazon dataset run
*** Steven ran into a tourney selection issue
* Alex, Anshul, and Sumit
** Alex working on Kaggle Colab notebook
*** May start from scratch and update embeddings file
** Anshul and Sumit are working on Amazon dataset in Colab
*** Colab is disconnecting and reconnecting on model fit process and yielding no error
*** Reached out to Stocks team for guidance
* I opened up the discussion about the PyTorch implementation
** What are the difficulties oh a hybrid implementation? What will it require?
Subgroup Meeting
* Cameron and Steven
** Cameron had outdated Amazon file, updated, and restarted run
** Steven is running Amazon dataset on PACE but hitting max recursion error
*** Anish guided Steven through issue in-meeting
* Alex, Anshul, and Sumit
** Alex working on running Amazon Dataset
** Colab issue persists
*** Sumit resolved unrelated issue then took video of the disconnection/connection issue
*** Anshul will post to stackoverflow and switch gears to a different baseline model
* Will review PyTorch installation/requirements
** Team discussed starting with population level implementation
*** Once implemented, will discuss with team about finer levels of implementation

===Week of Feb. 22nd===
Breakout Meeting
*Cameron and Steven
**Troubleshooting PACE-ICE instances
***Cameron will build and maintain de facto yaml file to prevent new members from the same pain
*Alex, Anshul, and Sumit
**Alex working on Kaggle and LSTM notebook
***Working through issues with embeddings
***Once resolved, will add LSTM primitive
**Anshul is working on Amazon dataset in Colab
***Colab is disconnecting and reconnecting on model fit process and yielding no error
**Sumit will focus on Anshul's Colab notebook to help troubleshoot
*I will add remaining primitives to the documentation
Subgroup Meeting
*Cameron and Steven
**Cameron resolved PACE issues and will run test next
**Steven troubleshooting PACE (had working session in meeting with Anish's guidance)
***PACE now works using selection nsga2 but tourney still has issues
*Alex, Anshul, and Sumit
**Alex will create documentation on EMADE data types and data pairs in Sphinx
**Anshul is working on Kaggle notebook
***Colab is disconnecting to runtime, reached out to stocks team for help
**Sumit and Anish are also lending help on Colab notebook
*I am finishing [https://www.notion.so/Natural-Language-Processing-6ab51406b2164470ab0fb16675dbdee6 documentation] today
**Will dive into PyTorch, starting with Anish's PyTorch [https://github.gatech.edu/emade/emade/blob/nn/src/GPFramework/pytorch_methods.py methods] file
=== Week of Feb. 15th ===
Breakout Meeting
*Cameron and Steven
**Setting up PACE-ICE instances
**MySQL and disk quota roadblocks but working through
**Will reach out to Anuurag, Maxim, or Pulak to resolve
*Alex, Anshul, and Sumit
**Alex to reach out to Zutty about potential documentation work
**Anshul is working on Amazon dataset
**Sumit found more literature
*Anish finished Amazon train/test split dataset, will work on literature method for pre-processing dataset next
*I will add half of the remaining primitives to the documentation
Subgroup Meeting
*Cameron and Steven
**Completed setting up PACE-ICE instances
**Steven tested instance on toxicity dataset, will run a short pass on Amazon dataset
*Alex, Anshul, and Sumit
**Alex working on prepping CIFAR10 dataset
***Hitting a few roadbumps using the chest x-ray script
***Will need to reshape and onehotencode
***Will reach out to Zutty about documentation
**Anshul is working on Kaggle notebook
***Resolved error where embeddings were not stacking properly
***Colab is disconnecting to runtime
**Sumit working on Amazon dataset
*Anish has Amazon [https://github.gatech.edu/athite3/amznreviews/tree/master dataset] all ready to go (passed off to Steven)
*I am plugging away on [https://www.notion.so/Natural-Language-Processing-6ab51406b2164470ab0fb16675dbdee6 documentation], will request Zutty join next breakout to discuss refactoring to PyTorch
=== Week of Feb. 8th ===
Monday Breakout
* Cameron and Steven are setting up PACE-ICE
* Anish is finishing up pre-processing and will dabble with chest x-ray dataset
* Alex, Anshul, and Sumit are collecting literature for baseline of Amazon (have one paper from Kaggle so far)
* I will be adding document level primitives to the [https://www.notion.so/Natural-Language-Processing-6ab51406b2164470ab0fb16675dbdee6 Notion page]
Weekly Meeting
*Team is focused on getting their machines set up and finalizing the pre-processing of the Amazon dataset
**Sumit found a few papers that used the Amazon dataset
***[https://ieeexplore.ieee.org/document/8768887 Linguistically independent sentiment analysis]
***[https://www.sciencedirect.com/science/article/abs/pii/S0167739X20309195 An Attention-based Bidirectional CNN-RNN Deep Model for sentiment analysis]
**Cameron and Steven are getting PACE set up -- currently getting MySQL working
**Alex is working on learning how to format data (specifically the Amazon dataset) for use in EMADE and wants to create an example walkthrough document
**Anish is half done with dataset pre-processing
**I'm working on document level primitives
Next week look ahead
* Should have a few runs in PACE
* Primitives documentation should be nearly complete

=== Week of Feb. 1st ===
Subgroup meeting (Monday)
* Discussed overarching goals of group
** What areas or work were of interest to members
** Issue concerning EMADE's Neural Architecture Search implementation finding only trivial solutions
*** Does issue lie in:
**** Pre-processing of unbalanced datasets
**** Implementation (or other?)
** Focusing on just NLP as previous term was dabbling in several areas (and had more members)
Subgroup meeting (Friday)
* Finalized team's direction for the term
** Refocusing team's efforts to just NLP applications
** Discussed first steps and divided into subteams to task efficiently
** Toxicity dataset was highly unbalanced, obscuring reason why EMADE NAS is only finding trivial solutions
*** By using Amazon dataset (50/50 binary classification), underlying NAS issue will hopefully be easier to resolve
* Term goals
** Compare EMADE to Keras on [https://www.kaggle.com/bittlingmayer/amazonreviews Amazon dataset]
*** Alex, Anshul, and Sumit will focus on Keras implementation
*** Cameron and Steven will focus on EMADE implementation
** Ensure NAS implementation in EMADE works properly and robustly
*** Anish has started troubleshooting effort to find where NAS implementation is failing
**** He's working on a cross validation and dataset balancing
** Document NLP primitives
*** I will document NLP primitives in Notion (or other formats as requested)

=== Week of Jan. 25th ===
* Weekly meetings will be at 4PM EST on Fridays

== Neural Architecture Search ==

Meetings: 2:00 pm on Fridays, Virtual

=== Week of September 6th - 12th ===
* First Subteam meeting of the semester
* Meeting began with some information about setting up emade on individuals machines as well as getting signed onto the team trello board.
* Cameron Whaley followed up by giving background information about the subteam and topics of neuroevolution. 2 articles were provided to be read by the team and some helpful resources for remembering deep learning topics were provided.
** Article 1: https://arxiv.org/pdf/1703.00548.pdf
** Article 2: https://arxiv.org/pdf/2002.04634.pdf
** Help resources: https://docs.google.com/presentation/d/1v33k5I9b-_MIR9f3QhO4U81HJaBRwWqt6xzoSecDsoA/edit#slide=id.p
* Finally, Cameron Bennett reviewed the top 6 ideas we plan to work on in order to improve neural architecture search in emade. These include:
** Triviality Detection
** Novelty Detection
** Bug fix for swap_layer method
** Speeding up emade processes
** Adding unit tests
** Introducing some pause functionality
* Everyone's tasks for the week is to setup emade and ensure they can at least run locally on their own machine.

=== Week of September 13th - 19th ===
* This meeting ran for the allotted hour and consisted of task layouts and debugging people's emade issues.
* During the beginning of this meeting, Cameron Bennett went through 5 newly added tasks on the trello board in detail.
* These tasks consisted of:
** Debugging/Reproducing issue with test_swap line 300 EMADE.py
** Modifying the reuse parameter within EMADE.py to allow reuse of all individuals after starting a previously stopped EMADE run
** Adding CIFAR-10 Input Template to EMADE
** Modifying eval_methods.py to check if test_data being input has 90% same classification
** Adding a method for keeping track of NNLearner layer frequency
* While discussing the above tasks, Cameron detailed starting points, purpose and ideal outcomes of each task so that members had a solid grasp of the expected outcome. 
* After discussing each task, Cameron Whaley provided input regarding some blockers for the first task stating that he would need some time to reproduce the issue he noticed before anyone will be able to work on that task.
* Once all questions from team members were answered, Cameron Bennett expressed that each member is responsible for finding and beginning work on a task as soon as they are setup in EMADE.
* The remainder of the meeting consisted of Cameron Whaley answering debugging questions for each team members current PACE-ICE Emade setup. 
* Connor Yurkon was able to finish his setup for PACE-ICE during the meeting.
* Devesh and Lucas expressed a need for more time until they are set up fully. 

=== Week of September 20th - 26th ===
* This meeting ran for the allotted hour and consisted of task layouts and debugging people's emade issues.
* The beginning of this meeting consisted of updates from team members regarding their assigned tasks:
** Connor communicated some blockers he ran into regarding the ability to debug the code he is currently trying to add to EMADE.py. It was suggested that he try to either utilize a dataset with a reducedInstances value of something small(such as 0.1) or he could try to reproduce the method functionality within a seperate python script since he knows what method he is trying to edit. 
** Devesh has completed his task of adding the Input_Template file for CIFAR-10 data. He currently should have a pr from a forked version of EMADE which will be merged by Cameron Whaley at some point. [https://github.gatech.edu/emade/emade/pull/198 Attached PR]
** Cameron Whaley remembered how he produced the swap_layer bug and has provided us an emade worker out file which should contain some evolved nnlearner individuals which we can use to identify our mutation problem.
** Cameron Bennett has done a re-run of emade on the amazon dataset with the new triviality objective function. In an attempt to identify useful nnlearners, Cameron Whaley has provided him a jupyter notebook with some methods for analyzing individuals from a run. It is possible he will need to re-run in order to utilize a seeded amazon group.
* After updates were given, the rest of the meeting time consisted of ideas for generating additional tasks for members. The ideas discussed are:
** Introducing a global "supernet" which would operate like a dynamic acyclic graph which stores information about submodel accuracy as well as weights of most optimal subnet based on a prior generation run. This discussion was based on a paper provided by Devesh which analyzes the effectiveness of combining a supernet NAS with validation runs for verifying accuracy. This idea was a bit loaded and awfully similar to what EZCGP is currently working on. Therefore, we put it to the side and have decided not to move further with it. [https://www.ijcai.org/proceedings/2020/0441.pdf Linked Article]
** Introducing Weight Sharing to the existing NNLearner evolution process in EMADE. One idea involving weight sharing was to create a method which can locate all of the shared subnets within every individual in a single generation and replace them with the most optimally trained weights. Ideally this would speed up the number of generations to see optimal performance out of similarly structured individuals. Currently, a task exists for anyone interested in exploring how to locate similar subnets within EMADE individuals.
** Introducing writes to disk which would improve memory usage within EMADE. This is moreso a general infrastructure interest however, noone was able to generate any concrete ideas around it due to our unfamiliarity with EMADE memory usage. Cameron Whaley made a recommendation that we introduce a parameter to the existing Inputschema which allows users to set a maximum amount of time for training any nnlearner individual. The idea behind this is that, if we enable networks to learn for as long as possible, we could reach more optimal individuals sooner without having to go through the evolution process as frequent. This is something we'd like to experiment with and see what results can be generated by this. We've created a task for anyone interested in adding this parameter.
* Before concluding, Connor requested that we have a meeting to help he and other members get a better understanding of EMADE directories and the evolution process.
* Cameron Whaley has completed the run resume feature but we want to test it and determine success on results.

=== Week of September 27th - October 1st ===
* This meeting ran for the allotted hour and consisted of mainly team updates along with a work session.
* Earlier in the week, a session was held in which team members got a better understanding of the evolutionary algorithm within EMADE.py. Cameron Whaley and Cameron Bennett cohosted the presentation and made a recording available [https://bluejeans.com/playback/s/SGxxpyPUJ5s0nYcJr3u8YIFWMHURHhIdWlyRUsbp6a90TRncYZaBXubs33FwJxj4 here].
* This presentation covered the EMADE Object as a whole and sub methods such as create_representation, setObjectives and buildClassifier. In addition, we covered some helper methods within the EMADE object which might be useful for people debugging in emade such as: handleWorker, my_str and my_similar.
* Our presentation included a couple of mating/mutation methods that we thought would be relevant for our team such as: swap_layer, cx_ephemerals and concat_healer. 
* Finally, we finished off by going through the entirety of the main master algorithm loop within EMADE.py. This included a discussion of the different arguments within the master algorithm such as MUTPB, CXPB and reuse. Different chunks of the main loop were explained such as the one which handles creation of new individuals.
* The remainder of the presentation is available for viewing on the bluejeans link above or [https://docs.google.com/presentation/d/1rVF5gObTu8hWLPCUKir1163ntRXxUsrYoe-9aw6hEog/edit#slide=id.gf4c8a6a88f_0_39 here] on google slides.
* In addition to this presentation, we held a smaller work session with team member Lucas Zhang in which we talked about how standalone tree evaluator works within the context of his swap_layer task. Cameron Whaley provided him an analyze.ipynb notebook an discussed how he could leverage an existing master.out file full of marked crossover individuals(potentially buggy ones) to reproduce the swap_layer issue that causes individuals to go from properly compiling to failing with message "Tree missing valid datatype for primitive". 
* Cameron B was unable to make the worksession held at our normally scheduled friday meeting and Cameron Whaley graciously hosted in place.

=== Week of October 4th - October 8th ===
* This week's meeting consisted of team updates in addition to some helpful debugging tips provided by Cameron B:
** Cameron B has finished his task of triviality detection and concluded that the individuals produced using the triviality evaluation method are not much different from the individuals being produced without them. The strongest indicator of this was that in comparing 2 emade runs (1 run without triviality method and 1 with) by generation 5, both runs had the same number of individuals with the structure shown [https://drive.google.com/file/d/1yNSxFgcHl5CvTEbmL-DE6j8gRAyU36Mp/view?usp=sharing here]. Additionally, it was noticed that the majority of the NNLearners being produced did not provide any valid indications of success due to the overbearing amount of errors that are appearing. An ideal next step will be to isolate these errors and handle them before trying to integrate additional changes.
** Cameron W worked on merging some changes from the nn branch of emade this week which has led to some conflict issues within the existing nn-vip branch. In an attempt to resolve the issues, he issued a PR of some reverted commits from the branch. However, he also proposed that we utilize a forked version of EMADE separate from the main repo in order to allow us more freedom to change EMADE and make PR's to a unified master branch custom to our purposes. For this reason, we are all currently shifting to working off a fork of the EMADE branch from Cameron's account. Finally, Cameron W has contributed some changes to the EMADE repo(on his own fork) which force emade to only produce NNLearners. Ideally this will allow us to isolate issues with NNLearners quicker and debug issues at a better pace.
** Connor has provided a PR with some dataset removals which will lighten the size of EMADE in hopes of being able to utilize the github repo for EMADE within PACE. 
** Devesh is working on connecting the input template parameter for nnlearner training time to the keras fit method which will limit the amount of time an individual is allowed to train by a specified amount. 
** Justin H is currently finished setting up PACE-Ice on his machine and is ready to work on his first task. Cameron W has suggested that he contribute a PR which removes the existing amazon dataset and cleans up the script for generating the amazon files.
** Lucas Z has also finished setting up PACE-Ice on his machine and is ready to work on his first task. His first task is to do an Emade run and look at the error strings being generated by the NNLearners within the run. From this he should be able to debug issues found within primitives present in EMADe.
* In addition to the updates provided above, there was a discussion had about how automatically defined functions could be leveraged to in a way similar to modules within the CoDEEPNEAT neuroevolution process. Cameron B shared some high level knowledge about the CoDEEPNEAT structure and Cameron W proposed ADF's be used as the "modules" within our context. In order to analyze the viability of this, we will need to rewrite some of the ADF generating code.

=== Week of October 11th - October 15th ===
* This week's meeting consisted of team updates in addition to a work session:
** Cameron W ran into 2 issues earlier in the week after pushing his updated nn-vip branch on EMADE which were:
*** Nested NNLearners. See example. Basically, a neural network is created, modifies a datapair with its classifications, then returns that datapair to... another neural network which overwrites those classifications with its own. Emade is making a ton of these and they take a really long time to evaluate (like, hours on a V100 gpu). It's always been possible for emade to make nested NNLearners, but with my changes it's a lot more frequent. The current fix is to auto-fail any individual with >1 NNLearner primitive, but I'd prefer to stop them from generating in the first place
*** ADFs mostly aren't valid. I modified the ADFs that emade makes so that their input is a datapair and they return a layerlist. So emade is making subtrees of layers (good), but not all of them have a layer primitive that can handle a datapair (like an embedding layer) so they mostly fail.
** Cameron B attempted some changes to resolve these compilation issues. 
*** In order to solve prevent nested NNLearners, a second data pair(EmadeDataPairNNF) was created and the 'MAIN' PrimitiveTypedSet had it's output changed to this second datapair to prevent it from being a nestable primitive to itself. Additionally, passthrough layers for datapairs and other primitives were used to remove any generate errors caused by DEAP while generating new individuals. The changes can be seen in this method screenshot [https://drive.google.com/file/d/1stLyEPliYeShUBzM3SL0veoCT12XMBtc/view?usp=sharing here]. 
*** In order to fix some of the ADF issues, ADFs are now structured to only take in a layerlist input and output a layerlist input. Any input or output layers necessary are added within the NNLearner method or through the use of an input terminal. 
*** Finally, the 'MAIN' PrimitiveSetTyped is no longer allowed to generate nnlayerlists with primitives outside of the adf's generated. This is an attempt at implementing a module/blueprint structure similar to that seen within this paper discussing [https://arxiv.org/pdf/2002.04634.pdf CoDEEPNEAT].
** Devesh is currently taking a different strategy with implementing his task, he will attempt an emade run which simply hardcodes the parameter for time limit to fit. If this works then it should be easier to then connect that value back to the input template file/schema. 
** Justin H has provided a PR for the cleaned-up Amazon dataset along with the new script. He is currently working on adding a testing method that utilizes an NNLearner statistics table, within the SQL database, to generate all of the evolving tree structures of a single individual(provided their hash value) over the course of its lifetime.
** Connor Y has provided a PR that allows team members to keep the entirety of EMADE on Pace including the git folder. This will allow people using pace to simply pull down any changes directly from the repo instead of having to SCP frequently. He is currently back on track with his task to generate an NNLearner layer frequency counter within EMADE.
** During the hackathon, Cameron W proposed a fix for the accuracy error issue noticed when doing EMADE runs. Currently, in the nn-vip-learner-fix branch of our forked repo most all NNLearners that are generated can compile but they all produce the same accuracy error score which is likely due to some issue within the NNLearner method. We'll run EMADE again with the changes to see if we get different scores this time around. Image of the issue [https://drive.google.com/file/d/1FSRtK-vADkCAPgt1_f2jYEBwt8B5-7AU/view?usp=sharing here]
** Just an update on the current structure of NNLearners can be seen [ here]
[[Category-AAD]]