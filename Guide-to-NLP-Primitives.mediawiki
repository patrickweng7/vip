= Text Processing Methods =
Located [https://github.gatech.edu/emade/emade/blob/nlp-nn/src/GPFramework/text_processing_methods.py here] in EMADE github.

== Tokenizer ==
[https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html Tokenization] breaks up a document into pieces by tossing out spaces, punctuation, and potentially other characters. Additionally, documents may be tokenized down to the word, subword (n-gram), or character level. Tokenization is necessary for text data to be processed by common and state of the art NLP models.

A few definitions
* A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing
* A type is the class of all tokens containing the same character sequence
* A term is a (perhaps normalized) type that is included in the information retrieval system's dictionary

Tokenization is non-trivial as the methodology used can have varied and unintentional results. Note in the image [https://blog.floydhub.com/tokenization-nlp/ below] that a simple sentence can be broken up into numerous ways. 

[[files/Tokenizing.png]]

Tokenizing on white spaces may seem like the intuitive initial method to try but contractions become an obstacle (one word can have multiple permutations resulting in one word with multiple tokens). Punctuation also leads to issues with contractions; they are not split between the two contracted words (resulting in nonsense words). Instead, tokenizing on rules (for instance: all white spaces, some punctuation, and on contractions) leads to some of the best results and offers the greatest flexibility.

     def tokenizer(data_pair, max_length, num_words):
         """Uses keras tokenizer to make the text into sequences of numbers which are mapped to the words
         Args:
             data_pair: given dataset
             binary: True if you want to (use 1 or 0) represent if a word exists in the dataset and false if you want to put the number of times a word exists at the spot instead of 1
             Ngram_start: the lower end of the ngram range
             Ngram_end: the higher end of the ngram range, both ngram_start and ngram_end are used to represent the ngram_range
             whichstopword: represents which stop word list to use 
         Returns:
            the data_pair where the train and test data are tokenized
         """
         train_data = data_pair.get_train_data().get_numpy().flatten()
         test_data = data_pair.get_test_data().get_numpy().flatten()
     
         #max_length = max_length % 600 + 100 #100
         #num_words = num_words % 2000 + 5000 #1000
         tokenizer = Tokenizer(num_words = num_words)
         tokenizer.fit_on_texts(train_data)
         x_train = tokenizer.texts_to_sequences(train_data)
         x_test = tokenizer.texts_to_sequences(test_data)
     
         x_train = sequence.pad_sequences(x_train, maxlen = max_length)
         x_test = sequence.pad_sequences(x_test, maxlen = max_length)
         
         data_list = []
     
         for transformed, dataset in zip([x_train, x_test], [data_pair.get_train_data(), data_pair.get_test_data()]):
             instances = cp.deepcopy(dataset.get_instances())
             for i, instance in enumerate(instances):
                 instance.get_features().set_data([transformed[i]])
             new_dataset = EmadeData(instances)
             data_list.append(new_dataset)
         data_pair = EmadeDataPair(train_data=(data_list[0], None),
                                            test_data=(data_list[1], None))
         vocabsize = np.amax(x_train) +1
         return data_pair, vocabsize, tokenizer
     
     
     stop_words1 = ["in", 'of', 'at', 'a', 'the', 'an']
     stop_words2 = "english"
     stop_words3 = None
     stop_words4 = stop_words.ENGLISH_STOP_WORDS
     stop_words5 = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']
     
     list = [stop_words1, stop_words2, stop_words3, stop_words4, stop_words5]
     MAX_NGRAM_VALUE = 3

== Bag of Words Vectorizer ==
[https://machinelearningmastery.com/gentle-introduction-bag-words-model/ Vectorization] generates a representation of a document where each unique word is a vector. The Bag of Words method represents the document as a collection of words and their respective counts. The information about the order or structure of the words in the document is discarded. For example, take the first few lines from "A Tale of Two Cities":

it was the best of times, <br>
it was the worst of times, <br>
it was the age of wisdom,  <br>
it was the age of foolishness, <br>

Now create the list of unique words.

"it"<br>
"was"<br>
"the"<br>
"best"<br>
"of"<br>
"times"<br>
"worst"<br>
"age"<br>
"wisdom"<br>
"foolishness"<br>
Using BoW, create vectors for each line.

it was the best of times = [1,1,1,0,1,1,1,0,0,0]<br>
it was the age of wisdom = [1,1,1,0,1,0,0,1,1,0]<br>
it was the age of foolishness = [1,1,1,0,1,0,0,1,0,1]<br>

Note that there is no information about the order or structure of each line, only the presence of words.

     def count_vectorizer(data_pair, binary, ngram_start, ngram_end, whichStopWordList):
         """Vectorize text data using traditional bag of words techniques
         Args:
             data_pair: given dataset
             binary: True if you want to (use 1 or 0) represent if a word exists in the dataset and false if you want to put the number of times a word exists at the spot instead of 1
             ngram_start: the lower end of the ngram range
             ngram_end: the higher end of the ngram range, both ngram_start and ngram_end are used to represent the ngram_range
             whichStopWordList: represents which stop word list to use
         Returns:
             the data_pair where the train and test data are vectorized
         """
         #STEMMING code for future reference!!
         # stemmer = PorterStemmer()
         # analyzer = CountVectorizer().build_analyzer()()()
     
         # def stemming(doc):
         #     return (stemmer.stem(w) for w in analyzer(doc))
     
         ngram_start = ngram_start % MAX_NGRAM_VALUE + 1
         ngram_end = ngram_end % MAX_NGRAM_VALUE + 1
         if (ngram_start > ngram_end):
             ngram_start, ngram_end = ngram_end, ngram_start
         whichStopWordList =  whichStopWordList % len(list)
     
         train_data = data_pair.get_train_data().get_numpy().flatten()
         test_data = data_pair.get_test_data().get_numpy().flatten()
         vectorizer = CountVectorizer(stop_words = list[whichStopWordList], binary = binary, ngram_range = (ngram_start, ngram_end))
         x_train = vectorizer.fit_transform(train_data)
         x_test = vectorizer.transform(test_data)
     
         data_list = []
         #print(x_train)
         for transformed, dataset in zip([x_train, x_test], [data_pair.get_train_data(), data_pair.get_test_data()]):
             instances = cp.deepcopy(dataset.get_instances())
             for i, instance in enumerate(instances):
                 instance.get_features().set_data(transformed[i])
             new_dataset = EmadeData(instances)
             data_list.append(new_dataset)
         data_pair = EmadeDataPair(train_data=(data_list[0], None),
                                            test_data=(data_list[1], None))
     
     
         #print(type(x_test))
     
         #print(data_pair.get_train_data().get_instances()[1].get_features())
     
     
        #data_pair = GTMOEPDataPair(train_data = GTMOEPData(x_train), test_data = GTMOEPData(x_test))
         #print(data_pair.get_train_data().get_numpy())
         #gc.collect(); 
         return data_pair

== TF-IDF Vectorizer ==
[https://monkeylearn.com/blog/what-is-tf-idf/ Term Frequency Inverse Document Frequency] is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is accomplished by calculating the term frequency (how many times a word or phrase show up in a document) and the inverse of the frequency of the word across multiple documents. As the number of occurrences in a given document increases the measure but the number of documents that contain that word decrease the measure.

Definitions

* Term Frequency: count of a word in a document, can be adjusted by length of document or weighted by the count of the most frequent word in that document
* Inverse Document Frequency: frequency of a word across multiple documents (logarithm of total number of documents divided by the documents that contain the word of interest)

The full equation (top) with term frequency (middle) and inverse document frequency (bottom) expanded.

tfidf(t,d,D) = tf(t,d) * idf(t,D)<br>
tf(t,d) = log(1 + freq(t,d))<br>
idf(t,D) = log(N/count(d in D: t in d))

TF-IDF vectorizer calculates the above weight in addition to vectorizing.

     def tfidf_vectorizer(data_pair, binary, ngram_start, ngram_end, whichStopWordList):
         """Vectorize text data using TFIDF bag of words techniques
         Args:
             data_pair: given dataset
             binary: True if you want to (use 1 or 0) represent if a word exists in the dataset and false if you want to put the number of times a word exists at the spot instead of 1
             ngram_start: the lower end of the ngram range
             ngram_end: the higher end of the ngram range, both ngram_start and ngram_end are used to represent the ngram_range
             whichStopWordList: represents which stop word list to use 
         Returns:
             the data_pair where the train and test data are vectorized
         """
         ngram_start = ngram_start % MAX_NGRAM_VALUE + 1
         ngram_end = ngram_end % MAX_NGRAM_VALUE + 1
         if (ngram_start > ngram_end):
             ngram_start, ngram_end = ngram_end, ngram_start
         whichStopWordList =  whichStopWordList % len(list)
     
         train_data = data_pair.get_train_data().get_numpy().flatten()
         test_data = data_pair.get_test_data().get_numpy().flatten()
     
         vectorizer = TfidfVectorizer(stop_words = list[whichStopWordList], binary = binary, ngram_range = (ngram_start, ngram_end))
         x_train = vectorizer.fit_transform(train_data)
         x_test = vectorizer.transform(test_data)
         print(x_train)
         data_list = []
         for transformed, dataset in zip([x_train, x_test], [data_pair.get_train_data(), data_pair.get_test_data()]):
             instances = cp.deepcopy(dataset.get_instances())
             for i, instance in enumerate(instances):
                 instance.get_features().set_data(transformed[i])
             new_dataset = EmadeData(instances)
             data_list.append(new_dataset)
         data_pair = EmadeDataPair(train_data=(data_list[0], None),
                                            test_data=(data_list[1], None))
         return data_pair

== Hashing Vectorizer ==
The [https://kavita-ganesan.com/hashingvectorizer-vs-countvectorizer/#.YDNMIWXPyHs Hashing Vectorizer] accomplishes the output as the Count Vectorizer but has an optimization that allows for very large datasets. Count Vectorizer keeps the dictionary of unique tokens and as the dataset scales, so do the memory requirements. Hashing Vectorizer runs the tokens through a hash function (typically [https://en.wikipedia.org/wiki/MurmurHash MurmurHash3]) and maps them to a column position. The dictionary of tokens is not kept, only the matrix where the columns are determined by the hash function and the frequency count of those tokens in the document. This of course has the notable drawback of not being able to retrieve the token from the column ID. In addition, the number of columns is specified by the user and if the number of unique tokens exceeds the value input, hash [https://en.wikipedia.org/wiki/Collision_(computer_science) collision] occurs; multiple unique tokens are mapped to the same column, distorting the counts.

     def hashing_vectorizer(data_pair, binary, ngram_start, ngram_end, whichStopWordList):
         """Vectorize text data using bag of words techniques but by hashing the words to make it efficient
         Args:
             data_pair: given dataset
             binary: True if you want to (use 1 or 0) represent if a word exists in the dataset and false if you want to put the number of times a word exists at the spot instead of 1
             ngram_start: the lower end of the ngram range
             ngram_end: the higher end of the ngram range, both ngram_start and ngram_end are used to represent the ngram_range
             whichStopWordList: represents which stop word list to use 
         Returns:
             the data_pair where the train and test data are vectorized
         """
         
         ngram_start = ngram_start % MAX_NGRAM_VALUE + 1
         ngram_end = ngram_end % MAX_NGRAM_VALUE + 1
         if (ngram_start > ngram_end):
             ngram_start, ngram_end = ngram_end, ngram_start
         whichStopWordList =  whichStopWordList % len(list)
     
         train_data = data_pair.get_train_data().get_numpy().flatten()
         test_data = data_pair.get_test_data().get_numpy().flatten()
     
         vectorizer = HashingVectorizer(stop_words = list[whichStopWordList], binary = binary, ngram_range = (ngram_start, ngram_end))
         x_train = vectorizer.fit_transform(train_data)
         x_test = vectorizer.transform(test_data)
         data_list = []
         for transformed, dataset in zip([x_train, x_test], [data_pair.get_train_data(), data_pair.get_test_data()]):
             instances = cp.deepcopy(dataset.get_instances())
             for i, instance in enumerate(instances):
                 instance.get_features().set_data(transformed[i])
             new_dataset = EmadeData(instances)
             data_list.append(new_dataset)
         data_pair = EmadeDataPair(train_data=(data_list[0], None),
                                            test_data=(data_list[1], None))
         return datapair

== Word2Vec Vectorizer ==
[https://thedatasingh.medium.com/introduction-713b3d976323 Word2Vec] is a group of models that produce word [embeddings](https://machinelearningmastery.com/what-are-word-embeddings/); a representation where tokens that have the same meaning have similar representation. In other words, similar tokens are grouped together in feature space with similar vectors. Word2Vec uses a shallow neural network that are trained to reconstruct the context of words.  Each token has a vector composed of numerous (typically hundreds) of elements which represent that token's relation to other tokens. 

The models understand language enough to do word [https://towardsdatascience.com/different-techniques-to-represent-words-as-vectors-word-embeddings-3e4b9ab7ceb4 math].

King - Man + Woman = Queen

Word2Vec uses either a continuous bag of words (look at surrounding words to predict the middle word) or Skip-grams (given a word, predict the surrounding words).

     def word2vec(data_pair, whichStopWordList, size, window, min_count , sg):
          """Uses gensim's word2vec vectorizer to make a word2vec model which creates vectors associated with each word. Afterward, each row of text becomes an average of each of the vectors associated with its' words
          Args:
              data_pair: given dataset
              whichstopword: represents which stop word list to use 
              size: corresponds with the dimension of the word vector
              window: the minimum distance between the vectors, smaller window should give terms that are more related bigger window increases accuracy but takes much longer
              min_count: word has to appear this many times to affect the model
              sg: true if skip-gram technique is going to be used false if continuous bag of words technique is going to be used to make the word2vec model
          Returns:
             the data_pair where the train and test data are vectorized with averaged word2vec vectors
          """
          size = size % 500 + 5 #10 #this corresponds with number of layers in the word
          window = window % 500 + 1 #minimum distance between vectors, smaller window should give your terms that are more related bigger value increasing accuracy but takes much longer
          workers = 8 #workers corresponds with number of cores you have more workers better should probably keep this constant
          min_count = min_count % 3 + 1 #word has to appear this many times to affect the model
          technique = -1
          if sg == True:
              technique = 1
          else:
              technique = 0
          sg = sg % 2 #use cbow or skip-gram method, skip-gram method is better but much slower, and parameter range is different for sg
          train_data = data_pair.get_train_data().get_numpy().flatten()
          test_data = data_pair.get_train_data().get_numpy().flatten()
          stop_words = list[whichStopWordList % len(list)]
         
          def tokenize(val):
              ans = text_to_word_sequence(val)
              ans = [a for a in ans if not a in stop_words]
              return ans
          reviews = np.concatenate((train_data, test_data))
          words = [tokenize(val) for val in reviews.tolist()]
     
          model = gensim.models.Word2Vec(sentences=words, size= size, window = window, workers = workers, min_count=min_count, sg = technique)
     
          def method(list, wv):
              mean = []
              for word in list:
                  if word in wv.vocab:
                      mean.append(wv[word])
                  else:
                      mean.append(word)
              mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)
              return mean
          x_train = words[:train_data.shape[0]]
          x_train = np.array([method(review, model.wv) for review in x_train])
          x_test = words[test_data.shape[0]:]
          x_test = np.array([method(review, model.wv) for review in x_test])
     
          data_list = []
          for transformed, dataset in zip([x_train, x_test], [data_pair.get_train_data(), data_pair.get_test_data()]):
              instances = cp.deepcopy(dataset.get_instances())
              i = 0;
              for instance in instances:
                  instance.get_features().set_data(np.array([transformed[i]]))
                  i+=1
              new_dataset = EmadeData(instances)
              data_list.append(new_dataset)
     data_pair = EmadeDataPair(train_data=(data_list[0], None),
                                        test_data=(data_list[1], None))
     return data_pair

== Sentiment Analysis ==
[https://towardsdatascience.com/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17 Sentiment Analysis] identifies and extracts subjective information (such as positive, negative, or neutral) from a document. Sentiment can be derived by applying a dictionary (nltk [http://www.nltk.org/howto/sentiment.html Vader] for instance) that contains words and their sentiment values against a document or by using a model that has been trained on relevant data. 

Below is an example using Uber customer's feedback on Facebook (top) and Twitter (bottom) post filtering unrelated content..

[[files/Fb_uber_sa.png]]

[[files/Tw_uber_sa.png]]

Note that the output is by definition subjective and highly sensitive to the dictionary or model used.

     def sentiment(data_pair, sentence_vec):
     
         def document_sentiment(data, sentence_vec=False):
     
             sentiments = []
     
             for review in data:
     
                 review_sentiments = []
     
                 if sentence_vec:
                     sentences = nltk.sent_tokenize(review)
                     for sentence in sentences:
                         blob = TextBlob(sentence)
                         ps_list = np.array ( [blob.sentiment.polarity, blob.sentiment.subjectivity] ) # Polarity and subjectivity list
                         review_sentiments.append(ps_list)
                 else:
                     blob = TextBlob(review)
                     for word in blob.words:
                         word = re.sub('[^A-Za-z0-9]+', '', str(word)) # remove punc.
                         wordBlob = TextBlob(word)
                         ps_list = np.array ( [wordBlob.sentiment.polarity, wordBlob.sentiment.subjectivity] )
                         review_sentiments.append(ps_list)
                 # print(np.mean(review_sentiments, axis=0))
                 # print(review_sentiments)
                 sentiments.append(np.mean(review_sentiments, axis=0))
     
             return np.array(sentiments)
     
         train_data = data_pair.get_train_data().get_numpy().flatten()
         test_data = data_pair.get_test_data().get_numpy().flatten()
         first_review = train_data[0]
         # Convert to sentiment
         x_train = document_sentiment(train_data, sentence_vec)
         x_test = document_sentiment(test_data, sentence_vec)
         
         # for i, x in enumerate(x_train):
         #     print(f"xtrain[{i}]: {x}")
     
         # for i, x in enumerate(x_test):
         #     print(f"xtest[{i}]: {x}")
         # print(first_review)
     
         data_list = []
     
         for transformed, dataset in zip([x_train, x_test], [data_pair.get_train_data(), data_pair.get_test_data()]):
             instances = cp.deepcopy(dataset.get_instances())
             for i, instance in enumerate(instances):
                 instance.get_features().set_data([transformed[i]])
             new_dataset = EmadeData(instances)
             data_list.append(new_dataset)
         data_pair = EmadeDataPair(train_data=(data_list[0], None),
                                            test_data=(data_list[1], None))
         return data_pair

== Stemmatizer ==
[https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html Stemming and lemmatization] is the process of reducing permutations of a given word in a document to a common base form (car, cars, car's, cars' → car). Stemming is a base implementation that will typically truncate a word, assuming the base word remains intact (car's → car) but does not handle more sophisticated permutations. Lemmatization uses a vocabulary and a morphological analysis of words in an effort to arrive at the dictionary form of the word, the lemma (am, are, is → be).

A [https://devopedia.org/lemmatization comparison] graphic below:

[[files/Stem_vs_lem.png]]

     def stemmatizer(data_pair, func_index=None, pos_tagger_index=None):
         """Stemming and lemmatization primitive
         Args:
             data_pair: given dataset
             func: func
             lemma: Input lemmatizer if given, won't use otherwise
         Returns:
             the data_pair after being transformed
         """
         if func_index == None or pos_tagger_index == None:
             return data_pair
         train_data = data_pair.get_train_data().get_numpy().flatten()
         test_data = data_pair.get_test_data().get_numpy().flatten()
     
         #stem/lemmatize here
         func = funcs[func_index % len(funcs)]
         pos_tagger = pos_taggers[pos_tagger_index % len(pos_taggers)]
     
         lam = lambda one_review, func, pos_tagger: stemlemmatize(one_review, func, pos_tagger)
         x_train = np.array([lam(r, func, pos_tagger) for r in train_data])
         x_test = np.array([lam(r, func, pos_tagger) for r in test_data])
     
         #repackage stemmatized data into data_pair and return.
         data_list = []
         for transformed, dataset in zip([x_train, x_test], [data_pair.get_train_data(), data_pair.get_test_data()]):
             instances = cp.deepcopy(dataset.get_instances())
             for i, instance in enumerate(instances):
                 instance.get_features().set_data(transformed[i])
             new_dataset = EmadeData(instances)
             data_list.append(new_dataset)
         data_pair = EmadeDataPair(train_data=(data_list[0], None),
                                            test_data=(data_list[1], None))
         return data_pair

== TextRank (unfinished) ==
[https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf TextRank] is a graph-based model that decides the importance of each vertex (in this case word or token) given the entire graph (document). In addition, it accounts for edge weights between vertices, allowing for greater contextual understanding of words in a given document. the output of a given text is a graph that can be visualized. See example from the developers paper.

The words in the document are ranked and connected with an edge if they occur within N words of each other.

[[files/textrank example.png]]
     def textRank(data_pair):
         """Computes TextRank for given dataset. UNFINISHED
         Args:
             data_pair: given dataset
         Returns:
             the data_pair where the train and test data are vectorized
         """
         from nltk.corpus import stopwords
         
         train_data = data_pair.get_train_data().get_numpy()
         test_data = data_pair.get_test_data().get_numpy()
         
         #This code doesn't make any sense!!
         # Extract word vectors
         #word_embeddings = {}
         #with open('/nv/pace-ice/agurung7/vip/emade/glove_word_embeddings.pickle', 'rb') as f:
         #    word_embeddings = pickle.load(f)
     
         # remove punctuations, numbers and special characters
         clean_sentences = pd.Series(sentences).str.replace("[^a-zA-Z]", " ")
     
         # make alphabets lowercase
         clean_sentences = [s.lower() for s in clean_sentences]
     
         # get stopwords from nltk
         stop_words = stopwords.words('english')
     
         # function to remove stopwords
         remove_stopwords = lambda sen: " ".join([i for i in sen if i not in stop_words])
             
         data_list = []
         #print(x_train)
         for transformed, dataset in zip([x_train, x_test], [data_pair.get_train_data(), data_pair.get_test_data()]):
             instances = cp.deepcopy(dataset.get_instances())
             for i, instance in enumerate(instances):
                 instance.get_features().set_data(transformed[i])
             new_dataset = EmadeData(instances)
             data_list.append(new_dataset)
         data_pair = EmadeDataPair(train_data=(data_list[0], None),
                                            test_data=(data_list[1], None))
         return data_pair