= Description =
We are a subteam for the [https://vip.gatech.edu/wiki/index.php/Automated_Algorithm_Design Automated Algorithm Design] VIP group. (Previously we were named the [[Automatically Defined Functions]] subteam.) Modularity is the property of a system where it can be abstracted down into a relatively independent, replicable, and composable subsystems (or modules). Many of the concepts surrounding modularity are also biologically/evolutionary inspired, for example, biological nervous systems exhibit highly modular structure at different levels, and it has been proposed that natural selection for evolvability would promote modular structure formation. Although modularity usually adds overhead to system design, it is often the case that a modular system is better than a traditional system. Our focus is on improving EMADE's architecture to support this more modular line of thinking, for example:
* Finding ways to abstract individuals to encourage code reuse/explainability and improve EMADE’s search/optimization capabilities
* Modifying the normal evolutionary process in some way to help modularize portions of individuals
* Creating “building blocks” that can help with the genetic process

Modules/building blocks can be regarded as targeting an isolated subproblem that can be handled separately from other subproblems or are an important part of solving the subproblem. As each module is concerned with a certain subtask, the modules can be designed to be loosely coupled and can add more explainability to the problem and subsequent solutions. It also facilitates exaptation, where existing structures and mechanisms can be reassigned to new tasks and encourage the reuse of components. Ideally, if the portions we abstract are “good” they will help EMADE’s optimization process or be useful in some way. It may also lead to novel solutions not found by traditional GP.

EMADE already uses machine learning models as a form of abstraction. Primitives can be models instead of simple operations found in GP. In fact EMADE already uses a very primitive modular structure from genetic programming known as Automatically Defined Functions (ADFs). However, our subteam has been focusing on a different structure known as Adaptive Representation through Learning (ARLs). For a more in depth explanation on both systems, please read the original literature cited in the "Literature" section below. 
* ADFs: 
** [https://mitpress.mit.edu/books/genetic-programming Genetic Programming]
*** Koza, J. R. (1994). Genetic programming. Cambridge, MA: MIT Press.
** [https://mitpress.mit.edu/books/genetic-programming-ii Genetic Programming II]
*** Koza, J. R. (1998). Genetic programming Ii: automatic discovery of reusable programs. Cambridge, MA: MIT.
* ARLs:
** [https://drive.google.com/file/d/1A7cUhIqIxnX7LBfEtSIY-ZAO7Lrb6gXs/view?usp=sharing Discovery of Subroutines in Genetic Programming]
*** Justinian P. Rosca and Dana H. Ballard. 1996. Discovery of subroutines in genetic programming. Advances in genetic programming: volume 2. MIT Press, Cambridge, MA, USA, 177–201.
** [https://www.aaai.org/Papers/Symposia/Fall/1995/FS-95-01/FS95-01-011.pdf Towards Automatic Discovery of Building Blocks in Genetic Programming]
*** Rosca, Justinian. (1996). Towards Automatic Discovery of Building Blocks in Genetic Programming.
** [https://dl.acm.org/doi/pdf/10.5555/2934046.2934055 An Analysis of Automatic Subroutine Discovery in Genetic Programming]
*** Dessi, A., Giani, A., & Starita, A. (1999). An Analysis of Automatic Subroutine Discovery in Genetic Programming. GECCO.

= Current Members =
* [https://vip.gatech.edu/wiki/index.php/Notebook_Gabriel_Qi_Wang Gabriel Wang]
* [[Notebook Kevin Lin Lu|Kevin Lin Lu]]
* [[Notebook Regina Ivanna Gomez Quiroz|Regina Ivanna Gomez Quiroz]]
* [[Notebook Bernadette Gabrielle Santiago Bal|Bernadette Gabrielle Santiago Bal]]
* [[Notebook Vincent H Huang|Vincent H Huang]]
* [[Notebook Angela Young|Angela Young]]
* [[Notebook Xufei Liu|Xufei Liu]]

= Past Members =
* [[Notebook Aryender Singh|Aryender Singh]]
* [[Notebook Ruarai Eoin O'tighearnaigh|Ruarai Eoin O'tighearnaigh]]
* [[Notebook Jacob Yahia Wang|Jacob Yahia Wang]]
* [[Notebook Weiyao Tang|Weiyao Tang]]
* [[Notebook Krithik Thiyagarajan Acharya|Krithik Thiyagarajan Acharya]]
* [[Notebook Benson Chau|Benson Chau]]
* [[Notebook Joel Ye|Joel Ye]]
* [[Notebook Aaron McDaniel|Aaron McDaniel]]
* [[Notebook Reagan Jui-Keng Kan|Reagan Kan]]
* [[Notebook Avni Tripathi|Avni Tripathi]]
* [[Notebook Abhiram Venkata Tirumala|Abhiram Tirumala]]
* [[Notebook Chidozie Alexander Onyeze|Chido O]]

= Schedule =
We currently have our subteam meetings on Bluejeans at 2:00 PM EST.

[https://bluejeans.com/616992142 BlueJeans Link]

[https://bluejeans.com/2821727431 Backup Link if we need to record the meetings]

= Github Description =
We currently have all our work on a separate fork, which was originally branched off the [https://github.gatech.edu/emade/emade/tree/detection_processing detection_processing] branch of master EMADE. We also use emade-viz for some analysis and AUC calculations, which we have also created a separate fork for. Many of the branches on our fork were used for previous experiments so they have not been deleted in order to preserve the work that has been done. However, this means that some of the branches are also outdated. The following will have a description of every branch.

[https://github.gatech.edu/gwang340/emade Subteam EMADE Github Fork]

[https://github.gatech.edu/gwang340/emade-viz/tree/ADFonlyAUC Subteam EMADE-Viz Github Fork]

====== master ======
* Original branch with working architecture, should be updated to reflect changes from new experiments. Do not use until update

====== fitness_ARL ======
* Legacy branch for the original frequency method for selecting ARLs

====== intelligent_adfs ======
* Legacy branch for a combination of frequency and fitness for selecting ARLs

====== feature/DifferentialFitness_NoRefactor ======
* Uses differential fitness as a heuristic for selecting ARLs, read the original ARL paper for a more in depth explanation of differential fitness

====== feature/DifferentialFitness ======
* Same functionality as the previous branch, but does a lot of refactor of certain features and may have bugs.

====== adf_selection ======
* Legacy branch for basic selection method experiment

====== new_selection ======
* Expansion of the previous branch, with a greater emphasis on weighting individuals with more ARLs for selection

====== arg_fix ======
* Branch that allows for primitives to take in ARG0, needs to be merged into master

====== RODataPairOnly          ======
* Branch that restricts selecting ARLs to just having EmadeDataPairs as both its input and output

====== PrimitiveAnalysis          ======
* Branch with tools used for analyzing AUC and pareto fronts

====== emade-CacheV2 ======
* Pull in changes from master EMADE's CacheV2 branch, needs to be further tested

====== sphinx          ======
* Branch used for Sphinx and code documentation

====== ADFRefactor ======
* Based off some of the changes in DifferentialFitness refactor, goal was to make more objects instead of our convoluted data structure. Needs to be tested

====== ARL_Refactor ======
* Planned branch to properly rename all 'ADF' instances in our code to ARL

= Current Efforts =
Our current ARL architecture is fairly simplistic, as we sometimes chose the simplest option and made certain assumptions during its initial creation. One example of this is that ARLs are all depth 1 subtrees of an individual. However, we now believe we are limited by our simplistic view so we are now trying to expand the complexity of the ARLs that we create. The following are some rough ideas of how we want to rearchitecture our work.
* '''Generalized depth'''
** We want to expand our architecture to support any size ARL. Currently our depth 1 size "limitation" is caused by how we search for ARLs within the population. We search through every individual for combinations of parent and children nodes, and keep track of the frequency of the combination appearances in the population. In order to expand this, we need to think of better ways to search for ARLs within the population. Previously we researched Apriori algorithms for this purpose, but they were more related to graphs and "creating" combinations of nodes not "finding" combinations of node. However, this line of thinking is not necessarily wrong, we simply chose the simplest option at the time.
* '''Partial ARLs'''
** Currently all ARLs are complete subtrees, so every root node is connected to all its children. However, not every child node may be useful, which leads to the concept of "partial/incomplete" subtrees being used for ARLs. Previously this was somewhat ignored as we expected partial ARLs from depth 1 trees to not be very useful. However, if we expand the architecture to support any size ARL we should also account for and create these partial ARLs in order to create more variability within the ARLs and prevent less useful primitives from always being included within ARLs. However, we do worry about if this would affect the computational complexity of this process, so we need to be careful on how we approach this.
* '''Database Storage'''
** Currently all ARLs are treated as lambda functions within EMADE, and we store certain information within the database for reference, such as the actual lambda expression and the input/output types. However, this decision was made before we allowed data manipulating primitives and learners to be included within ARLs, which we have now changed and seen better results in our experiments. While this change to allow data modifying primitives in ARLs has led to better results it has also led to the creation of some bugs and revealed some oversights. Since we don't store the pickled versions of the expressions (the actual GP tree for individuals and primitives), unfortunately some primitives when compressed into an ARL lose some information and attributes, leading to errors. We hope to actually store the picked nodes in the database now, and instead of just relying on lambda functions for everything, we can "expand" and "contract" ARLs when evaluating to go between the single abstracted ARL node and its proper full tree representation. This would allow us to store more information, and may potentially lead to possibilities such as ARL mating/mutation/coevolution and ARLs become "tiny individuals" in a sense. 
* '''Sphinx Documentation'''
** Overall we need better documentation for our efforts. This wiki page is to keep better track of our goals and conceptual work. Sphinx will help us document our codebase and create guides and API documentation for our changes. Our current goal is to create up to date Sphinx documentation for our repo to both leave a lasting legacy for ease of use and to familiarize ourselves more with the code base.

= Potential Future Work =
* '''Diversity'''
** Modularity generally encourages code reuse, but that raises the question, are we limiting the search space by encouraging reuse? We want some quantifiable way to measure diversity, and would be nice if it was generalizable for EMADE overall either as an objective or as a standalone tool. It may also be worth implementing a diversity measure as a type of heuristic when selecting these ARLs.
* '''Selection Method'''
** This work revolves around modifying the evolutionary selection method to help encourage the spread of ARLs throughout the population. The basic assumption for why this would work is that if ARLs are “useful” having more of them in the population would be more useful. Therefore we should select more individuals with ARLs so that they can allow for the propagation of ARLs throughout the population.
* '''ADF Integration'''
** ADFs may be a good direction to look into as they are what the master branch of EMADE uses and what most literature references to. ARLs and ADFs are not mutually exclusive, and it may be interesting to see how these two architectures interact with each other, as they have their own strengths and weaknesses. The original EMADE's ADF structure is also very simplistic, but the original literature treats ADFs as a very architecture heavy self-modifying process, which unfortunately is not implemented in EMADE yet. If there is enough support, ADF experimentation and expansion may also be a worthwhile endeavor. 
* '''Mutation Method'''
** Similar line of reasoning with the selection method work, to help spread ARLs and for more potential changes with the population we could try making a mutation function that replaces certain nodes with ARLs. This may be beneficial to expand complexity, as we have limited ARLs to be data manipulating types of primitives, so if an ARL is complex, but the individual is not, the mutation may benefit the individual.
* '''MNIST Experiments'''
** In previous semesters all our experiments have been run on the Titanic dataset. While we have seen some good results, we have never reached any significance. We believe that by using our architecture on a more complex problem, such as MNIST where we can leverage more signal and spatial primitives, we will see some better results. 
*'''Evolving ARLs'''
**ARLs are abstracted combinations of nodes that can no longer be modified once it has been abstracted. While this can be beneficial as it prevents too many destructive changes, it only works with the assumption that ARLs are inherently good, and may limit the searching capabilities in EMADE. To help counteract this downside and help with the underlying assumption, if we could have a secondary optimization/evolutionary process that could improve our existing ARLs over time, it could help prevent some of these problems. If we go the evolutionary route, similar to ADFs, we could potentially allow for mating, mutating, co-evolving, aging, etc. ARLs to help optimize our pool which would then help EMADE's overall optimization process.
** (Previous outdated page) [[ADF: Evolving ADFs]]
* '''ARL selection heuristics'''
** As mentioned, our method for selecting ARLs is fairly simplistic. There is no definitive way to know which primitives will be beneficial, therefore there is no definitive way to know which primitives to select for ARLs. However, there have been modularity metrics referenced in the literature, as well as a plethora of existing heuristics for selecting "useful" portions of individuals. We ourselves, have seen improvements with better heuristics for choosing combinations of nodes for ARLs, therefore this area of research may still be worthwhile.
** (Previous outdated page) [[ADF: Entropy Informed ADF Generation]]

= Resources =

== Presentation Slides ==
* [https://docs.google.com/presentation/d/1fPMmlTXOuPC2PACyJzmm3X6htNpVnMZcQhw1Da-9VPo/edit?usp=sharing Fall 2019 Midterm Presentation]
* [https://docs.google.com/presentation/d/1o7aoCDuJgwq7mgVcsqZ-5j67A0-Qknx1qza3bkVaNhY/edit?usp=sharing Fall 2019 Final Presentation]
* [https://docs.google.com/presentation/d/1VzJBOXUmKNmMjk3AgiRHe3Uq8gaHrWDserQgtQwZtOo/edit?usp=sharing Spring 2020 Midterm Presentation]
* [https://docs.google.com/presentation/d/1pM_VSVOVw0l5aHGx5YjvkpRcBvNPixVkjcpAnIzLzPk/edit?usp=sharing Spring 2020 Final Presentation]
* [https://docs.google.com/presentation/d/1ZTS5ij1kNA8cymFyIYWU1fkuJe3PuNVl_H2IyzB22RQ/edit?usp=sharing Fall 2020 Midterm Presentation]
* [https://docs.google.com/presentation/d/1KU-tlra_DXV93JOS6NjQ5RB24Urj0-M8F99U3YsQQ70/edit?usp=sharing Fall 2020 Final Presentations]
* [https://docs.google.com/presentation/d/1IO3LjZZYo4sKHMQILDyWWY-EngNwMHFWmh3ntrWZdh8/edit?usp=sharing (Outdated) Fall 2020 Possible Future Efforts]
* [https://docs.google.com/presentation/d/1nivJn2MfO-Amf-yvODm7LTLPNlpyeGXjzLXxgbbqvD4/edit?usp=sharing (Outdated) ADF Intro Spring 2020]
* [https://docs.google.com/presentation/d/1yrkD411TYEVQ8OMiqODLsoXdDiVo43PZBZpW9-1TNlo/edit?usp=sharing (Current) Modularity Intro Spring 2021]

== Video Recorded "Lectures" ==
* [https://bluejeans.com/s/GaOjlZ5qu3P Modularity Intro]
* [https://bluejeans.com/s/MkVzKyIhA0q/ Discussion session on re-architecture] 

== Literature ==
* [https://drive.google.com/file/d/1A7cUhIqIxnX7LBfEtSIY-ZAO7Lrb6gXs/view?usp=sharing Discovery of Subroutines in Genetic Programming]
** Justinian P. Rosca and Dana H. Ballard. 1996. Discovery of subroutines in genetic programming. Advances in genetic programming: volume 2. MIT Press, Cambridge, MA, USA, 177–201.
** Good intro to ARLs
* [https://www.aaai.org/Papers/Symposia/Fall/1995/FS-95-01/FS95-01-011.pdf Towards Automatic Discovery of Building Blocks in Genetic Programming]
** Rosca, Justinian. (1996). Towards Automatic Discovery of Building Blocks in Genetic Programming.
** Good intro to ARLs
* [https://dl.acm.org/doi/pdf/10.5555/2934046.2934055 An Analysis of Automatic Subroutine Discovery in Genetic Programming]
** Dessi, A., Giani, A., & Starita, A. (1999). An Analysis of Automatic Subroutine Discovery in Genetic Programming. GECCO.
** Analysis and testing of Rosca’s concept
* [https://mitpress.mit.edu/books/genetic-programming Genetic Programming]
** Koza, J. R. (1994). Genetic programming. Cambridge, MA: MIT Press.
** In depth look into Genetic Programming
* [https://mitpress.mit.edu/books/genetic-programming-ii Genetic Programming II]
** Koza, J. R. (1998). Genetic programming Ii: automatic discovery of reusable programs. Cambridge, MA: MIT.
** Continuation of the first book with the focus on ADFs
* [https://link.springer.com/article/10.1007/s10462-019-09706-7 A review of modularization techniques in artificial neural networks]
** Amer, M., Maul, T. A review of modularization techniques in artificial neural networks. ''Artif Intell Rev'' '''52,''' 527–561 (2019). https://doi.org/10.1007/s10462-019-09706-7
* [https://link.springer.com/chapter/10.1007/978-3-540-88906-9_54 Self-adaptive mutation only genetic algorithm]
** Shiu, K. L., & Szeto, K. Y. (2008). Self-adaptive mutation only genetic algorithm: An application on the optimization of airport capacity utilization. In C. Fyfe, D. Kim, S.-Y. Lee, & H. Yin (Eds.), ''Intelligent Data Engineering and Automated Learning – IDEAL 2008'' (Vol. 5326, pp. 428–435). Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-540-88906-9_54
** Interesting method brought from this article is increasing the mutation probability for less fit and decreasing the mutation probability for more fit individuals in the population.
** Similar to a mutation method that the NLP team implemented last semester
* [https://ieeexplore.ieee.org/abstract/document/7748328 A Survey of Modularity in Genetic Programming]
** G. Gerules and C. Janikow, "A survey of modularity in genetic programming," ''2016 IEEE Congress on Evolutionary Computation (CEC)'', Vancouver, BC, Canada, 2016, pp. 5034-5043, doi: 10.1109/CEC.2016.7748328.
** Previously there was no intrinsic value of a subroutine, since subroutine's fitness didn't differentiate from other building blocks of code.
** Sometimes, created routines aren't removed which leads to unfit routines (which actually may be an issue with our architecture)
*** As the GP run happens, we track newly created building blocks that are added to the function set and should remove them if they are "unfit".
** Rosca and Ballard used subroutines or building blocks with their own fitness functions to identify useful ones and add them to a function set of an evolved genetic programs.
** Ways to keep track of usefulness
*** Structural complexity - number of nodes in a tree for subroutine
*** Evaluation complexity - number of nodes in a tree and number of times a call is made to the routine
*** Evaluation complexity - keeps track of "call hierarchies"
*** Description complexity - uses minimum description length MDL
** Subroutines are subtrees with a depth between 2 and 4 (for the original ARL architecture, our implementation only has a depth of 1)
** Paper describes some methods and heuristics to find these subroutines
** Also suggests that low fitness subroutines are replaced with mutations - etc randomly generated routines
** The original ARL papers had mixed results on traditional GP problems, the hope is that with larger building blocks in EMADE like machine learning models and image processing primitives we can see better results
* [https://www.sciencedirect.com/science/article/pii/S0020025520302632 Asymptotic resolution bounds of generalized modularity and multi-scale community detection]
** Asymptotic resolution bounds of generalized modularity and multi-scale community detection. (2020). ''Information Sciences'', ''525'', 54–66. https://doi.org/10.1016/j.ins.2020.03.082 
* [https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.152.9427&rep=rep1&type=pdf A Brief Overview of Population Diversity Measures in Genetic Programming]
** Hien, N. T., & Hoai, N. X. (2006). A Brief Overview of Population Diversity Measures in Genetic Programming . ''Proceedings of the Third Asian-Pacific Workshop on Genetic Programming'', 128–139.
** The paper discusses several metrics used in the literature for measuring population diversity.
** The first category is edit-distance based measures, wherein nodes of individuals are directly compared in order to compute a distance between individuals. For example, one such edit-distance metric measures the number of substitutions, insertions, and deletions needed to transform one tree to another.
** The second category is space mapping distance, wherein genotypes of a population are mapped onto a plane, and the smallest rectangle containing all points is computed.
** The third category compares subtrees of individuals in order to determine similarity. One such technique involves calculating the difference between the union and intersection of two individuals.
** Less commonly used measures include history diversity, wherein the parents of individuals are tracked, and the distance between individuals is given by how distant their common ancestor is
** The last measure discussed suggests using entropy in order to measure the disorder of the population.
* [https://dl.acm.org/doi/pdf/10.1145/3376916 Indicator-based Multi-objective Evolutionary Algorithms: A Comprehensive Survey]
** Falcón-Cardona, J. G., & Coello, C. A. C. (2020). Indicator-based multi-objective evolutionary algorithms: A comprehensive survey. ''ACM Computing Surveys'', ''53''(2), 1–35. https://doi.org/10.1145/3376916
** '''Paper tries to resolve issue of Multi-Objective Evolutionary Algorithms (MOEAs) underperforming with >2/3 objectives by introducing IB-MOEAs'''
*** '''Indicator-Based MOEAs (IB-MOEAs) -''' use '''quality indicators''' that are used to assess different metrics of '''approximation sets''' (paper defines this as an approximate Pareto front)
*** '''Quality Indicators (QIs)''' - measure performance based on capacity, convergence, and '''diversity'''
* [https://dl.acm.org/doi/pdf/10.1145/3321707.3321718 Convergence and Diversity Analysis of Indicator-based Multi-Objective Evolutionary Algorithms]
** Falcón-Cardona, J. G., & Coello, C. A. C. (2019). Convergence and diversity analysis of indicator-based multi-objective evolutionary algorithms. ''Proceedings of the Genetic and Evolutionary Computation Conference'', 524–531. https://doi.org/10.1145/3321707.3321718
** Authors from previous paper also made another analyzing the diversity and convergence of approximation sets/results from these IB-MOEAs
** Key Notes:
*** '''Quality Indicators (QIs)''' are further clarified as ''functions that assign a real value to one or more approximated Pareto fronts, depending on their specific preferences''
** '''3.3 Diversity Analysis'''
*** Used Solow-Polasky Diversity Indicator and the Riesz s-energy indicator
* [https://www.researchgate.net/profile/Michael_Emmerich/publication/235412985_On_Quality_Indicators_for_Black-Box_Level_Set_Approximation/links/0c960514cad7fc4ead000000.pdf On Quality Indicators for Black-Box Level Set Approximation]
** Emmerich, Michael & Deutz, André & Kruisselbrink, Johannes. (2013). On Quality Indicators for Black-Box Level Set Approximation. 10.1007/978-3-642-32726-1_4.  
** Lists several indicators that measure how well a set of points approximates a level set and goes into detail behind the math calculations behind different quality indicators
** Solow-Polasky Diversity Indicator is gone into detail here to discuss how to calculate diversity.
*** Note that this ended up being used for diversity oriented search
*** This makes it interesting for measuring bio-diversity, as its value can be interpreted as the number of species
* [https://www.researchgate.net/publication/3418773_Diversity_in_Genetic_Programming_An_Analysis_of_Measures_and_Correlation_With_Fitness Diversity in Genetic Programming: An Analysis of Measures and Correlation With Fitness]
** Burke, Edmund & Gustafson, Steven & Kendall, Graham. (2004). Diversity in Genetic Programming: An Analysis of Measures and Correlation With Fitness. Evolutionary Computation, IEEE Transactions on. 8. 47 - 62. 10.1109/TEVC.2003.819263. 
* [https://arxiv.org/abs/1703.00548 Evolving Deep Neural Networks]
** Miikkulainen, R., Liang, J., Meyerson, E., Rawal, A., Fink, D., Francon, O., Raju, B., Shahrzad, H., Navruzyan, A., Duffy, N., & Hodjat, B. (2017). Evolving deep neural networks. ''ArXiv:1703.00548 [Cs]''. http://arxiv.org/abs/1703.00548
* [https://ieeexplore.ieee.org/document/6387221 Cooperative Coevolution of Automatically Defined Functions with Gene Expression Programming]
** A. Sosa-Ascencio, M. Valenzuela-Rendón and H. Terashima-Marín, "Cooperative Coevolution of Automatically Defined Functions with Gene Expression Programming," ''2012 11th Mexican International Conference on Artificial Intelligence'', San Luis Potos, Mexico, 2012, pp. 89-94, doi: 10.1109/MICAI.2012.15.
* [https://www.researchgate.net/publication/229046942_Co-evolutionary_automatically_defined_functions_in_genetic_programming Co-evolutionary automatically defined functions in genetic programming]
** Lukas, Anthony & Oppacher, Franz. (2009). Co-evolutionary automatically defined functions in genetic programming. 
* [https://link.springer.com/chapter/10.1007/BFb0040831 Co-evolving functions in genetic programming: Dynamic ADF creation using GliB]
** Ahluwalia, M., & Bull, L. (1998). Co-evolving functions in genetic programming: Dynamic ADF creation using GliB. In V. W. Porto, N. Saravanan, D. Waagen, & A. E. Eiben (Eds.), ''Evolutionary Programming VII'' (pp. 809–818). Springer. https://doi.org/10.1007/BFb0040831