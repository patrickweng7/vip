== Team Member ==
[[files/Aazia_Azmi_Profile.jpeg|thumb|123x123px]]
Team Member: Aazia Azmi

Email: aaziaazmi@gatech.edu

Cell Phone: 470-312-3125

Interests: Machine Learning, Web Development, Baking, Cats

== April 26, 2021 ==

=== Individual Notes: ===
* Gabe couldn't get a master process started so we just used Colab to run the master process
* Later I managed to get Colab Pro for longer runs
* Devan and I finally tried to do runs with the "average" parameter set to "weighted" and we managed to get 3 working objectives.
[[files/3_objectives.PNG]]

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add slides and prepare for final presentation
|Completed
|April 26, 2021
|April 30, 2021
|April 29, 2021
|}

== April 19, 2021 ==
=== Team Meetings/Lecture Notes: ===
* Peer Evals will close on 4/27 at 4 pm
* Final presentations are on 4/30 6-8:50pm
* Notebooks will be due on Saturday night

=== Individual Notes: ===
* We realized that the reason that the objective functions were not working out for us was that they were meant for binary classifications.
* I suggested and tried a new objective function called Cohen Kappa and it gave us a different score from F1
* [https://github.gatech.edu/gwang340/emade/commit/66172267e6ab2e632eb49e93c245c45d245f24d8 My changes]
* Devan also suggested changing the "average" parameter in the scikit methods to 'weighted" based on [https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/ this reference] but we did not get a chance to try it

=== Subteam Meeting Notes (April 25): ===
* All new members are tasked with doing baseline runs.
* Whoever is doing the master runs needs to do so directly from their machine because Colab terminates processes after 12 hrs.
* After Devan shared our progress with objective functions, it was decided that we will continue with F1 and Cohen Kappa score functions.

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Figure out times to do baseline runs with other new members
|Completed
|April 25, 2021
|April 25, 2021
|April 25, 2021
|-
|Do baseline runs with team
|Completed
|April 25, 2021
|April 30, 2021
|April 29, 2021
|}

== April 12, 2021 ==
=== Team Meetings/Lecture Notes: ===
* Since we currently have two objectives that give the exact same value, we should try to come up with a new objective
* Suggested: precision/recall metrics or confusion matrix data for objective

=== Subteam Meeting Notes (April 18): ===
* Differentiating between feature data and stream data: Feature data like the titanic dataset directly goes into learners for prediction and Stream data (images) require a feature extraction step (stream data -> feature data) before going into learners.
* Currently both objectives are giving the same values and we aren't able to generate enough valid individuals so Devan and I were assigned to look into changing the objective functions next week

=== Individual Notes: ===
* All objective functions are in eval_methods.py and we can select which one to use in our run in the input XML file.
* Most of the functions in eval_methods use [https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics scikit metrics]. NOTE: the functions are usually in the form of 1 - *scikit metric function* because we are trying to minimize the value returned by the function.
* They all seem to pass "individual" as a parameter but not use it. This is because of some formatting reasons.

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look into the source code for the objective functions as well as into scikit metric methods
|Completed
|April 18, 2021
|April 25, 2021
|April 22, 2021
|-
|Experiment with evaluation methods to fix our objective function problem
|Completed
|April 18, 2021
|April 25, 2021
|April 22, 2021
|-
|Set up a meeting time to work with Devan
|Completed
|April 18, 2021
|April 25, 2021
|April 18, 2021
|}

== April 5, 2021 ==
=== Team Meetings/Lecture Notes: ===
* Statistic for emade presentation about hypothesis testing and determining statistical significance
* Tests to find statistical significance: Student's t-test and Welch's t-test
* Welch's t-test is useful when expected observation is unknown

=== Subteam Meeting Notes (April 11): ===
* Make a copy of [https://colab.research.google.com/drive/1i_niAH2dxqdsdA-SMU3tYXCOW0DMRUXK?usp=sharing this notebook] shared by Gabriel and follow the instructions to get emade runs
* We want to get a successful run and increase diversity in our individuals

=== Individual Notes: ===
* Ran into trouble while trying to run reinstall.sh, fix was to run
 !sudo apt install dos2unix
 !find . -type f -print0 | xargs -0 dos2unix
* Ran emade for close to 50 generations but could not get any valid individuals (except what was seeded) but I might have just gotten unlucky

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run emade on mnist
|Completed
|April 5, 2021
|April 12, 2021
|April 7, 2021
|-
|Read papers from the light reading section of the intro presentation
|Completed
|March 29, 2021
|April 5, 2021
|April 15, 2021
|}

== March 29, 2021 ==
=== Team Meetings/Lecture Notes: ===
* Assigned to Modularity team which meets on Sundays 2-4 pm

=== Subteam Meeting Notes (April 4):: ===
* [https://docs.google.com/presentation/d/1nivJn2MfO-Amf-yvODm7LTLPNlpyeGXjzLXxgbbqvD4/edit#slide=id.g720ad7ae25_2_82 Intro presentation]
* Seeding is just adding individuals to the database to speed up the process
* Due to some naming convention ADFs are actually ARLs
* This semester we're working on getting runs on mnist data

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Send GaTech email to Gabriel to be added to the github repository
|Completed
|March 29, 2021
|March 5, 2021
|April 2, 2021
|-
|Read papers from the light reading section of the intro presentation
|Completed
|March 29, 2021
|April 5, 2021
|April 12, 2021
|}

== March 22, 2021 ==
=== Team Meetings/Lecture Notes: ===
* Presented ML, MOGP, and EMADE presentations to the entire class
* Watched all the subteam presentations (EZCGP, Stocks, Modularity, NLP) to choose one

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Send team preference to Dr. Zutty by completing assignment on canvas
|Completed
|March 22, 2021
|March 28, 2021
|March 26, 2021
|}

== March 17, 2021 ==

=== Team Meetings/Lecture Notes: ===
* Spent most of the class solving errors that came up while trying to connect to the server. I finally managed to connect to the server after Rishit gave us the right IP address to connect to (he obtained this IP from his VPN connection)
* A teammate still can't connect to the server (possibly due to firewall stuff)
* in data_splitter.py, everything before line 22 can be changed. Truth data needs to be concatenated (added to the last column) to the rest of the data.

=== Subteam Meeting Notes: ===
* I was assigned to make changes to the splitter file, to run emade without any changes made to data_splitter and evaluation functions, to make changes to the wiki group page, and add my part to the group presentation
* Because connecting remotely gave us errors, it was decided that we will run emade locally
* Through slack and help from other VIP members, we found out that we did not understand Full Dataset Positives and Negatives correctly (we thought it was just FNR and FPR). Here is the link to the changes I made to evaluation functions in [https://github.gatech.edu/aazmi6/VIP-emade-Bootcamp-Subteam-4-Titanic/blob/master/evalFunctions.py evalFunctions.py] to get FNR and FPR values
* We also found out that our pareto front was supposed to be a 3D graph, and so we squashed it down to 2 dimensions. [https://colab.research.google.com/drive/1UQZkKmY0OzaF2OJvNhbslgITb_lpAV29?usp=sharing Pareto Front Code]
* [https://github.gatech.edu/aazmi6/VIP-emade-Bootcamp-Subteam-4-Titanic/blob/master/titanic_data_splitter.py My changes to titanic_data_splitter.py]

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Find good times to meet with team using lettucemeet
|Completed
|March 17, 2021
|March 22, 2021
|March 17, 2021
|-
|Edit data_splitter.py for the team based on ML and GP assignment
|Completed
|March 19, 2021
|March 22, 2021
|March 19, 2021
|-
|Run emade without any changes made to data preprocessing and evaluation functions
|Completed
|March 21, 2021
|March 22, 2021
|March 21, 2021
|-
|Make changes to evalFunctions.py to get FNR and FPR instead of full dataset values
|Completed
|March 21, 2021
|March 22, 2021
|March 21, 2021
|-
|Update team wiki page
|Completed
|March 21, 2021
|March 22, 2021
|March 22, 2021
|-
|Make changes to the slides for the presentation
|Completed
|March 21, 2021
|March 22, 2021
|March 22, 2021
|}

== March 10, 2021 ==

=== Team Meetings/Lecture Notes: ===
* <launchSize> and <minQueueSize> are used for changing how many individuals at minimum need to be evaluated before moving to the next gen and how many individuals need to be in the queue at minimum.
* Log files (.err and .out) are generated in the directory from where you run emade looking into which can make debugging easy

=== Subteam Meeting Notes: ===
* The team met on Monday (03/15) to try and connect with Rishit's server
* We had trouble connecting to the server and his fix was to connect to gatech VPN. But we still could not connect.
* Install deap == 1.2.2 to get pareto front
* For local run, despite what was taught in class, setting <localPythonCommand> to simply python gave me error-free results, and setting it to my anaconda environment python gave errors.

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with team to connect to Rishit's database
|Completed
|March 10, 2021
|March 17, 2021
|March 15, 2021
|-
|Look into how to add data preprocessing
|Completed
|March 10, 2021
|March 17, 2021
|March 19, 2021
|}

== March 3, 2021 ==

=== Team Meeting/Lecture Notes: ===
* Presented in class today and so did Team 5
* To run emade, we just type into the cmd line
 py src/GPFramework/launchGTMOEP.py templates/input_titanic.xml
* Change the location of python in the first block in XML file and change database details in the second block
* The datasets block is for splitting the input file into 5 different files for Monte Carlo
* Our preprocessing needs to be added into datasets/titanic/titanic_data_splitter.py
* The evaluation code block is where evaluation functions are specified and <workersPerHost> specifies how many evaluations to run in parallel (change this to 2-3 for a laptop)

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Connect with localhost and toy around with emade
|Completed
|March 3, 2021
|March 10, 2021
|March 5, 2021
|-
|Help all teammates set up emade
|Completed
|March 3, 2021
|March 10, 2021
|March 13, 2021
|}

== February 24, 2021 ==

=== Team Meeting/Lecture Notes: ===
* The first 3 teams presented their Titanic ML + GP work
* We will present next class

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Intsall Emade
|Completed
|February 24, 2021
|March 3, 2021
|March 2, 2021
|}

== February 17, 2021 ==

=== Team Meeting/Lecture Notes ===
* Discussed the Titanic problem and multi-objective evolutionary algorithms that we would have to implement in our next assignment.
* For the next assignment, we can use DEAP but not its evolutionary algorithms

=== Subteam Meeting Notes: ===
* We discussed what primitives we would use and finalized add, subtract, multiply, negative, maximum, minimum, square, sin, cos, tan
* We used mutShrink and mutUniform as our mutation function and cxOnePoint as our mating function.
* We decided to use selBest and SPEA2 as our selection functions
* After we finished working on our assignment, I was assigned to work on the slides for our presentation and to update our wiki page (can be viewed on group page)

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with subteam to work on our GP methods
|Completed
|February 17, 2021
|February 24, 2021
|February 20, 2021
|-
|Complete Slides
|Completed 
|February 17, 2021
|February 24, 2021
|February 21, 2021
|-
|Update team wiki page
|Completed
|February 17, 2021
|February 24, 2021
|February 23, 2021
|-
|}

== February 10, 2021 ==
=== Team Meeting/Lecture Notes ===
* Assigned to subteam 4. Group page: [[Group 4]]
* The team lead is [[Notebook Rishit Mohan Ahuja|Rishit Mohan Ahuja]].
* The first assignment is the [https://www.kaggle.com/c/titanic Titanic Challenge]. We must each come up with an algorithm to solve this challenge and they must all be codominant.
* We must all use the same pre-processed data.

===Subteam Meeting Notes===
* We explored our dataset and found that Cabin had a lot of NULL values (77%) and hence decided to remove it from our training set.
* We then encoded our categorical features. For the name feature, we decided to use name titles and the length of the name (which surprisingly benefitted our accuracy).
* We substituted our NULL values with the average value of the feature
* Based on a correlation matrix that shows the correlation between different features, I made suggestions on what features could be dropped from the training set
* I worked on our Neural Network algorithm which I implemented using scikit (can be viewed in notebook on group page)
 Accuracy: 0.8271186440677966, Kaggle Score: 0.70813

 True Negatives 157
 False Positives 34
 False Negatives 17
 True Positives 87
* Finally, I plotted the FNR and FPR values for each algorithm to ensure codominance (can be viewed on group page)

===Action Items===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with subteam to discuss data pre-processing
|Completed
|February 10, 2021
|February 17, 2021
|February 15, 2021
|-
|Work on ML Algorithm for Titanic Challenge
|Completed 
|February 10, 2021
|February 17, 2021
|February 16, 2021
|-
|Check the codominance of team algorithms
|Completed
|February 10, 2021
|February 17, 2021
|February 16, 2021
|-
|}

== February 3, 2021 ==
=== Notebook Self-Evaluation ===
https://drive.google.com/file/d/1I0bQLMgpJ1usBnkE0RyPcBNdzRAZe96h/view?usp=sharing

=== Team Meeting/Lecture Notes ===
* Genetic Programming Life Cycle: New Gene Pool → Evaluation → Genes w/ scores → Fitness Computation → Genes w/ fitness → Selection → Parental Genes → Mating → Child Genes → Mutation → New Gene Pool
* Gene Pool is the set of Genomes that are to be evaluated. A Genome can be defined as the genotypic description of an individual's DNA. Search space can be defined as a set of all genome which is massive and we use GP to explore this space.
* Objective transforms the search space (genotypic description) into an objective space (phenotypic description). NOTE: the number of dimensions in the objective space is equal to the number of objectives.
* The process of evaluation associates a genome/individual with a set of scores. These scores can be:
** True positive: expecting something to be true when it is true
** False Positive: predicting something to be true when it is actually false
* Sensitivity or True Positivity Rate (TPR) = TP/P = TP/(TP+FN). Specificity (SPC) or True Negative Rate (TNR) = TN/N = TN/(TN+FP). We want these values to be as close to 1 (the bigger the better). In this class, we try to minimize, so we redefine TPR = 1- TPR and TNR = 1 - TNR (the smaller the better).
* Accuracy (ACC) = (TP + TN)/(P + N) = (TP+TN)/(TP+FN+TN+FP). For accuracy, bigger is better
* Negative Predictive Value (NPV) = TN / (TN+FN). For NPV, bigger is better
* An individual is Pareto Optimal if it is not outperformed in every objective by another individual. Selection is driven by favoring Pareto Optimal individuals. Visual Trick: Picture a rectangle between the point and the origin, if the rectangle of one individual is perfectly within the rectangle of a second individual, then the second individual has been dominated. However, if two individuals are a little bit in each other's triangles, then they are Pareto optimal.
* Nondominated Sorting Genetic Algorithm II: Separate the population into nondomination ranks. Individuals are selected using binary tournament. Between two individuals in different Pareto ranks, the one in the lower Pareto rank wins. For two individuals in the same Pareto rank, we pick the one that is less crowded. We can calculate this by finding the sum of Euclidean distances between an individual and every other individual in the same Pareto rank. Then we pick the one with the highest sum. This is done to increase phenotypic diversity. 
* Strength Pareto Evolutionary Algorithm II: We compute the strength (S) of each point. We calculate how many individuals, each individual dominates. We then calculate the rank (R) of each individual which is equal to the sum of strengths of the individuals that dominate it (NOTE: this value is zero for Pareto individuals). 
** For objects of the same rank, we calculate the distance to the kth nearest neighbors and calculate R + 1/(σ^k + 2). Hence, more crowded individuals will have a fractionally higher rank.

=== Lab Notes ===
* Part of Lab 2 is the same as part 1, except this time our desired function is -x + sin(x^2) + tan(x^3) - cos(x). Additionally, this time we will minimize two objectives, mean squared error and the size of our tree.
* We define a separate individual in addition to the population of 300 individuals. We use this individual to compare it with individuals that dominate the individual and those that are dominated by the individual.
* After this, we run our evolutionary algorithm.
** Best individual is: negative(cos(multiply(add(cos(sin(cos(sin(cos(tan(x)))))), cos(x)), tan(x))))
** Best fitness score is: (0.2786133308027132, 15.0)
[[files/Aazia_Lab2_Fitness1.PNG|center|thumb]]
* We visualize our objective space and Pareto front
** Area Under Curve: 2.3841416372199005
[[files/Aazia_Lab2_Pareto1.PNG|center|thumb]]
* To reduce the area under the curve, I simply changed my mutation function to mutUniform.
**Best individual is: add(sin(x), negative(x))
**Best fitness score is: (0.7201260772387972, 5.0)
[[files/Aazia_Lab2_Fitness2.PNG|center|thumb]]
**This changed my AUC to 0.3064205003288344 which is a reduction of 87%.
[[files/Aazia_Lab2_Pareto2.PNG|center|thumb]]

===Action Items===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|E-mail 'Rate Yourself' to Dr. Greg
|Completed
|February 3, 2021
|February 3, 2021
|February 3, 2021
|-
|Complete Lab 2
|Completed 
|February 3, 2021
|February 10, 2021
|February 10, 2021
|-
|Complete Self-Evaluation
|Completed
|February 3, 2021
|February 12, 2021
|February 10, 2021
|-
|}

== January 27, 2021 ==
=== Team Meeting/Lecture Notes ===
* Instead of having an evaluator function that gives us the objective of the individual, the individual will be the function itself
* Tree Representation of f(x):
** Nodes are called primitives and represent functions, eg: +, -, x, /
** Leaves are called terminals and represent parameters, eg: integers, variables
** The output is produced at the root of the tree, eg: 1+ 2 x 4G
* Lisp Preordered Parse Tree
** The operator is followed by the inputs
** To obtain f(x) from the parse tree in the form of an expression, simply perform an in-order traversal of the tree
* Crossover in Genetic Programming using Parse Trees is done by exchanging subtrees to produce different children
* Mutation in Genetic Programming is done by:
** Inserting a node/subtree
** Deleting a node/subtree
** Changing a node
* Symbolic regression is used to evolve solutions using primitives
* For example, we can use symbolic regression to evolve a solution to y = sin(x) by using the Taylor series formula for sin(x).
* Primitives for solving this problem: factorial, sin function (although this would trivialize the problem), cost, tan, exponent
* In order to evaluate the tree, we can measure the difference between the output and the truth that we obtain after running f(x)
===Lab Notes===
* Symbolic Regression is a type of evolutionary algorithm. For this lab, our individuals will inherit DEAP's PrimitiveTree instead of lists. More about PrimitiveTree can be found at https://github.com/DEAP/deap/blob/master/deap/gp.py
* We maintain a primitive set and add primitives to it. We follow the same steps as in Lab 1 in defining an individual, population, and evaluation function
* The evaluation function finds the squared sum of the difference between our desired function's output and our individual's output. Our desired function is x^4 + x^3 + x^2 + x^1.
* Implement a function to compile the trees to the toolbox
* Original Output (using uniform mutation function):
** Best individual is subtract(subtract(subtract(x, x), negative(x)), subtract(negative(subtract(multiply(subtract(subtract(x, x), negative(x)), x), negative(multiply(x, multiply(x, x))))), multiply(multiply(x, multiply(x, x)), x)))
** Best Fitness Score is 7.602982495923859e-17
[[files/Original_Output.png|center|thumb]]
* Output after adding primitive
** Best individual is add(add(square(square(x)), x), add(multiply(x, x), multiply(multiply(x, x), x)))
** Best Fitness Score is 1.0321736972101162e-16
 pset.addPrimitive(np.absolute, arity=1)
 pset.addPrimitive(np.square, arity=1)
[[files/Primitive_added_Mutation.PNG|center|thumb]]
* Output after adding NodeReplacement mutation
** Best individual is add(multiply(x, add(multiply(x, multiply(x, x)), multiply(x, x))), add(multiply(x, x), x))
** Best Fitness Score is 1.0760604870374914e-16
[[files/Node_Replacement_Mutation_Output.PNG|center|thumb]]

===Action Items===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Lecture Notes for the week into notebook
|Completed
|January 27, 2021
|February 3, 2021
|January 27, 2021
|-
|Review Material on GP
|Completed 
|January 27, 2021
|February 3, 2021
|January 26, 2021
|-
|Work on Lab 2 Part 1
|Completed
|January 27, 2021
|February 10, 2021
|February 4, 2021 
|-
|}

== January 20, 2021 ==
===Team Meeting/Lecture Notes===

* '''General Information:'''
** Syllabus Overview: [[Syllabus-Spring-2021|Syllabus]], Semester schedule: [[Calendar-Spring-2021|Calendar]]
** Join BootCamp meetings, Wednesdays 5:00 - 5:50 pm
** Join Transition Meeting on Monday, March 22, 5-8 pm
** Join Main Meetings after the transition, Mondays 5-6 pm
* '''Lecture Content'''
** Genetic Algorithms: Evaluate each individual in a population of solutions. If stopping criteria are not met, a new generation is created through mating/mutation and the evaluation step is performed again.
** Keywords:
*** Individual: A specific candidate in the population
*** Population/Generation: a group of individuals whose properties will be altered to obtain a new generation
*** Objective: A characteristic that is to be optimized by evolution
*** Fitness: Relative comparison to other individuals
*** Evaluation: A function that is used to compute the objective of an individual
** Selection is used to give preference to better individuals by allowing them to pass on their genes. Selection methods:
*** Fitness Proportionate: Random pick where each individual has a chance proportional to their fitness
*** Tournament: Tournaments are performed between individuals and the winners are selected for mating
** Genetic Algorithms for creating a new generation:
*** Mating (Crossover): Mate individuals (use one or more parts of both parents to form chid)
*** Mutation: Introduce random modification to the child to introduce diversity
** Genetic Algorithm Steps:
**# Random initialization of population
**# Performing evaluation to determine the fitness of the population, if stop criteria are not met move to next step
**# Select Parents from the population using selection methods
**# Perform crossover on parents creating a population
**# Perform mutation on the population to create diversity
**# Return to step 2
** One Max Problem:
*** The problem begins with individuals that contain a list of 100 values that are either zero or one
*** The goal is to eventually produce an individual that contains all ones

=== Lab Notes ===

* '''One Max Problem: To generate an individual with maximum 1s'''
** Create fitness and individual classes. We define the tuple (1.0,) as our weight because we are trying to maximize our objective (maximum sum).
** Using the DEAP's toolbox, define a function to initialize an individual by using random.randInt (to generate a boolean) 100 times. Using this function, define another function to initialize a population of n individuals.
** Implement an evaluation function which in this case is the sum of booleans in an individual
** Using the toolbox, define functions to mate (Two-point crossover), mutate (random independent chance of flipping each bit), and select (tournament of size 3)
** Starting with a randomly initialized population of 300, evaluate the population and assign fitness values. Then proceed into the evolution loop where selection, mating, and mutation are performed. Replace the older generation with offsprings and stay in the loop if max has not been found.
** For each generation, we study statistics that include Minimum, Maximum, Average, and Standard Deviation
** After running the code, I observed that the loop was terminated most times before reaching the 40th generation with a minimum between 87-89, an average between 97-99, and a standard deviation between 2.2-2.4.

* '''N Queens Problem: To generate positions for queens with minimum conflicts'''
** Create fitness and individual classes. We define the tuple (-1.0,) as our weight because we are trying to minimize our objective (minimum conflicts between queens).
** Using the DEAP's toolbox, define a function to initialize an individual by using random.sample (to generate a permutation of [0, n)). Using this function, define another function to initialize a population of n individuals.
** Implement an evaluation function that counts for each position, how many queens are in its diagonals. If the number of conflicts at a position is greater than 1, it is added to the overall count of conflicts. this overall count of conflicts is returned by the evaluation method.
** Define functions to mate (Partially-Matched crossover), mutate (shuffle attributes of input), and select (tournament of size 3)
** Starting with a randomly initialized population of 300, evaluate the population and assign fitness values. Then proceed into the evolution loop that runs for 100 generations where selection, mating, and mutation are performed.
** As done previously, for each generation, we study statistics that include Minimum, Maximum, Average, and Standard Deviation
** After running the code, I observed that the minimum was achieved around the 25th generation with a maximum between 10-11, an average between 1.5-2.5, and a standard deviation between 1.8-2.2.
[[files/NQueens_Fitness_vs_Generation.png|center|thumb]]

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Join Slack and Piazza
|Completed
|January 20, 2021
|January 27, 2021
|January 25, 2021
|-
|Install Jupyter Notebook
|Completed 
|January 20, 2021
|January 27, 2021
|January 25, 2021
|-
|Complete Lab 1 (Genetic Algorithms with DEAP)
|Completed 
|January 20, 2021
|January 27, 2021
|January 26, 2021
|-
|Set up notebook and update with lecture + lab notes
|Completed
|January 20, 2021
|January 27, 2021
|January 26, 2021
|-
|}