== Team Member ==
[[files/GeorgiaTechBuzz.jpg|thumb|123x123px]]

Team Member: Aditya Kumaran

Email: akumaran6@gatech.edu
Cell Phone: +1 4708380468

Interests: Writing fiction, Music, Reading, Sports

== September 22, 2021 (Week 5) == 

=== Lecture Notes: ===
*  Discussed last week's project
    *  Often lost pareto optimal solutions in favour of codominance
*  This week's task:
    *  Multiple objective genetic programming to find a set of pareto optimal algorithms for the same titanic data set
    *  Tree takes in the same inputs as ML model
    *  Simple primitives allowed (logical, mathematical)
    *  Strong or loosely typed GP allowed
    *  Need to write an algorithm – allowed to use selection, crossover, mutator, but no stealing algorithms. Need to write our own genetic loop.
        *  Don’t use selTournament for selection – it’s not multiple objective, only ever actually considers the first value in a tuple.
    *  Need to write evaluation function – false negative and false positive rate are the criteria.
    *  Compare pareto front of ML and GP frontiers
    *  Submit .csv with columns of passengerID, and one each of the pareto optimal algorithms’ prediction of an individual's survival
    *  Create a presentation to share our findings; compare ML and GP.
*  Presentation about presentation
    *  Group members on the first slide, title, date
    *  Pareto frontier lines are important
    *  Area under the curve
        *  Can always make a 1.0 false positive and 1.0 false negative
        *  Want to minimize area under curve
        *  GP often has punctuated equilibrium (takes a few generations to find a new pareto individual)
    *  Page numbers are helpful for audience to reference material
    *  Can write text in slides, but don’t read off of it


=== Individual Notes: ===


=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Weekly Notebook
|Complete
|September 22, 2021
|September 27, 2021
|September 25, 2021
|-
|Meet with assigned group to work on "Titanic - Machine Learning From Disaster" presentation
|Complete
|September 22, 2021
|September 27, 2021
|September 25, 2021
|}

== September 15, 2021 (Week 4) == 

=== Lecture Notes: ===
*  Split into temporary project groups
*  Introduced to Kaggle Competitions, Titanic project
*  Use Scikit for predictors and models. Learn about pandas, etc.
*  Results are score based on objectives: false positive, false negative.
*  Use files train.csv, test.csv, predictions.csv, and the python notebook to structure project.
*  Python Notebook:
    *  Pandas is like Excel/Google Sheets for python, can be used to read train.csv and test.csv
    *  Need to clean data sets (train and test the same way); use isna to find N/A values and replace them with averages/modes of their columns.
    *  Encode any strings to ints and replace them in their columns (more useful in ML).
    *  "Feature" = something that describes data. In this case, not the "survived" column.
    *  Allocate training data for training and testing: (not to be confused with test.csv data)
        *  x_train = top x rows of train.csv
        *  y_train = survived x rows of train.csv
        *  x_test = bottom (n - x) rows of train.csv
        *  y_test = survived (n - x) rows of train.csv
    *  Use Scikit score function to evaluate predictions
*  In groups, our algorithms must be codominant - to do so, the train and test partitions need to be the same. The random state parameter needs to be the same to ensure codominance - common method of preprocessing all around.
*  We have to individually submit our final predictions file with results codominant with our groups.
*  Use Scikit documentation to learn ML modules, models, etc.

=== Individual Notes: ===
*  Scheduled online meeting with subteam.
*  Communicated via discord, went through Python Notebook found at Reference Materials at https://github.gatech.edu/emade/reference_material/tree/master/Lectures/Lecture_4_ML
*  Brainstormed attributes that might factor into the survival of an individual on the Titanic, made a list at https://docs.google.com/document/d/1WVhgmRNwyJxAAaGPhp5YT6-aHzeGc_kS8ewx94U4Myw/edit
*  I shared my screen and communicated with group as we began editing the Titanic python notebook.
*  Chose that Pclass (affects availability of lifeboats), Sex (women and children boarded lifeboats first), Age (Elderly were given preference), SibSp (those with larger families would have wanted to stay together), Parch (those with dependents would have wanted to protect each other), and Embarked (Presumably the port you boarded from would affect which cabins were available, and therefore where you were when the ship sank) were influential to the model, dropped the other parameters.
*  Created a NaN_map to fill in missing Age and Embarked values. Created a column map to alias Embarked and Sex values to comparable integer values for the model to interpret.
*  Tried using first DecisionTreeClassifier and eventually RandomForestClassifier and changed their input parameters. Ex.- one iteration ran RandomForestClassifier(n_estimators = 100, max_depth = 5, min_samples_leaf = 5, criterion = entropy, random_state = 2). These were tried and adjusted through trial and error, using the SciKit documentation to learn which parameters each constructor takes in. NOTE: RandomForestClassifier requires importing 'ensemble' from sklearn.
*  Further adjusted values to try and optimize the preliminary prediction scores. Printed confusion matrices for each iteration and saved 5 (one for each group member) trials as predictions.csv. NOTE: Although adjusting parameters did change the accuracy scores slightly, the change wasn't monumental enough to affect the confusion matrices' objectives throughout the experiment. We can conclude that the algorithms are all codependent.
*  Created a GitHub repository for shared files (Python notebook, generated csv files) and shared with group members: https://github.gatech.edu/akumaran6/Titanic-Problem
*  Pareto Optimal Frontier using varying classifiers:
    *  Aditya = AdaBoostClassifier. FP = 32, FN = 21.
    *  Rohan = DecisionTreeClassifier (min_samples_leaf=30). FP = 9, FN = 45. 
    *  Manas = RandomForestClassifier (parameters above). FP = 18, FN = 29. 
    *  Adithya =  MLP. FP = 26, FN = 26.
    *  Yisu = SVM (used svm.SVC, sigmoid kernel). FP = 0, FN = 104. 

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Weekly Notebook
|Complete
|September 15, 2021
|September 22, 2021
|September 19, 2021
|-
|Meet with assigned group to work on "Titanic - Machine Learning From Disaster" ML project
|Complete
|September 15, 2021
|September 22, 2021
|September 19, 2021
|}

== Self-Assessment (September 11, 2021) ==

https://drive.google.com/file/d/1ugXxwFcjq7Q7bHHLOAo8wkhAZRtk9Bai/view?usp=sharing

*  Student Name: Aditya Kumaran
*  VIP Team: N/A
*  Semester: 1

*  Notebook Maintenance:
    *  Name and contact info: 5
    *  Teammate names and contact are easy to find: N/A
    *  Organization: 5
    *  Updated at least weekly: 5
*  Meeting Notes:
    *  Main meeting notes: 5
    *  Sub-teams' efforts: N/A
*  Personal Work and Accomplishments:
    *  To-do items: clarity, easy to find: 5
    *  To-do list consistency (weekly or more): 5
    *  To-dos and cancellations are checked and dated: 5
    *  Level of detail: personal work and accomplishments: 13
*  Useful Resource:
    *  References: 9
    *  Useful resource for the team: 15

*  Column totals: Poor = 0, Intermediate = 0, Exemplary = 97
*  Total out of 100: 97

== September 8, 2021 (Week 3) == 

=== Lecture Notes: ===
*  Multiple Objective Optimization:
    *  Multiple metrics together, like finding a mate in nature
        *  Algorithms look for speed, reliability, memory usage, consistency, accuracy
        *  Genome = description of an algorithm
            *  DNA
            *  GA = set of values
            *  GP = input for functions (tree structure, string)
            *  Drive selection by favouring pareto optimal indivudals (But also want to maintain diverstity by fiving all individuals some possibility of mating)
        *  Nondominated sorting genetic algorithm II (NSGA II)
            *  If we remove the pareto frontier, then the next set of points becomes the next rank, and then loop doing that.
            *  Lower ranked individuals beat higher ranked individuals
            *  Ties are broken by “crowding distance” (summation of normalized Euclidian distances to all other points within the front)
                *  Higher crowding distance wins
        *  Strength pareto evoluationary algorithm 2 (SPEA2)  
            *  Each individual has strength S (number of others in the population it dominates)
            *  Each individual has rank R (sum of the S’s of individuals that dominate it)
            *  Paretos are nondominated and have rank 0
            *  Fitness = R + 1/(dk + 2), where dk is the distance to the kth nearest neighbour

=== Individual Notes: ===
*  Continued Lab 2, starting section "Multi-Objective Genetic Programming"
*  Created new fitness and individual classes for multi-objective problem - aim to minimize objectives 'mean squared error' and 'tree size'.
*  Added primitives sin, cos, tan (all arity=1) and reinitialized toolbox with functions from last week.
*  Added an objective to evaluation function, by compounding primitive functions with 'points' in the mean squared term.
*  Defined pareto_dominance() function to visualize objective space. 
*  Initialized population of 300 with an additional individual for comparison.
*  Sorted population into dominated and dominators with respect to the comparison individual. Plotted sorted population in objective space.
*  Ran main evolutionary algorithm.
    *  Best individual is: negative(cos(multiply(add(cos(sin(cos(sin(cos(tan(x)))))), cos(x)), tan(x)))) with fitness: (0.2786133308027132, 15.0)
    *  Area Under Curve: 2.3841416372199005
*  The goal now is to reduce the area under the graph by 25%
    *  I first tried to add the mutate function with the altMutate function (mutInsert) by using both one after the other.
    *  Best individual is: subtract(multiply(x, sin(subtract(add(x, x), cos(x)))), cos(x)) with fitness: (0.1636098302024022, 12.0)
    *  Area Under Curve: 3.619531087140837 (increased)
    *  Then I tried completely replacing the mutate function with altMutate
    *  Best individual is: tan(multiply(subtract(subtract(cos(sin(x)), multiply(x, multiply(subtract(cos(multiply(x, multiply(x, x))), multiply(x, x)), negative(cos(multiply(subtract(cos(x), multiply(x, x)), negative(cos(negative(cos(add(sin(negative(x)), cos(multiply(x, x))))))))))))), multiply(x, x)), negative(cos(negative(cos(cos(multiply(subtract(cos(x), multiply(x, x)), negative(cos(negative(cos(add(x, x))))))))))))) with fitness: (0.0422300662166498, 63.0)
    *  Area Under Curve: 12.279788182341475 (increased significantly)
    *  It seems that complicating the evaluation leads to a greater area, so I'll try to make the calculations simpler.
    *  I tried changing the variables in the evolutionary algorithm: MU=100, LAMBDA=120.
    *  Best individual is: subtract(sin(multiply(x, subtract(sin(x), cos(x)))), cos(x)) with fitness: (0.24471230030517566, 11.0)
    *  Area Under Curve: 2.7332976348871716 (increased, but closer to the original)
    *  Finally, I removed the np.tan(points**3) from the calculation of the sqerrors, increased MU to 150, and decreased LAMBDA to 50.
    *  Best individual is: subtract(multiply(x, x), cos(x)) with fitness: (0.5796581408872937, 6.0)
    *  Area Under Curve: 1.796355292509189
    *  Based on the best individuals above, it seems my estimation was correct, and simpler functions lead to lower 'area under curve' values.

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Self-Grade Rubric
|Complete
|September 8, 2021
|September 15, 2021
|September 12, 2021
|-
|Update Weekly Notebook
|Complete
|September 8, 2021
|September 15, 2021
|September 12, 2021
|-
|Finish "Lab 2 - Genetic Programming and Multi-Objective Optimization.ipynb" with JupyterLab
|Complete
|September 8, 2021
|September 15, 2021
|September 12, 2021
|}

== September 1, 2021 (Week 2) == 

=== Lecture Notes: ===
* Learned how to add images to notebook via 'git clone'
* Introduced to genetic algorithms:
    *  Individual is now the function itself
    *  Practices running data through the individual instead of evaluating the individual AS the data.
        *   Ex.- 0,1,2,3,4,5 -> Individual -> 0,1,4,9,16,15 -> Evaluate
 	*   Individual function here is squaring
 	*   Evaluator would match output data to truth data for accuracy
    *  Tree representation 
        *   Represents a program.
        *   Made up of nodes (primitives, functions) and leaves (terminals, end of the tree. Parameters or input data)
        *   Read bottom to top node
        *   Stored as a ‘lisp treeordered parse tree’
        *   [+,*,3,4,1]
             *  First is root
             *  Next two are '*' and 1
             *  '*' has two inputs, and they come before 1
    *  Crossover
        *  Single-point crossover is just exchanging subtrees
             *  Starts by randomly selecting a point in the tree
             *  Subtrees are swapped to produce children
    *  Mutation
        *  Inserting a node or subtree
        *  Deleting a node or subtree
        *  Changing a node


=== Individual Notes: ===
*  Imported libraries from deap required for genetic programming (algorithms, base, creator, tools, gp)
*  Created fitness and individual classes, which will be represented as a tree structure made of primitives. Evaluation compiles the primitive tree from leaves to root node.
*  Initialized primitive set and added primitives like mathematical operators (add, subtract, multiply, negative). Added custom primitives np.deg2rad(arity=1) and np.ceil(arity=1). Using functions with arity=2 often lead to errors, but I think these have to do with the functions' domains - this is most consistent when choosing functions with a domain of all reals that output reals as well.
*  Registered four tool functions for expr (returns a tree based on a primitive set and maximum and minimum depth), individual, population, and compile (makes the tree into a function).
*  Defined evaluation function, comparing the compiled function with the function we're trying to generate by minimizing mean squared error.
*  Registered genetic operators for evaluate, select (tournament select, 3 per pod), mate (one point crossover), expr_mut, mutate. Added alternate mutation method, gp.mutInsert (inserts branch at a random position in individual).
*  Performed the customary evolutionary loop and outputted the same statistics.
    *  Achieved 1.16e-16 minimum fitness in best individual: add(x, multiply(x, add(multiply(x, x), add(x, multiply(x, multiply(x, x))))))
*  Tried adding images to notebook; git push failed. Going to try troubleshooting in lesson.

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Weekly Notebook
|Completed
|September 1, 2021
|September 8, 2021
|September 2, 2021
|-
|Begin "Lab 2 - Genetic Programming and Multi-Objective Optimization.ipynb" with JupyterLab
|Completed
|September 1, 2021
|September 8, 2021
|September 1, 2021
|}


== August 25, 2021 (Week 1) == 

=== Lecture Notes: ===
* Learned about project requirements: Anaconda, JupyterLab, Python deap
* Introduced to EMade's GitHub, Wiki, Personal Progress Notebooks, Python Jupyter Notebooks, Slack
* Introduced to Genetic Algorithms:
    *  Allegories for how DNA works.
    *  Population-based solution.
    *  Many solutions to your problem, each being a “genome” and can mutate either individually or in groups 
* "Individual"
    *  Specific candidate in the population
    *  Like one person's DNA
* "Population"
    *  Group of individuals that will be altered
* "Objective"
    *  Performance measure of individual
    *  Is an objective measurement, not relative
* "Fitness"
    *  Relative measure of performance
* "Evaluation"
    *  Computes the objective of an individual
* "Selection"
    *  Gives preference to better individuals to pass on their genes
    *  Can be:
        *  Fitness Proportionate = fitness value is proportional to the probability of being selected for mating
        *  Tournament = from a group of individuals, the winner (best individual) is selected for mating. More random than fitness proportionate.
* "Mating"/"Crossover"
    *  Represents mating between individuals, in that DNA is taken from both places
    *  Can be a single point splice, double point splice, etc.
* "Mutation"
    *  Making a small change to an individual (changing a person's DNA)
    *  Purpose is to maintain diversity
* Algorithms
    *  Initialize population
    *  Evaluate population to get objective, fitness
    *  Loop through:
        1. Select parents
        2. Mating actions
        3. Mutations
        4. Determine fitness
        5. (until the best individual is acceptable)

=== Individual Notes: ===
* Downloaded and installed Anaconda Individual Version for Windows 10
* Using Anaconda Navigator, I launched JupyterLab
* Retrieved the DEAP Problem from the Calendar, under the Assignments column for the first week. Saved the file as .ipynb
* Imported the .ipynb into JupyterLab via 'New -> Text File'
* Opened a new Terminal window in JupyterLab, and used 'pip install deap' to install deap

''' OneMax Problem '''
* Using toolbox.register and tools.initRepeat, we'll create an individual with a list of 100 booleans (either 0 or 1).
* Writing the evalOneMax() function to evaluate the total fitness of an individual, we sum all of the 100 bits an individual carries.
* Defined four tool functions for evaluation (evalOneMax()), mating (2 point crossover), mutation (independent probability of bit flipping = 5%), and selection (tournament style, 3 per pod).
* Initialized population of 300, mapped the evaluation function to the population using: map(toolbox.evaluate, pop). Assigned individuals their fitness values as properties.
* Defined an evolutionary loop (40 generations), and performed tournament selection on the population, cloning the selected offspring to create separate instances from the previous iteration.
* Matched the even terms with their adjacent odd terms and called toolbox.mate() with 50% probability. Deleted the mated offspring's fitness values.
* Mutated individuals with 20% probabilities, deleted the mutated offspring's fitness values. 
* Re-evaluated the modified offspring and assigned their newly evaluated fitness values. Replaced old population with offspring.
* Calculated max, min, mean, and standard deviation statistics for new population.
* Ran main():
    * Achieved 100% maximum fitness in 31 generations.
    * Achieved 100% maximum fitness in 39 generations.
    * Achieved 100% maximum fitness in over 40 generations (99% maximum in 40 generations).
    * Achieved 100% maximum fitness in 34 generations.
    * Achieved 100% maximum fitness in 39 generations.

''' N Queens Problem '''
* Created fitness and individual classes for an nxn chessboard (sample is 20x20). Fitness is weighted negatively, since the goal is to minimize conflicts between the queens on the board.
* Created individuals using toolbox_q.permutation (returns randomized list of numbers less than n, representing the queens' columns), since there is only one queen per column.
* Count the number of queens on each diagonal for evalNQueens(individual), and sum the total number of conflicts on both left and right diagonals.
* Writing the partially matched crossover function for two individuals. Chose two random crossover points, and swapped the individuals' bits between those indices.
* Wrote the mutation function for individuals with a given probability of each attribute to be swapped with another random index (indpb).
* Implemented custom mutation function, swapping an additional term that's halfway between the index term and the randomly selected term.
    * def mutationCustom(individual, indpb):
    *     size = len(individual)
    *     for i in range(size):
    *         if random.random() < indpb:
    *             far_index = random.randint(0, size - 2)
    *             if far_index >= i:
    *                 far_index += 1
    *             middle_index = (i + far_index) / 2
    *             individual[i], individual[middle_index], individual[far_index] = \
    *                 individual[middle_index], individual[far_index], individual[i]
    * 
    *     return (individual, )
* Registered four tool functions for evaluation (evalNQueens()), mating (partially matched), mutation (independent probability of bit flipping = 2/n), and selection (tournament style, 3 per pod).
* Performed the same evolutionary loop as in OneMax and outputted the same statistics.
* Ran main() with mutShuffleIndexes():
    * Achieved 0 minimum fitness in 32 generations.
    * Achieved 0 minimum fitness in more than 100 generations (1.0 minimum in 100 generations).
    * Achieved 0 minimum fitness in more than 100 generations (1.0 minimum in 100 generations).
    * Achieved 0 minimum fitness in more than 100 generations (1.0 minimum in 100 generations).
    * Achieved 0 minimum fitness in 81 generations.
* Ran main() with mutationCustom():
    * Achieved 0 minimum fitness in more than 100 generations (1.0 minimum in 100 generations).
    * Achieved 0 minimum fitness in more than 100 generations (1.0 minimum in 100 generations).
    * Achieved 0 minimum fitness in 15 generations.
    * Achieved 0 minimum fitness in more than 100 generations (1.0 minimum in 100 generations).
    * Achieved 0 minimum fitness in 74 generations.
* Learned that my custom mutation function often gets to a minimum of 1.0 quickly, but routinely fails to reach a minimum of 0.


=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up personal notebook page
|Completed
|August 25, 2021
|September 1, 2021
|August 25, 2021
|-
|Join Slack
|Completed
|August 25, 2021
|September 1, 2021
|August 25, 2021
|-
|Complete "Lab 1 - Genetic Algorithms with DEAP.ipynb" with JupyterLab
|Completed
|August 25, 2021
|September 1, 2021
|August 29, 2021
|}
