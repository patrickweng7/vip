== Team Member ==
Name: Angela Young

Email: ayoung97@gatech.edu

Interests: UI design, machine learning, dance, baking

= Fall 2021 =
* [[Modularity|Modularity Subteam]]
Members:
*[[Notebook Bernadette Gabrielle Santiago Bal|Bernadette Gabrielle Santiago Bal]]
*[[Notebook Tian Sun|Tian Sun]]
*[[Notebook Vincent H Huang|Vincent H Huang]]
*[[Notebook Xufei Liu|Xufei Liu]]
== Week 1: August 23, 2021 ==

=== Main Team Meeting ===
* Meeting in Klaus 1440
* In person meetings mainly!
* Sub Team formation, reshuffling, and creation
* Important Dates: Oct 25 (Midterm Presentations), Dec 10 (Final Presentations)
* Rank sub teams assignments

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Fill out Subteam rankings
|Complete
|August 23, 2021
|August 30, 2021
|August 30, 2021
|}

== Week 2: August 30, 2021 ==
=== Main Team Meeting ===
* Virtual meeting today
* My laptop is very close to dying, rip
* Discussed what ideas each subteam had for the semester

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Meet with subteam to discuss semester plan
|Complete
|August 30, 2021
|September 13, 2021
|September 8, 2021
|}

== Week 3: September 8, 2021 ==
No main team meeting on Monday due to Labor Day

=== Sub Team Meeting ===
* Discussed what we wanted to do for this semester, especially when there's so few of us
* Ideas: Follow through on last semesters projects, Start looking into stock data
* Need to look into database and repo issues now that Gabe is gone :(
* Need to set up subteam meeting
* Discuss potentially all meeting with Dr. Zutty on Fridays (Since half the team is in the time-conflist meeting)
** Dr Zutty said no :(

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Fill out lettucemeet for subteam meeting
|Complete
|September 8, 2021
|September 10, 2021
|September 9, 2021
|}

== Week 4: September 13, 2021 ==

=== Main Team Meeting ===
* PACE-ICE should be active for everyone now (need to be on campus or using the VPN)
** New guide made on EMADE wiki
* Self evaluations due today
** Just put score and what you want to improve
** Rubric is on notebooks page
* Anish and Gabe will be serving as mentors especially for the subteams they were on
* Door to Klaus 1440 needs buzzcard access now!

NLP
* Had subteam meeting last week on Wednesday
* Goal for semester is question answering system problems
* Outlined steps to take to reach their goal
* All team members should have EMADE set up on their computers now
* What does success look like?
** Getting datasets to work on EMADE, and be able to distill q/a models

NAS
* Had a meeting on Friday
* Discussed the general idea of the subteam and gave background to team members
* Rule out trivial solutions produced
* Plan is to check on everyone's EMADE situation and make sure runs can be completed
* Assigning tasks that can be done now to team members

Stocks
* Did portfolio optimizations last semester, now want to optimize results with a new paper and Monte-Carlo techniques
* Make sure everyone on the team is set up on PACE-ICE
* Have a literature review to see what they want to do
* Look for things that can be done with ML and can be reproducible

Modularity
* Will be extending last semester's work
* Split up into two sub sub teams
* Goals were to review the code and identify what needed to be changed
* Also continuing runs on MNIST
** Potentially collaborating with NAS to solve problems
* Need to work with Gabe to get repo fork set up (Bernadette is working on this)

Image Processing
* Met last week and started setting up goals for this new subteam
* Did a dive into the literature to see what's already out there
* Find something that can be added to EMADE
* Familiarize team with what's already in EMADE and see if certain algorithms are in there or not
* Guide to CV Primitives in EMADE wiki
* A lot of image processing uses open cv

=== Sub Team Meeting ===
* Reviewed what the team discussed on Friday
* Zutty discussed making a paper outline for Gecco conference, could be a good project even if the results aren't publishable
* Bernadette has been working on AWS remote database with Gabe
** Makes it easier to have everyone connect to a seperate server and don't have to deal with GT firewalls
* Doing visualizations? Will need to learn how to make those closer to midterm presentations
** Not too bad, just need data in CSV format
* Can reach out to Gabe to fix bugs...
* Potential tasks: clean up code and documentation
* Need to met up with sub sub teams to coordinate tasks on Thursday

=== Notebook Self-Evaluation ===
* Notebook Maintenance: 25/25
* Meeting Notes: 15/15
* Personal Work and Accomplishments: 30/35
* Useful Resource: 25/25
* Total: 95/100
* Comments: I currently haven't done much individual work since we are still trying to get the Github repo and server set up.

=== Sub Team Meeting: September 16 ===
* Meeting in CoC lobby today!
* 

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Notebook self evaluations 
|Complete
|September 13, 2021
|September 13, 2021
|September 13, 2021
|}

== Week 4: September 20, 2021 ==

=== Main Team Meeting ===
Image Processing
* Began tasking themselves, found a great paper to reference that used neural nets
** Can use selection methods and potentially add those to EMADE
** How to make sure primitives stay together during evolution and don't get split up
* Summary of team: Improving EMADE for image classification
* Hoping to start coding next week. Don't want to be creating neural nets, synced up with Anish about that
* Hyper-features are difficult to find papers on, can really only find papers on hyper-parameters

Stocks
* Finished secondary literature review
* Found new things to try and play around with. Fundamental analysis was decided against because there wasn't enough data to work off of.
* Found a paper using genetic algorithms that could generalize the individuals to work on any stocks
** They take individuals and take "rules" that can be applied forwards. Used one technical indicator for this

Modularity
* Got started on extending ARL runs
** Construct vs evalute an individual
** Check that it's regestered in the primitive set pset
** Goal shifted from extending depth to seeing how depth affects individuals performance
* Tried to do MNIST run on Google colab, but we agreed to start moving towards using PACE-ICE since it was difficult to get everyone properly set up on Google

NLP
* Decided on objective functions officially
** F1 score - more lenient
** Number of parameters - want to reduce the complexity
* Haven't seen smooth parato fronts with those objective functions

NAS
* Went over tasks based upon the ideas generated for the semester

=== Individual Exploration: September 20 ===
* Got started setting up potentially doing a PACE-ICE run
* Used new PACE-ICE guide for reference ([[Guide to Using PACE ICE#Configuring_EMADE_on_PACE_New_Guide | link]])
 ssh ayoung97@pace-ice.pace.gatech.edu 
 and then enter GT password next
** Need to be on campus or using VPN
* Set up SSH Key with command: 
 cat ~/.ssh/id_rsa.pub
* ...cry

=== Sub Team Meeting: September 23 ===
* Got started on working on PACE-ICE together
* Tried to get the SQL database working but kept running into issues

=== Individual Exploration: September 23 ===
* Continued following ([[Guide to Using PACE ICE#Setting_Up_MySQL | Guide]])
* Unsure if .my.cnf file was configured correctly
** Try using vim to create file
 vim .my.cnf
** Paste in contents then change USERNAME to ayoung97
** To save and quit: :wq
* Also got error when attempting to install the database
** "Can't create test file"

=== Individual Exploration: September 27 ===
* Actually used vim to create the .my.cnf file, also deleted 'scratch-old' directory in PACE-ICE
* Local MySQL instance command: 
 mysqld_safe --datadir='/storage/home/hpaceice1/ayoung97/scratch/db'
* Tested to see if the database is working


=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Set up MySQL in PACE-ICE
|Complete
|September 20, 2021
|September 27, 2021
|September 27, 2021
|}

== Week 5: September 27, 2021 ==

=== Main Team Meeting ===

NAS
* Got updated on what everyone was working on
* Cameron figured out how to replicate a bug found earlier
* Plan to merge a CIFAR file into the repo
* Didn't realize individuals needed to be seeded
* Introduced the idea of a global "supernet" which is a graph that stores information about submodels and previous generations
** Similar to the concepts EZCGP works on

Image Processing
* Looking at selection methods, compare NSGA-I and NSGA-II
* Looking at pre-processing data
* Also looking at pre-packaging
* Figured out blockers on PACE-ICE

Stocks
* Completed literature reviews
** Still trying to decided what paper to go forward with
** Lots of time series analysis
* Looked more into general knowledge about stock trading and algorithmic stock trading

Modularity
* Got started on using PACE-ICE
* Have managed to get MySQL set up, will make sure everyone on the team is set up at the next meeting
* Need to finish adding new databases to the repo before uploading that to PACE-ICE and starting runs

NLP
* Designed the primitives and infrastructure for getting set up
* Came up with solutions for issues they ran into
** Have two inputs that need to be handled
** Need to improve the prediction method(?)

=== Individual Exploration: September 28 ===
* Type "exit" to quit out of PACE-ICE
* Need to check with team about database status
* Should set up a baseline run first maybe
* Got started on getting a conda environment set up in PACE-ICE and installing all packages needed for EMADE runs
** conda activate emade
* Figure out how to get EMADE on PACE-ICE!!!!

=== Sub Team Meeting: September 30 ===
* Continued trying to get everyone set up on PACE-ICE
* Everyone had issues getting connected to their own node
* Personally got a "lost connection" error one time instead of a "connection not found" error


=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Get EMADE transferred to PACE-ICE
|Incomplete, moved to next week
|September 27, 2021
|October 4, 2021
|/
|}

== Week 6: October 4, 2021 ==

=== Main Team Meeting ===

* Peer evals due Friday at 4pm!
* Notebooks are also due Friday in the evening

Image Processing
* Continued on implementing selection methods and mating/mutation methods
** Solved seeding files not working error, arg 0 in input layer that wasn't supposed to be there

Modularity
* "add all subtrees" bug, trees are too big for python to efficiently compute
** Refactor architecture, or only look at subtrees with emade datapairs
** Have code reviews and unit testing to prevent these bugs
* Still don't have PACE-ICE up, might switch back to Google Colabs
** Some of the benefits we thought we'd have aren't actually true
* Stocks team is breaking up, some members might join us now that we're tackling some stocks data

NLP
* Started on infrastructure changes in the code
* Have NN learner take in a second datapair

NAS
* Gave presentation to new members on EMADE and the mating/mutation methods that would be relevant to the team

=== Sub Team Meeting: October 7 ===
* Met virtually today
* Bernadette got PACE-ICE to connect to the MySQL server!
* Attempted to replicate her steps but was unsuccessful
* Xufei discussed reading a paper and potentially doing some paper writing of our own
* New team member! Diptendu is from Stocks and is interested to see how our runs will go


=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Get EMADE transferred to PACE-ICE
|canceled since not using PACE-ICE
|September 27, 2021
|/
|/
|-
|Complete peer-evals
|Complete
|October 4, 2021
|October 8, 2021
|October 8, 2021
|}

== Week 7: October 11, 2021 ==

=== Main Team Meeting ===

None due to Fall Break

=== Sub Team Meeting: October 14 ===
* Met virtually today
* Xufei and Bernadette seemed to get the PACE-ICE server running
* I realized I should clone Vincent's fork of EMADE and got started on that

=== Individual Exploration: October 14 ===
* Deleted the old copy of EMADE that was Gabe's fork
* Had to refer to my own notebook to see what I did to reinstall EMADE last semester, and the steps were somewhat out of order so I'll condense the commands to run here
 git lfs install
 git clone https://github.gatech.edu/vhuang31/emade.git
 *Open a new terminal window*
 cd emade
 conda create -n emade python=3.7  --- deleted old conda enviroment to replace
 conda activate emade
 conda install opencv
 conda install numpy pandas tensorflow keras scipy psutil lxml matplotlib PyWavelets sqlalchemy networkx cython scikit-image mysqlclient pymysql scikit-learn 
 pip install xgboost lmfit multiprocess hmmlearn deap opencv-python
* Ran into an issue installing hmmlearn. Also needed to go eat so I decided to work on it another day

=== Individual Exploration: October 16 ===
* Continued trying to get EMADE set up
* Saw a command for if hmmlearn fails to install on Windows but I use a Mac so I didn't think it would work
* Tested it, and it did work! So the next steps for installing emade for me includes making sure hmmlearn is built
 conda install -c conda-forge hmmlearn
* Now to see if all the files build correctly
 bash reinstall.sh
* Got tensorflow error just like last semester 😡 
* Running the "which" commands that seemed to work last semester... 
 which python3
 which python
 which pip
* tensorflow uses Python 3.7 while my conda env seems to be running Python 3.6
** Not sure how to fix this error 😭 
* Going to start over but use this command to create a conda env with the right version of Python
 conda create -n emade python=3.7
* FIX: edit the reinstall file
 vim reinstall.sh
* Change `python3` to `python` since running which python3 makes it use the wrong version
** Running which python makes it use the anaconda version which is the version we want to use
* Next is to get emade copied into PACE-ICE!
* Make a copy of emade but without the .git directories (use cmd + shift + . to see hidden files) I called this `emade1`
 cd ~
 scp -r emade1 ayoung97@pace-ice.pace.gatech.edu:~
* Change the directory name in PACE-ICE
** 


=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Reclone EMADE
|Complete
|October 14, 2021
|October 18, 2021
|October 16, 2021
|-
|Resolve hmmlearn error
|Completed
|October 14, 2021
|October 18, 2021
|October 16, 2021
|-
|Resolve tensorflow error
|Completed
|October 16, 2021
|October 18, 2021
|October 16, 2021
|}

== Week 7: October 18, 2021 ==

=== Main Team Meeting ===

None due to Fall Break

=== Sub Team Meeting: October 21 ===
* Got set up again on Google Colab to try to complete some runs on the Stocks dataset
* Unfortunately I kept running into various bugs that prevented me from even starting one

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Finish working on mid-term presentation slides
|Complete
|October 21, 2021
|October 25, 2021
|/
|}

== Week 8: October 25, 2021 ==

=== Main Team Meeting: Mid-Term Presentations ===

* In the Ford building atrium today!

NLP
* Natural language processing: Question answering (information retrieval)
* Used SQUAD Dataset from Stanford that has 100K answerable and 50K unanswerable questions
* Also went over 2 separate papers for their projects!

Bootcamp 1 
* Ran some preprocessing
* Emphasized how they did learn from their mistakes and completed their models
** Accidentally deleted the survived column

NAS
* Was hard to get new individuals that outperformed the seeds
* Most individuals in EMADE aren't neural networks
* Short term goal is to use EMADE to create more complex architecture

Bootcamp 2
* Ran into a lot of issues when trying to run EMADE
* Discussed how they would've liked to have known what the typical number of individuals to use was

Image Processing
* New team this semester
* Conducted experiments based on an existing paper and dataset
* Also worked to find selection methods as well as mating and mutation methods

Bootcamp 3
* Added a third objective of tree size to their evaluation function
* Found that runs were completed faster on a Macbook with an M1 chip

Stocks
* Use EMADE on time series data
* Objective: want to be better than randomly buying and selling
* Chose a new paper to work with this semester 
** Couldn't replicate last semester's paper
** Having trouble replicating this semester's paper too

Bootcamp 4
* 

Modularity
* That's us!
* [https://docs.google.com/presentation/d/1Lus6qHH9vwdfaLxcBg50PBBOl56qF_A7wFGT4F-1hlI/edit Slides]

=== Sub Team Meeting: October 28 ===
* No meeting due to mid-term presentations
* Split up tasks to work on for the rest of the semester
** Will be assisting Vincent with ARL depth runs

== Week 9: November 1, 2021 ==

=== Personal Exploration: November 1 ===
* Learned how to make code blocks in "MediaWiki" -_-
* Just start a new line with a space
 Like this line

=== Main Team Meeting ===
*

=== Sub Team Meeting: October 28 ===
*

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|task
|Inomplete
|November 1, 2021
|November , 2021
|/
|}

= Spring 2021 =
[https://bluejeans.com/904633358 Main Team Meeting Link]
* [[Modularity|Modularity Subteam]]
** [https://bluejeans.com/616992142 Meeting Link]
Members:
*[[Notebook Gabriel Qi Wang|Gabriel Wang]]
*[[Notebook Kevin Lin Lu|Kevin Lin Lu]]
*[[Notebook Regina Ivanna Gomez Quiroz|Regina Ivanna Gomez Quiroz]]
*[[Notebook Bernadette Gabrielle Santiago Bal|Bernadette Gabrielle Santiago Bal]]
*[[Notebook Vincent H Huang|Vincent H Huang]]
*[[Notebook Xufei Liu|Xufei Liu]]

== Week 1: January 25, 2021 ==

=== Main Team Meeting ===
* Important Dates to Note
** March 22 - extended lecture time (5-8pm)
** April 30 - Final Exam Time (6-8:50pm)
* Things will run similarly to last semester (all virtual)
* Continued with team selection to get an idea of numbers, also to see what teams would be formed this semester
** Personally switched from EzCGP to Modularity

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Fill out lettucemeet
|Complete 
|January 25, 2021
|January 27, 2021
|January 25, 2021
|}

== Week 1: January 31, 2021 ==

=== Team Meeting ===
* Discussed general plans for this semester
** Will review literature for first few weeks
** Also look at previous research done by team to see if anything looks interesting
** Things to research: ARLs, ADF
* Went over each team members' comfort level with EMADE
** Personally not that comfortable since I haven't used it too much (another thing to research?)
* Gabriel gave some places to start looking 
** GECCO - conference on genetic and evolutionary computation
** Can also set if there are any problems to see how to solve, datasets to look into, etc
* Gabriel also mentioned making videos to explain concepts

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Get sources for research
|Complete 
|January 31, 2021
|February 7, 2021
|January 31, 2021
|}

== Week 2: February 1, 2021 ==

=== Main Team Meeting ===
* Weekly team reports wiki page needed to be made
* Stocks:
** Plan to do more EMADE coding
** Play with selection methods and evolutionary processes
** Figure out more long-term goals
* EzCGP:
** Discussed removing transfer learning step
** Want to look into creating new primitives
* NLP:
** Did not discuss future goals yet, stayed after to discuss 
* Modularity:
** Looking into diversity and ARL expansion (architecture work)
** Need to catch up new members and look into literature to get more long term goal

=== Subteam Meeting ===
* Review literature for this week, hopefully find something interesting
** Bring in at least one article to discuss
* Gabriel will record presentations on previous work done to catch up the new members 

=== Personal Exploration - Feb 6 ===
* Watched Gabriel's video introducing modularity ([https://docs.google.com/presentation/d/1yrkD411TYEVQ8OMiqODLsoXdDiVo43PZBZpW9-1TNlo/edit#slide=id.p slides])
* Aiming to optimize EMADE's process by creating blocks of reusable code
** Hopefully the next step beyond machine learning models that actually exist
* ARLs
** When a useful tree is found, "lock" it to prevent changes
** Allow any individual to call a subtree from a pool of useful ones
* Note: some ARLs are incorrectly called ADF in the Modularity fork, but they are actually ARLs

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Find some literature to based a project off of
|Complete 
|February 1, 2021
|February 7, 2021
|February 7, 2021
|-
|Watch the new member presentation
|Complete
|February 1, 2021
|February 7, 2021
|February 6, 2021
|}

== Week 2: February 7, 2021 ==

=== Team Meeting ===
* Discussed papers each team member found
* Summary of a few (all are on the [https://vip.gatech.edu/wiki/index.php/Spring_2021_Sub-team_Weekly_Reports Subteam Weekly Reports] page)
** Review of modularity techniques in neural networks
** Optimizing mating/mutation methods for diversity
** Various ways of measuring diversity in individuals
** An overview of modularity
* Topics of papers seemed to fall into two categories: ARL architecture vs diversity measures
** Everyone was leaning towards ARL architecture, so we are not splitting into two teams and will instead look into potentially making subtopics to focus on
* Need to establish more of a foundation and find more literature explaining the subject

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Add article to Weekly Reports page
|Complete
|February 7, 2021
|February 8, 2021
|February 7, 2021
|}

== Week 3: February 8, 2021 ==

=== Main Team Meeting ===
* Market Analysis
** further discussed semester goals, and aligned it with research papers
** looked for something like last semester to base research off of
** see how EMADE can be used with the paper
** test on more stocks than just what is listed on the paper
** next week:
*** have a paper to work on
*** have ideas on changes needed to make on EMADE
*** similar process to last semester overall though
** essentially goal is to replicate paper's results in EMADE and optimize the results to hopefully outperform the original results

* EzCGP
** researched some papers on transformers and hyperparameters
** also updated a problem file and removed references to transfer learning
** tested minGPT and trained on Google COLAB
* Modularity
* literature review, everyone brought a paper and discussed it
** reading each others papers and asking each other questions on it
* lots of ideas on topics, but everyone seemed to want to focus on modularity
* next week
** outside of literature, want to get more architecture focused
** probably won't have anything concrete to show yet
** document changes made in the past as a way to have evidence
** write up pieces of the code using spanks? sphinx?
*** Go to for writing up/documenting code
* NLP
** Focused on what the goals for this term are
** Purely NLP, address major issue from last semester
*** EMADE's neural architecture search would return only trivial solutions
** dividing up the problem into multiple sub-problems

=== Subteam Meeting ===
* The lecture on EMADE's source code has not be completed
* Not sure what sphinx? is, will look into it
* peer coding to looking to the literature 
* link to literature aggregation not complete yet, will be on the main vip subteam page
** references to literature is currently empty
* Dr. Zutty joined the call to talk about Sphinx
** sphinx is a tool for documentation
** automatically pull from code document and will format it in a nice website
** uses restructured text to create html files from the source code
** relies on documentation format
** can click link in html file to directly go to that section in the source code
** still a work in progress but should be a good starting point for the rest of the semester
* peer programming sessions might be thursday/friday, we'll see

=== Team Meeting - Feb 14 ===
going over an in depth explanation of EMADE's source code as well as discussing what future architectures for ARLs should look like

Meeting recording: https://bluejeans.com/s/MkVzKyIhA0q/

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Add summary of article to weekly report page
|Complete
|February 8, 2021
| -
|February 15, 2021
|-
|
|
|
|
|
|}

== Week 4: February 15, 2021 ==

=== Main Team Meeting ===
* Stocks
** Goal was to find a research paper for the rest of the semester
** Found a good paper to use that is similar enough to what they did last semester
** Hoping to broaden usage to more generic datasets
** Hope to have piecewise linear representation set up to create labels
* EzCGP
** Got PACE-ICE set up and did some runs but they would fail maybe due to memory issues
** Replaced normalization primitive with equalize 
** Look into why the run died, probably isn't due to memory issues
* Modularity
** Look into how to expand ARLs past one depth tree
** Got starting looking at the source code, will start on sphinx soon
** Set up wiki page with resources
* NLP
** Looked into papers on amazon data set
** Planning to set up a walkthrough on how to run EMADE as well as working on notion documentation

=== Subteam Meeting ===
Recording from yesterday's meeting is posted

In regards to sphinx: might be ambitious but aim to have results next meeting

Try to have a work session Thursday/Friday

Things to consider doing:

Start trying to edit functions, since they're mostly self contained

Expanding depth function

Just be able to explain any changes made

Think about direction to go in, form pairs to tackle problems together

=== Team Meeting - Feb 21 ===
Continuing documentation using Sphinx. Take a function from source code and document it.

Split into two groups, one is looking at depth of trees and the other is looking into how objects are stored.

=== Personal Exploration - Feb 21 ===
* Downloaded Sphinx through terminal with the following command. It did seem like it had been installed before, but the command went through and reinstalled it.
**   pip install -U Sphinx
* Deleted previous EMADE repository and cloned Modularity's EMADE fork ([https://github.gatech.edu/gwang340/emade link])
** Ran into disk space issue, will try to resolve that. 
** Deleted unnecessarily large files from my computer, which hopefully has resolved the issue.

* While that's downloading, I decided to watch and take notes on the recording of the presentation from last week

==== Overview of EMADE in general (0:00 - 13:) ====
* Most of Modularity's work is self-contained in the adfs.py file (Under GPFrameworks directory)
** Gabriel has the repository opened in Visual Studio Code
* The work that isn't self-contained uses the database
* EMADE works off of a MySQL database
** Probably won't need to mess with this too much
* Most things regarding the database are contained in the sql_connection files
* In EMADE, primitives are python functions, and we try to group them by types which each get their own file
** Ex: signal_methods is for signal processing type primitives. One primitive is a fast Fourier transform. Another, spatial_methods, is for image processing like filters and gradients.
** Pretty much any file that ends with "_methods" is a file specific to primitives
** Most important is probably learner_methods which is where the machine learning comes in.
* EMADE.py is the main file in terms of GP functions
** Handles mating/mutation, multi processing
** Basically a really big file that handles everything really nicely
* standalone_tree_evaluator takes in a hash and evaluates that one individual in the database so you don't have to do a full run to evaluate it

* This session will be focusing on the contents of the adfs.py file
** Where most of the subteam's work is self-contained
* Modifying the traditional genetic processing pipeline
** Normal process: Select the individuals for the next generation then mate and mutate them, make children, move onto the next generation.
** Adding an extra step between selection and mating/mutating
** Select the individuals we want to continue on to make children or mutate, then look through those individuals for good portions to modularize (in this case, ARLs)
** Once those portions have been found, add it to a global pool to replace primitives with when mutating
* Condensed multiple ARL nodes into one to prevent too much change from mating/mutation
* There is an ADF specific block in the EMADE file. How we're writing to the database, and how we're handling ARLs.
* When the function gets called, it calls another function: update_representation
** After selecting, we need to update what they look like since they're now an ARL instead of a traditional node
* In adfs.py, the class ADFController has all the functions that we need, including the update_representation method which calls all the other methods in the class.
** First it calls _find_adfs which finds adf candidates to modularize with. Looks through every individual and finds parent/children node combinations.
*** DSF(?) traversal, uses a dictionary to store every combination of parent node and its children. Also store the frequency of parent/child combinations.
*** Once all the pairs have been found, we return the dictionary of all the parent child node combinations and their frequencies.
** Now we take that dictionary and the nodes are now candidates to be selected from using the _get_best_adfs method.
*** No actual way of defining what makes an ARL good (at least so far), but there is some intuition about what a good ARL is. Trying to find a good way to determine if an ARL is good or not is the basis of most of our research.
*** Then we call _generate_adf which takes in the population and the frequencies and generates a probability function which makes it so that a candidate that shows up more often (higher frequency) will be more likely to be picked.
**** This probability function is not completely set in stone, could change in the future
**** After that we will have a set definition of what the ARL will be, which is comparable to a function since it has inputs and outputs.
*** The ARL then gets turned into a lambda, put into a primitive set, then written into the database

====== Notebook Self-Evaluation ======
[https://drive.google.com/file/d/1-C9QnAWUmfIcQLcTe4OGgTxb0BHKdI7H/view?usp=sharing Rubric]

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Notebook self-evaluations
|Complete
|February 15, 2021
|February 22, 2021
|February 21, 2021
|-
|Take notes from the source code and architecture discussion video
|Complete
|February 21, 2021
|February 28, 2021
|February 23, 2021
|}

== Week 5: February 22, 2021 ==

=== Main Team Meeting ===
* Stocks
** Decided to do option with one Monty Carlo fold per stock
** Some confusion over paper, but will talk more about it this week
** Working on fixing code to match paper's results and add paper's primitives to EMADE
** Focusing on preparing an EMADE dataset 
** Which stocks will be used?
*** Paper mentioned three different ones: NASDAQ stocks, some Taiwanese stocks too
*** Using same time periods, and will get aggregate of results on the six folds (maybe less since not all the Taiwanese stocks are in the US)
* EzCGP
** running PACE and debugging it
** Was able to do a successful run, was a minor issue - still ironing out bugs
** Got first gen to work properly, but it died out soon after
*** Going to tournament select and seemed to try to compare two incompatible types of individuals
* Modularity
** Sphinx documentation is complete for all but two methods
** Split team into two task groups
** Discussed redoing the architecture, restructuring the depth of the trees
** Working on what can be stored in the database, currently lambdas, want to put subtrees (like tiny individuals)
** Asked Jason to join our sub team meeting
* NLP
** Primarily about getting things set up
** ran into a few issues with PACE ICE
** was able to do tests on datasets, but ran into a few snags for some
*** Amazon dataset had dependency issues
** Making sure instances are prepped 
** Finished up documentation
** Would also like Jason to join sub team meeting 

=== Subteam Meeting ===
* All individuals are lists, is in preorder traversal
* Know parity of each node, so can tell how many children there are
* The bigger the ARL the better? Means you have found a large part of a tree that works
* Should other primitive types be allowed to be taken in by bigger ARLs
** Might add unnecessary bloat 
* How is an ARL formed, and how can it be improved?
** Goal is to reuse useful components
* How do you know if a complete tree is useful?
** Not really able to tell, but uses the fitness results of comparing the child to the parent
* Open up the frequency - if common pieces of code are found, it's probably more useful
* ARLs are used to minimize destructive changes
** Condense useful nodes into one single node that represents the ARL
** As generations go on, it's harder to mutate
* Potentially create genetic duplicates to have one version with an ARL and have one without
** Initial individuals started off small, then got even smaller due to shrinking... but was still performing well
** Make an individual without the arl, just the complete tree, to hopefully make a new tree that might be better
** Will probably be more on the database side of things

* Crossover doesn't introduce any new genetic material, mutating and headless chicken mating(???) does
* Bernadette volunteered to help with generating ADFs
* Looking at the fitness of the individuals, trying to get a formula to find good ARLs?
* Get a depth 2 tree to work, be able to show a partial tree?

=== Team Meeting - Feb 28 ===
Gabriel gave Ivanna and I two tasks to choose from: genetic duplicates and tree storage in the database

Bernadette started on generate_adfs, Xufei started on the search_adfs method, Kevin mentioned find_adfs, and Vincent said he had chosen a third method, so everyone in that group had mutually exclusive methods they were working on. 

Genetic duplicates:

ARLs are formed by finding useful subtrees and condensing them into one big node with the same inputs

Concern about that then getting condensed into a larger node that then never has any genetic variation
[[files/GabrielDiagramGeneticDuplicateExplanation.png|thumb|Gabriel's explanation for the reasoning behind genetic duplicates]]

Want to allow indivs to explore the search space while keeping good indivs

So why don't we just duplicate the tree so that we have one version with an ARL node and one without the ARL

Want to see if this will help with diversity issues

EMADE creates 512 initial indivs for the first generation, randomly created with primitives in EMADE

After that, not much randomness is introduced, only two ways

Choose two indivs to be parents and make two children from that, or mutation occurs (swapping a node for a random primitive)

The thought was that ARLs are new primitives and EMADE can make new indivs with those primitives

However, that's not how EMADE works, new indivs are not made after the initial generation

At the end of each generation:

Insert genetic duplicate into the database to allow for more changes

Make some random individuals with the existing primitives we have 

Both are in the hopes that new genetic material will be inserted into the population

In EMADE.py -> master algorithm

In adfs.py -> contract adfs

In sql_connection_orm_master.py -> adfs()

Meet with Gabriel and Ivanna after main meeting tomorrow to look at code together

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Examine methods given by Gabriel
|Complete
|February 28, 2021
|March 1, 2021
|March 1, 2021
|}

== Week 6: March 1, 2021 ==

=== Main Team Meeting ===
* Midterm evaluation week!
** Complete peer evals this week, due friday
** Notebooks are also due tonight, deadline is extended by a day

* Stocks
** Algorithm works well, just have a few minor bugs to work out
** Working on developing a more volume based technical indicator
** Also looked into a technical indicator library you can download for python, want to see if it's possible to install in EMADE
** As for paper, team is in a good spot, and they can figure out how to understand the paper by looking into citations and sources
* EzCGP
** Fixed pipeline issues
** In terms of focus: looking into benchmarking and evaluting endividual training times
** See if framework is good architecture or not
** Did a few runs as debugging issues, wasn't a good as they were hoping for 
* Modularity
** Almost done with documentation of code
*** One more commit away from finishing
** Working towards testing larger depth trees
* NLP
** Turny(?) selection isn't working but NSGA is
** Reached out to Stocks team for help with Colab
** Wrapping up documentation
** Expecting a couple runs this week

=== Subteam Meeting ===
* Tree depth team is going well
* Gabriel made two new branches in the github repo: genetic_duplicate and tree_database
* Each branch has todos within the files for Ivanna and I to complete
* I will be working on the genetic_duplicate branch, which has todos in the EMADE.py, adfs.py, and sql_connection_orm_master.py files

=== Personal Exploration -  Mar 5 ===

==== EMADE.py ====
Not entirely sure what these ToDos are asking for or how the code works :(

==== adfs.py ====
contract_adfs - How to get an new indiv_index for new individual instead of inserting the new individual at the old individual index?

==== sql_connection_orm_master.py ====
adf_update - how to insert a new row instead of deleting/overwriting the previous row?

=== Team Meeting - Mar 7 ===
* Team updated each other on progress
* Unfortunately had to force shut down computer before I could save my changes, so I lost my notes for this day :(
* Did get answers for my questions, but may have to ask again or look at my Slack message history to recover some information
* Did try doing an EMADE run, but ran into package issues when runing the reinstall command (Tensorflow)

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Complete peer evals
|Complete
|March 1, 2021
|March 5, 2021
|March 5, 2021
|-
|Get questions about todos within the genetic_duplicate branch asnwered
|Complete
|March1, 2021
|March 8, 2021
|March 7, 2021
|-
|
|
|
|
|
|}

== Week 7: March 8, 2021 ==

=== Main Team Meeting ===
* Grades will be given back soon
* Stocks
** Continued work towards getting a run of EMADE done
** Started preparing the datasets moving forward
** Confusion over how paper got optimal threshold, but decided since it's not that important to just figure it out themselves
* EzCGP
** Visualizing results from last week's run
** Blocks themselves are working well
** Might remove preprocessing to see if that is actually helping or not
** Worried that it might be over-fitting
* Modularity
** Depth team - has all the individual components done, just need to merge and then can start experiments
** Database team - still looking into certain things
** One fear since arls are converging and are inherently dense, worried abt running out of genetic material
*** Want to create new material after each generation
** Gabriel asked about tracing genealogy in EMADE 
** Have a good idea of where to put things, but still working on organizing
* NLP
** Hit a few snags but otherwise had a good week
** Had a run on the Amazon dataset, got peculiar results
** Hope to be set up for a baseline run by next week
* Midterm presentations are coming up! Would be a good idea to start putting together a draft for next week
* Also will be assigned new students the week after, so have slides on what qualities and what roles those new students should have

=== Subteam Meeting ===
* Vincent and Bernadette - both still testing components
* Kevin might start early on midterm presentation since he finished his part early
* Asked tensorflow error question, will keep team updated on status
* Will probably rename source code variables to be clearer

=== Personal Exploration - Mar 9 ===
* Xufei helped to try and resolve the tensorflow error
* Deleted EMADE again
* reinstalled git lfs first
* cloned the repo
* created a conda environment
 conda create --name emade
* need to activate the environment before each run:
 conda activate emade
* Also switched Python versions in the conda enviroment
 conda install python=3.7
* I reinstalled all the packages and tried to set up a run, however I still got the tensorflow error :(
* So there might be a different reason for the error

=== Team Meeting - Mar 14 ===
* Xufei: finished with search_adfs, tried to push changes but there might've been a merge conflict
* Me: couldn't test anything but I will push the changes and hopefully someone who's emade works can test it for me
* Bernadette: pushed changes to generate_adfs
* Kevin: will make final changes after all the changes get pushed and make sure everything works
* Hoping to get some runs in for the midterm presentation
* Ivanna made slides, so contribute to what you were working on
* Will practice presenting to Drs. Rohling and Zutty, and will send out a lettucemeet to determine the time

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Look through Ivanna and my own notebooks to see 
if there was a solution already found for the tensorflow error
|Complete
|March 8, 2021
|March 14, 2021
|March 9, 2021
|-
|Push code to GitHub
|Complete
|March 14, 2021
|March 15, 2021
|March 14, 2021
|-
|Fill out lettucemeet for practice presentation
|Complete
|March 14, 2021
|March 15, 2021
|March 15, 2021
|}

== Week 8: March 15, 2021 ==

=== Personal Exploration - Mar 15 ===
* Attempt to fix the tensorflow error part 2!
* Deleted all files related to python and anaconda so that I could reinstall them
* Instructions for installing on Mac here: [https://docs.anaconda.com/anaconda/install/mac-os/ link]
* Recreated conda enviroment with the commands below
 conda create --name emade
 conda activate emade
 conda install python=3.7
* Next will be to install the opencv command and the really long commands found in the EMADE repo
* Result: Still getting the same error

=== Main Team Meeting ===
* Still working on midterm grades, but will give them back through canvas by the end of tomorrow
* Next week is presentation day!
* Stocks
** Did a run w the new primitives, but it didn't go too well
** Only got one or two additional valid individuals, even after seeding with 30 valid individuals
** Will meet with Dr. Zutty after meeting to figure out what's going on
** Goal is to improve performance and work on presentation
* EzCGP
** Working on figuring out a new way to confirgure individuals to prevent errors
** Going to add dense layers to see if that fixes it
** Working on producing benchmarks
* Modularity
** Waiting on availabilities to get a practice presentation time
** Taking a while to do a run
** Getting a weird error (ME!)
** Going to change how things are stored in the database still
** Plan for this week is to work on presentation for next week
* NLP

=== Subteam Meeting ===
* Vincent - has not actually finished testing his part, but will explain what's going on so far
** adding_all_subtrees - given an individual, add all subtrees starting from length one to the root node index
* Tried to help me with tensorflow error, but nothing seemed to work
** Will probably consult Dr. Zutty about ideas

=== Individual Exploration - Mar 16 ===
* Attempt to fix the tensorflow error part 3!
* Dr. Zutty had me run three commands last night
 which python3
 which python
 which pip
* Not sure if these commands helped my computer find the right files to use, but I was under the assumption they were essentially informative commands (like checking what version of a software you have)
* Reran the reinstall command this morning, and everything installed correctly

=== Personal Explanation - Mar 20 ===
* Created slides explaining the genetic duplicate process

=== Team Meeting - Mar 21 ===
* Met to practice presenting for tomorrow
* Chose slides to present, 18-20, 23
* Feedback: 
** Less text heavy slides
** More visualizations and pseuodeocode for students who haven't seen these concepts before
** Question asked: Why not start with ARL generating after a baseline run so that we can better see what functions are better, instead of doing it from the start?
* Went through each slide one by one to give specific feedback
** Broke up my own slides on Genetic Duplicates into four slides with visual representations on each slide.
** Also added speaker notes so I'll know what to say during the actual presentation better

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Get EMADE up and running
|Complete
|March 15, 2021
|March 22, 2021
|March 16, 2021
|-
|Finalize Presentation slides
|Complete
|March 15, 2021
|Marcch 21, 2021
|March 21, 2021
|}

== Week 9: March 22, 2021 ==

=== Main Team Meeting - Midterm Presentations ===
Stocks
* Looking into stock market trend analysis with EMADE
* Researching and implementation
** Found new literature for this semester than was more consistent and wanted to create a model that would be better than the one described in the paper
** Wrote Technical Indicator(TI) functions for data preprocessing
*** Want ones that account for volumetric data
* Defining Scope and Reviewing Literature
** The paper used a Piecewise Linear Representation (PLR) to recognize trends in stock price movement
* Implementation
** TIs used include simple moving average, bias, relative strength index, moving average, stochastic oscillator, Williams, transaction volume, and single day change for everything
** To better match the paper, they wanted to expand the TIs used
** Want PLR Algorithim to make trends in stock prices visible
** These trends convert to trading signals and gind threshold value using genetic algorithms
** Exponential smoothing uses NN trained with TIs to predict trading signals
** Aim to smooth out the prediction of buying or selling
* Current issues
** Figuring out implementation details and generating proper thresholds per stock
** Some individuals are a little "laggy" in their predictions
** After seeding, want to generate valid individuals
* Results
** Ran for 77 generations using 100 individuals
** Got about a 20% profit with tree size of 148 for the best individual
* Future
** Hope to get more TIs implemented in the run
** Look into different time granularities
** Also maybe look at other stocks (ex: penny, crypto)
** Check out neural network evolution
** Isolate evolution for TIs

Bootcamp #1
* Created a chart comparing correlations of features to each other to tell what columns to use
* GP performed better than ML overall
* Had minimal data cleaning and only dropped Name, Embarked, and PClass
* Ran for 21 generations and found 36  pareto individuals
** Runs faster for cluster run due to shared computer power, and less redundancy
** EMADE was most accurate and most diverse

EzCGP
* Introduce ezCGP
** Is a graph based instead of tree based structure that uses custom primitives and data types
** Uses a block structure so there's not much mixing
* Midterm Progress
** Focusing more on Neural Architecture search, want to recreate results without relying on transfer learning
** Hope to validate assumptions
** Transformers perform both vision and language tasks
*** Sounded useful, so looked into using them with genetic algorithms
**** Wasn't much existing research
** Untrained model performed well after 10+ hours of training
*** 10+ hours is problematic for the scope of the problem
** Ran experiments on training parameter bench-marking
*** Converged almost completely after 20 epochs
** Wanted to test if ezcgp framework would create architecture complex enough without using transfer learning blocks
*** First simply removed the block and ran a few generations
*** However, there was an issue of over-fitting

Bootcamp #2
* Extract title from each passenger's name, can extrapolate missing ages from title
* Imputed Fare from PClass
* One Hot Encoding for Sex, didn't want to prioritize one over another
* Dropped Cabin
* Dropped Embarked as they didn't want to over-fit
* Minimized FNR and FPR by squishing
* Ran with a pop of 300 individuals for 40 generations
* GP performed better than ML
* Ran EMADE for 25 generations
* EMADE performed slightly worse compared to MOGP and ML
Modularity (Our presentation!)
* Comments/Questions:
** Any knowledge/issues of space complexity? Considering using EMADE's worker functions?
*** More worried about time complexity since it takes more time to complete a run than we'd like. Our experience is that ARLs don't take that much more time to create. 
*** Our ARLs take place after mating/mutation. No, we don't use the worker processing currently. Could consider looking into it though
Bootcamp #5
* Preprocessing
** Created a chart comparing correlations of features to each other to tell what columns to use
* GP 
** Normalized features before passing them in
** Tested different selection methods, settled on NSGA2
* ML
** Did a very large run on 1500 generations but the individual was likely over-fitting, so stuck with first run of 50 individuals
* EMADE
** Modified dataset like before
** EMADE wasn't significantly better than GP or ML

NLP
* Using Neural architecture search to take on EMADE
** Using Kaggle Amazon dataset
* Found a lot of trivial solutions last semester, so want to focus on why those happened for this semester
* Chose PACE-ICE, standardizes runs and is easy to set up for the first time
** Issues with database and run times though
** Fixed some merge and recursion errors recently
** Also corrected errors in misc files which were deprecated or had minor errors as well as lingering bugs in new mutation functions
* Started documenting NLP Primitives using Notion
** Covers 7 implemented primitives and a work in progress one
* Reviewed how to get a dataset up and running in EMADE
* Added Amazon Product Reviews Dataset
** Binary Classification (pos vs neg sentiment)
** Balanced Dataset, has similar number of pos and neg examples
* Got a baseline model on Kaggle, but was unable to replicate that result due to multiple issues on Colab
** Realized model can't be replicated in EMADE due to tree based structure because of the concatenate layer
* FastText, may consider using it as a primitive in EMADE
* Completed one non-seeded run
** Mostly to highlight why seeded runs are needed for NNLearners
* However, a seeded run is giving the same results
** Will prioritize fixing this
* Want to figure out why these individuals are failing
** Look into debugging the primitives
* Future Work
** Add PyTorch functionality
** Has high compatibility with SOTA attention-based models as well as deeper learning functionality
*** Obstacle is refactoring code from Keras to PyTorch, resource prohibitive
* Goals
** Get completed runs on Amazon dataset
** Figure out where trivial solutions are coming from
** improve quality of runs and identify which primitives are failing a lot
** Have a minimum-viable-implementation for population-level PyTorch runs
Bootcamp #3
* Dropped Name, Ticket, and Cabin due to missing information or irrelevance
* Filled in missing values for Age and Fare
* One Hot Encoded Sex and Embarked
* Observed some disparities among class
* GP and ML
** Added 9 primitives to GP to try and evolve more complex individuals
** Had 200 generations
** Used tournament selection on the initial population
* EMADE
** Ran for 26 generations which took about 3 hours
** Had difficulties getting estimates for FPR and FNR
** Most of the optimal individuals had AdaBoostLearner as the outer primitive
* ML was fastest time wise, but EMADE has better AUC over time
* GP was best at individual tuning and had the highest number of pareto front individuals

Bootcamp #4
* PClass had a low correlation with other columns and Name wasn't really useful as a string, so both columns were dropped
* Used average value to fill in missing values for age
* ML ended up doing better than GP
* EMADE
** Ran for 20 generations, and resulted in 12 individuals on the pareto front
* GP did best, ML did worst
* However, EMADE might've done best if it had gotten to run for longer

=== Team Meeting - Mar 28 ===
* No real updates for today, discussed expectations for the rest of the semester
* What are first semester students going to do?
* Do some baseline runs while the rest of the code is being debugged
* Think about what a first semester student would like to have resource wise
* Potentially focus on diversity
** Write mutation functions
** Find literature, if it gives a numerical way to calculate diversity, could write separately from EMADE
* Use the first week to think of ideas, or make a tentative decision now for first semesters
* Do some baseline runs to collect data
** Use colab for this so it doesn't run on your own computer
* Research areas to do work in
* Try and collect resources for first semester students
* Regarding genetic duplicates:
** Need to call an insert function and then maybe write a duplication function to call

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Get started on a baseline run
|Complete
|March 28, 2021
|April 4, 2021
|April 4, 2021
|-
|
|
|
|
|
|}

== Week 10: March 29, 2021 ==
Starting here, the VIP wiki site is hosted on a new server. However, the editing GUI is now not functional, and we have to use the "Edit Source" button to add entries. As I am not used to editing in this format, I will be taking notes on a seperate Google Doc and pasting in entries weekly.
=== Main Team Meeting ===
* Assigned new semester students to teams now
** Each team gained 7 members :o
Stocks
*Discussed ideas on how to further optimize emade w more fitness functions
* Came up with a few ideas to make stronger individuals
* Considered Dr. Zutty’s suggestion of the Monte Carlo approach
ezCGP
* Looking at dense layers
Modularity
* Taking a week off to figure out what to do with the one month we have left
* Look into runs using the MNIST dataset
NLP
* Prioritizing nontrivial results over anything else 

=== Subteam Meeting ===
* Sent forms for returning and new students to give feedback
=== Personal Exploration - Apr 4 ===
* Xufei and I worked on getting a baseline run up and running through google colab
* Used the burner google accounts to store the emade-cloud folder generated in emade
* Ran into a file not found error (resolved in the team meeting)
* Got a run going! However, none of the individuals generated were valid :(
* Will need to look into why the seeded individuals weren't valid

=== Team Meeting - Apr 4 ===
* Gabe was helping out the first semester students and also looked into arl_update with Vincent and Kevin
* Ivanna is doing implementation stuff
* Kevin finished the lambda method where we have to find number of parameters
* Xufei and I have been working on getting a baseline emade run set up
** Ran into a file not found error
** Turned out the files were incorrectly named (there was an extra “s” in the file)
* Bernadette said she would also like to work on doing baseline run instead of trying to get started on a new project
* New students can do baseline stuff too
* Schemas need to be uniquely named so we don’t run into issues. Currently, Xufei and I are using the name “mnist_hoes”

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Figure out how to fix issues with run
|Complete
|April 4, 2021
|April 12, 2021
|April 11, 2021
|-
|
|
|
|
|
|}

== Week 11: April 5, 2021 ==
=== Main Team Meeting ===

Stocks
* Spent week on-boarding new students
** Explained to them how emade works and what their codes changes were and their goals with emade
** General tasking given such as research into technical indicators

EzCGP
* Ran for 50 generations with population of 8, got 68.4% accuracy 
* Diversity was very limited
* Did a second run with larger population, which had more diversity but decreased accuracy

Modularity
* Starting MNIST benchmark runs
* Sticking with F1 and accuracy scores as it’s slightly simpler
* Not sure how long to run for yet as this is a new dataset
* Depth problem will continue to be workshopped

NLP
* Trained new members and got them tasked
* Getting everyone set up in PACE
* Gave an EMADE 101 presentation, and will also give a NN presentation later
* Figured out why crashing: size of dataset was too large

==== Statistics for EMADE Presentation ====
* Since EMADE has elements of randomness, how can we show that the changes we made were actually the cause of improvements?

=== Personal Exploration - Apr 7 ===
* Attempted to get seeding of benchmark run completely successfully
* Kevin got a successful run, so Xufei and I decided to see if seeding the run from my laptop would work better
* Unfortunately, I got the exact same error message as Xufei did last week, so we're still at square one on what to do

=== Team Meeting - Apr 11 ===
Wiki was down again so these are notes from a Google Doc again
* Gabriel was working on depth trees, ran into a bug.
* Kevin is working on contract_arl still because it doesn’t fully work yet.
** Seems like they need to be able to access a primitive
* Xufei and I have been working on the MNIST dataset, but both of us ran into issues with a file being unzipped when it shouldn’t be.
** Can try to rewrite cloning script so it will work better on Macs
** Ivanna offered to help us out with getting EMADE cloned into Google Drive
* Bernadette has also been trying to do runs on MNIST
* Can set up a work session later, Gabriel said maybe Friday afternoon
* Will send out a lettucemeet to figure out a time
* There’s an additional step needed for individuals to be valid in MNIST that we didn’t need in Titanic because MNIST uses images which produces stream data
** If we’re finding that the seeded individuals are valid, then why aren’t they making valid children?
** Maybe generations aren’t long enough, maybe the primitives aren’t crossing over well.
** Might want to wait to discuss with Dr. Zutty since it doesn’t look like we’ve been able to create many valid individuals.
** It would be nice to have set data and analysis to have for tomorrow though

=== Personal Exploration - Apr 11 ===
* Xufei and I managed to reclone the emade-cloud folder, and it was much easier to upload this time
* We ran a master process from my computer, and she ran a worker process.
* We created 75 valid individuals over 48 generations! Only 2 of the seeded individuals were valid, but we did get many individuals that were better than the seeded ones. 
** Of the 75, 2 achieved an accuracy score of 1
** These two both use learnerType(‘DEPTH_ESTIMATE’)
** I am worried these two individuals might be overfitted, but will discuss during tomorrow’s meeting
* Overall, there’s a notable trend where we have 10 individuals whose accuracy scores are above 0.88, then the rest are below 0.46
[[files/AYandXLFirstBenchmarkRunMNIST.png|left|frame|200px|Screenshot of valid individuals generated listed by highest accuracy]]
(Unfortunately I can't figure out how to make the image smaller without a "file not found" error showing up)

== Week 12: April 12, 2021 ==
Main Team Meeting
* Dr. Zutty is aware the wiki has been on and off, so updating the page less frequently is expected
* However, there should still be weekly updates in the content by taking notes on your own

Stocks
* On Monday, had a shorter meeting after the stats review
* Considering using Welch’s test on individuals
* Continue to fix smaller errors found and to make more runs over the weekend
* Going to start splitting up tasks after this meeting
* Consider how seeds affect the results of the run

EzCGP
* Made small additions to code in order to test results
* Analyzed why individuals have 4-5 nodes, not more
** Individuals with more nodes seemed to be selected against (Are generated, but not chosen)
* New members are working on visualization

Modularity
* Majority of us were running and analyzing MNIST
* Currently looks like others aren't really doing as well as Xufei and I (only about 15 valid individuals out of 900)
* Not sure how many generations it took, but likely close to our number of generations
* Also wondering what data structure learnerType is, use ephemeral 

NLP
* Have most of new memebers set up on PACE now, they will be working on the "evolution" problem
* Got results from a few runs, will discuss later today
* Also gave new students "Neural Networks 101" presentation

Subteam Meeting
* Reported Xufei and I's results to Gabriel, turns out we had the accuracy score wrong! It's actually the smaller the better
** However, that just means we have a lot more good individuals than we thought
* However, we had to share the unfortunate name of our schema: mnist_hoes
* Gabriel and Dr. Zutty both said our individuals looked really good (yay!)
* Xufei informed them we plan to create visualizations this week for our valid individuals
** The individuals work by seeing if the pixel they're looking at is light or dark, and assigning a value to it that way. This then allows it to figure out the pattern that forms different numbers
** Also the actual good individuals had SKLearners on them
** Measure the AUC under the pareto front; however, the two accuracy scores are exactly the same, so there's no way to make a proper pareto front
* Hard-coded to run only 50 generations because of Tianic dataset
* Set parameters for useArl (useAdf?) to True in templates
* seedingTests.py and gp_Frameworks.py?
* Gabriel said he would send over the visualization code to Xufei and I after the meeting

=== Personal Exploration - Apr 12 ===
* Xufei and I decieded we would try a seeded run using ARLs this time
* Inside the input_MNIST XML file, we set the "<shouldUseADFs>" parameter to true, and created a new schema called "mnist_AX"
* However, the run was unsuccessful and I sent a screenshot of the error message to Gabriel
* He said it seemed like the adfcontroller object wasn't properly initialized, and that we might be using the wrong XML file
* Unclear what exactly that meant, so will inquire what the correct file is and where to find it

=== Team Meeting - Apr 17 ===
* Discussed the database, split into 2 teams
* ARL/contract, storage in db: Bernadette, Krithik, Ivanna, Gabriel
* ARL evaluation/expansion, reading from db: Angela, Xufei, Rishit, Gabriel
* EMADE.py
** evaluate() and evaluate_individual() methods
** wrapper_methods.py

 ####################################################
 # TODO If 'adf' is discovered while evaluating,
 # run the expand_adfs() method to expand out the
 # tree again, then recompile the individual as
 # expected
 ####################################################

== Week 12: April 19, 2021 ==
=== Main Team Meeting ===
* End of semester is coming up!
* Notebooks are due May 1 by midnight
* Peer-evals are due on Apr 27 at 4pm
Stocks
* Cleared up a misunderstanding with first-semester students
* Looking for an algorithm that will work on any stock but may need to be trained for that specific stock
EzCGP
* Visualization team added parameters to graphs
* Issues with GPU building model, allocater ran out of memory trying to allocate an individual
Modularity
* Finally have something running without errors with the new depth, although there are a few edge cases still causing errors
* MNIST, still not getting consistently good runs
NLP
* PACE installations are going well, was able to complete multiple successful runs
Subteam Meeting
* Just had a meeting yesterday, so no updates from then

== Week 12: April 26, 2021 ==
=== Personal Exploration - Apr 26 ===
* Returning students all worked together to complete some runs with new seeding individuals.
* Did have to start over but the schema we're currently using is "mnist_old_arl_1"
* Vincent is acting as the master, and the rest of us are workers.
* Have gotten 13 generations, 370 individuals, and 29 valid individuals so far.

=== Main Team Meeting ===
* Between 20-25 minutes per group for presentations plus q&a
* [https://docs.google.com/presentation/d/1SLLHwjsy-ZHV4OqAXDBclBeTzqrSbyCNCJNRZs0a8Kc/edit#slide=id.g812dce5abf_2_2 Modularity Slides]
* Notebooks are due on the last day of class, however to allow for focus on the presentation, they will be graded after 11:59pm on Saturday
* Double check midterm feedback,
Stocks
* Did more seeded runs with graphs showing a particularly successful individual that was generated

EzCGP
* Did some work on researching primitives
** Scraped layers used by common pre-trained networks
* Experimented with mating methods

Modularity
* Have new objectives and want to do essentially three experiments to test various changes
* Mostly just wrapping up and finishing these changes

NLP
* Pushed a large update to the github, had a huge performance increase

Subteam Meeting
* Old ARLs team has been completing runs however, there's been a few issues
** First issue was that Vincent accidentally connected to the VPN part way through the run
** Second issue was with deap, got an index out of range error
* Not sure if this is specific to one person or not, so will test with someone else as master
* First semester students have been working together to complete some runs, but have been having issues with colab
* Gabriel will be working with them to run a master process

=== Final Presentation - Apr 30 ===
* Notebooks are due tomorrow at midnight, no revisions after then 
Stocks
* Objectives
** Implement TA-Lib indicators in emade
** Increase Evolvability of emade individuals
** Test on a larger dataset and different stocks
** Incorporate data analysis and statistical evaluations of individuals 
** Implement technical indicators
* Piecewise Linear Representation 
** Threshold value determines number of piecewise functions 
* Exponential Smoothing
* Divided team into three main subgroups:
** Literature Review and Research
** Data Analysis of runs
** EMADE Implementation
* Changes to EMADE included removal of TriState and Axis parameters to TI primitives
* Added new Datasets to optimize (Larger Date Range)
* TA-Lib Technical Indicators were redone and Indicators without TA-Lib were calculated manually
*EMADE Results: completed two long runs
** 300+ generations with 4 objectives. New objective: Normal CDF on Distribution, correlated with Profit Percentage. AUC of pareto front decreased over the generations
** 300+ generations with 3 objectives. Removed Profit Percentage since it was correlated with another objective. Also saw an overall decrease in AUC of pareto front, but had a spike from restarting the run in the middle.
*EMADE Analysis
** New objective function: Monte Carlo Simulations. Calculate profit on stock with random buy and sell trades and compare to performance of Individuals. Individuals performed better than randomness.
** Buy/Sell Hold and Buy/Sell Lag
* Primitives Analysis
** The algorithm uses historical data and technical indicators to predict stock, so bad TIs can mess up the results of this analysis.
* Future work: Comparing levels of generalization of optimal models, statistical analysis of seeding on AUC, create bounded objective functions, applying fundamental analysis in addition to Technical Analysis, Look at effectiveness of emade on different intervals of trading, look at another paper to base more research off of.

EzCGP
* The teams uses a Cartesian block structure that is specific to ezCGP
** Certain nodes are active while others are inactive to see which ones work the best
** There are stages that blocks are grouped by so the code passes through a certain order
*** This includes preprocessing and data classification
* They have removed augmentation and preprocessing as blocks since the midterm
** As a result, they got higher training accuracy and lower validation accuracy. Runs are much faster as a result though
** Also resulted in overfitting issues and a lack of connected layers
* Hope to take out transfer learning and create their own replica of CIFAR-10
** Worked on visualizations of genomes and researched/tested some new mating methods
* Population lacked diversity and individuals had few layers so the team decided to manually analyze individuals
** Individuals in the initial population matched the targeted population
** Larger individuals tended to do worse, potentiality more room for mistakes compared to smaller individuals
** Could look into incrementally larger individuals
* Completed testing of pooling and drop out to compare experiments to midterm benchmarks
** Using the new improvements, they had a 68.5% accuracy which is much more than 56.3% and pooling could improve the evolve architecture
** On the other hand, dropout layers didn't do well
* Looking at dense layers - would adding fully connected layers improve performance?
** Got a lower validation and training accuracy as well as low diversity and did not do as well as SOTA or transfer learning
* Improved how visualizations are generated
** Added inactive nodes, layer arguments, and node numbers.
** Multiple individuals can be visualized as needed
** Uses command-line interface to easily visualize individuals
* Want to work on having better seeding
** Read through online code from TensorFlow's GitHub page
* CGP paper overview and crossover
** Looked at symbolic regression problems
** Each generation has the best individuals from the previous generation and offspring
* Meta parameter search can get expensive for computational efforts but the mean result is defined by the fitness
* New mating method through using one-point crossover
** Runs with mating converge faster than runs without mating
* Point mutation: Look into a percentage of the parent's genes to create a new child genotype
* For next semester - find new mating methods, existing CNN architectures

NLP
* Take an evolutionary approach to NLP via neural architecture using emade
* Different layers are primitives within emade’s tree structure
* Previous semesters work:
** Focused on developing functionality and primitives of Neural Architecture Search (NAS) within emade
** Branched into computer vision applications 
** Had issues with trivial solutions being generated
* This semester:
** Worked on streamlining how the team ran EMADE and chose a simpler dataset to work with
** Ran emade and examined the shortcomings of their implementation of NAS
** Got two team members set up on ePACE-ICE and wrote documentation to help the rest of the team get set up
*** In the process, they ran into a few errors but were able to resolve some of them. This included no up to date conda environment, mysql errors, and support for multiple worker nodes. 
** In EMADE, they phased out broken primitives and fixed bugs in new mating/mutation functions.
* Primitives used
** Pretrained embedding layers: a NN layer which takes in a vector representation of words learned by a predefined vocabulary
** Primitives Documentation: Have 50 primitives documented
* Resources created: EMADE, PACE, and NN/NLP slide decks/guides
* Amazon Product Reviews Dataset: A binary classification and balanced dataset. Only used half the train data, as it was too big for emade
* Baseline Models
** Benchmarking - FastText Model achieved 91.73% accuracy, the number for EMADE to beat
*** Seed for the run already beat this number, so question is whether emade can improve upon the seeds.
*** AUC of results compared to seeds only marginally decreased
* Non-trivial solutions
** Best individual has a 92.8% accuracy after 22 generations, and this experiment has replicated similar results
** Improvements were very strong at the start but later becomes more marginal after a sharp drop-off
** With the first run, we see there were 17 Pareto optimal individuals, though 7 were seeded
** The elapsed time of individuals was a metric for complexity with an AUC of 0.027
** The second run was 21 generation but only had 4 individuals on pareto front that weren’t seeded, and had an AUC of 0.04.
*** Outperformance of seeds became more obvious past generation 20, with best accuracy being 0.9313.
** Takeaways: No discernable pattern in misclassified reviews
*** Longer reviews weren’t as good, and dataset could be labeled better
* Future work
** Increase network complexity and decrease failure rate through examining the structure
** Also might want to return to CV and avoiding multilabel datasets
** Improve EMADE’s outlook when seeded poorly
** Look at NNLearners as subtrees

Modularity
* How were 30 seeded individuals selected? Best individuals plus some extra
* Why such small populations? Committed some changes late and ran out of time for larger runs. Also because MNIST is a more complicated dataset

Wrap Up
* Make sure to document all efforts and complete notebook
* Next semester will be moving to Microsoft Teams

= Fall 2020 =
Bootcamp Team #3

EzCGP Subteam

== August 19, 2020 ==

=== Lecture Notes ===
* Genetic Algorithms - a search through a space of possible parameters using bio inspired principles
** Create new generations by mating/mutating the best individuals from the previous generation
** Goal is to eventually create the most fit individual
* Individuals - one specific candidate in the population
** One genome or solution, typically a list
** Assigned a fitness score based on how well it fulfills the goal
* Population - group of individuals undergoing evolution
** Properties will be changed over time
* Objective - a value given to an individual that is either maximized or minimized
** Increase the objective through the evolutionary algorithm
* Fitness - relative comparison of an individual to the population using the objective
* Evaluation - how to calculate the objective from an individual
* Selection - how to find the most fit individuals
** Fitness Proportionate - the higher the fitness, the more likely to be selected for mating
** Tournament - randomly choose a group of individuals, pick out the best from each group
* Mating/Crossover - offspring are created by combining "genes" from parents
* Mutation - small changes to an individual, random modifications to maintain diversity

=== Lab 1 - N Queens Problem ===
* The mutation function I wrote essentially swaps the integers at two random indices (and checks to make sure the two numbers are different).
* While the objective wasn't necessarily reached faster, the maximum of each generation over time does decrease more dramatically, which in turn makes the average lower as well
<gallery widths="550" heights="150" perrow="2">
files/Lab1NQueensMutationAY.png|The mutation function written.
files/Lab1NQueensMyResultsAY.png.png|The results of my mutation function
</gallery>

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Join Slack
|Complete 
|August 19, 2020
|August 26, 2020
|August 24, 2020
|-
|Create Notebook
|Complete
|August 19, 2020
|August 26, 2020
|August 24, 2020
|-
|Complete Lab 1
|Complete
|August 19, 2020
|August 26, 2020
|August 24, 2020
|}

== August 26, 2020 ==

=== Lecture Notes ===
* Last week's lecture: Genetic Algorithms
* Instead of taking an individual and having a function evaluator to obtain objective scores, the individual is the function itself
** Represent the program as a tree structure
*** Nodes (primitives) represent functions
*** Leaves (terminals) represent parameters
** Lisp preordered parse tree
*** Operator followed by inputs
**** The trees for f(x) = 3 *4 + 1 can be written as [+, *, 1, 3, 4]
*** Crossover in GP is simply exchanging subtrees
**** Start by picking a random point on the tree to find a subtree
**** Exchange subtrees to produce children
*** Mutation in GP 
**** Inserting/deleting a node or subtree
**** Changing a node
*** Evaluating a tree
**** Feed a number of input points into the function and measure error between outputs and expected value

=== Lab 2 Part 1 - Symbolic Regression ===
* Add two new primitives to the toolbox
** Chose a square and a remainder function (both with arity = 1) to add
* Also added a mutation function, mutNodeReplacement
* This lowered the maximum and seemed to reach the objective in fewer generations as well

* There were some errors in calculating the average, which may be due to the primitives that were added
<gallery widths="400" heights="300">
files/Lab2Part1ResultsAY.png|
</gallery>

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Complete first half of Lab 2
|Complete
|August 26, 2020
|September 2, 2020
|August 30, 2020
|}

== September 2, 2020 ==

=== Lecture Notes ===
* Multiple Objectives in MOGA and MOGP
** Example: "What do you look for in a date?"
*** Evaluating multiple aspects of a person, not just one
** Evaluating different vectors, ex: scoring higher in one class, but it's not that high compared to other students
*** Scores: True vs False positives
** Classification Measures (binary world)
*** Data Set contains positive(P) and negative(N) samples)
**** Classifier predicts positive or negative
***** Results in a [https://en.wikipedia.org/wiki/Confusion_matrix confusion matrix]
***** True positive/negative vs False positive/negative (type I/type II errors)
***** Maximization measures
****** Sensitivity or True Positive Rate (TPR): TP/P 
****** Specificity or True Negative Rate (TNR): TN/N
****** Precision or Positive Predictive Value: TP/(TP+FP)
****** Negative Predictive Value: TN/(TN+FN)
****** Accuracy (ACC): (TP+TN) / (P+N)
***** Minimization measures:
****** False Negative Rate (FNR): FN/P = FN/(TP+FN) = 1-TPR
****** Fallout or False Positive Rate (FPR): FP/N
****** False Discovery Rate: FP/(FP+TP)
** Pareto Optimality
*** Individual is Pareto if there is no other individual in the population that outperforms the individual on all objectives
*** The set of all Pareto individuals is the Pareto frontier
**** Trying to discover the true frontier
*** Drive selection by favoring Parato individuals
** Nondominated Sorting Genetic Algorithm II (NSGA II)
*** Separated into nondomination ranks
**** Rank 0, 1, 2, etc.
*** Individuals are selected using a binary tournament
**** Lower ranks beat higher ranks
**** Ties are broken by crowding distance
***** Summation of normalized Euclidian distances to all points within the front
***** Higher crowding distance wins
***** Want more diversity 
** Strength Pareto Evolutionary Algorithm 2 (SPEA1)
*** Each individual is given a strength based on how many others it dominates in the population
*** Also given a rank based on the sum of the strengths of the individuals that dominate it
**** Pareto individuals are nondominated and receive a rank of 0
*** A distance to the kth nearest neighbor is calculated and a fitness is obtained using this distance and the rank

=== Lab 2 Part 2 - Multi Objective Genetic Programming ===
* To lower the AUC of the problem, I simply changed the mutation function from mutUniform to mutNodeReplacement
** This lowered the AUC from <code>2.463792426733847</code> to <code>0.9798779140805436</code> which is roughly a 60% decrease
<gallery widths="400" heights="300">
files/Lab2Part2ResultsAY.png|
</gallery>

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Complete second half of Lab 2
|Complete
|September 2, 2020
|September 9, 2020
|September 8, 2020
|-
|Send email with self assessment
|Complete
|September 2, 2020
|September 2, 2020
|September 2, 2020
|}

== September 9, 2020 ==

=== Lecture Notes ===
* Introduced to Kaggle and the Titanic machine learning lab
* Also given our subteam groups to work with
** [https://vip.gatech.edu/wiki/index.php/Notebook_Maxim_Daniel_Geller Maxim Geller], [https://vip.gatech.edu/wiki/index.php/Notebook_Vincent_H_Huang Vicent Huang], [https://vip.gatech.edu/wiki/index.php/Notebook_Krithik_Thiyagarajan_Acharya Krithik Acharya]
* Assigned a self-graded assignment for notebooks
** [https://drive.google.com/file/d/1OkINrQa-OO3hvT8HfeybacBtVvAl-9RQ/view?usp=sharing Rubric]

=== Subteam Meeting ===
* Couldn't think of usefulness of "Name" and "Ticket" columns, so dropped them
* Looked at some jupyter notebooks on Kaggle for inspiration on what to do
* One mentioned that they created a new "Deck" column from "Cabin" because there was correlation between whether or not a passenger survived
* "Deck" essentially combined different cabins mostly based on what social class had rooms there
** Also created an "M" deck for passengers with unknown cabins

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Notebook Self Grading Assignment
|Complete
|September 9, 2020
|September 16, 2020
|September 15, 2020
|-
|Meet with subteam to discuss task
|Complete
|September 9, 2020
|September 16, 2020
|September 9, 2020
|-
|Experiment with the data and use Neural Networks to predict survivability
|Complete
|September 9, 2020
|September 16, 2020
|September 15, 2020
|}

== September 16, 2020 ==

=== Subteam Meeting ===
* Created a Github repo for project
* Reviewed MOGP from lab 2, and examined previous semester bootcamp groups to see what they had done
* Decided to use strongly typed primitives as they would give us more control over inputs and outputs
* Chose to use NGSA-II for selection

== September 23, 2020 ==
Final Presentation:  https://docs.google.com/presentation/d/1597u_k5FujjsxW72xXI3dQB0GkeUyko_JRNvBnZZu_U/edit#slide=id.g99b3431920_2_4

GitHub: https://github.com/maximgeller/Titanic-Multiple-Objectives

== September 30, 2020 ==

=== Lecture Notes ===
Download and set up EMADE

====== Switching python verisons: ======
* conda install python=3.6

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Set up EMADE on computer 
|Complete
|September 30, 2020
|October 7, 2020
|October 11, 2020
|}

== October 7, 2020 ==

=== Lecture Notes ===

=== Subteam Meeting - October 13 ===
* Set up EMADE worker process using Google Cloud to host the data

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Run EMADE
|Complete
|October 7, 2020
|October 14, 2020
|October 11, 2020
|}

== October 14, 2020 ==

=== Lecture Notes ===
* Checked in with each group to see how the EMADE process is going.

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Set up slides for presentation
|Complete
|October 13, 2020
|October 19, 2020
|October 18, 2020
|}

== October 19, 2020 ==

=== Presented EMADE findings ===
* Presentation link: 

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Submit sub-team preferences
|Complete
|October 19, 2020
|October 24, 2020
|October 23, 2020
|}

== October 26, 2020 ==

=== Lecture ===
* Assigned EzCGP team
* EzCGP team status:
** Reviewing code and want to implement paper 
*** Newly updated branch, working on issues

=== Subteam Meeting ===
* Introduced to team, set up next meeting for bootcamp students

=== Individual Work – October 29, 2020 ===
Instructions [https://github.com/ezCGP/ezCGP/wiki/New-VIP-Student-ToDo's here]
* Successfully cloned the repository to my computer
* Could not complete the next steps as I could not find the .git folder

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Set up Github repo for EzCGP
|Ran into issues
|October 26, 2020
|October 31, 2020
|Continued into next week
|}

== October 31, 2020 ==

=== Subteam Meeting Notes ===
Intro to Cartesian Genetic Programming (CGP) [https://docs.google.com/presentation/d/13COwMU_s9w_Jq8dbYY8YuYVM6Gxtdpb7TVCDkTAlWTs/edit?pli=1#slide=id.p (slides)]
* Directed Acyclic Graph (DAG)
** Used instead of the tree format
* Unique features:
** Reusable nodes
** Fixed length for "main" nodes
** Inactive nodes
*** up to fixed length can contribute to the algorithm
*** Mimics latent genes, silent phenotypes
*** Identify active vs inactive by working backwards from the output
*** Ignore inactive connections
** Implementation
*** One row, multiple columns for nodes
*** "1+4" evolutionary strategy
**** No mating
**** Mutation of parent creates four offspring
**** Mutate until an active node gets affected
Intro to EzCGP [https://docs.google.com/presentation/d/1HMyyie9ILQwdZ1e5jSMmVhDl_N0wOTZ8WpBCSRvk7-I/edit#slide=id.p (slides)]
* Custom DAG + Custom primitives and data types + Custom features?
* Represent DAG with a list of dictionaries
** One node is one element in the list
**One main node is a dictionary with keys "inputs", "args", and "function"
***"inputs" - list of indices that feed into the node
***"args" - list of indices in a list of arguments to use
***"function" - a function that takes in an array and an integer, and returns a list
*Intro to blocks
**Data preprocessing vs data classification primitives
***Classify as two separate genome structures, have two smaller genomes inside of a larger genome
***Sub-genomes referred to as blocks
**All individuals in a population have unique genetic codes but have the same concept of what an individual is
***Block Definition
***Individual Definition
***Definition vs Material: concept behind what a generic individual or block should look like vs what makes the individual or block unique.
*Universe - a Class
**The execution of the evolution fo a population given a set of rules for evolution
*The Problem - a Class
**User customizes the Universe
Resolved missing .git folder
* Was hidden because of a weird macOS quirk ([https://scriptingosx.com/2016/12/on-hidden-files-especially-library/ link])
** Keyboard Shortcut: cmd + shift + "."

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Finish setting up Github repo for ezCGP
|Complete
|October 26, 2020
|November 5, 2020
|November 1, 2020
|}

== November 2, 2020 ==

=== Main Meeting Notes ===
* Stocks team
** Integrated first years
** Added primitives to emade
** Troubleshooting errors
* EzCGP
** Research team met up to work on the code to implement a paper
*** Working on fixing bugs
** New students will work on code review and get caught up after main meeting
* NLP
** Getting new students caught up
** Working on coding, and meeting soft deadlines

* Modularity
** Got some results from alternative selection
*** Statistical significance achieved 
*** How to further improve
** Got new students set up, get them set up on colab

=== Team Meeting Notes ===
* Introduction to Neural Networks
** Rows are observations, Columns are ways to describe it + final column is a binary true/false is a Class (ten total)
** Linear Regression
*** XA + B = y
**** a<sub>0</sub>x<sub>0</sub> + ... + a<sub>9</sub>x<sub>9</sub> + b = y + error
*** A -> 10 x 1
**** linear transformation to change X to go from 500 x 10 to be 500 x 1
** A set of linear transformations + non linear functions
** layers -> each layer is a linear transformation + nonlinear function
** data -> ||| (3 layers)
** X<sub>input</sub>A<sub>0</sub> + B<sub>0</sub> = X<sub>0</sub>
** X<sub>0</sub>A<sub>1</sub> + B<sub>1</sub> = X<sub>1</sub>
** X<sub>1</sub>A<sub>2</sub> + B<sub>2</sub> = X<sub>2</sub>
* For example: take an image, want to recognize what's in the image
** Take chunks of the image, a few pixels at a time to make a kernel?
** Maps a number for each chunk of an image to a feature space
* We need tensorflow
** import tensorflow as tf
** tf.keras....
** Build our model/graph (DAG)
** Tensorflow uses graph, keras uses model
** input = tf.keras.Input_Layer(image_size = (500, 600, 3))
*** Returns a method ? 
*** First layer
** method = tf.keras.Conv2d(kernal = (3, 3))
** x = method(input) #x being the data
** Do more convulutions?
** y = tf.keras.Conv2d()(x)
*** Last layer
** tf.keras.Model(input, y)
* Slides ([https://docs.google.com/presentation/d/11UVq33iR0u-7ilrL90SifMdX0cb5ACwkpRD9braMko4/edit#slide=id.p link])

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Ask for help with test run
|Complete
|November 1, 2020
|November 5, 2020
|November 5, 2020
|}

== November 5, 2020 ==

=== Team Meeting Notes ===
* New team members: focus more on genetic programing/block stuff first, work on neural architecture on the side
** Run titanic dataset on ezCGP
*** Experiment, how fast they evolve, how quickly can we get competent individuals 
*** How does the evolutionary process change when we introduce block
*** Get familiar w the ezCGP process
** Connect emade datapair to process
*** Primitives emade uses?
**** Operator file
** *figuring out what first semester students can do based on what we have done*
** Features to features

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Ask for help with test run
|Complete
|November 5, 2020
|November 9, 2020
|
|}

== November 9, 2020 ==

=== Main Meeting Notes ===
* Stocks team
** Found inconsistencies in paper they're referencing
* EzCGP
** Worked on setting up PACE
** Figured out all issues and did some runs, got some results
** Fixed and updated some cv2 methods
* NLP
** Trying to figure out why they're getting an error
* Modularity
** First semesters ran through colab
** Compared runs to compare difference from benchmark run

=== Team Meeting ===
* Troubleshooting PACE-ICE
* Got codepairs for titanic working
* Look at Slack thread from earlier today for pACE-ICE info
** ssh someuser3@pace-ice.pace.gatech.edu
** Branch of 2020f, make new class of evaluate block
** [[Guide to Using PACE-ICE|vip link]] 

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Have PACE-ICE set up
|Complete
|November 9, 2020
|November 12, 2020
|November 10, 2020
|}

== November 12, 2020 ==

=== Team Meeting Notes ===
* Discuss final presentation details
* Troubleshooting various teammates' issues
* Want to finish research by Thanksgiving
* Assigned first semester students a task to get used to using PACE ([https://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/ MNIST])

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Look at MNIST instructions
|Complete
|November 12, 2020
|November 23, 2020
|November 14, 2020
|}

== November 16, 2020 ==

=== Main Meeting Notes ===
* Figuring out out to give midterm grades... Canvas?
* Keep detailed journals, want to makes sure you could replicate results
* Stocks
** Completed sanity checks, got results for all but one run
*** Contact author of paper to see why slight discrepancy 
* EzCGP
** Plan to write a script, and also figuring out pace-ice
* NLP
** Slow week, working on experimentation run to get results to compare
** No solution for GPU issue still
** Maxim Geller made a resource for how to use EMADE on PACE
* Modularity 
** Slow, week, still doing runs, worker error instructions, but seems fixable

=== Sub Team Meeting Notes ===
* Got seeding done
* Checked in with first semester students with assigned task ([https://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/ MNIST])

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Try downloading MNIST 
|Ran into some issues with Terminal
|November 12, 2020
|November 23, 2020
|Pushed to next week
|}

== November 19, 2020 ==

=== Team Meeting Notes ===
* Rodd found he has access to PACE-ICE
* Debated cutting dataset in half for faster runs
* Did seeding
* Moved on to presentation discussions 
** Want to make sure everyone in the team talks
** Divided up presentation sections and assigned everyone one part
** I am in charge of the introductions and the future tasks section

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Continue with MNIST
|In Progress
|November 12, 2020
|November 28, 2020
|November 19, 2020
|-
|Contribute to slides
|Completed
|November 19, 2020
|November 23, 2020
|November 22, 2020
|}

== November 23, 2020 ==

=== Main Meeting Notes ===
* Grades up to October are out on Canvas
** Reminder to self to keep notes on personal explorations too, not just meeting notes
* Stocks
** Went over weekend findings - what function the paper they're referencing was using to get the trendscore
** Contacted paper's author, no response
** Trained on S&P 500 data
* EzCGP
** prioritizing base runs now that EMADE and PACE are working
** got one 8 hr test run and visualized the individuals, plan to boot RAM
** working on presentation 
*** needs to be 20-25 mins, so go into more detail
** evolution is being "greedy"
*** unsure about exact meaning of this
*** seems to be because of a dramatic increase in individuals (20 -> 400)
* NLP
** focus on getting runs and results
** runs are split bwteen PACE and ICEHAMMER
** Asked questions related to feature extraction and signal methods
* Modularity
** datapair - got runs on ICEHAMMER
*** looks promising, but no statistical significance yet
**** sample size is 4

=== Sub Team Meeting Notes ===
* Need to work on slides
** assigned a rough estimate of how long each section should talk 
** stick to that time limit, under time is better than over time
** ask questions in slack as needed
** write speaker notes as well
** introducing self not whole team
** check with other slides to add more detail to the future tasks section
* Set up a meeting before next Monday 
** currently looking at Saturday since this is Thanksgiving week
* Make sure notebooks are up to date
** Refer to fellow sub team members' notebooks for comparison

=== Personal Exploration ===
* Finally started looking into the MNIST dataset using [http://rasbt.github.io/mlxtend/user_guide/data/loadlocal_mnist/ this site]
* Downloaded the files from [http://yann.lecun.com/exdb/mnist/ here] as using the commands given by the site above was causing issues (got warnings about a binary file for one of them)
* Used the command given to unzip the files
** gunzip t*-ubyte.gz
* Wasn't sure where to go from here, will revisit with a subteam member

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Ask for help with MNIST next steps
|Complete
|November 12, 2020
|November 28, 2020
|November 23, 2020
|}

== November 25, 2020 ==
Thanksgiving day, sub team meeting pushed to Nov 28

=== Personal Exploration - Nov 27 ===
* Met with [[Notebook Xufei Liu|Xufei Liu]] to discuss MNIST
* Copied the code from [http://rasbt.github.io/mlxtend/user_guide/data/loadlocal_mnist/ here] into a Python file
[[files/AY MNIST Code.png|alt=Image of the python code used|none|thumb|Image of the python code used]]
* Installed MNIST using a conda command ([https://anaconda.org/conda-forge/mnist source])
 conda install -c conda-forge mnist 
* Installed mlxtend using a pip command ([http://rasbt.github.io/mlxtend/installation/ source])
 pip install mlxtend
* After running the python file in terminal, I got the same results as the site
* Assisted Xufei with some troubleshooting (turned out she only downloaded one of the four files)

=== Sub Team Meeting - Nov 28 ===
* Discussed results and updated each other on the presentation progress
* [[Notebook Hoa Vinh Luu|Hoa Luu]] shared a PACE-ICE resource on notion ([https://www.notion.so/PACE-ICE-GPU-for-Ez-CGP-8be7a2e57c6649229f36505d093952dd link])
* Went through our rough outline slide by slide (not everyone was in meeting unfortunately) and checked in with the member(s) assigned to that slide
* Got some updates for Future Tasks, but requested to meet with Rodd after meeting to ensure my understanding of the slides was correct

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Get results from MNIST
|Complete
|November 12, 2020
|November 28, 2020
|November 27, 2020
|-
|Update speaker notes
|Complete
|November 28, 2020
|November 30, 2020
|November 29, 2020
|}

== December 2, 2020 ==

=== Presentation Notes: ===
*Slides: [https://docs.google.com/presentation/d/1cbx_daOsFvMZIgBQvVnmiJBmmm-Lvha61Mfe68Ej7c8/edit#slide=id.p link]
Dec 4: I took notes by hand, but forgot to upload them before notebooks were due
*Notebooks are due Dec 3 at midnight
*Stocks
**Discussed inconsistencies they saw with their research paper
**Went over primitives they were working with
**Plan to run stream to features on normalized data, test larger time ranges, and find new literature to work with
*NN
**Compared what they were doing to a research paper (accuracy vs AUROC)
**Developed a new mating/mutation function
**Worked on Wikidetox dataset and chest x-ray dataset
**Plan to do multi-task learning, find more datasets to work with, and implement more complex mutation scheme
*Modularity
**Ran four different experiments
**Using ARLs seems to be intuitively better, but they haven't seen results quite yet
**Currently using Titanic dataset, want to switch over to MNIST
***Performed some runs, seems to be slower than Titanic dataset and has trouble creating valid individuals 
**Plan to evaluate diversity over time and integrate ARLs and ADFs