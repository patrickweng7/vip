== Team Member ==
Team Member: Animesh Agrawal

Email: anagrawal@gatech.edu

Cell Phone: 404-543-3133

Interests: Machine Learning, Robotics, Python

==January 7, 2019 ==

=== '''Team Meeting Notes:''' ===
* Genetic Algorithms
** A process of mating and mutating individuals of a given population based on a certain fitness function such that after numerous repetitions of this process, the algorithm will produce the best individual whose fitness cannot get any better.
** Objective: a value that can be used to evaluate individuals and is a value that the evolutionary algorithm aims to maximize or minimize.
** Evaluation: a function that computes the objective of an individual.
** Fitness: A relative comparison to all of the other individuals in a given population.
** Selection: gives individuals with higher fitness a better chance to pass on their genes. Two forms of selection include:
*** Fitness Proportionate: probability of mating is proportionate to the individual's fitness value.
*** Tournament: conducts tournaments amongst individuals of a certain tournament size and selects the winners for mating
** Mate/Crossover: represents mating between individuals
** Mutate: introduces random changes to maintain diversity
** There are various evolutionary algorithms to create the best individual and follow the general flow of:
*** 1. Randomly initialize population
*** 2. Determine fitness of population
*** 3. Select parents from population (based on fitness)
*** 4. Perform crossover/mating amongst selected parents to form next generation population
*** 5. Perform mutation on next generation
*** 6. Repeat steps 2-5 until the best individual of the current population meets the requirements

=== '''Sub-Team Notes:''' ===
* N/A: No team assigned

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review class lecture slides with notes
|Completed
|January 7, 2019
|January 13, 2019
|January 13, 2019
|-
|Finish the first part of the DEAP Lab #1: One Max Problem
|Completed 
|January 7, 2019
|January 14, 2019
|January 14, 2019
|}
==January 14, 2019==

=== '''Team Meeting Notes:''' ===
* Genetic Programming
** Same as Genetic Algorithms except Mating and Mutation are different due to change in representation of genome of individuals
** Uses graphs: tree structure
** Nodes --Primitives (+,-,*,%)
** Leaves - Terminals
** Instead of evaluating genome, the genome tree itself is the function/evaluation
*** Ex: 1 + (3 * 4) = [+, 1, *, 3, 4]
** Mutations, many ways to mutate individuals: change primitive function of a node, randomly generate a tree and insert it into the genome tree, delete a tree and move its children up etc.
* DEAP
** Primitive function set
** Arity: number of inputs to a primitive function in the Primitive function set
* Symbolic Regression
** y = sin(x)
** Primitive set: +, -, *, /, !, exp (exponents)
** Terminals: x, constants

=== '''Sub-Team Notes:''' ===
* N/A: No team assigned

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
| Review class lecture slides with notes
| In Progress
| January 14, 2019
| January 21, 2019
| January 21
|-
| Finish the last part of the DEAP Lab #1 and first part of the DEAP Lab #2 
| In Progress
| January 14, 2019
| January 21, 2019
| January 28
|}

==January 28, 2019 ==

=== '''Team Meeting Notes:''' ===
* Multi-Objective Optimization
** Gene Pool: set of genome to be evaluated during the current generation\
*** Genome: Genotypic description of an individual analogous to DNA
*** Search Space: set of all possible genomes (Important to consider as it may affect how many generations we should run for and whether we need to add more primitives
** Evaluation: A function that takes in an individual (as a set of parameters in GA or a string in GP) and maps each individual to a set of scores based on objectives we want to maximize or minimize
*** Some examples of these scores include True Positives (TP) and False Positives (FP) 
*** Objectives: The set of measurements against which each individual is scored against 
*** Phenotype 
** Objective Space: Set of objectives
** Evaluation is essentially a function from the Search Space to the Objective Space (the phenotype description of each genome in the search space
** Classification Measures: Confusion Matrix: {| class="wikitable" ! !Predicted Positve !Predicted Negative |- |Actual Positve (P) |True Positive (TP) |False Negative (FN) |- |Actual Negative (N) |False Positive (FP) |True Negative (TN) |}
**Maximization Measures:
***Sensitivity/True Positive Rate (TPR): TPR = TP/P = TP/(TP + FN)
****Also known as hit rate or recall
***Specificity (SPC) or True Negative Rate (TNR): TNR = TN/N = TN/(TN + FP)
**Minimization Measures:
***False Negative Rate (FNR): FNR = FN/P = FN/(TP + FN) = 1 - TPR
***Fallout or False Positive Rate (FPR): FPR = FP/N = FP/(TN + FP) = 1 - TNR
**Other Measures:
***(Want to maximize) Precision of Positive Predictive Value (PPV): PPV = TP/(TP + FP)
***(Want to minimize) False Discovery Rate (FDR): FDR = FP/(TP + FP) = 1 - PPV
***(Want to maximize) Negative Predictive Value (NPV): NPV = TN/(TN + FN)
***(Want to maximize) Accuracy (ACC): ACC = (TP + TN)/(P + N) = (TP + TN)/(TP + TN + FP + FN)
**Fitness Computation
***Objective Space: Each individual is evaluated using objective functions including mean squared error, cost, complexity, TPR, TNR etc.
***The Objective score calculated for each individual can be used to map each individual to a point in the Objective Space (Phenotype of the individual)
**Pareto Optimality:
***An individual is Pareto if there is no other individual that outperforms it in '''all''' objectives
***The set of Pareto individuals forms the Pareto Frontier
***Selection favors Pareto individuals but is able to maintain diversity by giving all individuals some chance of mating
**Non-Dominated Sorting Genetic Algorithm II (NSGA ||)
***Population separated into non-domination ranks where the lower ranked non-domination frontier individuals dominate all individuals in higher ranked frontiers
***Individuals Selected using binary tournament
***Ties on any given front are broken using crowding distance
****Crowding Distance: Summation of normalized Euclidean distances to all points within the front
****The higher crowding distance individual wins the tie
**Strength Pareto Evolutionary Algorithm 2 (SPEA2)
***Each Individual possesses a strength ''S'': the number of individuals in the population the given individual dominates
***Each Individual possess a rank ''R'': R is the sum of the ''strength'' ''S'' 's of the individuals that dominate the given individual
***The distance to the k<sup>th</sup> nearest neighboring individual (σ<sup>k</sup>) is calculated to obtain fitness: R + 1/ (σ<sup>k</sup> + 2)
=== '''Sub-Team Notes:''' ===
* N/A: No team assigned

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review class lecture slides with notes
|Completed
|January 14, 2019
|January 28, 2019
|January 28, 2019
|-
|Finish the last part the DEAP Lab #2
|Completed 
|January 28, 2019
|February 4, 2019
|February 3, 2019
|}
==February 4, 2019 ==

=== '''Team Meeting Notes:''' ===
* Introduction Titanic Dataset and Kaggle's get started beginner Machine Learning competition
* Went over basic python code and useful libraries useful for data processing and training a model over dataset
* Discussed briefly topics of feature selection/extraction, model selections, data splitting with cross validation and hyper parameter tuning.
* Formed bootcamp subteam

=== '''Sub-Team Notes:''' ===
* N/A: No team assigned

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review example python code for Kaggle Competion
|Completed
|February 4, 2019
|February 11, 2019
|February 8, 2019
|-
|Enter Titanic Dataset Competition on Kaggle using any Machine Learning Mode
|Completed
|February 4, 2019
|February 11, 2019
|February 11, 2019
|-
|Meet with team, ensure co-dominance with all members and obtain Pareto front of each members Kaggle code
|Completed
|February 4, 2019
|February 11, 2019
|February 10, 2019
|}
__FORCETOC__
__NEWSECTIONLINK__
==February 8, 2019 ==

=== '''Team Meeting Notes:''' ===
(Boot Camp Sub-team Meeting - Group 6)
* Met with group to discuss Kaggle Titanic Data Set Competition assignment: Details of assignment, how to go about tackling the problem, individual responsibilities, and plan to complete all required tasks by Monday.
* Went over briefly how to improve our current programs, models and methods for data processing
* Discussed current Kaggle scores with those who had already processed the dataset,  trained a model, made predictions and submitted to Kaggle.
* Discussed ways to ensure we arrive at a co-dominant Pareto front and making sure we all split out training set the same way to avoid confusion and miscalculations in Pareto Front

=== '''Sub-Team Notes:''' ===
* N/A: No team assigned

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Assign tasks to each member and complete individual code to pre-process titanic data and use a machine learning model to make predictions
|Completed
|February 8, 2019
|February 9, 2019
|February 9, 2019
|-
|Make co-dominant Pareto front ensuring that each member is co-dominant with all other members
|Completed
|February 8, 2019
|February 10, 2019
|February 10, 2019
|}
__FORCETOC__
__NEWSECTIONLINK__
==February 10, 2019 ==

=== '''Team Meeting Notes:''' ===
* (Boot Camp Sub-team Meeting - Group 6)
* Met with bootcamp sub-team to discuss and modify certain individual's data processing, machine learning model, hyper parameters, amongst other things, such that each persons False Positive and False Negative scores were co-dominant.

=== '''Sub-Team Notes:''' ===
* N/A: No team assigned

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Obtain a Pareto optimum graph where each members scores are codominant with that of each other member
|Complete
|February 4, 2019
|February 11, 2019
|February 10, 2019
|}
__FORCETOC__
__NEWSECTIONLINK__
==February 11, 2019 ==

=== '''Team Meeting Notes:''' ===
* Introduction to applying GP methods to Titanic Dataset competition and went over sample (basic) python code with implementation of GP on Titanic Dataset
* Similar concepts used as LAB #2, with the same supporting libraries to make implementation much easier
* Moved on from using SKLearns python ML libraries and began discussing and working on Titanic Dataset problem using GP instead of ML keeping the Data Processing portion mostly untouched.
* Discussed with bootcamp sub-team possible meeting times, how to collaborate / go about working on group program in place of individual programs like last week.

==== Individual Work and Code for Titanic Dataset Competition: ====
* Data Processing
** Dropped names, passenger ID, Ticket and Cabin
** Dropped SibSp and Parch and extracted a new feature 'familySize' from these features that represents the size of any given individual's family on board the ship
** Extracted a new feature 'Title' from the names of each person such as "Mr.", "Mrs.", "Dr.", "Master" etc.
** Processed the age and fare features by imputing NaN values with median values and then splitting them into different bin ranges (split evenly into 5 bins for age and 4 for fare)
** Processed the embarked feature by imputing NaN values with mode values and then mapping each embark port to a unique integer
** Split data into training (66%) and cross validation (33%)

* Model Selection and Tuning
** Tested various SKLearn ML models and found that overall Support Vector Classifiers (SVC) and Random Forest Classifiers produced the best accuracy scores
** After tuning hyper-parameters on the cross-validation set on both models, found that both models had similar scores with SVC being slightly better
* Results:  
** Machine Learning Model: Support Vector Classifier (SVC)  
** Kaggle Score: 0.78947  
** Confusion Matrix: {| class="wikitable" |True Positive (TP): 80 |False Negative (FN): 24 |- |False Positive (FP): 22 |True Negative (TN): 169 |}

=== '''Sub-Team Notes:''' ===
* N/A: No team assigned

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review example GP python code for Kaggle Competion
|Completed
|February 11, 2019
|February 18, 2019
|February 18, 2019
|-
|Implement as a group a GP solution using basic primitives on Kaggle Competition (Titanic Dataset)
|Completed
|February 11, 2019
|February 18, 2019
|February 18, 2019
|}
__FORCETOC__
__NEWSECTIONLINK__
==February 15, 2019 ==

=== '''Team Meeting Notes:''' ===
(Boot Camp Sub-team Meeting - Group 6)
* Discussed progress individual members made in their attempts to implement GP to the Titanic Dataset problem on their own
* Found a means to collaborate and eventually submit one program/solution for our group implementation of GP by using Kaggle's Kernel collaborative features including a basic Version Control System integrated into their online platform
* Went over problems and confusions members had concerning how a GP implementation would work in a multi-objective setting with multiple features/genes/parameters
* Went over basic sudo-code to resolve common mistakes in writing the crucial evaluation function for the GP

==== Individual Work and Code for Titanic Dataset Competition with GP: ====
* Data Processing left untouched from previous code for Titanic Dataset Competition without GP
* Implemented a Strongly typed GP and ensured that the return value must be restricted to boolean values representing whether the sampled individual from the dataset survived or not
* Used basic primitives such as addition, subtraction, multiplication, logarithms, exponents, trigonometric functions etc.
* Created basic self defined function that simply convert float to boolean to allow GP to convert input float valued features and any intermediate calculated floats into an output that is of type boolean as restricted by the strongly typed GP
* Using same split as last time for training and cross-validation set, implemented evaluation function by compiling individual GP tree into a function to make predictions on train data. Then calculated false positive and false negative values by comparing individual's predictions on training data to actual truth values of each sample in train data.
* The evaluation function then returns the false positive and false negative as a tuple
* Maintained mutation and mating function from LAB #2 only changing the selection method from tournament selection to selSPEA2 from tools in the DEAP library
* Maintained for a large part the same code as learnt in part 2 of LAB #2 regarding multi-objective GP for the remainder of the program
Results:
[[files/Objective Space with initial randomly generated population.png|center|thumb|The blue point is the given individual we set aside and compared all the other individuals to. The black points are uncomparable, the green points are dominated by the given individual, and the red points dominate the given individual.|600x600px]]
[[files/Fitness vs. Generation Graph.png|center|thumb|In the graph above, the orange and red plots represent tree size and the blue and green plots represent MSE.|600x600px]]
[[files/Final Generation Pareto Front Curve with AUC.png|center|thumb|Non-dominated individuals from the final generation are plotted along the Pareto frontier and the Area Under the Curve (AUC) is calculated below.|600x600px]]In the end I kept arriving at the same value for the best fitness of (0, 238) (False Positive, False Negative) and arrived at the conclusion that GP with basic primitives was simply not as effective as using a single optimized ML model as programmed last time. However, there was likely significant area for improvement in my GP implementation for the Titanic Dataset Competition possible by optimizing various sections of the code such as choosing different probabilities, running for more generations, changing mating and mutation function, modifying my evaluation function, adding other primitives etc.

=== '''Sub-Team Notes:''' ===
* N/A: No team assigned

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Continue to implement and improve as a group a GP solution using basic primitives on Kaggle Competition (Titanic Dataset)
|Completed
|February 15, 2019
|February 18, 2019
|February 18, 2019
|}
__FORCETOC__
__NEWSECTIONLINK__
==February 18, 2019 ==

=== '''Team Meeting Notes:''' ===
* Introduction to EMADE and its installation
* End Goal: Use EMADE's software to implement more successful and advanced GP's by using ML models as primitives in place of the more basic mathematical operations
* Began installation process

=== '''Sub-Team Notes:''' ===
* N/A: No team assigned

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install and try running EMADE
|Completed
|February 18, 2019
|February 25, 2019
|February 24, 2019
|}
__FORCETOC__
__NEWSECTIONLINK__
==February 25, 2019 - March 1st, 2019 ==

=== '''Team Meeting Notes:''' ===
* Setup mySQL Database and made appropriate changes to input_titanic.xml file to link the two together.
* Set reuse variable to 1 so the EMADE stores results from current generation to continue running further generation next time EMADE is started.
* Changed 5-Fold data set from default to Sean's processed dataset using same 5-Folds. Seans processing allowed him to achieve the lowest almost 0/0,  FP/FN score, but due to high levels of overfitting, his kaggle score was not as high as mine which had a higher FP/FN score but generalized better. Since our objective was to minimize the FP/FN scores, we decided to use his preprocessed data instead of the default data (more information on how he processed his raw data can be found in his [[Notebook Seungil Kim|notebook]] under the March 1st entry). I decided to store his preprocessed data in the following directory within my local EMADE repo: datasets/titanic/ourVersion.
* Minor changes to worker / host and GPU settings.
* Rest of the settings, objectives and file were left to default values.
* My mySQL database was set-up as the master database.
* Our bootcamp subteam met on March 1st again to have other teammates connect to my master database, however, my database ran into a few issues and so we decided to make Sruthi's database the master database
** More specifically, I had lost authorization to my own SQL database whilst trying to change my user account's password to something more basic.
* After going to help desk and trying to set up Sruthi's database as master, we ran into more problems regarding getting a master-worker connection to her computer's database even over eduroam network despite all of us turning off our firewalls (we couldn't even ping her ip address). Luckily, I managed to fix my database by resetting it and setting up my account with the right password. We were then able to have most members connect to my database successfully as workers.
[[files/Input titanic.xml.png|center|thumb|My input_titanic.xml file after setting up mySQL database and changing dataset to custom processed K-Fold dataset.]]

=== '''Sub-Team Notes:''' ===
* N/A: No team assigned

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Setup mySQL Database and input_titanic.xml file
|Completed
|February 25th, 2019
|March 4th, 2019
|March 3rd, 2019
|-
|Meet with boot camp sub-team to have rest of the members connect to master and run EMADE as workers 
|Completed
|February 25th, 2019
|March 4th, 2019
|March 4th, 2019
|}

== March 4th-9th, 2019 ==

=== '''Team Meeting Notes:''' ===
* Discussed how we should go about making our midterm presentation and what main objectives to include
* Began collecting results using EMADE and having as many members connect to my master database as workers
* Initially we struggled to understand EMADE outputs through terminal and the output files and so we ended up running several runs of EMADE (not continuous) and so we took the opportunity to visualize our EMADE results by taking part in the user study being conducted by the EMADE Viz Team
* After meeting with the EMADE Viz Team we were able to acquire great and accurate visualisations of our EMADE runs with relative ease and were able to better analyze our results. Most importantly, we were also able to look at a graphical visualisation of some of our pareto optimum individuals at any given generation!
* In particular, I went through the EMADE Viz software visualisations and noticed several issues with our runs of EMADE (leading up to the presentation day, we weren't aware that the worker databases also had access to the same data in my database and so other members didn't know that they could also use the EMADE Viz software to analyze our results). Most notably, it seemed as though some individuals had never finished evaluating or were never queued to be evaluated from previous generations to the one being run by EMADE currently. This was something that after discussing with the EMADE Viz team led me to believe that most likely there were some issues with the way in which EMADE was accessing and interacting with the SQL database. The other notable issue seemed to be the fact that hypervolume calculations seemed to be extremely unreliable and volatile (often hypervolume data would even dramatically increase across generations).
* A lot of our results and visualisations can be found on our [[Bootcamp Sub-team Spring 2019 - Titanic EMADE#Group 6|Bootcamp Subteam Page for the EMADE results section]] (Group 6).
=== '''Sub-Team Notes:''' ===
* N/A: No team assigned

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Make Midterm presentation
|Completed
|March 4th, 2019
|March 10th, 2019
|March 10th, 2019
|-
|Run EMADE to get best individual's FP/FN scores on titanic dataset 
|Completed
|March 4th, 2019
|March 10th, 2019
|March 10th, 2019
|}
__FORCETOC__
__NEWSECTIONLINK__
== March 11th, 2019 ==

=== '''Team Meeting Notes:''' ===
* Gave our [https://docs.google.com/presentation/d/1qOIhmgg9fOdSk5NohSP1jzdBPoqsWjGenV8YBT41PGc/edit#slide=id.p presentation] group to showcase our work over the first half of the semester learning about EMADE and Genetic Programming during our bootcamp.
* All of the main research subteams presented their midterm presentations to give first semester students a quick recap of what each subteam is about, why we should consider joining their subteam and what progress they have made so far this semester.
__FORCETOC__
__NEWSECTIONLINK__
== March 18th, 2019 ==

=== Spring Break ===
__FORCETOC__
__NEWSECTIONLINK__
== March 25th, 2019 ==

=== '''Team Meeting Notes:''' ===
* Got assigned to the Deep team.
* Due to the fact that the Deep team doesn't really work directly with EMADE and is developing its own software library ezCGP that uses a different type of Genetic Programming, Cartesian Genetic Programming, new members were introduced to the current team members, what they are currently working on and have worked on, and followed along with the presentation given by Mr.Rodd on how the ezCGP framework in particular works and the differences between Cartesian Genetic Programming and just regular Genetic Programming that we had just learned in the bootcamp.
=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Go over Mr.Rodd's presentation to better understand the ezCGP framework and methodology
|Completed
|March 25th
|April 1st, 2019
|March 31st, 2019
|-
|Begin to clone the ezCGP repository and play around with the problem.py file in the Symbolic Regression branch to get a practical understanding of the current code base and ezCGP framework.
|Completed
|March 25th
|April 1st, 2019
|March 31st, 2019
|}
__FORCETOC__
__NEWSECTIONLINK__
== April 1st, 2019 ==

=== '''Team Meeting Notes:''' ===
* Were introduced in more depths to the two subteams that had been recently formed by the former members. Team A would focus on making the ezCGP framework easier to use and more user friendly and team B would work on adding more primitives and extend the framework to solve regression problems as well.
* First-semester students continued to play around with the Symbolic Regression branch to better understand the ezCGP framework and Mr.Rodd went over the practical aspect of writing code in tensorflow and the general tensorflow framework for building neural nets in python briefly. We went over tensorflow because the ezCGP framework uses different neural network layers and model as genes and thus an understanding of tensorflow is essential to better understand the ezCGP codebase.
* The problem.py file essentially uses the ezCGP framework to form a model for y=1/x using the taylor series expansion using basic primitive operators such addition. Since mating is rather difficult to implement in CGP and is still being researched, ezCGP relies mostly on mutations to create new individuals in each generation. The problem.py file uses a subset of various mutation functions that have been defined in the mutate_methods.py file.
* I looked through some of these methods and also briefly traced some of the other files such as operators.py that contained all of the basic primitives such as addition and multiplication and just ran the main.py file and played around with the selection of mutation methods and basic primitives available to ezCGP.

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Continue learning about ezCGP through SymbRegression branch
|Completed
|April 1st, 2019
|April 8th, 2019
|April 5th, 2019
|-
|Select a subteam to join
|Completed
|April 1st, 2019
|April 8th, 2019
|April 1st, 2019
|}
__FORCETOC__
__NEWSECTIONLINK__
== April 8th-11th, 2019 ==

=== '''Team Meeting Notes:''' ===
* Caught up on progress made by each of the subteams so far and explained to Aniruddha the progress made by the first-semester students in exploring the SymbRegression branch and understanding the ezCGP framework as a whole.
* Were tasked with a bitesize issue to fix in the tensorflow_nn branch to introduce us to the codebase and get us started with contributing to the ezCGP library. I quickly came up with a solution to the problem and pushed my changes to the main repository. In particular, the issue was concerning the fact that the number of epochs run wasn't easily configurable from problem.py file where many of the parameters and configurations for ezCGP are setup. I resolved this issue by first adding a parameter n_epochs to the problem.py skeleton block dictionary. Next, I worked on integrating this new parameter into the blocks.py file which would run the skeleton block. Specifically, I made changes to include this new parameter in the constructor and integrate it into the tensorblock_evaluate function that runs the evolutionary process using tensorflow.
* In my original push, I made a small error of not referring to my new n_epochs variable in the blocks object as '''self.'''n_epochs and thus, the tensorblock_evaluate wasn't using the same variable setup in the constructor and in the problem.py skeleton_block. This minor error was fixed by Aniruddha.

=== Sub-Team Meeting Notes (11th April): ===
* Normally I have a conflict during the sub-team meeting times on Thursday 4:30pm, but I decided to attend the VIP subteam meetings and catch up on class notes afterwards. I didn't attend the previous sub-team meetings since joining the Deep team because I had an in class midterm the week before and so I had to attend the previous two weeks' Thursday class.
* I decided to join team A to get a more practical understanding of the ezCGP framework in anticipation for work to be done in the Fall semester and so I worked on team A's task for the week which was to allow the ezCGP framework to be scalable and to be able to handle large data sets that cannot be held in one file in memory.
* We implemented this feature by first adding another parameter large_dataset to problem.py's skeleton_block dictionary that should be passed in a tuple of consisting of a list of filenames and a function that can be used to read the data contained within each of the files. This parameter's default value is '''None''' as it's assumed that most of the time the user is able to manage using just 1 file with all of the data and would pass in values for large_dataset if required. [[files/New large dataset parameter.png|center|thumb|700x700px|An example of passing in the cifar-10 dataset across 3 files with the get_cifar10_batch method that is able to read the data from these files.]]
* The other major changes involved with our implementation was then changing how the tensorflowblock_evaluate code worked by creating and if else clause that checks to see if the user has passed in a large dataset or not. If large_dataset is set to '''None''', then we run the original code, but otherwise we the add an additional loop to go through each of the files and read each files data using the provided function as shown below:[[files/Tensorflowblock evaluate function.png|center|thumb|700x700px|Our implementation for reading large datasets in the else clause when a large data set is in fact passed in by the user. The highlight lines indicate the additional loop that accounts for each of the files and reads them using the inputted function.]]
*Unfortunately, the way ezCGP is set up in its code, when a user does decide to input a large dataset, they must still pass in data for the x_train and y_train variables even though doing so is uneccessary and the data passed into these variables would go unused. This is because, ezCGP uses the data passed into these variables to create placeholders when creating its tensorflow models and so passing in '''None''' to these variables would result in an error as '''None''' doesn't have valid dimensions to create placeholders with. Thus, we decided to come up with a quick fix by creating a method that creates dummy data (a numpy array of 0s) with the correct dimensions to allow ezCGP to create proper placeholders for its tensorflow models and not crash.
[[files/Dummy method.png|center|thumb|700x700px|Creates dummy data so that user doesn't have to supply redundant data for x_train and y_train variables when using the large dataset feature.]]

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add a new parameter in the problem.py to allow users to more easily set the number of epochs run
|Completed
|April 8th, 2019
|April 15th, 2019
|April 8th, 2019
|-
|Implement a new feature that allows ezCGP to handle large datasets in the tensorflow_nn branch
|Completed
|April 8th, 2019
|April 15th, 2019
|April 11th, 2019
|}
__FORCETOC__
__NEWSECTIONLINK__
== April 15th-21st, 2019 ==

=== '''Team Meeting Notes:''' ===
* First-semester students were tasked with testing and training the best individuals from the ezCGP runs done by other members for the MNIST, CIFAR-10 and CIFAR-100 datasets.
* Learned how about Google Collab as a platform to run python tensorflow models (while collaborating with multiple members and being able to use a GPU runtime environment to speed up runtime for large number of epochs)
* We were given access to Aniruddha's previous code from when he was learning to use google collab and testing individuals obtained from ezCGP as reference point.

=== Sub-Team Meeting Notes (18th April): ===
* While I wasn't able to successfully complete the assigned task in google collab yet, I managed to trace through the code Aniruddha provided as a python notebook.
* With help from Aniruddha, Mohan (another first-semester student) and I were able to start a google collab notebook and make a tensorflow model for the best individual from the MNIST run in ezCGP. We essentially setup the layers as described in the terminal output from ezCGP and used a lot of the functions defined in operators.py to form our tensorflow layers. With Aniruddha's previous code, we were also able to train the best individual's neural network model and obtain a training and testing accuracies and obtain the following visualisation of our results:
[[files/MNIST Best Individual Neural Network Training and Testing Accuracies.png|center|thumb|700x700px|Testing and Training accuracies obtained during and after training the neural network model of the best individual from ezCGP's run on MNIST data set.]]
* Here is a link to the code I wrote with Mohan's help and by referencing Aniruddha's google collab notebook: https://colab.research.google.com/drive/1yyQ4x4iO1cy9NR5qhk8oaoygt7zdjLC3

=== Individual Work (20th-21st April): ===
* With our final project and class meeting fast approaching, I helped out by training the neural network models and obtaining similar learning curve graphs for some of the best individuals obtained for the CIFAR-100 dataset. I also ran some of the individuals with and without image augmentation to see if we could obtain a better testing accuracy by creating a more varied dataset to train the model.
* The following graph is the learning curve for the best individual's neural network from the ezCGP run on the CIFAR-100 dataset without image augmentation ([https://colab.research.google.com/drive/1zwTF8UEiLvRCXsvXAqDxvOwgJtNe_r5f link to code]):
[[files/Best Individual CIFAR-100 Learning Curve without image augmentation.png|center|thumb|700x700px|Learning curve for the Best Individual obtained from ezCGP's run on the CIFAR-100 dataset without image augmentation.]]
* The following graph is the learning curve for the best individual's neural network from the ezCGP run on the CIFAR-100 dataset '''with''' image augmentation:
[[files/Learning Curve for Best Individual with Image Augmentation.png|center|thumb|700x700px|Learning curve for the Best Individual obtained from ezCGP's run on the CIFAR-100 dataset with image augmentation.]]
* The following graph is the learning curve for another individual obtained from ezCGP's run on the CIFAR-100 dataset that seemed to have potential to have a good testing accuracy with image augmentation ([https://colab.research.google.com/drive/1pSbh-V9JQP4O-2n9d7jBvkvLSJ8C2M_j link to code]):
[[files/Learning curve for a pareto optimum individual from the ezCGP run on CIFAR-100.png|center|thumb|818x818px|Learning curve for a pareto optimum individual from the ezCGP run on CIFAR-100 with image augmentation applied onto the training data of the neural network model.]]

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Produce learning curve visuals for various individuals obtained from ezCGP runs on the MNIST, CIFAR-10 and CIFAR-100 datasets
|Completed
|April 15th, 2019
|April 21st, 2019
|April 20th, 2019
|}
__FORCETOC__
__NEWSECTIONLINK__
== April 22nd, 2019 ==

=== '''Team Meeting Notes:''' ===
* Give [https://docs.google.com/presentation/d/10t_-9TvkV_GwWpHTPa6wG7tWbhio_NAjP8u-f-bfWqE/edit#slide=id.p final end of semester presentation] to entire Automated Algorithms VIP team

== August 26th, 2019 ==

=== Team Meeting Notes: ===
* Caught up on missed class from previous week
* Due to time conflicts, new subteams had already been formed with most members already having joined one of the subteams
* Got to observe and go around looking at some of the new topics of teams in need of more members
* Joined the Bloat subteam

=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Begin reading neat GP paper
|Complete
|August 26, 2019
|August 30, 2019
|August 29, 2019
|}
== August 30th - September 9th, 2019 ==

=== Individual Work: ===
* Begun reading up on research papers on theories explaining Bloat and ways to go about reducing its impact on efficiency
* Catching up on team meeting discussed papers as I was unable to make it to this weeks subteam meeting
* No meeting next week due to long weekend
* neat-GP paper notes:
** Most popular definition for bloat is an increase in program size that doesn't correspond to an increase in performance
** Crossover-Bias theory is one of many that attempts to explain the formation of bloat evolution encourages bloat to appear as it punishes smaller individuals more than larger individuals for a given performance
** Fitness-causes-bloat-theory (FCBT) is another explanation formation of bloat which largely states that there are many solutions for a given fitness and that there are many more large solutions with bloat than the most efficient bloat free solution and so GP tends towards larger program sizes as there is a higher chance to encounter an optimal fitness value.
** neat-GP is a bloat control method based on:
*** NeuroEvolution of Augmenting Topologies (NEAT) a heuristic designed specifically for evolution involving neural networks
**** Uses speciation to protect novel solution topologies (favors new tree structures or individuals with unique ordering of nodes) and promote complexity across generations (favor more complex individuals and solutions as more generations have passed)
*** Flat Operator Equalization that shapes program size distributions to be even across generation (uniform distribution)
**neat-GP-SC is a variant of proposed neat-GP that uses subtree crossover instead of NEAT-Crossover (for neural nets)
**The results on standard symbolic regression datasets shows that neat-GP-SC was the only bloat control GP evolution that maintained or even bettered the optimal fitness values from regular GP while having a lower average tree size (an indication of less bloat)
**The results on classification however, show that both neat-GP and neat-GP-SC gave similar performances to regular GP but actually had larger average program sizes. Only full neat-GP (using the specialized NEAT-based crossover) method was able to maintain similar performances while significantly reducing average program size.
=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finish reading neat GP paper
|Complete
|August 30, 2019
|September 9, 2019
|September 9, 2019
|}
== September 9th - 15th, 2019 ==

=== Individual Work: ===
* Finished reading neat GP paper (notes can be found above)
* Assigned work on creating more advanced individual based bloat metrics/quantification in DEAP completely separate from EMADE for ease of implementation, exploration and testing.
* Current ideas to approach task:
** Create a hash method or data structure to check for nodes that aren't affecting their inputs and may be considered as bloat
** Varying inputs to subtrees or nodes of individual and checking for constant value suggesting that the root node or entire subtree may be considered to be bloat
*After meeting with team on Friday (September 13th), decided to focus on the latter approach to check if the subtree/nodes of an individual were constant as an identifier of bloat and to test it on the Titanic dataset using DEAP GP
*ADF team member met with me during our team meeting to discuss the issue of being able to effectively parse the subtrees of a given Primitive Tree object of an individual in DEAP
*Dr.Zutty suggested to check out the searchSubtree method in DEAP's framework to help approach this problem.

=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Begin initial implementation of more advanced individual bloat metrics
|Complete
|September 9, 2019
|September 16, 2019
|September 15, 2019
|-
|Review neat GP paper as necessary
|Complete
|September 9, 2019
|September 16, 2019
|September 15, 2019
|}
== September 16th - 22nd, 2019 ==

=== Individual Work: ===
* Implemented preprocessing of data from Kaggle's Titanic dataset and DEAP GP with simple primitives that obtains an accuracy of approximately 81.7% after 50 generations:[[files/GP Evolution with Basic Primitives on Titanic Dataset.png|center|thumb|750x750px|Basic GP evolution run for 50 generations with very basic primitives on the Titanic Dataset. Best individual achieved approximately 81.7% accuracy on training data and is displayed at the very bottom.]]
* Working with best individual from last generation to quantify the amount of bloat it contains.
* Need to find a way to manipulate or easily parse/access any subtree of a given individual given what the root node of that subtree is
* Begun to look into the searchSubtree method in more detail on [https://github.com/DEAP/deap/blob/master/deap/gp.py DEAP's Github] and find a way to translate the python slice object it returns into the list of nodes that form the given subtree that can then be used to create a Primitive Tree object for that given subtree
* After tracing the code for the structure of a Primitive Tree object in DEAP, found out that a Primitive Tree is just a list of Primitive objects that are each of the nodes in that individual's tree. This simple python code takes the slice object from the searchSubtree function and effectively create a Primitive Tree object for every nodes' subtree:
*[[files/Parsing Subtrees.png|center|thumb|Takes a Primitive Tree object and creates a list of every subtree of the original object as their own Primitive Tree objects.]]

=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Implement a basic DEAP GP algorithm on Titanic dataset problem by Kaggle on basic primitives (including processing of input data)
|Complete
|September 16, 2019
|September 23, 2019
|September 22, 2019
|-
|Work on quantifying bloat from some of the pareto optimum individuals (be able to quickly parse subtrees of any given individual)
|Complete
|September 16, 2019
|September 23, 2019
|September 22, 2019
|}
== September 23rd - 29th, 2019 ==

=== Individual Work: ===
* Finally resolved issue of parsing subtrees of an individual with some help from ADF team members and Dr.Zutty using the slice objects returned by DEAPs searchSubtree method
* Now able to generate every subtree of a given individual and store each subtree as a Primitive Tree object type that can then be compiled by DEAPs toolbox object
* Working to run each subtree as a compiled function on the processed training data to check for constant outputs
* After resolving some issues with being able to run each subtree as a function on all of the training data, I was able to get results for each subtree and can start using the data returned to generate bloat metrics for the best individual and hopefully repeat for all of the pareto optimum individuals from the last generation.
* Here is some of the code that simply prints out the results of each of the subtrees on the training data[[files/Running each Subtree of the Best Individual on the Processed Training Data.png|center|thumb|500x500px]]

=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run each subtree on processed training data as input and check for constant outputs to determine bloat
|Complete
|September 23, 2019
|September 30, 2019
|September 29, 2019
|}
== September 3oth - October 6th, 2019 ==

=== Individual Work: ===
* Need to now be able to parse through outputs on processed training data from every possible subtree of a given individual and determine if that node or its subtree can be considered to be bloat.
* Challenges:
** How do we determine if the root node of the subtree or the subtree is to be considered as bloat given constant output from all inputs
** What if the root node just outputs one of its inputs (doesn't have to output a constant)? Need to check for this case so that only root node is marked as bloat whenever this is the case.
** Time complexity of running each subtree as a function on training data and then determining bloat
*Measuring scale for bloat: percentage of nodes of individual's tree that our quantifier considers to be bloat

=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Work to finish advanced bloat quantification if possible and check results on pareto optimum individuals from Titanic DEAP GP
|Complete
|September 30, 2019
|October 13, 2019
|October 11, 2019
|}

== October 7th - 13th, 2019 ==

=== Individual Work: ===
* Setup first implementation of individual level bloat quantifiers using the outputs generated from each of the subtrees of an individual as shown in the September 23rd - 29th above and the criteria described in entry directly above. 
* For nodes that output constant output from a given subtree of an individual, the script checks the output on each training example in the training dataset and checks for variation in output as shown below:[[files/Constant Output Subtree Bloat Detection.png|center|thumb|750x750px|Python code to detect subtrees that are bloat because they output constant output over the entire training data input.]]
* For nodes that simply passes one of their descendant's output as output over the entire training dataset were detected using the following code:[[files/Passing Descendant Output Bloat.png|center|thumb|750x750px|Code for detecting nodes that simply pass one of their descendants' outputs as output over the entire training dataset]]
* Finally, the function counts the number of nodes that weren't bloat by iterating over the isBloat array and returning as a percentage the number of nodes that '''aren't''' considered to be bloat (based on the two criteria described above).
* Next step is to verify that bloat detection works as intended and analyse bloat found in the pareto optimum individuals from the DEAP GP run on the Titanic dataset.

=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Work to finish advanced bloat quantification if possible and check results on pareto optimum individuals from Titanic DEAP GP
|Complete
|September 30, 2019
|October 13, 2019
|October 11, 2019
|}

== October 14th - 20th, 2019 ==

=== Individual Work: ===
* Ran individual bloat metric on the 10 highest accuracy individuals and analysed results.
* Bloat detection seems to be working well and accurately.
* Almost no node with constant output was found. Rather there were many nodes that were inefficient and bloat because they were simply passing one of their direct or indirect inputs (output from a descendent) as output.
* Noticed that most of the cases of bloat occurrences were due to the min and max primitives which would always output one of their inputs that's always less than or more than all of the other inputs
* Here is an example of an individual and the subtrees whose root node is bloat:
 sin(sub(x2, protectedDiv(protectedDiv(x1, min(cos(cos(x6)), x4)), protectedDiv(protectedDiv(min(min(sub(neg(cos(protectedDiv(min(min(protectedDiv(x2, mul(sin(protectedDiv(x7, x2)), cos(x4))), x7), x4), min(neg(x8), x2)))), sub(x8, x7)), x7), x4), cos(protectedDiv(x5, x1))), x2))))

 Score: 95.65217391304348 percent

 Bloat Nodes:
  
 14 :  min(sub(neg(cos(protectedDiv(min(min(protectedDiv(x2, mul(sin(protectedDiv(x7, x2)), cos(x4))), x7), x4), min(neg(x8), x2)))), sub(x8, x7)), x7)
  
 32 :  min(neg(x8), x2)
  
* Note that the score of 95.65217391304348%  represents the number of nodes that '''aren't''' considered to be bloat by the individual level bloat quantifier.
* Worked on and finalised midterm [https://docs.google.com/presentation/d/1cJccFW1RC8qtmwskcuaJ-xSwl0Heyv_IwihF0YMfo4k/edit#slide=id.g6407a4f5c6_0_5 presentation] for October 21st.

=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Analyse Individual Level Bloat Quantification and work on midterm presentation
|Complete
|October 14, 2019
|October 20, 2019
|October 20, 2019
|}

== October 28th - November 4th, 2019 ==

=== Individual Work: ===
* Started to looking into integrating individual level bloat quantification into EMADE
* Goal for EMADE implementation is to run a EMADE GP run on the titanic dataset, take the individuals from the final generation, run individual level bloat quantification on each individual and finally also 'clean' each tree using the detected bloat nodes and removing them from the tree. This 'cleaned' tree would then be fed the training dataset as input to check that its output matches its output before all of its bloat nodes were removed. In theory, based on the criteria for detecting bloat in an individual, the output of this 'cleaned' tree should match exactly with its output before 'cleaning'.
* Started looking into Eric Frankel's fork of EMADE on the detection processing branch

=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Begin integration of individual level bloat quantification into EMADE and peruse EMADE codebase again to refamiliarize myself
|Complete
|October 28, 2019
|November 4, 2019
|November 4, 2019
|}

== November 5th - 11th, 2019 ==

=== Individual Work: ===
* When trying to run EMADE on the titanic dataset ran into a lot of problems with conda environment setup and various libraries not being installed
* Spent majority of the week to resolve these errors as it wouldn't be possible to write a custom python file within GPFramework that pulls information about the pset and dataset from the inputted xml file
** Solution: run conda uninstall package_name, package2_name, ... for all of the packages that are causing the conda environment to be 'inconsistent'. Then iteratively install using pip3 or conda all of the packages that cause launchEMADE.py to not execute properly. This error seems to stem from the installation of varying versions of certain packages that may not even be necessary to run EMADE.
*Read how launchEMADE.py is able to parse input xml file and pull necessary parameters
*After discussing with the team about current goals, decided to make a shift in goals to attempt to do an EMADE GP run on the Titanic dataset that 'cleans' all of the individuals in the current generation every 5 generations or so,  whilst collecting data on population bloat scores, pareto optimum individual level bloat scores and accuracies for each individual.

=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Resolve conda environment errors preventing EMADE run on the titanic dataset
|Complete
|November 5, 2019
|November 11, 2019
|November 11, 2019
|-
|Continue to peruse EMADE database and look into how to go about integrating individual level bloat quantification
|Complete
|November 5, 2019
|November 18, 2019
|November 18, 2019
|}

== November 12th - 18th, 2019 ==

=== Individual Work: ===
* After analysing and looking into the logistics of doing an EMADE GP run that cleans all individuals every 5 generations and after discussing extensively with team, came to the conclusion that it would be unfeasible as in my current implementation of individual level bloat quantification, EMADE would be required to iterate over each individual, generate each of its subtrees, calculate raw outputs of each subtree on the entire training dataset and then analyse outputs of all subtrees of an individual to generate a score and 'clean' all of its bloat nodes. The runtime would be far too high and would entirely defeat the purpose of removing bloat as it takes much more processing power to remove bloat nodes than the increase in efficiency for removing that bloat in the first place. 
* Interacted with conflict semester students to better understand how they were progressing and using cache table hash values and after discussing with team, decided to switch to using cache table hash values for individual level bloat analysis rather than recalculating outputs of each of the subtrees.  
* However, it was pointed out to me that the cache table may not necessarily store the hash and output of every subtree (stored as a method in the cache table) of every individual in any given generation. After discussing with Austin, I found out that by setting the timeThreshold in the input xml file to 0, the cache table would be forced to store the hash values of every subtree/method and may crash or run out of memory on disk depending on the dataset EMADE is running on and for how long. 
* Assigned Esther (first semester student who just joined the team) to test if the cache table did in fact store each of the subtrees of every individual with given time threshold parameter set to 0. 

=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look into adapting individual level bloat metric to just compare hash values to detect bloat and how to, during an EMADE run, analyse all of the individuals in the database and replace them with some 'cleaned' individual
|Incomplete (Dropped)
|November 12, 2019
|November 19, 2019
|
|}

== November 19th - 25th, 2019 ==

=== Individual Work: ===
* Due to time constraints and lack of sufficient knowledge to effectively interact with the Database during an EMADE run to be able to analyse bloat in the entire population on an individual level using the cache table, after discussing with the team, I decided to go back to my previous goal of simply running bloat analysis and 'cleaning' trees at end of an EMADE run on the final generation. Furthermore, since Esther ran into some issues with getting the caching table to work on her EMADE titanic run, we decided to also go back to the original idea of manually calculating the outputs of every subtree in every individual in the final generation and letting the algorithm take its time as it serves its purpose of making final generation individuals more efficient and gives a measure for the bloat in a given run (and thus can be use later down the line to score bloat removal/ control methods that we develop).
* Started implementing a custom script that is able to pull information from an xml file and a .csv file of the individuals table exported after running EMADE. Looked at launchEMADE.py, standalone_tree_evaluator.py and EMADE.py to get the primitive set and create an instance of emade to be able to generate and compile primitive trees of individuals pulled from the .csv file.
* Started adapting individual level bloat quantification code for EMADE.

=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create a python script that gets individuals from final generation of a given EMADE run and is able to get the corresponding datasets, primitive set and emade instance to be able to run individual level bloat quantification code within EMADE.
|Complete
|November 19, 2019
|November 25, 2019
|November 25, 2019
|}

== November 26th - December 2nd, 2019 ==

=== Individual Work: ===
* Started to work on generating a primitive tree for each individual in the final generation from the .csv file of the individuals table. Unfortunately, Esther wasn't able to run EMADE on the titanic dataset for many generations without crashing or erroring out. Luckily, Jeffrey Minowa, a time conflict student in our team who was working on implementing bloat removal using EMADE's caching table, had data from a successful EMADE run on the titanic dataset that ran for 27 generations and was able to send me the individuals table as a csv file. However, after pulling the final generation individuals, I found that DEAP's in built gp.PrimitiveTree.from_string() constructor errored out and wasn't able to parse EMADE generated individuals likely due to the EMADE specific machine learning primitives and terminals.
* After discussing with Austin, I looked into Austin's code (parse_tree()) in seeding_from_file.py to parse an individual's tree structure as a string and return a list of the primitive and terminal nodes in order that could then be passed into the Primitive Tree constructor to generate a Primitive Tree for a given individual. I adapted his code and setup to run within my custom python code and tested it on the individuals from the final generation from Jeff's data. Unfortunately, it was largely unsuccessful because Austin's parse method was set up for the most up to date version of detection processing and Jeff's data was from an older run using an older seeding file and different primitives set.
* On Austin's suggestion, I switched to an older commit of the detection processing branch where the seeding_from_file.py had a slightly different parsing method that should work with the individuals generated by EMADE from Jeff's run on the titanic dataset. However, once again, I was unsuccessful in adapting the parsing method to handle certain unrecognized primitives, learner types, modifiers, tri and quad states, and ensemble primitives.  After adapting it to not error out on any of the individuals, I ended up only being able to correctly parse 6 individuals out of 261 total individuals in the final generation as the remaining individuals were parsed incorrectly and returned a different individual (sometimes vastly different individual as shown below):
** Original Individual Tree Structure:
*** Learner(ARG0, ModifyLearnerList(learnerType('RAND_FOREST', {'n_estimators': 100, 'criterion': 0, 'max_depth': 3, 'class_weight': 0}, 'ADABOOST', {'n_estimators': 50, 'learning_rate': 1.0}), [12, -7]))
** Parsed output:
*** LearnerType('DEPTH_ESTIMATE', {'sampling_rate': 1, 'off_nadir_angle': 20.0})
* I went on to try to run individual level bloat analysis on the 6 individuals that I was able to correctly parse. However, I ran into more problems with finding the correct DEAP toolbox to compile the individuals Primitive Tree into a function that could then be run on the training dataset. After defining my own toolbox using the primitive set I initialized from input, I was able to compile each of the 6 individuals and all of their subtrees:[[files/DEAP toolbox for compiling Primitive Tree.png|center|thumb|700x700px|Custom toolbox that simply registers the compile method using the same primitive set I used to parse the individual from the csv file and is able to successfully compile the parsed individuals Primitive Tree into a function.]]
* I looked into the method handleWorker in EMADE.py and the evaluate_indivual that takes a compiled EMADE individual's function and evaluates it on the training dataset (not the same implementation as my code from DEAP as EMADE contains machine learning primitives that need to be fit to the training data). However, due to the implementation of constants.py and the way terminal nodes are processed in EMADE, for most subtrees and the overall individual itself, the compiled function failed to run on the training dataset.
* The current python code to parse and eventually run individual level bloat quantification on the final generation can be found in the following [https://github.com/Awesommesh/VIP_Bloat_IndividualQuantification_DEAPGP repository] and is named finalGenIndividualBloat.py. This file must be placed in the GPFramework directory along with the other source code and can be run just like running EMADE using the following command in terminal from the root directory: python src/GPFramework/finalGenIndividualBloat.py templates/<emade xml file>.  '''Before''' running this file, the csv file of the individual table from EMADE is needed and line 554's pd.read_csv functions passed in string must be manually replaced with the path to the csv file on your local machine (this can be optimized to just have the user input this path in the future once the individual bloat detection code is fully integrated within EMADE).
* Due to limited time, I was unable to resolve parsing of individuals and evaluation in EMADE and thus, couldn't integrate my individual level bloat quantification from DEAP to EMADE. I was however able to update my DEAP implementation to now 'clean' an individual of all of its bloat nodes alongside bloat detection and analysis. I also added a fix to an error in the original individual level bloat quantification method that didn't consider certain nodes as bloat in the case where a node passes one of its descendants outputs as its own output. More specifically, the previous version of bloat quantification would only recognize the root node that was passing the same output as bloat whereas all of the children of that root node (and the root node itself) that aren't the part of the subtree of the descendant whose output is being passed along are all considered to be bloat and wouldn't be included in a 'cleaned' version of the individual. The final code for my DEAP GP run on the titanic dataset and the individual level bloat quantification methods can be found [https://github.com/Awesommesh/VIP_Bloat_IndividualQuantification_DEAPGP here]. Below is an example of an individual from the final generation that contained some bloat that was detected and then cleaned:
 add(neg(protectedDiv(neg(cos(sin(sin(x7)))), min(x8, x2))), min(min(max(protectedDiv(sin(x7), sub(x2, protectedDiv(protectedDiv(protectedDiv(max(mul(mul(x2, x4), x7), x5), protectedDiv(protectedDiv(x1, x7), x2)), x7), x2))), protectedDiv(neg(neg(neg(x1))), x2)), sin(neg(x4))), max(protectedDiv(neg(x2), sub(x2, x3)), x2)))
 Score: 83.01886792452831 percent
 8 :  min(x8, x2)
 9 :  x8
 45 :  max(protectedDiv(neg(x2), sub(x2, x3)), x2)
 46 :  protectedDiv(neg(x2), sub(x2, x3))
 47 :  neg(x2)
 49 :  sub(x2, x3)
 50 :  x2
 51 :  x3
 52 :  x2
 Cleaned Tree:  add(neg(protectedDiv(neg(cos(sin(sin(x7)))), x2)), min(min(max(protectedDiv(sin(x7), sub(x2, protectedDiv(protectedDiv(protectedDiv(max(mul(mul(x2, x4), x7), x5), protectedDiv(protectedDiv(x1, x7), x2)), x7), x2))), protectedDiv(neg(neg(neg(x1))), x2)), sin(neg(x4))), x2))
* The first string is the original indivdual followed by its bloat score which is just the number of nodes not considered bloat as a percentage of the total number of nodes in the individual's tree. Next is a list of indices of the nodes that  were found to be bloat along with their subtree. The last 2 lines contains the cleaned individual with the exact same accuracy as the original individual.

=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Work on final presentation, integrating individual bloat level quantification into EMADE and writing a clean method
|Partially incomplete (Integration into EMADE dropped)
|November 19, 2019
|December 1, 2019
|December 2, 2019
|}

== January 13th - January 19th, 2020 ==

=== Team Meeting Notes: ===
* Began discussing goals and objectives for the semester and reflecting on past semesters progress
* Decided to move forward with the neat-GP paper's implementation of GP to naturally control bloat implicitly by maintaining uniform populations across speciated individuals
* Plan to test the effectiveness of EMADE with neat-GP integration at controlling bloat for titanic dataset using previous semester's basic population level bloat quantification metric

=== Individual Work: ===
* Looked at previous semester's individual level bloat control metrics to re-evaluate whether it would be possible to fix and integrate neat-GP into EMADE before the end of the semester so that we can also use it as another bloat quantification metric to measure (goal of gathering more data this semester to show statistically whether neat-GP integrated into EMADE reduces bloat during the evolutionary GP run or not)
* Determined that it may be possible to use individual level bloat quantification metrics if we are able to finish neat-GP integration and have time to resolve tree parsing errors in EMADE. For the moment, we have decided to prioritize implementing the full neat-GP algorithm into EMADE
* Reread/reviewed neat-GP paper and implementation in order to be able to begin working on integrating one of the steps of the neat-GP algorithms into the master algorithm within EMADE.

=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review last semester's individual level bloat metrics
|Complete
|January 13, 2020
|January 27, 2020
|January 27, 2020
|-
|Review neat-GP paper
|Complete
|January 13, 2020
|January 20, 2020
|January 20, 2020
|}

== January 20th - January 26th, 2020 ==

=== Individual Work: ===
* Began work on implementing neat-GP crossover method to be integrated into EMADE and replace EMADE's crossover method.
* Began debugging and reading through neat-GP crossover code and helper methods. They can be found [https://github.com/saarahy/neatGP-deap/blob/master/crosspoints.py here].
=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Begin implementing neat-GP crossover method and integrating it into EMADE
|Complete
|January 20th, 2020
|February 2nd, 2020
|February 2nd, 2020
|}

== January 27th - February 2nd, 2020 ==

=== Individual Work: ===
* Working of my branch of Eric's fork of EMADE, I imported neat-GP crossover code and begun debugging / integration of helper methods and main crossover method.
* First version of crossover has been implemented and integrated into EMADE's master algorithm and needs to be tested by running EMADE on titanic dataset.
* Eric was able to resolve tree parsing by loading DEAP individual trees in EMADE from their pickle file which can be found [https://github.gatech.edu/efrankel6/emade/blob/speciation/src/GPFramework/species-verification.ipynb here]. A variation of this method may be interesting to look into later in the semester to load final generation individuals in EMADE to then be passed to the individual level bloat metrics that are currently largely failing due to incorrect tree parsing algorithms for the current version of EMADE.
=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Begin implementing neat-GP crossover method and integrating it into EMADE
|Complete
|January 20th, 2020
|February 2nd, 2020
|February 2nd, 2020
|-
|Test neat-GP crossover integration into EMADE
|Complete
|February 2nd, 2020
|February 9th, 2020
|February 20th, 2020
|}

== February 3rd - February 9th, 2020 ==

=== Individual Work: ===
* Database setup in EMADE was corrupted / incoherent so had to create a fresh database for an EMADE run with crossover on titanic database
** Ran into couple of issues setting up input_titanic.xml at first when setting up connection, but was able to debug most of the environment and MySQL issues
** The current major problem that I am looking into is that EMADE seems to error out upon initialization of the 0th generation population as it is unable to insert individuals into the database due to a SQL duplicate primary key error:  (1062, “Duplicate entry ‘central’ for key ‘PRIMARY’“) [SQL: ‘INSERT INTO host (id) VALUES (%(id)s)’]
** Full Stack Trace: 
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1236, in _execute_context
*** cursor, statement, parameters, context
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 536, in do_execute
*** cursor.execute(statement, parameters)
*** File "/anaconda3/lib/python3.6/site-packages/PyMySQL-0.9.3-py3.6.egg/pymysql/cursors.py", line 170, in execute
*** result = self._query(query)
*** File "/anaconda3/lib/python3.6/site-packages/PyMySQL-0.9.3-py3.6.egg/pymysql/cursors.py", line 328, in _query
*** conn.query(q)
*** File "/anaconda3/lib/python3.6/site-packages/PyMySQL-0.9.3-py3.6.egg/pymysql/connections.py", line 517, in query
*** self._affected_rows = self._read_query_result(unbuffered=unbuffered)
*** File "/anaconda3/lib/python3.6/site-packages/PyMySQL-0.9.3-py3.6.egg/pymysql/connections.py", line 732, in _read_query_result
*** result.read()
*** File "/anaconda3/lib/python3.6/site-packages/PyMySQL-0.9.3-py3.6.egg/pymysql/connections.py", line 1075, in read
*** first_packet = self.connection._read_packet()
*** File "/anaconda3/lib/python3.6/site-packages/PyMySQL-0.9.3-py3.6.egg/pymysql/connections.py", line 684, in _read_packet
*** packet.check_error()
*** File "/anaconda3/lib/python3.6/site-packages/PyMySQL-0.9.3-py3.6.egg/pymysql/protocol.py", line 220, in check_error
*** err.raise_mysql_exception(self._data)
*** File "/anaconda3/lib/python3.6/site-packages/PyMySQL-0.9.3-py3.6.egg/pymysql/err.py", line 109, in raise_mysql_exception
*** raise errorclass(errno, errval)
*** pymysql.err.IntegrityError: (1062, "Duplicate entry 'central' for key 'PRIMARY'")
*** The above exception was the direct cause of the following exception:
*** Traceback (most recent call last):
*** File "src/GPFramework/didLaunch.py", line 118, in <module>
*** main(evolutionParametersDict, objectivesDict, datasetDict, stats_dict, misc_dict, reuse, database_str, num_workers, debug=True)
*** File "src/GPFramework/didLaunch.py", line 108, in main
*** database_str=database_str, reuse=reuse, debug=True)
*** File "/anaconda3/lib/python3.6/site-packages/GPFramework-1.0-py3.6.egg/GPFramework/EMADE.py", line 679, in master_algorithm
*** cache_dict=None)
*** File "/anaconda3/lib/python3.6/site-packages/GPFramework-1.0-py3.6.egg/GPFramework/sql_connection_orm_master.py", line 156, in add_host
*** self.session.commit()
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1023, in commit
*** self.transaction.commit()
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 487, in commit
*** self._prepare_impl()
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 466, in _prepare_impl
*** self.session.flush()
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2446, in flush
*** self._flush(objects)
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2584, in _flush
*** transaction.rollback(_capture_exception=True)
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 67, in __exit__
*** compat.reraise(exc_type, exc_value, exc_tb)
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 277, in reraise
*** raise value
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2544, in _flush
*** flush_context.execute()
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/unitofwork.py", line 416, in execute
*** rec.execute(self)
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/unitofwork.py", line 583, in execute
*** uow,
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
*** insert,
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py", line 1063, in _emit_insert_statements
*** c = cached_connections[connection].execute(statement, multiparams)
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 980, in execute
*** return meth(self, multiparams, params)
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/sql/elements.py", line 273, in _execute_on_connection
*** return connection._execute_clauseelement(self, multiparams, params)
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1099, in _execute_clauseelement
*** distilled_params,
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1240, in _execute_context
*** e, statement, parameters, cursor, context
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1458, in _handle_dbapi_exception
*** util.raise_from_cause(sqlalchemy_exception, exc_info)
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 296, in raise_from_cause
*** reraise(type(exception), exception, tb=exc_tb, cause=cause)
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 276, in reraise
*** raise value.with_traceback(tb)
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1236, in _execute_context
*** cursor, statement, parameters, context
*** File "/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 536, in do_execute
*** cursor.execute(statement, parameters)
*** File "/anaconda3/lib/python3.6/site-packages/PyMySQL-0.9.3-py3.6.egg/pymysql/cursors.py", line 170, in execute
*** result = self._query(query)
*** File "/anaconda3/lib/python3.6/site-packages/PyMySQL-0.9.3-py3.6.egg/pymysql/cursors.py", line 328, in _query
*** conn.query(q)
*** File "/anaconda3/lib/python3.6/site-packages/PyMySQL-0.9.3-py3.6.egg/pymysql/connections.py", line 517, in query
*** self._affected_rows = self._read_query_result(unbuffered=unbuffered)
*** File "/anaconda3/lib/python3.6/site-packages/PyMySQL-0.9.3-py3.6.egg/pymysql/connections.py", line 732, in _read_query_result
*** result.read()
*** File "/anaconda3/lib/python3.6/site-packages/PyMySQL-0.9.3-py3.6.egg/pymysql/connections.py", line 1075, in read
*** first_packet = self.connection._read_packet()
*** File "/anaconda3/lib/python3.6/site-packages/PyMySQL-0.9.3-py3.6.egg/pymysql/connections.py", line 684, in _read_packet
*** packet.check_error()
*** File "/anaconda3/lib/python3.6/site-packages/PyMySQL-0.9.3-py3.6.egg/pymysql/protocol.py", line 220, in check_error
*** err.raise_mysql_exception(self._data)
*** File "/anaconda3/lib/python3.6/site-packages/PyMySQL-0.9.3-py3.6.egg/pymysql/err.py", line 109, in raise_mysql_exception
*** raise errorclass(errno, errval)
*** sqlalchemy.exc.IntegrityError: (pymysql.err.IntegrityError) (1062, "Duplicate entry 'central' for key 'PRIMARY'") [SQL: 'INSERT INTO host (id) VALUES (%(id)s)'] [parameters: {'id': 'central'}] (Background on this error at: <nowiki>http://sqlalche.me/e/gkpj</nowiki>)

* Attempted to resolve strange database insert error that should be handled by EMADE when it creates database schema tables etc.

=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Test neat-GP crossover integration into EMADE
|Complete
|February 2nd, 2020
|February 9th, 2020
|February 20th, 2020
|}

== February 10th - February 16th, 2020 ==

=== Individual Work: ===
* Was able to resolve database issue by creating a fresh database and reseting environment on a new branch (crossover2) of master (running off of master branch version of EMADE with launchGTMOEP.py instead of detection processing version) on Eric's fork of EMADE
* Ran standard EMADE on the titanic dataset for 66 generations to have a baseline run to compare neat crossover with. Visualization of the final pareto front is presented below:
[[files/66 Generation EMADE with Neat Crossover Pareto Front.png|center|thumb|450x450px|AUC not available due to negative hypervolume as individuals were performing better than their goal.]]
* Currently running EMADE with neat crossover locally for a similar number of generations.

=== '''Current Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Test neat-GP crossover integration into EMADE
|Complete
|February 2nd, 2020
|February 9th, 2020
|February 16th, 2020
|-
|Finish EMADE run with neat crossover
|Complete
|February 17th, 2020
|February 23rd, 2020
|February 21st, 2020
|}

== February 17th - February 23rd, 2020 ==

=== Individual Work: ===
* Was able integrate neat crossover into EMADE and test it by running EMADE using neat crossover on titanic dataset for 68 generations. The visualization of the final pareto front is presented below:[[files/Pareto front gen 68.png|center|thumb|450x450px|Hypervolume ~ 1162.32]] 
* Will look to perform final tests by taking several individuals from the regular EMADE run and mating them with each other using neat crossover. Will compare the parents and the resulting children in LISP form and verify that they are match the implementation presented in the neat-GP paper. 

'''Current Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finish EMADE run with neat crossover
|Complete
|February 17th, 2020
|February 23rd, 2020
|February 21st, 2020
|-
|Perform individual neat crossover and analyze for correctness in LISP format
|Ongoing
|February 17th, 2020
|February 23rd, 2020
|
|}

== February 24th - March 1st, 2020 ==
'''Individual Work:'''
* Noticed several major bugs after testing NEAT-Crossover for bugs such as not mating in-place as was being done by DEAP's one point crossover method in baseline EMADE.
* Additionally, NEAT-Crossover only produces one individual per pair of parents selected for mating and so had to update mating function to run NEAT-Crossover twice (while swapping the order of the parents) to produce 2 offspring per pair of parents.
* Forgot to run ./reinstall.sh to update codebase being run for previous runs from last week. Was able to spot this issue at team meeting on Friday February 28th and have corrected errors generated during integration

* Updated codebase for NEAT-Crossover can be found under the branch crossover2 on Eric's fork of EMADE: https://github.gatech.edu/efrankel6/emade
* Added debugging feature to output input parents and resulting mated individual into neatcrossover.txt file to verify that NEAT-Crossover is working as intended
* After completing a run over the weekend with updated NEAT-Crossover integration, was able to confirm that NEAT-Crossover seems to now be working as intended in the neat-GP paper by analysing mating instances logged in the neatcrossover.txt file generated over the course of the evolutionary algorithm.

'''Current Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run Trials of EMADE with fixed NEAT-Crossover Integration
|Complete
|February 28th, 2020
|March 8th, 2020
|March 8th, 2020
|-
|Check correctness of NEAT-Crossover implementation with fixes
|Complete
|February 28th, 2020
|March 1st, 2020
|March 1st, 2020
|}

== March 2nd - March 8th, 2020 ==
'''Individual Work:'''
* I ran Neat-Crossover on Baseline EMADE for at least 35 generations across 3 trials
* I performed p-test statistical analysis on the Neat-Crossover data and produced visualizations for each run using an adaptation of the visualizations.py file in EMADE
* I added my graphs and information regarding Neat-Crossover to the midterm presentation which can be found [https://docs.google.com/presentation/d/1OsK1UyTKBDKVUQhS4866ONOtRGTuCsb9uMZ8VDXICIE/edit here].
* Unfortunately, none of our results were statistically significant for Neat Crossover (likely due to the relatively high variation with only 3 trials run). Additionally, it seems as though Neat Crossover seemed to perform slightly worse than baseline EMADE (both based off of the older version of EMADE using the GTMOEP.py script) in terms of AUC at 35 generations. Despite this, there is some promise with regard to bloat reduction as Neat Crossover had a fairly lower average bloat value at generation 35 (although there was more variance and higher resultant p-value showing no statistical significance). It's possible that with more runs that Neat Crossover may be able to outperform baseline EMADE in terms of bloat with more trials to reduce variation in our results.
* It was hard to compare performance of Neat Crossover with Speciation and Fitness Sharing runs because our current version of Neat Crossover was implemented on an older version of EMADE with fewer primitives and had significantly '''lower''' AUC values throughout its evolutionary runs on baseline and with Neat Crossover when compared with results we obtained on the current version of EMADE. We may need to update our implementation of Neat Crossover to the latest version of EMADE to verify our results and then possibly move on to implementing Neat Crossover on top of fitness sharing and speciation versions of EMADE.

'''Current Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run 3 trials of NEAT-Crossover and Baseline EMADE for at least 35 generations each
|Complete
|March 2nd, 2020
|March 8th, 2020
|March 8th, 2020
|-
|Perform statistical analysis to compare effect of NEAT-Crossover on bloat and AUC
|Complete
|March 2nd, 2020
|March 8th, 2020
|March 8th, 2020
|-
|Prepare visualizations and analysis for midterm presentation
|Complete
|March 2nd, 2020
|March 8th, 2020
|March 8th, 2020
|}

== March 23rd - March 29th, 2020 ==

=== Covid-19 Disruption: ===
* Due to COVID-19 outbreak classes were moved online to distance learning and official restart of classes was delayed till start of April. As I decided to travel back home to Singapore and adjust to the time difference, I wasn't able to make progress during this week temporarily.

== March 30th - April 5th, 2020 ==
'''Individual Work:'''
* Due to 12 hour time difference, I caught up with Eric and Josh over slack. We decided that our midterm implementation of Neat Crossover on an older version of EMADE (with master evolutionary algorithm being implemented in the GTMOEP.py file) was difficult to compare with fitness sharing and speciation results that had been implemented on the latest version of EMADE (that has more primitives such as ensembling) as hypervolumes produced by runs on the older version of EMADE were significantly '''lower''' than those produced by the current version of EMADE (in fact the older version of EMADE would consistently reach lower hypervolumes than those reached over 50  generations on the current version of EMADE soon after initialization itself in much less time).
* Thus, to make our results more comparable, we decided to move our implementation of Neat Crossover and reintegrate it into the updated baseline EMADE.
* Since my EMADE dependencies were in conflict and causing several errors when running baseline EMADE, I found that making a conda environment for EMADE specifically was the best way to avoid errors resulting from dependencies that may change for other projects.

'''Current Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Setup newest version of EMADE on a new branch on Eric's fork of EMADE
|Complete
|March 30th, 2020
|April 5th, 2020
|April 5th, 2020
|-
|Begin integrating Neat Crossover code and bloat quantification metric into updated version of EMADE
|Complete
|March 30th, 2020
|April 5th, 2020
|April 5th, 2020
|}

== April 6th - April 13th, 2020 ==
'''Individual Work:'''
* Caught up with Eric and Josh in a private meeting to discuss progress made since the midterm presentation and our plans for the remainder of the semester up to the final presentation. We discussed problems that first years had been running into while running EMADE and ways in which we can organize tasks and runs in preparation for the final presentation. 
* I suggested that we all run 30 generation runs across all experiments on the Titanic dataset with largely the same constants that we had maintained during our trials for our midterm presentation (optimizing for 2 objectives false positives and false negatives, same initial population size, mating and mutation probabilities etc). Due to some runs getting stuck or hung while evaluating certain individuals or just taking very long after 20-25 generations as more valid individuals are present in the population, we decided to perform 30 generation runs so that we could aim to get 10 trial runs per experiment (helping to reduce variance and improve chances at getting statistically significant results) within the limited time left by having the first year students perform as many runs as possible before the final presentation. 
* I also proposed that we create a google drive folder where we can keep track of each run, sql database tables and master log files on the fly by having the first year students upload their runs as they are completed.
* After fixing several errors on the version of EMADE on the detection processing branch of Eric's fork of EMADE (the fork on which all of the research fundamentals code is based), I realized that there were some issues with versions between the main repository and the detection processing branch on eric's fork.
* Since the detection processing branch of Eric's fork wasn't up to date and wasn't working after stack tracing, to save time, I implemented neat crossover off of the speciation branch (Note: that speciation isn't factored into the parent selection process and so this implementation simply implements neat crossover and the basic bloat quantification methods on top of the current version of baseline EMADE). The code for it can be found on the speciationNeatCX branch of Eric's fork: https://github.gatech.edu/efrankel6/emade/tree/speciationNeatCX.
* Incorporated Eric's integration of individual timeout (Aaron from the ADF team pointed him to a commit that addressed this issue) which wasn't working as intended before.
* I was able to confirm that Neat Crossover was working as intended on a test run of EMADE with Neat Crossover.
* After having confirmed that Neat Crossover is working, I asked Eric to begin running some runs along with me so that we are able to get 10 trials before the final presentation.

'''Current Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Implement Neat Crossover on a branch off of the speciation branch on eric's fork of EMADE
|Complete
|April 6th, 2020
|April 13th, 2020
|April 13th, 2020
|-
|Test functionality and correctness of Neat crossover implementation on new branch
|Complete
|April 6th, 2020
|April 13th, 2020
|April 13th, 2020
|}

== April 14th - April 21st, 2020 ==
'''Individual Work:'''
* Met with the entire sub-team at our usual meeting time to discuss making of the final presentation and analysis of our results thus far. Team has overall done a great job of getting runs done in time for the final presentation. Eric and I plan to perform analysis on the results gathered by first year students and have visualizations ready for our mock presentation with Dr.Rohling and Dr.Zutty on Sunday. (we are yet to assign slides to first years, but most likely each first year will discuss a slide or two about background concepts or on their experimental results).
* I was able to do 9 runs of EMADE with Neat Crossover on the updated version of EMADE for at least 30 generations each. Eric got 3 runs done on the same (although his first run ran a bit too quickly likely due to some error in setup, so we may not use those results in our analysis). All of our runs' data and important tables can be found [https://drive.google.com/drive/folders/1y1EQwaHWMCKDlMOVALpo6ZztLyOYWmW3?usp=sharing here]. 
* Unfortunately, since I had received all the data for Neat Crossover runs a bit late, I wasn't able to add my graphs and analysis to our presentation in time for the mock presentation on Sunday. Despite this, our mock presentation went well and we were able to receive much needed feedback to improve our presentation:
** Add a table of contents or overview of topics covered in chronological order at the beginning of the presentation 
** Explain the bloat metric used to recap for the audience how it works and what it measures 
** Provide motivation/ discuss utility of bloat detection and removal in genetic and evolutionary algorithms 
** Explain in greater detail (before presenting data) the precise experimental setup for each experiment 
** Give the end results briefly before diving into the data and graphs even if the outcome of the experiment was negative. 
** Be specific and accurate with labeling of plots and use log scale for hypervolume due to large differences in magnitude between generations 
** Add 1 standard deviation above and below each line in the graph 
** Give big picture or main takeaway in textual form especially on purely graphical slides so that others who reference the presentation in the future don't depend on missing out on the actual oral delivery of presentation and to help audience come to the conclusion thats most important 
** Tie presentation in the end back to the motivation of bloat removal and detection 
*  Eric wrote up a Jupyter notebook that graphs average hypervolume and bloat per generation across all of our runs data stored in a single directory for various runs of fitness sharing with varying speciation thresholds. I adapted his notebook to handle and process the Neat Crossover data and I then performed visualization and analysis on the Neat Crossover runs (after receiving feedback on Sunday, particularly regarding our visualizations, we added 1 standard deviation above and below each plot, changed labels to be more precise, and used log scaling for hypervolume). My version of the notebook can be found [https://github.com/Awesommesh/VIP_Bloat_IndividualQuantification_DEAPGP/blob/master/Final%20Statistics.ipynb here].
*  The standard deviation plots helped give greater insight into our data showing the variation between trials for hypervolume and bloat at each generation. As mentioned in our final presentation, Neat Crossover diverged and was statistically worse performing in terms of hypervolume with almost no change average bloat at generation 30 (no statistically significant differences in bloat). However, it was interesting to note that Neat Crossover seemed to have a much lower standard deviation throughout the evolutionary run. 
**  I suspect this was likely because in our current setup, Neat Crossover was implemented directly on top of baseline EMADE without speciation being factored into the mating selection process. Thus, Neat Crossover would often receive two dissimilar individuals and produce offspring with majority of genetic makeup being inherited by one of the parent individuals. This meant that Neat Crossover runs were inherently less random as they performed fewer swaps during crossover and it didn't explore the objective space as rapidly as Baseline EMADE. It would be very interesting to see how Neat Crossover performs with fitness sharing and speciation where individual tree topologies are factored into the selection process and should be one of the subteams next steps for research next semester. However, based on the results for speciation with fitness sharing, it may just be the case that our speciation metric isn't well suited towards the. common individual structures generated by EMADE (often leaning towards degenerate "spine" tree topology).
*  Our final presentation can be found [https://docs.google.com/presentation/d/1mmyBsT76iPt4N7pM0oUf7c2qlhh34gBle978DpOBPrE/edit?usp=sharing here].
'''Current Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Perform as many runs of Neat Crossover as possible before final presentation (preferably 10 across Eric and I)
|Complete
|April 14th, 2020
|April 20th, 2020
|April 17th, 2020
|-
|Analyse Neat Crossover results and visualize data
|Complete
|April 14th, 2020
|April 20th, 2020
|April 19th, 2020
|-
|Add Neat Crossover information and slides to our final presentation (incorporating feedback received on Sunday's mock presentation)
|Complete
|April 14th, 2020
|April 20th, 2020
|April 20th, 2020
|-
|Prepare my sections on Neat Crossover for final presentation
|Complete
|April 14th, 2020
|April 20th, 2020
|April 20th, 2020
|}
__FORCETOC__
__NEWSECTIONLINK__