== Team Member ==
Team Member: Anshul Tusnial - NLP subteam, Semester 2

Email: atusnial3@gatech.edu

Cell Phone: 404-993-6730

Interests: Machine Learning, Algorithm Development, Graph Theory, Math, Languages

== Apr 30, 2021 ==
=== '''Final Presentation Notes:'''===
Stocks
* objectives since midterm:
** implementing TA lib indicators
** testing on larger datasets
** statistical evaluation of individuals
* overview of paper
** 1st part - piecewise linear representation 
** 2nd part - exponential smoothing
* task distribution
** lit review & research (including TI implementation)
** data analysis of runs
** emade implementation - modifying EMADE code & improving functionality
* Technical Indicators
** added 6 new TA-Lib indicators
** also added some indicators not available on TA-Lib by calculating them manually
* Run Results - completed 2 long runs
** 1st run - 328 generations, 4 objectives
*** profit percentage, average profit per transaction, variance profit per transaction, normal cdf on distribution
** 2nd run - 328 generations, 3 objectives
*** removed profit percentage objective bc it was highly correlated with profit percentage
* Notable Individuals
** Best performer - decision tree regressor on a bollinger band
*** very low variance profit and had 41% average profit percentage across the 6 stocks
** 2nd individual used Gaussian process and had an average 32% profit percentage
* Basleline comparisons
** monte carlo simulations - calculate profit on stock with random buy and sell trades to establish a baseline for how well a stock should have done
** also compared to a buy-hold strategy - best learner underperformed the buy-hold baseline
* Primitive analysis
** irrelevant TIs can skew results of the ML algorithm, so they studied which learners had the best performance
* Future Work
** optimize individuals for each stock separately
** analyze types of seeds impact on the AUC
** create bounded objective functions

ezCGP
* Overview
** is an example of Cartesian genetic programming
** graph-based structure composed of a DAG
** can re-use nodes, and has active/inactive nodes
* midterm recap
** removed augmentation and preprocessing stuff
** had high training accuracy but very low validation accuracy - means high overfitting
* Objective - recreate CIFAR-10 results without transfer learning
** expand primitive set to diversify population and fine tune training parameters
** also want to improve ability to visualize genomes
* Experiment - Individual Size - manually analyzed individual sizes for the first few generations since they noticed little diversity in in the individual sizes in the midterm
** result - larger individuals performed worse so they were selected against
** conclusion - larger architectures mean more room for mistakes in the architecture, so a possible solution is to look at mutations that add layers/mating (incrementally adding size)
* Experiment - Activation Function - limited architecture to only use ReLU since they noticed that every convolutional layer was using a different architecture
** Result - no significant improvement in performance (which was surprising)
* Experiment - Pooling & Dropout since most SOTA architectures include these, so they added max pooling, average pooling, and dropout primitives
** result - achieved significantly better results than the midterm (68.5% vs 56.3%) - pooling layers were really good, dropout layers didn't do too well
* Experiment - Dense Layers
** Added dense + dropout layer after convolutional layers to see if evolving these would improve performance
** result - after 12 generations, got validation accuracy of 69.6% which is better than midterm, however there was low diversity in the population and performance is just bad compared to SOTA - this is probably due to low total number of parameters
* Visualization (since previous iteration of visualization was not informative enough - didn't have number of parameters, layer arguments, etc.)
** added visualization of inactive nodes, named layer arguments, and node numbers
** allowed visualizing multiple individuals at once
** built an easy-to-use CLI to do so
* Comparative Study on Crossover in CGP Paper
** paper worked on symbolic regression
* Meta-Parameter Search
** a new way to run ezCGP to easily run different iterations of the same problems making only small changes between runs
* Mating - tried one-point crossover
** works by selecting a common crossover point and swapping the corresponding subtrees, works the same way in ezCGP
* Point mutation
** paper does this, takes a percent of parent's genes and mutates them to create a child genotype

== Apr 26, 2021 ==

=== '''Team Meeting Notes:''' ===
* Final presentations on Friday April 30th at 6PM

=== '''Sub-Team Notes:''' ===
* Need to make slide deck for final presentation - will do a run-through on Wednesday so we'll try to get our slides in by then.
* Wednesday meeting notes:
** Spent first half of the meeting making sure everyone had slides to go through. I gave a few of the slides I was originally covering to first semester students (Cameron B., slide 5, and Heidi, slide 16).
** Spent the remainder of the meeting finishing those slides and talking through the presentation. Scheduled an actual run-through for Thursday Apr. 29 (next day) at 6PM
* Thursday meeting notes:
** Finished the slide deck for final presentation and did a run-through - estimated time was at ~26 minutes, so will need to cut down a little bit but overall we're in a good position.
** Fixed issues on some slides (see individual notes)

=== '''Individual Tasks:''' ===
* We decided that since we had enough people doing runs, my time would be better spent making sure the presentation was ready. In addition to doing my own slides (see below), I helped first semester students with their slides to make sure all relevant material was covered.
* I also pointed out and fixed issues on some slides:
** Slide 7 - originally had information that was covered later in the presentation, we removed that from this slide
** Slide 26 - graph on this slide was titled accuracy, but the objective measured was actually error = (1 - accuracy)
** Slide 31 - GIF of pareto front by generation didn't have generation titles at the top, and one of the pareto fronts (gen 10) was wrong
*** there was an individual with FPR = 0 and FNR = .5 so adding the trivial solution of (0,1) to this pareto front was wrong in that case, needed to remove it
* I worked on the following slides:
** Slide 4 - overview of what an NNLearner is
** Slide 5 - (with Cameron B.) overview of a LayerList
** Slide 16 - (with Heidi) overview of resources created this semester to help future members get up to speed quickly
** Slide 18 - emphasized important aspects of Amazon dataset - binary classification, balanced dataset, too big for EMADE
** Slide 35 - ran Cameron W.'s tree-viz script to generate the tree for the individual on this slide
** Slide 38 - (with Cameron W.) examining by hand some of the misclassifications of the best individual to see if there was a discernable pattern
** Slide 41 - next semester ideas
* link to presentation here: [https://docs.google.com/presentation/d/13PfLTDQnA1Lct5-SkOJLb8uzvkISPMzfuC6GnMvAmsk/edit#slide=id.gc91e5cc51d_0_31]


=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Prepare for final presentation
|In Progress
|Apr 26, 2021
|Apr 30, 2021
|Apr 30, 2021
|}

== Apr 19, 2021 ==

=== '''Team Meeting Notes:''' ===
* Modularity - having trouble finding individuals that evaluate, think they need longer runs
* EZCGP - experimented with one-point crossover
* Stocks - adding new technical indicators and preparing for a run on today

=== '''Sub-Team Notes:''' ===
* Will which objectives to FPR and FNR from accuracy and num params and compare the results between runs from the two sets of objectives in the final presentation
* Discussed how to seed database before runs in PACE
* plan for the rest of the semester is to just get runs in
* Will meet Wed. Apr 28 for final presentation run-through

=== '''Individual Tasks:''' ===
* Having trouble with running EMADE on PACE - out file is always giving an error where the dataset files are just hashes, implying that git-LFS isn't installed properly, but I tried re-installing git-LFS with no success.
* Eventually (as of Sunday Apr 5) got it to work - still not sure what was wrong but just re-installed git-LFS and re-git cloned a few times and got it to work. Ran for 2 generations just to make sure it was working, which it *finally* is.
=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved

|-
|Figure out issue with git LFS
|Complete
|Apr 19, 2021
|Apr 23, 2021
|Apr 25, 2021
|}

== Apr 12, 2021 ==

=== '''Team Meeting Notes:''' ===
* Stocks is looking at different tests they can use to show significance
* Modularity had a small run and only a few valid individuals, they think a longer run will solve this issue
* Dr. Zutty wants us to submit a PR with the PACE fix into cacheV2

=== '''Sub-Team Notes:''' ===
* PACE 8 hour run limit can be bypassed by just reseeding the pareto front in a new run
* Most members are nearly done with PACE setup with just a few minor issues left
* Steven did a 24 hour run - no new individuals are beating seeded ones so we need longer runs
* Everyone is being tasked for the rest of the semester on one of 4 categories:
** Fix pretrained embedding layers - right now we are unable to edit the input size to the layer properly causing every individual with this layer to immediately fail
** evolution - figure out why NNLearners can't get very complex without breaking
** Allow NNLearners to take in other NNLearners as subtrees
** PACE-ICE - merge PACE functionality into cacheV2
* I tasked myself on debugging pre-trained embedding layers

=== '''Individual Tasks:''' ===
* Finish setting up PACE
** debugging more issues: fixed the issue with the yml file from last week by commenting gpframework out of the yml, and installed gpframework with reinstall instead. Got a new error saying there was a conflict with uncertainties=2.4.4 since there's a dependency on uncertainties>=3.0.1,  solved this by just pip installing uncertainties=3.0.1. I was afraid this would break things but didn't see any immediate issues.
** After these issues PACE seems completely set up, will look to get started on runs next week

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved

|-
|Get set up on PACE
|Complete
|Mar 22, 2021
|Apr 19, 2021
|Apr 16, 2021
|-
|Task myself on one of the new tasks
|Complete
|Apr 12, 2021
|Apr 16, 2021
|Apr 16, 2021
|}

== Apr 5, 2021 ==

=== '''Team Meeting Notes:''' ===
Dr. Zutty gave stats lecture:
* How to prove you're getting significant/meaningful results?
** EMADE collects some statistics - objective scores, AUC, number of pareto individuals, etc.
* Hypothesis testing:
** type 1 error - P(reject null hypothesis | null hypothesis is True) - alpha, False Negative
** type 2 error - P(fail to reject null hypothesis | null hypothesis is False) - beta, False Positive
** we want to compute the probability of observing a sample given the null hypothesis
** use a t-Test with t-statistic (x - mu)/(s/sqrt(n)) where s is the sample standard deviation
** test can be either 1 or 2 tailed

=== '''Sub-Team Notes:''' ===
* Dr. Zutty recommended we just reduce the size of the training set and move on from there
* Cameron made a guide on the wiki to help everyone set up on PACE, but since the wiki has been down he also made a youtube video that was very helpful
* we spent the meeting mostly troubleshooting PACE issues
* moved my NN/NLP presentation to Friday due to Dr. Zutty's stats lecture

=== '''Individual Tasks:''' ===
* Prepare for NN/NLP presentation [https://docs.google.com/presentation/d/1CB7nFttRU0psaFTDHHWIScy8nFkvT0X5bTc3T_En808/edit?usp=sharing]. Presentation is thorough and covers a wide range of topics:
** background on supervised learning - what is the task, how we know a model is good (loss functions), why neural networks
** Neural network background - I presented at the level of abstraction that a neural network is just a stack of layers that each do "something"
** what's in a layer - another abstraction, I treat a layer as an operation (computation) + an activation (non-linearity)
** what are common activation functions - I go through a few but mention that ReLU is generally great
** how are neural networks "trained" - feed forward & back-propagation, just a quick overview
** how to evaluate a model - train/test/validation split
** what if the model did poorly? - overfitting/underfitting, regularization, learning rate adjustment
** common types of layers used in NN/NLP tasks - dense layers, convolutional layers, recurrent layers, LSTM layers, attention
** NLP-specific concepts - NLP is natural language processing, overview of some common NLP tasks
** introduced Amazon dataset - sentiment analysis problem on a balanced binary classification dataset
** word embeddings - a meaningful way to encode textual data into vectors
** further resources - learn some math (matrix calculus) so you can read papers, learn about other NN architectures, different kinds of embeddings, take a class
* I still need to get set up on PACE
** running into some PACE issues:
*** having trouble installing the conda environment from the yml file Cameron and Steven provided - says "No matching distribution found for gpframework=1.0" and "Pip failed" - will ask Cameron to help debug

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Give NN/NLP presentation
|Complete
|Apr 5, 2021
|Apr 9, 2021
|Apr 9, 2021
|-
|Get set up on PACE
|In Progress
|Mar 22, 2021
|Apr 19, 2021
|
|}

== Mar 29, 2021 ==

=== '''Team Meeting Notes:''' ===
* 1st semester students assigned to subteams
* Dr. Zutty talked about debugging NNLearners since our team was having a little trouble, turns out some lines in EMADE.py redirect stdout so print statements don't work. Commenting those out solves the issue.

=== '''Sub-Team Notes:''' ===
* Introduced new members
* Everyone is tasked with getting set up on PACE, but since there's wiki issues this might be tough
* Realized Amazon dataset is just too big - tokenized is 14GB which is too much memory for PACE
** possible solutions - just cut out train dataset, reduce sequence length of embeddings
* Friday meeting Cameron W. gave a presentation on EMADE basics - EmadeDataPairs, important methods, and how to run EMADE

=== '''Individual Tasks:''' ===
* I will make and give a presentation on NLP/NN basics next week
* Link to presentation here: [https://docs.google.com/presentation/d/1CB7nFttRU0psaFTDHHWIScy8nFkvT0X5bTc3T_En808/edit?usp=sharing]
* Also need to get set up on PACE, but will likely save that for next week since making the presentation will take time

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finish NN/NLP presentation
|Complete
|Mar 29, 2021
|Apr 2, 2021
|Apr 2, 2021
|-
|Get set up on PACE
|In Progress
|Mar 22, 2021
|Apr 12, 2021
|
|}

== Mar 22, 2021 ==

=== '''Team Meeting Notes:''' ===
Midterm Presentations!
* Stocks
** use technical indicators to predict stock trends, and use EMADE to optimize current models
** two objectives - research (needed new paper since last semester's paper was inconsistent) and implementation (EMADE runs, add more technical indicator primitives)
** uses piecewise linear representation, exponential smoothing, and a dynamic threshold
** used AlphaVantage for stock data sources
** used all the TIs as the paper and then some extra - OBV, CMF, KVO, VPT
** had issues generating proper thresholds per stock, differs widely from the paper they're using
** Best individual does not use TI primitives and did very well on one ticker but not great on the others
** Future plans - add more TIs using TA-Lib (technical indicator library), add different time granularities, explore different derivatives (not just NYSE listed stocks), look at NN evolution in EMADE, only allow preprocessing & TI primitive evolution

=== '''Sub-Team Notes:''' ===
* Discussed plans for rest of semester and what to do with first semester students
** Looks like we will shift full focus to getting actual results on amazon runs and worry about everything else later
* Will get everyone set up on PACE so we can do runs
* Still need to figure out why NNLearners are failing

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get set up on PACE
|In Progress
|Mar 22, 2021
|Apr 5, 2021
|
|}

== Mar 15, 2021 ==

=== '''Team Meeting Notes:''' ===
* Stocks - finished EMADE run, could only evaluate 2-3 non-seeded individuals (lots of errors)
* EZCGP - shape mismatches make adding dense layers hard
* Modularity - is doing testing on multiple problems, will start runs soon

=== '''Sub-Team Notes:''' ===
* Fixed MySQL error on PACE
* Seeding file has issues - some typos? 
* Alex is trying to implement models as NNLearners but is running into issues due to concat layers & fasttext
* Will try to look at .out files to figure out why individuals fail
* Work on presentation

=== '''Individual Tasks:''' ===
* Work on presentation [https://docs.google.com/presentation/d/1bpIN_1nL6PB8fMq1yvEDQnuy_ktcSY87HV2nxNsvmas/edit#slide=id.gc84dce302c_2_50 slides] - my slides are specifically: Benchmarking - LSTM model and Semester Goals (slides 18 and 27)

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Work on midterm presentation slides
|Complete
|Mar 15, 2021
|Mar 19, 2021
|Mar 19, 2021
|}
== Mar 8, 2021 ==

=== '''Team Meeting Notes:''' ===
* Stocks - PLR and exponential smoothing are ready, will do an EMADE run
* EZCGP - got results from their run, not sure if preprocessing is doing anything, issues with dense layers
* Modularity - Added function to insert new random individuals at each generation

=== '''Sub-Team Notes:''' ===
* Steven & Cameron did runs on PACE
** Steven's was very long, Cameron's stopped after 2 hours and didn't get past gen 1
** Cameron isn't able to do a run for > 8 hours for some reason
* Jon will use PyTorch Lightning rather than normal PyTorch since it's implemented more closely to Keras

=== '''Individual Tasks:''' ===
* After building the model from scratch this week (link to notebook [https://colab.research.google.com/drive/1A6XaxDJ1qrQLXYkARDAfdH7JyN3Ifj7e here]), I had the exact same issue in Colab as before. I also found an encoding issue in the embedding file parser - the file wasn't specifically being opened under utf8 encoding. This didn't show up as an error message in colab, but the same problematic behavior (where only part of the embedding file is parsed) occurred both in colab and locally. Despite this fix, colab would not run the model.fit() cell. I found that the model.fit() does begin to run when I run the notebook locally (although I don't have a GPU so the model is way too large for me to fit locally), so it is definitely some sort of colab issue. Probably won't be able to fix this, so we will look at other models as a baseline. Sumit found some using FastText that he will try to run.

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Figure out error in model.fit()
|Complete
|Feb 22, 2020
|Mar 1, 2020
|Mar 8, 2020
|}
== Mar 1, 2021 ==

=== '''Team Meeting Notes:''' ===
* stocks - looking at implementing a python library that would add hundreds of indicators, implemented exponential smoothing
* ezcgp - ran cifar-10 without transfer learning, accuracy didn't change much
*modularity - wrote a new function to find ARLs but will have to change a lot of other stuff in pipeline to make it work
*do peer evals this week

=== '''Sub-Team Notes:''' ===
* Sumit running into a new issue with his embedding file; he'll use a different file. Not sure why I'm not having the same issues.
* Steven and Cameron running into the classing tourney select error - will downgrade deap version
* Still discussing PyTorch refactor, might not be worth it given how difficult it will be

=== '''Individual Tasks:''' ===
* Need to do peer evals
* Still having the same model.fit() issue, honestly not sure what else to try. Might try to build the model from scratch myself with the EMADE files and just work it that way. 3 exams this week though so won't be making much progress until next week.

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Figure out error in model.fit()
|In Progress
|Feb 22, 2020
|Mar 1, 2020
|
|}  

== Feb 22, 2021 ==

=== '''Team Meeting Notes:''' ===
* ezcgp - got PACE set up, made a shared conda file, still having memory issues on runs
*modularity - splitting ointo two teams - how to find ARLs and how to store them efficiently

=== '''Sub-Team Notes:''' ===
* Dr. Zutty stopped by, seems like we (Jon leading) will go ahead with trying to get PyTorch learners implemented in EMADE since: (1) important layers like transformers are easy to implement in PyTorch, (2) today, most papers/results in NLP field are implemented in PyTorch, so easy to compare
* Steven had trouble running Amazon dataset on PACE since the files didn't gzip properly - probably a Git LFS issue
* Cameron had PACE issues as well - something about reinstall not working?
* By Friday both Steven and Cameron were able to do runs in PACE, me and Sumit still having issues

=== '''Individual Tasks:''' ===
* here's the [https://colab.research.google.com/drive/1A6XaxDJ1qrQLXYkARDAfdH7JyN3Ifj7e#scrollTo=7cJFbfp8-3xC notebook]:
* Running into a new issue - I get all the way through the notebook without problems until the cell that runs model.fit() - when I run this cell, the Colab runtime momentarily disconnects, then reconnects without dropping any of the files uploaded to the runtime (which is weird), and the model.fit() cell is no longer running. Have no idea how to fix this - maybe it's some sort of memory issue with the embeddings file?
* Sumit is having a slightly different issue - his model.fit() seems to finish running, but the cell with model.evaluate() gives an error, implying that the model never fit properly in the first place
** I think it's likely these are the same issue manifesting in two different ways
* Sumit and I will reach out to Abhiram from the stocks team to see if he can help since he's very familiar with Colab 

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Figure out error in model.fit()
|In Progress
|Feb 22, 2020
|Mar 1, 2020
|
|}  

== Feb 15, 2021 ==

=== '''Team Meeting Notes:''' ===
* stocks - looking at adding new primitives/functionality like a new dataset with 2008 stock info, exponential smoothing, etc.
* ezcgp - having trouble with PACE - got it running for everyone, but they think there's memory constraints
*modularity - did documentation, will rewrite how ARLs are found to increase ARL complexity

=== '''Sub-Team Notes:''' ===
* Me and Sumit are working on the Kaggle model implementation - see below
* Steven & Cameron seem to have fixed PACE issues

=== '''Individual Tasks:''' ===
* Running into issues trying to run the Kaggle model on Google Colab. First, I made some changes to the notebook:
** replaced use of "CuDNNLSTM" layer with a traditional "LSTM" layer
* After having trouble running the notebook using the dataset train/test split we use in EMADE since it's compressed as a BZ2 file in the notebook, I decided to use the same files that the notebook used for the train/test split as well as the embeddings
* Running into an error (layer size mismatch) trying to stack the layers of the embeddings matrix, not sure what's causing this: <insert image>
* update - i tried to explore the issue by printing the sizes of the entries in the embeddings matrix, found nothing peculiar, ran the cell again, and there was no error - seems like it resolved itself? Not sure how that happened but I'll take it

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Figure out error in embeddings stack
|Complete
|15 Feb, 2021
|22 Feb, 2021
|21 Feb, 2021
|}  

== Feb 8, 2021 ==

=== '''Team Meeting Notes:''' ===
* Need to do notebook self evaluation. I have not been good about updating my notebook and am doing this evaluation nearly a month late, but will do better for the rest of the semester.
* stocks - goal is to read a paper and do more finance-specific research, as well as introduce more technical indicators
* ezcgp - trying to get rid of transfer learning, looking into using transformers
*modularity - doing paper review, want to make generated ARLs more complex, may look at CoDeapNEAT

=== '''Sub-Team Notes:''' ===
* Sumit found multiple papers with Amazon dataset, he and I both independently found a Kaggle model that looks very promising as a baseline since all of the layers it uses are already in EMADE: https://www.kaggle.com/anshulrai/cudnnlstm-implementation-93-7-accuracy
* Steven & Cameron still working on adding Amazon dataset to EMADE; currently trying to set up MySQL on PACE

=== '''Individual Tasks:''' ===
* Need to try to implement the Kaggle model to ensure that it is an accurate baseline
* Notebook self eval below:
[[files/Self Evaluation 2021.png|678x678px]]

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Build & Run Kaggle model
|In Progress
|Feb 8, 2020
|Feb 15, 2020
|
|}  

== Feb 1, 2021 ==

=== '''Team Meeting Notes:''' ===
* stocks - want to get more technical indicators as primitives
* ezcgp - discussing transfer learning and how to divide up work given the group size
* modularity - want to explore bigger ARLs

=== '''Sub-Team Notes:''' ===
* Spoke with Dr. Zutty about team goals - will look more closely at the NLP side and not as much at the CV side this semester
* Need to figure out why EMADE is only giving trivial solutions - to do this, need a balanced dataset (like Amazon dataset)
* Looking for papers that work with Amazon dataset to establish a baseline to compare EMADE against - me and Sumit will do this
* Steven & Cameron will set up on PACE
* Anish will get Amazon dataset integrated in EMADE

=== '''Individual Tasks:''' ===
* Need to look through literature to find models on Amazon dataset to establish a baseline with Sumit
** update - found LSTM-based model with 94% accuracy on Kaggle

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Research literature for Amazon dataset models
|Complete
|Feb 1, 2021
|Feb 8, 2021
|Feb 8, 2021
|}  

== Jan 25, 2021 ==

=== '''Team Meeting Notes:''' ===
* First meeting of the year; not sure which subteam I want to join this semester. Leaning towards staying with NLP but also thinking about stocks.

=== '''Sub-Team Notes:''' ===
* NLP sub team did not meet this week

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Join Subteam
|Complete
|Jan 25, 2021
|Feb 1, 2021
|Feb 1, 2021
|}  

== Dec 2, 2020 ==

=== '''Team Meeting Notes:''' ===
* Presentations!
* Stocks
** inconsistencies in research paper they were trying to replicate
** looking using genetic labeling to train individuals
** developed technical indicator primitives in EMADE - simple moving average, exponential moving average, moving average convergence and divergence, stochastic oscillator (KD%) (primitives from paper)
** also developed other primitives - relative strength indicator, William's R%, Ease of Movement, On-Balance Volume
** new technical indicators - Bollinger Bands, CCI, MFI
** results:
*** paper's truth values gave 8.65 profit and predicted values gave 24% profit; they think this was a fluke
*** EMADE run predicted values gave 5% profit while genetic labeling truth values gave 12% profit
** thoughts - seems like they spent the semester trying to add new primitives and replicate the paper's behavior, I'd be interested to see whether the new primitives they developed significantly help their results or not
* EZCGP
** added thresholding, normalize, blurring, and filtering primitives
** changed objective evaluation from accuracy and F1 score to precision
** used pip install for tensorflow rather than conda install on PACE
** Saved state of run (best 20 individuals) after every generation
** Initial run - best individual had .98 precision, .97 recall, .97 F1 
*** AUC jumped significantly (i think .92 to .98?) between generations 1 and 9
*** said that model performed well due to transfer learning layers and wanted to measure performance without those in the future, i'm not totally convinced that this is a bad thing though - transfer learning is good! why not use it
** future goals - perform more experiments on MNIST, research new mating methods
* Modularity
** Group primitives together as "building blocks" to use like single primitives in later generations
*** ARL - combine primitives into a single node in an individual, selected based on frequency and distribution
** ran on titanic minimizing FP and FN, seeded with 5 individuals and capped at 40 generations using differential fitness as heuristic
*** differential fitness - difference of child fitness - parent fitness, positive value = good child
*** obtained statistical significance by generation 16 but not sure why, converged with baseline at higher generations
** second experiment - used different selection method where individuals with more ARLs are more likely to be selected to increase the number of ARLs
*** statistical significance by generation 11
** third experiment - ARLs are only created through data pairs (I wasn't too sure about what this meant), supposed to reduce bloat
*** didn't find statistical significance
** fourth experiment - combine experiments 2 and 3
*** performed worse than other experiments, but this may have been due to small sample size
** future work - use ARLs on less trivial datasets like MNIST, combine ARLs and ADFs (i'm not super sure on the difference)
** thoughts - I don't think results on the Titanic dataset tell us very much about ARL performance given that it's just a binary classification problem (pretty trivial) - would be interested to see more results on datasets like MNIST

== Nov 23, 2020 ==

=== '''Team Meeting Notes:''' ===
* ezcgp - focusing on baseline runs; found a bug where too many individuals are added each generation
* modularity - started with runs using Icehammer, but don't see statistical significance yet since sample size is only 4. However ARLs are working as expected which signals promising results

=== '''Sub-Team Notes:''' ===
* Anish did a baseline run over the weekend
* Tejas is having trouble with PACE, something about setting up conda environment?
* CV team is finishing implementing primitives
* troubleshooting for runs on PACE, Anish & Pulak figured it out which is good
* results from disease prediction (CV) are really strange (only 50% accuracy)
** also not sure how paper defines "correct" classification (multilabel accuracy or not)

=== '''Individual Tasks:''' ===
* running into issues trying to get started with PACE - disk error issue when SCPing EMADE over from my machine, connection also times out sometimes; some other people (I think Tejas and Shreyas) also running into this issue
** update - not a huge deal if I can't get PACE running before the presentation as we have enough people that have it working that can do the runs
* Coordinate with Alex on issues he's having with adaptive mutation not running on his Icehammer run
** also need to fine tune good/bad mutation probabilities, but can't really do this without just doing a bunch of runs
** master.err file is empty but won't get past the first generation, meaning there's still something wrong with our implementation despite the ind.fitness.values fix
*** **UPDATE - works (as of 11/31) when we just seed in some individuals
* Analyze results from adaptive mutation run
** Best individual with error .0383: NNLearner(ARG0, OutpiutLayer(ARG0, GRULayer(33, seluActivation, 32, falseBool, falseBool, EmbeddingLayer(32, ARG0, randomUniformWeights, InputLayer(ARG0)))), 89, AdamOptimizer)
** Gen 24 hypervolume: 38287692.41437316
** Pareto Front:[[files/Pareto front adaptive.png|none|thumb]]
* Write presentation slide for adaptive mutation
** slides 13 and 26 in [https://docs.google.com/presentation/d/1Z8PjzaP_LjhyycpGSeXX7rIHdVYkxHzqg8vaKVEYhdU/edit#slide=id.g53d14b96c4_1_124 Final Presentation] (Adaptive Mutation Function and Adaptive Mutation Results slides)

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Write adaptive mutation slide
|Complete
|11/23/20
|12/3/20
|12/2/20
|-
|Coordinate with Alex on adaptive mutation runs
|Complete
|11/23/20
|12/3/20
|12/3/20
|-
|Set up PACE to help with runs
|On Hold
|11/16/20
|11/30/20
|
|}  

== Nov 16, 2020 ==

=== '''Team Meeting Notes:''' ===
* Final presentations set for Dec. 2nd (during final exam time)
* Dr. Zutty will be moving midterm grades to Canvas
* Stocks - reaching out to authors of paper to try to replicate one result that they can't replicate currently
* EZCGP - did first run of semester, but had some issues with colab timing out
* Modularity - slow week, doing runs

=== '''Sub-Team Notes:''' ===
* Dr. Zutty talked about adding novelty detection, but we're not sure if anyone will have time to get that done in time for presentations
* Everyone should work on setting up PACE
* Planned runs for final presentation - 
** 4 large runs (>500 generations) on Icehammer
** 10 regular runs (100 generations) on PACE
* Cameron has BERT layer almost implemented

=== '''Individual Tasks:''' ===
* Worked with Alex to write actual mutation_prob function in EMADE.py; commit should be on his notebook page and looks very similar to pseudocode from last week
* important differences from pseudocode:
** ind.fitness.values returns a tuple; need to figure out what those tuple values are. If one of them is accuracy, we'll just use that for simplicity rather than evaluating mutation probability based on multiple comparisons **UPDATE - ind.fitness.values is (accuracy, num_params); we'll just use accuracy and ignore num_params
** need to deepcopy the pareto front at the beginning of the function
* running into some issues using individual.fitness.values (type conflicts), need to ask Anish about the aforementioned tuple values

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Write preliminary adaptive mutation function
|Complete
|11/2/20
|11/20/20
|11/29/20
|-
|Ask Anish about what ind.fitness.values returns
|Complete
|11/21/20
|11/21/20
|11/21/20
|-
|Resolve type conflicts in individual.fitness.values
|Complete
|11/21/20
|11/28/20
|11/29/20
|-
|Set up PACE to help with runs for final presentation
|On Hold
|11/16/20/
|11/23/20
|
|}  

== Nov 9, 2020 ==

=== '''Team Meeting Notes:''' ===
* stocks - looking at irregularities in the paper they're following
* ezcgp - will start doing runs; got their code running on PACE
* Modularity - exploring why nnlearners aren't the most frequent ARLs after 40 generations

=== '''Sub-Team Notes:''' ===
* Maxim made a guide for EMADE setup on PACE; currently doesn't work for toxicity dataset but will be useful for runs for the final presentation
* Cameron is still working on implementing BERT layer, running into problems with its size
* Dr. Zutty & Tushna discussed adding primitives for CV

=== '''Individual Tasks:''' ===
* Work with Alex to finally write a preliminary adaptive mutation function
** first try to understand the current implementation for mutations:
** we see that the master algorithm has many separate mutation probability parameters, but the only ones that are actually used in the code are MUTNRPB for per-node mutations and MUTPB for per-layer mutations (add layer, remove layer, swap layer, mutate activations, mutate weight initializers). Currently all mutation probabilities are hard-coded to .01
** the mutate function takes in this mutation probability and a specific mutation function which it uses to mutate the individual
** we need to write a separate function that is called by the mutate function which returns a value for MUTPB; this function will be written in EMADE.py
*** function should accept as parameters: the pareto front (to compare the individual's fitness to), the individual, and good/bad probabilities (mutation probabilities for "good" and "bad" individuals - as a preliminary algorithm, we don't want a fancy scheme since that just makes it more difficult to see what's going on; we'll complicate the scheme as we go)
*** function computes average pareto individuals' fitness, and if the individual's fitness is greater, it returns the lower MUTPB, otherwise returns the higher MUTPB
* future implementation ideas:
** take into account the evolution's generation (i.e. later generations should have less mutations) - use some sort of weight decay, but need to be careful not to decay too quickly
** have per-mutation probabilities - if certain mutations show up more in better individuals, increase the probability of those mutations - could use some sort of discrete-time Markov chain as explained in the Vafaee, Nelson paper (see Sep. 21 notes)
* psuedocode:[[files/Psuedocode.png|none|thumb]]
[[files/Master algorithm header.png|none|thumb|485x485px]]
[[files/Mutate header.png|none|thumb]]

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Understand current mutation implementation
|Complete
|11/9/20
|11/16/20
|11/13/20
|-
|Write pseudocode for adaptive mutation
|Complete
|11/9/20
|11/16/20
|11/13/20
|-
|Find good bounds for good/bad mutation probabilities
|Complete 
|11/9/20
|11/16/20
|11/13/20
|}  

== Nov 2, 2020 ==

=== '''Team Meeting Notes:''' ===
* stocks - onboarding first semester students with code base
* ezcgp - research team did a code review with Rodd, is sort of merging with implementation team now
*modularity - onboarding first semester students with ARLs and EMADE

=== '''Sub-Team Notes:''' ===
* Discussed a new dataset we might use (sentiment classification found on Kaggle)
* discussed sub-architecture extraction
* CV team is starting to get results after trying a bounding-box approach, and seems like bounding-box won't really be pursued further

=== '''Individual Tasks:''' ===
* intro to NN resource canceled - 1st semesters are already very knowledgeable about NN architectures and don't need a quickstart
* meeting with Alex to make a preliminary adaptive mutation function - (got pushed to the following week)
* overall a slower week since I had a hell week for pretty much all my other classes, but I'll pick up the pace next week

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Write preliminary adaptive mutation function
|In Progress
|11/2/20
|11/7/20
|
|}  

== Oct 26, 2020 ==

=== '''Team Meeting Notes:''' ===
* stocks - somehow got a negative profit percentage from the run which shouldn't happen; also wrote new functions to test technical indicators
* EZCGP - research team began implementing research paper, and implementation team is working to fix bugs in the newest feature branch
* Modularity - slow week; will have new members perform runs

=== '''Sub-Team Notes:''' ===
* introductions
* overview of NN for 1st semester students & walked through neural_network_methods.py
* task 1st semester students
* EMADE enables ensemble flows & stacking (i.e. can stack an NNLearner module on top of a preprocessing module)
* Dr. Zutty asked some questions about the midterm presentation; I was a little confused

=== '''Individual Tasks:''' ===
* start on implementing adaptive mutation - look at EMADE.py
** need to think about actual mutation schemes - what actual probabilities to assign to good/bad individuals, do we take into account generation number, etc.
** talked to Alex about per-neuron vs per-layer schemes - challenge is that probability assignments in literature in per-neuron schemes probably don't generalize well to per-layer schemes since per-layer mutations have drastic effects on individuals
* make intro to NN resource for 1st semester students

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Make intro to NN resource for 1st semester students
|In Progress
|10/26/20
|10/30/20
|
|}  

== Oct 19, 2020 ==

=== '''Team Meeting Notes:''' ===
* Presentations!
*Stocks
**new subteam this year
**want to use EMADE to optimize market trading algorithms specifically and more generally use EMADE on time series regression
**methods include using existing research on genetic algorithms in market analysis, learn about various technical indicators, and use EMADE to optimize current trading algorithms
**2 subteams - research and implementation
***research subteam - create CEFLANN architecture, implement keras in EMADE, and research technical indicators
***implementation - run EMADE with regression and through colab
**first steps - what stock tickers, what technical indicators to use?
**use CEFLANN architecture bc it's computationally efficient
**had some baseline runs on SPY ticker from 2014 data
**added regression learners into EMADE
**future work - implement keras, more runs, profit calculation in EMADE, and more technical indicators
*Modularity
**abstracting parts of individuals to make "building blocks" for evolution
**ARLs - adaptive representation through learning, to introduce modularity into EMADE
***a dynamically evolved function that can be called by other functions
***useful to improve average fitness
**experiment - titanic dataset with FP FN objectives, 10 40 generation trials, compare with baseline of no ARLs

=== '''Sub-Team Notes:''' ===
* Need to figure out tasking for 

=== '''Individual Tasks:''' ===
* start on implementing adaptive mutation - look at EMADE.py
** work with Alex
* come up with resource for 1st semesters on intro to NN

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Implement adaptive mutation
|On Hold
|Oct 10, 2020
|Oct 31, 2020
|
|-
|Come up with resource for 1st semesters on intro to NN
|In Progress
|Oct 23, 2020
|Oct 30, 2020
|
|}  

== Oct 5, 2020 ==

=== '''Team Meeting Notes:''' ===
* stocks - abhiram added SGD optimizer and other regression functionality; they have all the data necessary to do a run on EMADE in the next few days; having issues with exploding standard error
* ezcgp - went through Augmentor pipeline documentation in subteam meeting; trying to get environment running on PACE but running into a "disc quota exceeded" issue but was fixed by reinstalling the repository
* nlp - ran EMADE with just mating function and different optimizers in 2 runs for 182 and 332 generations; many individuals were evaluating as "inf" even in later generations (due to input mismatch error) - need to fix this
** need to understand magnitude of the problem - having a good percentage of valid individuals is nice, but the real key is to make sure individuals that fail do so quickly
** most individuals in the pareto front have the GLoVe layer in it and do really well; that adds to the simplicity; maybe try to reward complexity instead of penalizing it?
* adf - new selection method Kevin wrote - when you select individuals for tournaments, select individuals with higher number of adfs to try to increase number of adfs
** working on adding MNIST dataset

=== '''Sub-Team Notes:''' ===
* Cameron is working on implementing a transformer; starting in pytorch but need it in keras eventually
* CV team - debugging some issues but colab is much smoother than PACE

=== '''Individual Tasks:''' ===
* start on implementing adaptive mutation - look at EMADE.py

== Sep 28, 2020 ==
=== '''Team Meeting Notes:''' ===
* nlp - ran new mating function in 2 separate runs, best individual was .035 after 94 generations; now trying runs with just one optimizer at a time (just adam optimizer gives similar results); talked about next steps - adaptive mutation (not sure how), pretrained word embeddings, adversarial regularization
* ezcgp - implementation team is working on getting new primitives in using things like maxpool and gaussian blur, also loading CIFAR dataset for testing
* modularity - has colab working but is running into PACE issues with the sep package

=== '''Sub-Team Notes:''' ===
* PACE is giving us a new issue every week; we should switch to colab so we at least have something working (rn it's the import sep error that the modularity team was having)
* figure out how to do thresholding for adaptive layers
* read about adversarial regularization - might not be super useful, used typically to refine a model and not as a general regularization technique
* add primitives to the runs

=== '''Individual Tasks:''' ===
* peer evaluation
* figure out a way to get generation's average fitness - talk to pulak
** ask Dr. Zutty about ways to get a generation's average fitness
* notebook is due on Monday, Oct. 5

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete peer evaluation
|Complete
|Sep 28, 2020
|Oct 2, 2020
|Oct 2, 2020
|-
|Look into averaging generation's fitness
|Complete
|Sep 28, 2020
|Oct 5, 2020
|Oct 11, 2020
|-
|Ask Dr. Zutty about getting generation average fitness
|Complete
|Sep 28, 2020
|Oct 5, 2020
|Oct 5, 2020
|-
|Polish notebook for due date
|Complete
|Sep 28, 2020
|Oct 5, 2020
|Oct 5, 2020
|}  

== Sep 21, 2020 ==

=== '''Team Meeting Notes:''' ===
* stocks subteam - research group read papers on stock market prediction, will discuss this week what to implement and what they want to emulate
* modularity - having PACE issues and colab issues but other teams have that figured out so they'll reach out
* nlp - read adaptive mutation paper but isn't totally applicable, trying to read more papers to improve understanding. Also adding branch crossover function and CV team has PACE working

=== '''Sub-Team Notes:''' ===
* discuss adaptive mutation paper (Vafaee Nelson discrete time Markov model) on Friday
* Alex and Tejas will also start looking at adaptive mutations, I need to work with them on reading the papers and getting comfortable with some of the math
* consider new mutation functions - hyperparameter mutation?
* consider 2-point crossover, batch normalization/adversarial regularization
* concatenate layers are being dominated - just aren't very fit

=== '''Individual Tasks:''' ===
* [https://ieeexplore.ieee.org/document/5381807 Vafaee, Nelson 2009]: talking about adaptive mutation, the paper introduces an idea called "nucleotide substitution" as a discrete time Markov chain, where at each time step, the probability of a type of node mutating is governed by a given stochastic (transition) matrix. It is known that every such Markov chain has a "stationary distribution vector" or "steady-state vector", where each entry in the vector is the probability of being in its respective state as time progresses. In this adaptive mutation scheme, at each generation in the evolution when fitnesses are calculated and the Pareto-optimal front is updated, the stationary distribution is updated to reflect the gene distribution in the Pareto-optimal front, and the transition matrix is updated to reflect the new stationary distribution. That generation's individuals are mutated by the new transition matrix, and the process repeats. This method is shown successful on a variety of problems, including the Sphere function, Schwefel's problem, the Generalized Rosenbrock function, and more. However, this has not been implemented in a neural network context, so we don't know about its potential success in EMADE.

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read & prepare to lead discussion on Vafaee, Nelson paper
|Complete
|Sep 21, 2020
|Sep 25, 2020
|Sep 25, 2020
|-
|Help Alex and Tejas get reading material on adaptive mutations
|Complete
|Sep 21, 2020
|Sep 23, 2020
|Sep 21, 2020
|}  

== Sep 14, 2020 ==

=== '''Team Meeting Notes:''' ===
* keep notebook updated
** personal work is the most important part - if you read papers, have them summarized and linked; if you do work, make it useful for others to follow along
* self-grading rubric is due next week (assessment of notebook)
* stocks subteam - research group read papers on stock market prediction
* ezcgp - focusing on reading papers, loading primitives into emade
* nlp - optimizers have been added as extra parameters
** blocker - some people don't have push access to the branch - send names to Dr. Zutty
** having some PACE issues

=== '''Sub-Team Notes:''' ===
* CV team is working on getting PACE set up
* we're still reading papers on adaptive mutation
* alex and I still need to create the branch crossover function
* Cameron is still reading about different word embeddings - try to get Bert into EMADE?
* need to get push access to the nn-vip branch
* we'll discuss the Libelli Alba paper in the meeting on Friday

=== '''Individual Tasks:''' ===
* [https://link.springer.com/article/10.1007/s005000000042 Libelli, Alba 2000]: describes an adaptive mutation function on individuals given as a sequence of bits where the most significant bits define "more" of the individual than the least significant bits. One idea described is that of a "similarity template" or "schema", which is a mask that individuals can fit if they have certain bits taking a given fixed value (and the rest left unspecified). The paper calls for evaluating the average fitness of individuals fitting given schema, and exponentially increasing the number of individuals in "good schema"; that is, schema whose average fitness is greater than the population's average fitness. This is achieved by lowering mutation probability for individuals whose fitness is greater than average, especially in the most significant bits, and increasing mutation probability among individuals whose fitness is lower than average. Given the emphasis on bit structure affecting schema (i.e. most significant bits are more important in the individual), it doesn't seem that it'll be wholly applicable to EMADE; however, we can take from it the idea of decreasing mutation probability for high-fitness individuals and increasing it for low-fitness individuals. 
* Self Evaluation: 
* [[files/Vip self evaluation.png|frameless|767x767px]] 

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read Vafaee, Nelson paper
|Complete
|Sep 14, 2020
|Sep 21, 2020
|Sep 18, 2020
|-
|Read & prepare to lead discussion on Libelli, Alba paper
|Complete
|Sep 14, 2020
|Sep 18, 2020
|Sep 18, 2020
|-
|Coordinate with Alex on swap branch mating function
|Complete
|Sep 14, 2020
|Sep 18, 2020
|Sep 18, 2020
|}

== Sep 7, 2020 ==

=== '''Team Meeting Notes:''' ===
* CV team issue - want to do preliminary run on PACE but moving forward want to get valid individuals quickly; is best process to get pretrained "decent" individuals?

=== '''Sub-Team Notes:''' ===
*want to identify the individuals with the .033 score and see if we can replicate this behavior
*want to add optimizer/activation/initializer-specific mutations
*look into adaptive mutation

=== Individual Tasks ===
* read papers on adaptive mutation
* [https://www.researchgate.net/publication/224619893_Using_genetic_algorithm_with_adaptive_mutation_mechanism_for_neural_networks_design_and_training Tsoy, Spitsyn 2005]: describes an adaptive mutation function on dense layers where mutations are adding/deleting nodes/edges; mutation is chosen with respect to the number of neurons and number of connections. The heuristic used is a function f that increases in value as the number of possible connections in the network increases (called linkage). The actual probabilities that a neuron/connection is added/removed are governed by 4 equations in the paper; the general idea is that the probability to delete a random neuron is highest, and the probability of adding a connection is also high. However as complexity increases, the probability that a connection is deleted increases. In general the applicability of this paper for our purposes is probably low, since it only works in a single dense layer.

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read Tsoy, Spitsyn paper
|Complete
|Sep 7, 2020
|Sep 14, 2020
|Sep 12, 2020
|-
|Coordinate with Alex on swap branch mating funciton
|Complete
|Sep 7, 2020
|Sep 14, 2020
|Sep 18, 2020
|}

== Aug 31, 2020 ==

=== '''Team Meeting Notes:''' ===
* CV team issue - want to do preliminary run on PACE but moving forward want to get valid individuals quickly; is best process to get pretrained "decent" individuals?

=== '''Sub-Team Notes:''' ===
*new github branch - [https://github.gatech.edu/emade/emade/tree/nn-vip nn-vip]
*CV team is trying to set up PACE
*concatenate layers isn't really working, want to figure out why
*look into adaptive mutation

=== Individual Tasks ===
* work on layer list crossover with Alex - want to implement branch-based nnlearner layer list crossover
** read current mating functions in emade_operators.py, specifically familiarize self with swap layer mutation and think about how that could be converted to mating function
** continue reading through DEAP's mating functions

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read mutation functions in emade_operators.py
|Complete
|Aug 28, 2020
|Aug 31, 2020
|Sep 3, 2020
|-
|Read through DEAP mutations source code
|Complete
|Aug 28, 2020
|Aug 31, 2020
|Sep 3, 2020
|}

== Aug 24, 2020 ==

=== '''Sub-Team Notes:''' ===
*I need to pick task(s) to work on
**I'm interested in mating/mutation, but I'm not very familiar with these in EMADE - I'll have to do some reading & research
***tasks: adaptive mutation layers (train mutation function to prefer "useful" layers), cross over branches of layerlists (not sure about this)

=== Individual Tasks ===
* pick tasks to work on for semester
* get EMADE working on colab

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Choose tasks to work on
|Complete
|Aug 24, 2020
|Aug 28, 2020
|Aug 26, 2020
|-
|Read mutation functions in emade_operators.py
|In Progress
|Aug 28, 2020
|Aug 31, 2020
|
|-
|Read tests in neural_network_mutations_test.py
|In Progress
|Aug 28, 2020
|Aug 31, 2020
|
|-
|Read through DEAP mutations source code
|In Progress
|Aug 28, 2020
|Aug 31, 2020
|
|-
|Call w/ Alex Liu to split up tasks in adding mating/mutation
|Complete
|Aug 28, 2020
|Aug 30, 2020
|Aug 29, 2020
|}

== Aug 17, 2020 ==

=== '''Team Meeting Notes:''' ===
* We are allowed to switch subteams if desired
* I want to stay on the NLP subteam - talk to Pulak to make that happen

=== '''Sub-Team Notes:''' ===
*Talked about the NLP team and goals for the rest of the semester; goals include:
**read new papers (WANNs, other Neural Architecture Search papers, weight sharing with TuNAS, Evolved Transformer, etc.)
**Get Colab working for nn branch
**Find better individuals on Wikidetox
**Add different word embeddings
**Make activation implementation more efficient
**Add new mating and mutation functions
*I need to pick task(s) to work on

=== Individual Tasks ===
* pick tasks to work on for semester
* get new laptop set up for EMADE
** issues included:
*** "ImportError: cannot import name 'EMADE'" - resolved by checking out nn branch instead of nlp-nn
*** "ModuleNotFoundError: No module named 'keras_pickle_wrapper'" - resolved by 'pip install keras_pickle_wrapper' (duh)
*** "ModuleNotFoundError: No module named 'sep'" - resolved by installing Microsoft Visual C++ and then 'pip install sep'
*** "gpframework 1.0 requires mysqlclient>=1.4.6, but you'll have mysqlclient 1.3.14 which is incompatible" - resolved by "pip install mysqlclient --upgrade"
*** in master.err - "pymysql.err.InternalError: (1049, "Unknown database 'nlp-nn'") - resolved (after lots of error checking) by renaming database to "nlpnn" in MySQL Workbench (since SQL apparently doesn't like dashes in database names)
*** in master.err - "ValueError: selTournamentDCD: individuals length must be a multiple of 4" - resolved by uninstalling deap ("pip uninstall deap"), then installing deap from source ("git clone https://github.com/DEAP/deap"), then installing deap again ("python setup.py install" in the deap directory)
** thanks to Anish for helping a ton with getting this to work

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get EMADE set up (new laptop)
|In Progress
|Aug 17, 2020
|Aug 24, 2020
|Aug 24, 2020
|-
|Choose tasks to work on
|In Progress
|Aug 17, 2020
|Aug 24, 2020
|Aug 26, 2020
|-
|Get MySQL set up (new laptop)
|Complete
|Aug 17, 2020
|Aug 24, 2020
|Aug 20, 2020
|}

== Apr 20, 2020 ==

=== '''Team Meeting Notes:''' ===
Presentations!

ADFs

- focus on modularity in EMADE

- modified genetic process to include an "ADFs" section with 4 subprocesses - find ADF candidates, select ADF nodes, insert ADFs into generation, and update primitive set

- they select ADFs based on frequency

- Differential fitness - measures improvement between an individual and its parents - if > 0, then it improved on its parents and is a candidate for ADFs

- 3 projects:

- primitive analysis - analyze how 'useful' the ADFs selected are

- ADF selection - create a selection method including ADF frequency

- differential fitness

- previously (in midterms) achieved statistical significance in using ADFs around generation 19

- Primitive Analysis:

- goal is to find effects of ADFs on the population and determine whether or not they are useful

- also provide some explainability for why differences are seen

- compared primitives appearing in ADFs to primitives appearing in baseline pareto front from a no-ADF run

- the average difference between the two is very low - around .23%

- ADFs are not as diverse as the pareto front

- results: evaluated the differences in average fitness between pareto individuals with and without ADFs

- found that most ADFs generally increase accuracy by about 1%, but haven't figured out why they work better

- the ADF increasing the accuracy the most improved by ~16%

- Differential Fitness

- motivation: Rosca 1995

- measures change in fitness between individual and parent

- compared differential fitness adf primitives with intel adf primitives

- learnerType was favored a lot by differential fitness

- differential fitness ADFs did not prefer nested ADFs

- results are not statistically significant, just a weak correlation

- differential fitness for some reason has more ADF types

- Selection Methods

- expectation: there should be a small increase in the number of ADFs

- benchmark and new selection method performed very similarly, but the variance in average number of ADFs per individual for the new selection method was much lower

- analyzed relationship between ADF frequency and AUC; there is very little correlation - shows that just adding ADFs without some guarantee of "goodness" doesn't really help

- TL;DR ADFs don't affect final AUC as much, they just help faster convergence

- Future Work

- code refactoring

- code documentation

- more runs and more samples

- more heuristics

- more complex selection/mating/mutation methods

- more complex data set (more fields with more complex relationships)

- longer runs might be good

Research Fundamentals

- basic idea - how to get rid of bloat

- bloat metric - population level metric that is intended to quantify how much mean program size changes with respect to fitness

- calculated as (normalized size change / normalized fitness change)

- speciation - species are assigned based on the shared topological structure of trees

- not actually based on the primitives in the trees; rather is based on the topological structure (arity)

- species are assigned greedily via a tree distance metric computed between tree sizes (wait till the metric is smaller than a threshold)

- fitness sharing - punish individuals from highly-populated species and protect more unique individuals

- for selection, generate a probability vector based on species population with a value for every individual in the population

- crossover - similar to one-point crossover but identifies common region between two individuals and then (1) swaps internal nodes randomly and (2) swaps tree branches randomly

- experiment: run EMADE on titanic for 30 generations with FP and FN as objectives

- vary parameters: distance thresholds, crossover method, etc.

- results:

- fitness sharing with speciation thersholds of .15 performed better than baseline but not statistically significant (in terms of bloat)

- with distance thresholds of .3 and .6, it actually did worse

- the distance metric used might not be well suited to EMADE

- neat crossover was statistically worse up to generation 30 in hypervolume, but there was no statistically significant difference in bloat

- there was very high variance between runs for each experiment

- there is a mysterios drop in the number of new individuals per generation around generations 16-30

- speciation=.15,.3 have low individual-species quotients (most individuals are in their own species); speciation=.6 has a quotient of 8.31 (much more)

- next steps - investigate tree distance metric and explore other options for metrics that might work better

- explore how neat crossover performs with speciation and fitness sharing

- investigate correlation between drop in #individuals and other metrics like hypervolume

- investigate intra-species characteristics and the evolution of species

NLP Time Conflict

- work on text summarization; have been creating numerous primitives

- goals: testing summarization primitives, automate running tests on PACE, create documentation for future semesters

- issues: PACE was a pain, spent a lot of time debugging SQL issues

- summary primitives:

- svc_scikit_multi: like SVC classifier but encodes positional information as well

- num_named_entities - counts the number of named entities in a corpus

- TFISF (term frequency-inverse sentence frequency) - way of assigning numbers to each sentence in a document to label the importance of that sentence. Rarer words have more significance.

- takes over 2 hours to run

EZCGP

- use CIFAR-10 dataset

- 1st semester students ran evolutions using ezCGP 1.0 on PACE using a single GPU

- wanted to find contemporary CNN architectures and implement them in tensorflow 2.0

- comparing effectiveness of ezCGP with similar networks

- results: achieved better test accuracies without any augmentations or data preprocessing

- some of the models they compare against were built for different image datasets

- I started having network issues right about here and wasn't able to get the rest of the presentation

=== '''Sub-Team Notes:''' ===
N/A

=== '''Action Items:''' ===
N/A

== Apr 13, 2020 ==

=== '''Team Meeting Notes:''' ===
* all subteams went through their progress for the week
* final presentations will take place next week, Apr 20
* Dr. Rohling will meet with each subteam before Monday to go through the presentations

=== '''Sub-Team Notes:''' ===
*As discussed below, we needed a new accuracy metric for the Toxicity dataset, so Mohan worked on that
*I reported my findings with the GloVe embedding vetors; since they worked so well, I was tasked with adding them as a primitive to EMADE. This ended up being difficult for 3 main reasons:
*#I needed to figure out where to place to GloVe file since it is >300Mbs large
*#While all the other first semesters were creating new primitives, I was editing the existing Embedding layer primitive so it could take an optional parameter to use GloVe embeddings instead of randomly initialized embeddings; thus a lot of the stuff covered in the meeting to help first semesters didn't really apply to me
*#I had to edit the tokenizer methods in text_processing_methods since I needed access to the tokenizer in order to create the embedding vectors
*Each first-semester student was given a primitive to add to EMADE; we were asked to write them, test a few generations in EMADE, and then push to the nlp branch on GitHub
**We realized later that first-semesters weren't able to push directly to the repository, so we forked the repository and sent in pull requests instead
*Finally, we were each asked to make a slide with our primitive for the presentation; mine is shown below: (see https://docs.google.com/presentation/d/1sfyO-eB262HKiVnPvO8vDu_6n4wR-Q1RJrIfldWo810/edit#slide=id.g745514b8c1_8_15)  [[files/GloVe Slide.png|frameless|600x600px]]

=== '''Individual Task:''' (GloVe in EMADE) ===
* link to GitHub commit below. In particular, see changes to files gp_framework_helper.py, neural_network_methods.py, and text_processing_methods.py
** https://github.gatech.edu/emade/emade/commit/54674b050e6f930568f211f840bfa9b3519771c9
* First, in the neural_network_methods.py file:
** I added the "use_glove" boolean to the EmbeddingLayer initialization to determine whether to use randomly initialized or GloVe embedding weights to initialize the Embedding layer
** Next, I wrote the get_glove_embedding_matrix method that reads GloVe vectors from a file and creates a dictionary where for each word in the vocabulary of the used tokenizer, that word maps to the corresponding embedding vector from the GloVe file  [[files/GloVe code.png|frameless|444x444px]]3
** Additionally, I edited the main block of the NNLearner function to use the new version of the Embedding layer
* Secondly, in the text_processing_methods.py file:
** The word tokenizer method originally didn't return the tokenizer used, but I needed that to be able to map words to their GloVe vectors, so I changed the return statement of tokenizer2 to also return the tokenizer. As we can see above, I used this tokenizer in the get_glove_embedding_matrix method.
* Finally, in gp_framework_helper.py:
** I simply changed the addPrimitive call for the EmbeddingLayer to also include a boolean value as below:
 pset.addPrimitive(nnm.EmbeddingLayer, [int, EmadeDataPair, bool], nnm.LayerList, name='EmbeddingLayer')
The trickiest part of the assignment was trying to figure out where to store the 300+ MB GloVe file.In the end, since I couldn't figure it out, I asked Anish, and he said to just leave it in src and he would take care of it. Moreover, while I was able to run EMADE on my machine, I did not see the GloVe embedding layers appear in any of the individuals. 

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add GloVe vectors to EMADE
|Complete
|Apr 13, 2020
|Apr 15, 2020
|Apr 17, 2020
|-
|Create slide for final presentation
|Complete
|Apr 13, 2020
|Apr 20, 2020
|Apr 19, 2020
|}

== Apr 6, 2020 ==

=== '''Team Meeting Notes:''' ===
* all subteams went through their progress for the week

=== '''Sub-Team Notes:''' ===
*We went over the completed notebooks and discussed results. The models were all achieving >98% accuracy; we think this had to do with the usage of pre-trained GloVe embedding vectors
*I was given the task of analyzing GloVe vector vs randomly-initialized Embedding layer performance on the movie reviews dataset since the Toxicity dataset was giving very high accuracy to nearly any results. 
**We later realized that the reason for the high accuracy was due to the accuracy metric Keras uses by default. The metric used is for single-label binary classification, but we were using it for multi-label binary classification. 
**For example, if the ground truth classification vector was (1,0,1,1) and our predicted vector was (1,1,1,1), we would want the accuracy for that instance to be 0, but it would actually end up being 3/4. 
**Thus, our models achieved crazy high accuracy easily just by getting "somewhat close".
*We also discussed getting EMADE working with Google Colab
**Anish made a setup guide for this directly after the meeting
**Need to use either port forwarding or a remote mysql database to get it to work
**Colab gives free GPU/TPU usage for hours at a time, so it is perfect for testing things in quick runs on EMADE

=== '''Individual Task''': (IMDB Dataset) ===
* Below is the link to a Jupyter Notebook with my work; it's on Google Drive rather than GitHub since I was using Google Colab to speed the training process of my neural networks
** https://drive.google.com/open?id=1Tm2eR-XHtx7zdne_NWD_ZVb8at7i7T_0
* I first preprocessed the text by removing punctuation and special characters, line breaks, single characters (a, i were common but didn't give much information), and multiple consecutive spaces [[files/Text preprocessing.png|frameless|450x450px]]
* I decided to compare and contrast the results of four different models. Each model consists of an Input layer, an Embedding layer, an LSTM layer, and two Dense layers.
** model1 - trainable embedding layer initialized to GloVe vector embeddings
** model2 - non-trainable embedding layer initialized to GloVe vector embeddings
** model3 - non-trainable embedding layer initialized to a matrix of ones
*** this simulates the effect of having no embedding layer, just random word tokenizations instead of word embeddings
** model4 - trainable embedding layer initialized randomly (in accordance to Gaussian normal distribution)
* The results pretty much followed what was expected:
** model1 - 89% accuracy after 10 epochs of training. The model actually started to overfit after just 5 epochs, as we can see from the graph of training vs validation loss below:   [[files/Model1 accuracy,loss.png|frameless|399x399px]]
** model2 - 87% accuracy after 15 epochs of training. The model began to overfit after 12 epochs.
** model3 - 50% accuracy after 10 epochs of training. I wanted to train it longer to see if more time would help, but training these models is actually quite slow, and each epoch took around 5 minutes to complete (even with a GPU), so I decided against it. This just shows that an embedding layer is vital in NLP tasks
** model4 - 87% accuracy after 10 epochs of training. The model started to overfit around the 10 epoch mark.
* I didn't do much hyperparameter tuning, other than fiddling with the number of epochs, so the accuracies actually could have gone higher. Nevertheless, I was able to show that the pretrained GloVe embedding vectors increase the accuracy of the model, as expected. Thus it is a good idea to try to get GloVe embeddings incorporated in EMADE.
* See the Google Drive link at the beginning of this section to view more detailed results and plots

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get EMADE set up in Google Colab
|Complete
|Apr 6, 2020
|Apr 8, 2020
|Apr 7, 2020
|-
|Analyze performance of pretrained GloVe vectors in movie reviews (IMDB) dataset
|Complete
|Apr 6, 2020
|Apr 13, 2020
|Apr 13, 2020
|}

== Mar 30, 2020 ==

=== '''Team Meeting Notes:''' ===
* all subteams went through their progress for the week

=== '''Sub-Team Notes:''' ===
*First semester students were told to work on either the Toxicity (NLP) dataset or the Chest X-Ray (CV) dataset; I chose the Toxicity dataset
*We discussed EMADE and seeding individuals into EMADE. Anish linked a guide he has been working on in the Slack; that was very helpful to read through
**talked about seeding trained individuals into EMADE (seeding_from_file)
**we work off the nlp-nn branch (git checkout nlp-nn)
**all the methods we have to mess with are for the most part in the src folder, files to note;
*** text_processing_methods - contains methods for tokenization/vectorization of text
*** neural_network_methods - contains layer creation and neural network layer primitive specifications
*** GPFramework - where you would go to add new primitives to EMADE

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Familiarize myself with Keras/TF API
|Complete
|Mar 23, 2020
|Mar 30, 2020
|Mar 30, 2020
|-
|Create model in Toxicity Jupyter Notebook
|Complete
|Mar 30, 2020
|Mar 30, 2020
|Mar 30, 2020
|}

== Mar 23, 2020 ==

=== '''Team Meeting Notes:''' ===
* All courses are online now due to COVID, so we'll use BlueJeans for the rest of the semester
* I was assigned to the NLP subteam

=== '''Sub-Team Notes:''' ===
*All the members introduced themselves
*Talked about the NLP team and goals for the rest of the semester
** focus on Neural Networks in EMADE, two main branches for the two datasets we use
*** toxicity dataset - NLP focused, using NLP principles in EMADE to classify toxic comments
*** chest x-ray dataset - CV focused, classify chest x-rays
** first semester students will work on defining new primitives in EMADE
*** first semester students will first familiarize themselves with the toxicity dataset
**** there's already a notebook in the nlp EMADE branch for ML with the toxicity dataset; we should familiarize ourselves with that and build a NN achieving > 85% accuracy on that dataset
**** the plan is to do partner coding, but not sure how that'll work with the online format of the course

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Pull nlp-nn branch of EMADE
|Complete
|Mar 23, 2020
|Mar 23, 2020
|Mar 23, 2020
|-
|Join nlp slack channel and read pinned documents
|Complete
|Mar 23, 2020
|Mar 23, 2020
|Mar 23, 2020
|-
|Familiarize myself with Keras/TF API
|Complete
|Mar 23, 2020
|Mar 30, 2020
|Mar 30, 2020
|}

== Mar 9, 2020 ==

=== '''Team Meeting Notes:''' ===
Group presentations:
# Automatically Defined Functions
#* a way to introduce modularity & reusability in EMADE
#* Example implementations:
#** Koza (ADF): coined term ADF, had subroutiens defined for each individual that evolved alongside them
#** Our ADFs are defined for an entire population; can think about it as a "useful" subtree used as a primitive
#* useful for:
#** improving a population's overall fitness
#** frequency: the more often a subtree is used throughout past generations, the more likely it'll be used in the future
#** fitness: if it's used often in high fitness individuals, it's a good subtree
#* data collected from runs:
#** AUC information, most frequent ADFs, most frequent primitives in ADFs, size, % individuals using ADFs, etc.
#* since we didn't find much improvement in ADFs, now comparing frequencies of primitives
#* want to look at more heuristics in the future
#** e.g. differential fitness - fitness difference between child and best parent
#* other future ideas:
#** evolving ADFs and using ADFs in the evolutionary process
#* team meetings on Thursday: 4:30-5:30
# NLP (1)
#* NLP - aims to improve computers' language comprehension
#* we want to do so using standard ML models
#* Text classification:
#** old primitives:
#*** vectorizers - convert text into vectors of numbers
#*** sentiment - converts text into vectors encoding sentiment
#*** stemmatizer - reduces words based on established word stems
#** This semester:
#*** lots of hypothesis testing to look at statistical significance of results
#** lots of time spent fixing issues running EMADE on PACE-ICE server
#* Also use Neural Networks:
#** goal: want to incorporate Keras API into EMADE
#** created NNLearner primitive - takes in a LayerList and Data Pair
#*** LayerList is what our layers look like
#*** EMADE can't do this the way it does with normal primitives because Keras doesn't pickle layers, so we had to do NNLearner
#*** works currently, but it's hard to train. Works currently on a single computer, but want to get it to work on distributed GPUs
#* meetings @ 4:30 on Fridays
# Research Fundamentals (Bloat Control)
#* bloat = increase in program size without corresponding improvement in fitness
#* why does bloat exist? 2 theories:
#** fitness causes bloat - bigger programs are on average fit more than smaller programs since they can explore more of the solution space
#** crossover bias theory - smaller trees don't get selected as much for crossover bc they're on average less fit
#* bloat is bad - more time, more memory, and less effective breeding
#* to measure bloat: use normalized size change / normalized fitness change
#** for fitness, use hypervolume (AUC for problems with more than 2 dimensions)
#* neat-GP is a bloat-control technique that aims to control bloat naturally (rather than explicitly limiting size of tree, etc.)
#* we subdivide population into "species" to protect solution complexity and encourage diversity
#* fitness sharing penalizes members from over-represented species
#* discourage inter-species crossover maybe?
#** results seem to say that unrestricted mating might be better
#** sample size is small so results are not statistically significant
#* future work: look at how altering speciation distance threshold affects fitness sharing and bloat'
#* meetings on fridays at 1:15PM

=== '''Sub-Team Notes:''' ===
* N/A - no team requirements for the week

=== '''Action Items:''' ===
None - Spring Break

== Mar 7, 2020 ==

=== '''Team Meeting Notes:''' ===
* N/A - meeting outside of scheduled class

=== '''Sub-Team Notes:''' ===
* We met at the CULC to get a run of EMADE in. We connected to Karthik's master process and SQL server and were able to run EMADE for 32 generations.
* I used SQL commands to keep track of how far we were in our run and to view the pareto individuals. For example, to view the pareto individuals, I used:
 SELECT * FROM titanic.individuals
 JOIN titanic.paretofront
 ON titanic.individuals.hash = titanic.paretofront.hash
* Initially I had trouble with getting the command to run, but then I realized that SQL uses a single "=" for comparison instead of a double "==" as I originally thought.
* We were worried that 32 generations wouldn't be enough, but we realized that we didn't have nearly enough computational power to run more generations in the timeframe we had.
* We also began working on the presentation, but realized that a majority of the presentation would have to be done on our own time outside of the meeting due to time constraints. I worked on the MOGP and Tradeoffs with MOGP slides. In addition, I took charge of overall presentation flow, grammatical errors, and overall formatting. The presentation is linked below:
** https://docs.google.com/presentation/d/1HKAtfxUGHAqXmvGPzLQAk5f3CI09wXYGTkR6E8MKtmQ/edit#slide=id.g52d13e7bd1_1_17

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete EMADE run
|Complete
|Mar 7, 2020
|Mar 8, 2020
|Mar 8, 2020
|-
|Complete MOGP slides in presentation
|Complete
|Mar 7, 2020
|Mar 8, 2020
|Mar 8, 2020
|-
|Complete formatting & overall presentation check
|Complete
|Mar 7, 2020
|Mar 9, 2020
|Mar 9, 2020
|}

== Mar 4, 2020 ==

=== '''Team Meeting Notes:''' ===
* This meeting was another work day on EMADE.

=== '''Sub-Team Notes:''' ===
* We were all ultimately able to connect to the master process, although some team members experienced some issues with their worker processes
* We started running some generations on Karthik's MySQL server, but of course didn't have time to run many during the meeting
* We plan to meet this weekend to get a good run of EMADE in

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up meeting time for the weekend
|Complete
|Mar 4, 2020
|Mar 5, 2020
|Mar 5, 2020
|-
|Run EMADE on Titanic dataset
|Complete
|Feb 29, 2020
|Mar 7, 2020
|Mar 8, 2020
|}

== Feb 29, 2020 ==

=== '''Team Meeting Notes:''' ===
None (hackathon day)
=== '''Sub-Team Notes:''' ===
* Most of the team was able to attend the hackathon; I attended for the whole time (2:00PM to 6:00PM)
* I figured out how to run the MySQL server by going to services.msc and starting the MySQL service from there
* I also got EMADE to download the git-lfs files successfully by using git-lfs pull
* We each tried to be the master process and have the other members connect worker processes to our server; after showing that we could all do it, we decided that Karthik would have the master process
* We were able to have individuals show up in his server, but didn't have time for more than 1 generation to run

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run EMADE on Titanic dataset
|Complete
|Feb 29, 2020
|Mar 4, 2020
|Mar 8, 2020
|}

== Feb 26, 2020 ==

=== '''Team Meeting Notes:''' ===
* Group work-day in meeting
* Anyone with MySQL 8.0 must downgrade to MySQL 5.7 since some EMADE features don't work on 8.0
* Presentation day will be March 9th

=== '''Sub-Team Notes:''' ===
* Set up EMADE
** I had to re-clone EMADE because git-lfs wasn't working properly - I tried to git-lfs the datasets but they were still just the file hashes; I still need to figure out how to fix that
* MySQL
** We decided that Karthik would run the master process. I tried to download MySQL but wasn't able to get the server running. MySQL works as a "service" on Windows; I need to figure out what that means and how to start a MySQL server. Since EMADE wasn't yet working for me, I wasn't able to connect to Karthik's server with a worker process

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get EMADE to work properly
|Complete
|Feb 26, 2020
|Mar 4, 2020
|Feb 27, 2020
|-
|Figure out how Windows "services" work
|Complete
|Feb 26, 2020
|Mar 4, 2020
|Feb 29, 2020
|-
|Set up MySQL server
|Complete
|Feb 26, 2020
|Mar 4, 2020
|Feb 29, 2020
|}

== Feb 19, 2020 ==

=== '''Team Meeting Notes:''' ===
Intro to EMADE - doing genetic programming using sklearn functions as primitives
* EMADE - Evolutionary Multi-objective Algorithm Design Engine
* Uses a multi-objective evolutionary search with high-level primitives (i.e. sklearn ML functions)
* need to finish setting up EMADE, including MySQL server and git lfs
* to run EMADE: run python src/GPFramework/launchGTMOEP.py templates/input_titanic.xml (after navigating to the top level directory) - first part is the module to run, second part is the input file
* to connect to mysql server from command line: mysql -h hostname -u username -p
* SQL queries: select, from, where

=== '''Sub-Team Notes:''' ===
* N/A - no team requirements for the week

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up MySQL server
|In Progress
|Feb 19, 2020
|Feb 26, 2020
|push to next week
|-
|Refresh SQL knowledge
|In Progress
|Feb 19, 2020
|Feb 26, 2020
|Feb 26, 2020
|}

== Feb 12, 2020 ==

=== '''Team Meeting Notes:''' ===
Project Presentations! Notes below:

==== Group 4: ====
* chose age, sex, passenger class, and siblings + family
* Used decision tree, stochastic gradient descent, random forest, and multilayer perceptron classifiers.
* primitives used: add, subtract, multiply, negative, sin , cos, arctan, sigmoid
** sigmoid worked on normalized data nicely
* allowed 4 arguments instead of just 1
* used gengrow instead of genhalfandhalf or genfull - gengrow gives more variance in tree size
* used NSGA2, which helps give points that are farther away from each other in the Pareto frontier
* MOGP worked just as well as standard ML; the AUC value for MOGP was much lower due to more data points

==== Group 5: ====
* removed cabin, ticket data; one-hot encoding for title from each name; removed name, embarked; normalized data; replaced missing entries with mean/mode values
* Models: voting ensemble with gradient boosting, histogram-based gradient boosting; stochastic gradient descent; SVM; random forest; Gaussian Naive Bayes; Passive Aggresive
* GP: used booleans, floating point +,-,*,/, logic operators
* ML did better than GP, but could have spent more time on GP

==== Group 2: ====
* replaced missing values with randomly sampled values
* used a lot of different classifiers
* primitives used: +, -, *, /, ^, and logic operators; applied sigmoid function on every "computation" operator
* trained on a random sample of the training data to reduce overfitting
* genetic program worked much better than ML classifiers

==== Group 1: ====
* used age, ticket price, sex, child/parent, #siblings, #parents/children
* replaced unknown values with mean values
* did 3-fold cross-validation to reduce bias
* used basic arithmetic and basic logic operators
* used cxOnePoint for mating; mutation exchanged subtrees with full trees
* more variety in performance of Pareto optimal MOGP individuals than ML individuals

=== '''Sub-Team Notes:''' ===
* N/A - no team requirements for the week

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up EMADE
|Complete
|Feb 12, 2020
|Feb 19, 2020
|Feb 19, 2020
|}

== Feb 5, 2020 ==

=== '''Team Meeting Notes:''' ===
* Genetic Programming for Titanic Problem:
** make an evolutionary loop
** use GP techniques learned to develop a Pareto optimal set of trees that classify points in the Titanic dataset on the same feature set used in the previous week's work
** can't use any of DEAP's algorithms for the evolutionary loop
** draw another Pareto frontier
* Presentation on last 2 weeks of work next week. Guidelines:
** talk about feature creation process
** talk about evolutionary loop
** compare and contrast algorithm performance
** include graphs, make sure Pareto front lines are going in the appropriate directions
*** include AUC calculations
** include page numbers
** include a "takeaway" box for each slide

=== '''Sub-Team Notes:''' ===
See https://github.gatech.edu/atusnial3/VIP----Automated-Algorithm-Design-3 for the code referred to in this section; see the file titanic_gp.ipynb

==== Sub-Team Meeting Feb 6, 2020: ====
* Met for 2 hours
* Created primitive set, registered basic DEAP functions, added same data pre-processing as in the previous week's work
* I personally wrote the evaluation function eval_ind to get the tuple of fp+fn, fp, fn to minimize
* Ran into a "recursion error" when trying to evaluate an individual, need to figure out how to fix that

==== Sub-Team Meeting Feb 8, 2020: ====
* Met for 2 hours
* Fixed the recursion error - turns out we entered the arity of one of our primitive set functions wrong
* Wrote the evolutionary loop; ran into trouble trying to figure out if an individual was Pareto with the given methods, so I personally wrote functions to do so

==== Sub-Team Meeting Feb 10, 2020: ====
* Met for 2 hours
* Fixed final issues with the evolutionary loop and Pareto dominance methods
* Decided presentation roles and begun work on presentation. Communicated via Slack and shared presentation on Google Slides. My part was to present the functions we wrote to determine if a given individual was Pareto or not (slides 9, 10).
* Presentation is linked here: [https://docs.google.com/presentation/d/1ICXOqBV7iUe1lpmjNrr2cEbk_yJKmNF5KFg9bf7ZDrE/edit?usp=sharing Project Presentation]
[[files/Pareto code.png|none|thumb]]

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add primitive set & do preprocessing 
|Complete
|Feb 5, 2020
|Feb 6, 2020
|Feb 6, 2020
|-
|Write the evaluation function
|Complete 
|Feb 5, 2020
|Feb 6, 2020
|Feb 6, 2020
|-
|Figure out the recursion error in the evaluation function
|Complete
|Feb 6, 2020
|Feb 8, 2020
|Feb 8, 2020
|-
|Write the evolutionary loop
|Complete
|Feb 8, 2020
|Feb 10, 2020
|Feb 10, 2020
|-
|Fix the vectorization issues in the Pareto dominance method
|Complete
|Feb 8, 2020
|Feb 10, 2020
|Feb 9, 2020
|-
|Finish presentation slides for Pareto dominance section
|Complete
|Feb 10, 2020
|Feb 12, 2020
|Feb 12, 2020
|}

== Jan 29, 2020 ==

=== '''Team Meeting Notes:''' ===
* Learned to use Kaggle, sklearn models, machine learning techniques to work on Titanic problem
**want to minimize the number of False Positives and False Negatives and create a Pareto optimal set of models
*Tips:
** learn to use pandas
** learn to use numpy
** read other Kaggle notebooks

=== '''Sub-Team Notes:''' ===
* Roadmap:
** clean the data
** identify feature set & engineer features
** split into train and test folds
** develop models (1 model per group member)
** evaluate models

==== Sub-Team Meeting Feb 2, 2020 ====
* Group preprocessed data. In particular:
** dropped unnecessary columns (Name, Ticket, Cabin, PassengerID)
** filled missing values in with either mean feature value (for numerical data) or mode feature value (for categorical data)
** classified sex in binary form - 0 for male, 1 for female
** mapped ages into 5 bins, each spanning a 16-year period
* Decided to use Age, Sex, and family_size as the feature set based on heatmap illustrating correlation between the variables
* Experimented with different sklearn models: in particular, I tried both a linear-kernel SVM (LinearSVC) and 2-layer neural network (Multilayer Perceptron). I obtained minimums of 19 FP and 37 FN with the MLP classifier, and 18 FP / 38 FN with the SVM.
* GitHub repo with code: https://github.gatech.edu/atusnial3/VIP----Automated-Algorithm-Design-3
** see titanic_solution_at.ipynb for code implementing the MLP and SVM classifiers, as well as relative accuracies with the other group members' models (at the bottom of the file)

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Titanic Data Preprocessing
|Complete
|Jan 29, 2020
|Feb 2, 2020
|Feb 1, 2020
|-
|Titanic Feature Selection/Engineering
|Complete 
|Jan 29, 2020
|Feb 2, 2020
|Feb 2, 2020
|-
|Titanic Model Selection
|Complete
|Feb 2, 2020
|Feb 5, 2020
|Feb 5, 2020
|}

== Jan 22, 2020 ==

=== '''Team Meeting Notes:''' ===
* Multiple Objective Programming
** Measuring algorithm performance:
*** for maximization measures:
**** TRP = TP/(TP + FN)
**** TNR = TN/(TN + FP)
*** for minimization measures:
**** FNR = FN/(TP + FN) = 1 - TPR
**** FPR = FP/N = FP/(TN+FP) = 1 - TNR
*** other measures
**** (maximize) PPV = TP/(TP + FP) [precision of positive predicted value]
**** (minimize) FDR = FP/(TP + FP) = 1 - PPV [false discovery rate]
**** (maximize) ACC = (TP + TN)/(TP + TN + FP + FN) [accuracy]
** Pareto optimality
*** an individual is Pareto if no other individual outperforms it on all objectives

=== '''Sub-Team Notes:''' ===
* N/A - not yet assigned to a team

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete DEAP Lab 2 Part 2
|In Progress
|Jan 22, 2020
|Jan 29, 2020
|Feb 1, 2020
|-
|Review multi-objective programming
|In Progress 
|Jan 22, 2020
|Jan 29, 2020
|Jan 29, 2020
|}

=== '''DEAP Lab 2 Part 2 Conclusions:''' ===
* Steps:
** create new fitness and individual classes for the multi-objective problem
** add 3 new primitives (sin, cos, tan)
** write evaluation function
** write pareto-dominance function (for visualization purposes)
** initialize a 300-individual population, and sort by pareto dominance (deap-given method)
** undergo 50-generation evolutionary algorithm
** visualize population (graphs attached)
** calculate AUC measure to evaluate performance
[[files/MSE vs Tree Size.png|none|thumb]]
[[files/Generation v fitness.png|none|thumb]]
[[files/Pareto front my.png|none|thumb]]

== Jan 15, 2020 ==

=== '''Team Meeting Notes:''' ===
* Genetic Programming Basics:
** Before, we took an individual and used a function evaluator to get an objective. Now, our individual is the evaluator, and it performs some operation on input data to receive an output that is then scored.
** We represent program as a tree - nodes are primitives (functions) and leaves are terminals (parameters)
*** Data propagates upwards and output goes to the root of the tree
*** Tree is stored as pre-order traversal
** Crossover in tree-based GP is exchanging subtrees
*** We randomly select 2 nodes as roots of subtrees and swap those subtrees
** Mutation can be: inserting, deleting, or changing a node (or subtree) 
* Example: Symbolic Regression
** Use primitives (operators) and terminals (integers, variable x) to approximate sin(x)
** To evaluate: feed a number of input points to get outputs, and use sum of squared errors (or MSE or RMSE or some other measure) as measure of error from ground truth

=== '''Sub-Team Notes:''' ===
* N/A - not yet assigned to a team

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete DEAP Lab 2 Part 1
|In Progress
|Jan 15, 2020
|Jan 22, 2020
|Jan 28, 2020
|-
|Review Genetic Programming
|In Progress 
|Jan 15, 2020
|Jan 22, 2020
|Jan 28, 2020
|}

=== '''DEAP Lab 2 Part 1 Conclusions:''' ===
* First we add primitives to the pset - traditional operators, as well as divide and factorial operators
** We have to create our own divide and factorial operators to make them input-safe. In divide, we return 0 for division by zero; in factorial, we take the floor and absolute value of our integers.
* Then (like before) we define a toolbox and register individual and population constructors
** We also register an expr function that generates a tree with min and max depth
* We then (like before) register genetic operators
* We run the algorithm for 40 generations; attached are the minimum fitness values by generation.
[[files/Minimum Fitness Values.png|300x300px]]

== Jan 8, 2020 ==

=== '''Team Meeting Notes:''' ===
* Introduced Genetic Algorithms:
* Vocab:
** objective - a value to maximize, describes individuals
** fitness - how good an individual is compared to other individuals
** evaluation - objective function for an individual (calculates objective)
** selection - how to determine which individuals will pass on their "genes"
** mating/crossover - once we've selected which individuals pass on their genes, this is how we combine the genes of those individuals via either single-point or double-point combination
** mutation - we add random modifications to genes to maintain diversity
* Algorithm:
*# Create a random population
*# Determine fitness of population by evaluating every individual
*# Repeatedly, until best individual is good enough:
*## select parents from population (randomly)
*## perform mating on parents to create new population (next generation)
*## perform mutation on new population
*## determine fitness of new population

=== '''Sub-Team Notes:''' ===
* N/A - not yet assigned to a team

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Join VIP Slack group
|Completed
|Jan 8, 2020
|Jan 15, 2020
|Jan 15, 2020
|-
|Install DEAP Python package
|Completed 
|Jan 8, 2020
|Jan 15, 2020
|Jan 10, 2020
|-
|Review class lecture slides
|Completed
|Jan 8, 2020
|Jan 15, 2020
|Jan 10, 2020
|-
|Finish DEAP Lab 1
|Completed
|Jan 8, 2020
|Jan 15, 2020
|Jan 15, 2020
|}

=== '''DEAP Lab 1 Conclusions:''' ===

==== Part 1 - One Max Problem ====
# We begin by defining the objective function (maximize the number of 1s in the individual's bit string), individual constructor, and population constructor.
# We then define the evaluation, mating, mutation, and selection functions for the genetic algorithm.
# We run the algorithm for 40 generations on a size 300 population, and print out the statistics. 
# Conclusions: We find that the average objective after 40 generations is ~99, and the max is 100 (as desired). However, running the algorithm multiple times, we see that it is not always the case that we achieve the desire maximum.

==== Part 2 - N Queens Problem ====
# As before, we begin by defining the objective function (which is now a minimization problem - to minimize the number queens attacking each other), the individual constructor (a random permutation of a list of integers [0, ..., size - 1]), and the population constructor.
# We define the evaluation function as the number of "collisions" along the diagonals of the chessboard.
# We define two distinct crossover functions - cxPartialyMatched and cxTwoPoint. The first function swaps pairs of queen positions between the two parents rather than swapping full pieces of the parents as in cxTwoPoint, which does not help us retain information from the parents.
# We define two mutation functions - mutShuffleIndexes and mutShuffleIndexes2. mutShuffleIndexes, for each gene in the individual, swaps the gene with another with probability indpb, whereas mutShuffleIndexes2 returns a random permutation of the individual with probability indpb.
# We run algorithm over 100 generations on a size 300 population with various parameters and functions, and find that using cxPartialyMatched, mutShuffleIndexes with indpb = 2.0/size, and selTournament with tournsize=5 reaches the minimum very quickly, as shown in the following graph:
[[files/Conclusions Graph.png|center|thumb|491x491px]]