== Team Member ==

Team Member: Benson Chau

Email: ben.chau@gatech.edu 

Cell-Phone: 404-434-5548

(Caching Subteam Wiki)[https://vip.gatech.edu/wiki/index.php/EMADE_Cache_Subteam]

== Jan. 7th, 2019 ==
'''Notes:'''
* First meeting stuff here; Meetings now in the COC.
* Flexibility, need to decide what team to join or if we should continue work on the caching branch.
* Meeting with Jason on Tuesday, 4:30pm Baker Building to discuss. 
'''Subteam Notes:'''
* Due to the large number of bugs on the caching branch, we could document, test, benchmark the work we've already done. 
* This would allow us to take up a list of all the bugs and fix them in a logical, easy to understand way that new people would be able to pick up later on.
* The caching branch is much cleaner than what is on the original, and is essentially a revision of EMADE plus caching.
'''Action items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Decide what the caching subteam should be doing next.
|Completed
|Jan. 7th, 2019
|
|Jan. 8th, 2019
|-
|Set up a meeting with Jason to get started on the caching bugs.
|Completed 
|Jan. 7th, 2019
|
|Jan. 7th, 2019
|}

== Jan. 8th, 2019 ==
'''Notes:'''
* Discussed what to do with Jason.

'''Subteam Notes:'''
* Decided on a mix of testing, benchmarking, and documentation for the caching branch.
* On Friday (Jan 11th), we'll fill up a table of action items for the next SCRUM. 

== Jan 11th, 2019 ==
'''Notes:'''
* Discussed with Samuel Huang what we'd do this semester, set plans for the next week.
* Filled the action tables for our individuals tasks and for the caching subteam as a whole.
'''Action items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Establish a documentation standard (in code and in documentation).
|No progress this week
|Jan. 11th, 2019
|
|Feb 3rd, 2019
|-
|Start a list of bugs on the caching branch, to be tested later.
|No progress this week
|Jan. 11th, 2019
|
|Jan 21st, 2019
|-
|Create a unit testing skeleton for the new caching features.
|No progress this week
|Jan. 11th, 2019
|
|
|}

== Jan 14th, 2019 ==
'''Notes:'''
* Set weekly meetings to Friday, 4:30pm.
* Discussed our plans for the week in the two minutes scrum today.
* Added information to the Wiki page.
* Need to fix EMADE on my machines, as of now seg-faults. 
* Getting EMADE to work on Google Cloud currently a priority; in the meantime, we can find a documentation standard and place to host it.

== Jan 18th, 2019 ==
'''Notes:'''
* Found a usable documentation tool to create our docs with.
* Here's an idea of a standard doc:
    name of the function
    function description: what's the function do? what does it take in, what does it output? what is it used for?
    relation to class/module/etc: where does this function come from?
    usage: show an example of how to use the function
  * See C++ docs for more inspiration.


'''Action items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Establish a documentation standard (in code and in documentation).
|Found inspiration in C++ docs, see http://www.cplusplus.com/reference/vector/vector/ and doc tool.
|Jan. 11th, 2019
|
|Feb 3rd, 2019
|-
|Start a list of bugs on the caching branch, to be tested later.
|No progress this week
|Jan. 11th, 2019
|
|Jan 21st, 2019
|-
|Create a unit testing skeleton for the new caching features.
|No progress this week
|Jan. 11th, 2019
|April 1st, 2019
|
|}

== Jan 21st, 2019 ==
'''Notes:'''
*Started a [https://github.gatech.edu/bchau7/mkdoc_emade_cache_test/tree/master Github repository] to store a testing version of the mkdoc utility in action using the EMADE cache as content for it. 
*Will be using the format above from Jan 18th; goals are to have a couple functions done by the end of the week (Jan 25th).
*A list of bugs and a CONTRIBUTING.md to also be made.

'''Action items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Establish a documentation standard (in code and in documentation).
|Found inspiration in C++ docs, see http://www.cplusplus.com/reference/vector/vector/ and doc tool.
|Jan. 11th, 2019
|
|Feb 3rd, 2019
|-
|Start a list of bugs on the caching branch, to be tested later.
|Created a contributing.md for the bugs in the current documentation repository. Needs polish, as creating such a bug list should be clear as to what the problems are so they can be fixed later on.
|Jan. 11th, 2019
|
|Feb 3rd, 2019
|-
|Create a unit testing skeleton for the new caching features.
|No progress this week
|Jan. 11th, 2019
|April 1st, 2019
|
|-
|Need to fix my EMADE build so it doesn't segfault on every machine I have.
|Performing a rebuild of the caching branch on my machine, from scratch.
|Jan 21st, 2019
|
|Feb 3rd, 2019
|}

== Jan 28th, 2019 ==
'''Notes:'''
* Created a [https://github.gatech.edu/pages/bchau7/mkdoc_emade_cache_test/ docs] page with mkdocs.
* Added some function documentation sections in GPFramework/, but I'm not 100% happy with what's there. 
** The plan is to rewrite them in accordance to the C++ docs in Jan 21st, 2019.
** I also want to reorganize the page to fit a bugs list/issues/resolved things.
*** Consider Bugzilla? Could be overkill, probably is overkill.
* Planning on filling in empty markdown files on the docs page this week as well.
* Continuing to rebuild EMADE on my machine (I've had to wipe it once over in the last week due to technical issues). As soon as the build on the main branch works, I should be good to go.

== Feb 3rd, 2019 ==
'''Notes:'''
* Fully migrated docs/ to mkdocs. 
** We can nuke the original mkdocs test repository.
** Can't path Github-Pages to docs/. 
*** So, it might be nice to write a small shell script or link it into the main executable for EMADE to run docs on a --docs flag
* Found a documentation style I like:
** http://www.doxygen.nl/manual/docblocks.html#pythonblocks
** Would be easier to use the initial sample, i.e @docstring.
* EMADE build has been fixed. We can use @SamuelHuang's Google Cloud instance to run EMADE on the same page.
** Changes will be made and added to [https://github.gatech.edu/bchau7/emade]

== Feb 8th, 2019 ==
'''Notes:'''
* Created more documentation for functions in data.py.
** Just need to migrate those changes into docs/.
* Have access to @SamuelHuang's Google Cloud instance.
** Need to set things up so we have
**# More space; not enough space to hold Anaconda packages and EMADE with only 30GB.
**# A working EMADE caching system that we can both work from to avoid conflicts.
'''Action items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add more documentation for the foreseeable future.
|Continuing until the end of the semester; migrate changes from paper to Git.
|Feb. 8th, 2019
|
|April 22nd, 2019
|-
|Migrate bug list to Git.
|Bug list exists, needs to be migrated to Git as well.
|Jan. 11th, 2019
|Feb 25th, 2019
|
|-
|Create a unit testing skeleton for the new caching features.
|To be considered post solving the other problems.
|Jan. 11th, 2019
|Feb 8th, 2019
|
|-
|Fix Google Cloud instance space issue, EMADE caching qsub bugs.
|Reimaging the google cloud instance. 
|Feb 8th, 2019
|
|Feb 18th, 2019
|}

== Feb 11th, 2019 ==
'''Notes:'''
* Working on merging qsub hotfix over to the caching branch.
** Want to avoid merging everything from master over just yet so we don't deal with a million merge conflicts. The current caching branch is a fork from [[git://imaging]], which has been awhile since then.
** Can't do much except documentation until this is fixed; preventing us from running the caching branch at all.

== Feb 18th, 2019 ==
'''Notes:'''
* Created more documentation for functions in data.py.
** Added one_for_all, many_to_many, all_to_one, etc.
** '''PROBLEM:''' Austin didn't update the docstrings to these functions as he was updating them. New docstrings need to be written to replace the ones he made.
* Fixed XML input schema merge issues.
** Replaced inputSchema.xsd and input_dota.xml to match the master branch's most current update, plus our caching feature.
* Slurm problems abound.
** Slurm can't parse our configuration file, which comes from [https://slurm.schedmd.com/slurm.conf.html here].

== Feb 22th, 2019 ==
'''Notes:'''
* Added more documentation for functions in data.py.
** Continuing fixing docstrings.
* Replaced the hard-drive of our Google cloud instance due to a lack of space after Anaconda build.
* Reinstalling EMADE onto the new system with new cache fixes.

== Feb 25th, 2019 ==
'''Notes:'''
* Slurm continues to be an issue, but one with a temporary solution. 
** We were able to create a new google cloud instance with a bigger hard drive and an EMADE cache branch fork that can function without support for Slurm.
** Internal code switches to grid-engine if it fails to run Slurm binaries (e.g squeue). 
** '''Problem:''' The whole point of slurm is to manage EMADE properly with software meant for running a cluster of computers. Thus, running EMADE across multiple cloud instances should have working slurm daemons for EMADE to function as intended.
*** We intend to fix this at some point, but haven't been able to find the correct configurations.
*** Currently, continuing to fix/clean the instance up. We have written a short tutorial on performing the right configurations on a compute engine instance that we can say for sure works. 
**** There is potential for a Dockerfile or more practically a bash script.
** This was a problem we were going to run into anyway, but replacing the hard drive now is a good call. EMADE and Anaconda take up a bunch of space.
*We are able to run input_dota.xml on the default GridEngine!
**'''Problem:''' It may or may not do anything. This requires much more debugging to figure out, but it does run without error. This is caching branch exclusive and it does work on EMADE master.

== March 1st, 2019 ==
'''Notes:'''
* EMADE cache is now completely runnable on a new instance!
** Spent much of the time today debugging code.
** Turns out much of the code relies on one being in the exact spot that you are expected to be in.
*** See `src/GPFramework/data.py:153`; this in particular defines the base directory that the caching data should be stored in for later retrieval. '''It forces the user to be in the emade source directory if you run any data, if the flag use_cache is enabled in the input template.'''
** Squashed master changes on input templates, `src/GPFramework/launchGTMOEP.py`, `src/GPFramework/didLaunch.py`.
*** Modified input templates to use SQL via TCP sockets and not Unix filesystem sockets.

== March 4th, 2019 ==
* Thoughts on next steps:
** Caching is relatively close to getting integrated into the master branch. Since we've already started merging some parts of master back into caching anyway, it'd be good to start doing a couple PRs at a time merging what we know already works. 
*** See [https://github.gatech.edu/bchau7/emade/commit/02cfcaf586e15b2081e5004f3ae1122a8547d42c] and [https://github.gatech.edu/bchau7/emade/commit/c4fcd1b17fb05eb1f16d9848c1e664810c03f2f3], one of which was done by me and the other by Sam respectively.
*** Again, '''another note''': some commits made by me are put under Sam's, because we share a Google Cloud instance with a limited amount of space (enough for one user account). I should tag them with my name, but I tend to forget very often.
*Added more documentation.
**Finished updating loader function docstrings, [https://github.gatech.edu/bchau7/emade/commit/93336d94a89e997eb652d2e37654e4d090b56fbf].
**Added a docstring for the grouper function in data.py, [https://github.gatech.edu/bchau7/emade/commit/54d2d341ae53e60e3a080162135424b83e353b35].
**Made some changes to out of date docstrings left behind from the last full feature update, [https://github.gatech.edu/bchau7/emade/commit/56bc4322170ed9a69710ffb01fc11dd60ffbc93c].
*Master changes to launchGTMOEP & didLaunch merged into caching, see [https://github.gatech.edu/bchau7/emade/commit/91078c61bbab95af40e53d257c95abc5e214b0c8].
*Considering generating graphs via the viz team API. If it's simple enough to merge, we could do that also.
*An updated task/action-item list to be posted here by March 8th, 2019.
*Start & finish midterm presentations before the 11th.

== March 8th, 2019 ==
* Created an updated task/action-item list, see below.
* Need to start working on our midterm presentations.
* Focusing more on fixing image data as our time is running out until the end of the semester (approx. 1 or 2 months).
** Debugging options: print statements, PDB, writing out to files; none are very appealing, but will have to do.
*** PDB is too annoying to use with EMADE (or rather I'm not experienced enough to use it in a multi-process application like this one).
*** Print statements only work if it is within the master algorithm.
*** Writing out to files will make the workspace messy. 
**Tracing the code downward from launchGTMOEP; planning on writing out a code map for future use in debugging (since stream data also is broken.
'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add more documentation for the foreseeable future.
|Continuing until the end of the semester; migrate changes from paper to Git.
|Feb. 8th, 2019
|
|April 22nd, 2019
|-
|Fix/Find out what's wrong with image data by tracing the code from launchGTMOEP.
|
|March 8th, 2019
|
|
|-
|Finish midterm presentations (March 11th, 2019)
|Unfinished, see [https://docs.google.com/presentation/d/1ZWmyfr1L2FiMxCui9nB0wpWteBDxGFPxqyAZGfE6SYg/edit?usp=sharing].
|March 4th, 2019
|
|March 11th, 2019
|-
|Consider merging parts of the up-to-date master branch into the caching branch as is to resolve issues merging in slowly as the caching branch is developed on.
|Merged didLaunch, launchGTMOEP. 
|March 4th, 2019
|April 8th, 2019
|
|}

== March 10th, 2019 ==
* Finished midterm presentations for tomorrow. 
* Tracing continues. I've just noticed that naming conventions are extremely weird even outside of the root of caching (data.py, where all the storing/loading cache functions reside). 
** For instance, `load_feature_data_from_file` dumps feature_data into a .npy file, but all the load cache functions load from a different name for all types, e.g `image__fold#_data.npy`. 
** Or, odd and unclean variables that are hard to understand all over data.py.
*Otherwise, I've been practicing a little bit for our presentation tomorrow, while adding a couple things as we go. 
**Find the midterm presentations [https://docs.google.com/presentation/d/1ZWmyfr1L2FiMxCui9nB0wpWteBDxGFPxqyAZGfE6SYg/edit?usp=sharing]here.

== March 17th, 2019 ==
* Sam @ I have decided what files we want to try merging into the caching branch from master.
** database_tree_evaluator.py
** didLaunch.py
** gtMOEP.py
** seeding_from_file.py
** methods.py
** selection_methods.py
*It might be useful to give this to new members. It would serve as 1) an exercise on how to push new features to a branch multiple people are working on at a time and 2) as a progress check for how compatible the current caching branch is with the master.
*Moreover, they shouldn't be too conflicting as long as someone does good-enough code review. 
'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add more documentation for the foreseeable future.
|Continuing until the end of the semester; migrate changes from paper to Git.
|Feb. 8th, 2019
|
|April 22nd, 2019
|-
|Fix/Find out what's wrong with image data by tracing the code from launchGTMOEP.
|Found weird file naming conventions when using binary dumps within the caching code, tracing continues.
|March 8th, 2019
|
|
|-
|Finish midterm presentations (March 11th, 2019)
|Completed, see [https://docs.google.com/presentation/d/1ZWmyfr1L2FiMxCui9nB0wpWteBDxGFPxqyAZGfE6SYg/edit?usp=sharing].
|March 4th, 2019
|
|March 11th, 2019
|-
|Consider merging parts of the up-to-date master branch into the caching branch as is to resolve issues merging in slowly as the caching branch is developed on.
|Added names of files we should try merging into caching from master.  
|March 4th, 2019
|April 8th, 2019
|
|}

== March 25th, 2019 ==
* Back from Spring break; '''Logistical stuff'''- Subteam introductions & recruiting.
** See @Anish, @Alex, @Eric, @Yash, @William. See the #cache channel on Slack for more details.
** Tasked new members with:
*** Reviewing/pulling/using the Caching subteam's branches.
**** Specifically, review `data.py` and `gtMOEP.py`.
***Merging master versions of several files against grid_slurm_integration.
****The purpose being that it would make it easier to merge the caching feature back into master once the caching branch has been fully finished.
****Also, keep cache up-to-date in any other case.
** Continuing to figure out what tasks we have and how to divide them; before we just had two members and it was relatively easy to coordinate. 
** Sam & I were tasked with code review and merging PRs.
'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add more documentation for the foreseeable future.
|Continuing until the end of the semester; migrate changes from paper to Git.
|Feb. 8th, 2019
|
|April 22nd, 2019
|-
|Fix/Find out what's wrong with image data by tracing the code from launchGTMOEP.
|Found weird file naming conventions when using binary dumps within the caching code, tracing continues.
|March 8th, 2019
|
|
|-
|Finish midterm presentations (March 11th, 2019)
|Completed, see [https://docs.google.com/presentation/d/1ZWmyfr1L2FiMxCui9nB0wpWteBDxGFPxqyAZGfE6SYg/edit?usp=sharing].
|March 4th, 2019
|
|March 11th, 2019
|-
|Consider merging parts of the up-to-date master branch into the caching branch as is to resolve issues merging in slowly as the caching branch is developed on.
|Added names of files we should try merging into caching from master.  
|March 4th, 2019
|April 8th, 2019
|
|-
|Review new PRs, merge them as they come in.
|Waiting for pull requests to come in.
|
|
|April 5th, 2019
|}

== April 1st, 2019 ==
*Merged in @Eric/@Yash, @William/@Sam's PRs, see [https://github.gatech.edu/bchau7/emade/pull/1 here] and [https://github.gatech.edu/bchau7/emade/pull/2 here] respectively.
**Still waiting on a PR from @Anish/@Alex.
**The merging and review has mostly gone well; one of the goals of having new members do this is to get familiar with what our general workflow should be.
*Suspended the unit-testing skeleton feature.
**The plan currently is to split our subteam into 3 subteams, each focused on enhancing the features of the cache.
***Benchmarking, Cache Invalidation, Image (e.g Fix Image Data). 
***Sam and I will fill image data together, while the rest of the subteam works on benchmarking or enhancing cache invalidation.

== April 5th, 2019 ==
* Meeting at 3:00PM on the focus of the new group members of the Caching subteam.
** Divided up into Benchmarking, Caching features (e.g a better caching schema), and 
*** The benchmarking team should create a tool to allow fast benchmarking (less setup) and get statistics in a clean, human-readable manner.
*** The caching team should create a new caching schema to be implemented in the Caching branch. 
*** The remaining members need to fix the image/stream data in the main branch.
**Merged in @Anish/@Alex [https://github.gatech.edu/bchau7/emade/pull/7 here].
'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add more documentation for the foreseeable future.
|Continuing until the end of the semester; migrate changes from paper to Git.
|Feb. 8th, 2019
|
|April 22nd, 2019
|-
|Fix/Find out what's wrong with image data by tracing the code from launchGTMOEP.
|Found weird file naming conventions when using binary dumps within the caching code, tracing continues.
|March 8th, 2019
|
|
|-
|Finish midterm presentations (March 11th, 2019)
|Completed, see [https://docs.google.com/presentation/d/1ZWmyfr1L2FiMxCui9nB0wpWteBDxGFPxqyAZGfE6SYg/edit?usp=sharing].
|March 4th, 2019
|
|March 11th, 2019
|-
|Consider merging parts of the up-to-date master branch into the caching branch as is to resolve issues merging in slowly as the caching branch is developed on.
|Added names of files we should try merging into caching from master.  
|March 4th, 2019
|April 8th, 2019
|
|-
|Review new PRs, merge them as they come in.
|Merged in all PRs. Need to try running EMADE as is. 
|
|
|April 5th, 2019
|}

== April 8th, 2019 ==
* A plan for the new members to get more familiar with the caching code in general:
** Have them '''fully''' read data.py and launchGTMOEP.py, where the majority of the caching code is stored. The other parts are also important (e.g caching schema), but they should read the bulk by the next week (or this week).
*The 'Fixing Image' team:
**We've been reviewing old commits to see what the odd-variable names are supposed to mean. One in particular is a 'c' or 's' that would be used to format log files. We did find the commit where it was removed, but couldn't tell what reason it was removed for.
***My personal theory is that it was a part of the batch parameter before it was removed in the last year, but for some reason this wasn't. Might have just been a mis-step.
***That having been said, it doesn't seem like that part of the code is ever run. We can run a full 24 hours of feature data without any error pertaining to that variable.
**I reverted that error, and we are going back through the code to find other bugs.
*On the other hand:
**Most of the team has set up Google Cloud instances at this point. 
**They seem to have a good focus from what I've asked about their work.

== April 15th, 2019 ==
* Continuing to go through the source code from top to bottom, once again.
** Having a lot of trouble pin-pointing exactly what's wrong with it. The more I look, the more confused I get with the nonsensical naming conventions, and the more Sam & I doubt feature data even works. 
** What we do know for sure at the moment is that individuals are not being evaluated at all when running input_image.xml (our image data sample). The number of individuals never decrease, so the master process just waits for workers to evaluate but they never do.
*** After around an hour, the tables show 509 separate individuals but no data in any other table.
** Assuming it works on feature data, one thing we could try is checking the differences between feature and image in terms of evaluating individuals or loading the data, code-wise. 
** Data seems to get saved to the disk. Loading is probably very, very broken due to the way load_from_cache works. 
** Debugging continues to be a problem; difficult to use PDB, leaving us with debugging via print statements and file I/O. 
*** Either way, trying to run and shut it down takes several minutes at a time (even more on the Google Cloud instance, which has recently been very uncooperative), and it's driving me crazy.
*In order to get image working properly, I had to revert all the pull requests made by our members.
**This was due to the fact that I had not done the code review properly and partially because of how incompatible some of the source files really are. 
'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add more documentation for the foreseeable future.
|Continuing until the end of the semester; migrate changes from paper to Git.
|Feb. 8th, 2019
|
|April 22nd, 2019
|-
|Fix/Find out what's wrong with image data by tracing the code from launchGTMOEP.
|Developing a code-map for later debugging. 
|March 8th, 2019
|
|April 22nd, 2019
|-
|Finish final presentations (March 11th, 2019)
|Unfinished, see [https://docs.google.com/presentation/d/15m_zXjYzh8NjSt8R-O-rFKncoHjmWP40Cz0Z09ACCn0/edit?usp=sharing]
|March 4th, 2019
|
|March 11th, 2019
|-
|Consider merging parts of the up-to-date master branch into the caching branch as is to resolve issues merging in slowly as the caching branch is developed on.
|See the action item below. 
|March 4th, 2019
|April 8th, 2019
|
|-
|Review new PRs, merge them as they come in.
|Reverted all PRs due to several problems.
|
|
|April 5th, 2019
|}

== April 21st, 2019 ==
* Wrapping things up; dealing with new found weird aspects of the caching branch. 
** Feature data works because the file names are actually right, just nonsensical in terms of the type of data it is actually being used for. For instance, the previous student hard-coded in the filenames for image, although that data might actually be something else. 
** If you run input_dota.xml, it creates dumps of subtree objects as "image_*.pyc", not as "feature_*.pyc", but still runs properly. We have two theories.
*** Feature data works, but the naming conventions are incorrect. 
*** Feature data doesn't work, the statistics are mostly theorized- several trees are NULL in that database. Moreover, @Eric found that the performance of our best individual is always worse running caching than if running the original. This is bizarre, but possible. 
**It might help us to create a code map for the caching branch for debugging in the future, as well as help improve the knowledge of our other teammates on the caching branch as a whole.
***See P[https://drive.google.com/file/d/1IzQCGOKC0G-S-4qAjgN-jzvZSWA1tpkO/view?usp=sharing] for the code map.
*Final presentation, spring 2019: [https://docs.google.com/presentation/d/15m_zXjYzh8NjSt8R-O-rFKncoHjmWP40Cz0Z09ACCn0/edit?usp=sharing]

== April 22nd, 2019 ==
* I feel that I deserve to pass, at the very least. We've encountered a lot of obstacles along the way, from preconceived notions about what worked in cache to Google Cloud having bad days. This is by no means an excuse for the amount of work I've done. I won't say that I don't want/deserve an A, but I will say I deserve more than a B. That being said, I'll be happy to get the grade I deserve. 
* Per the prior feedback during the last midterm review, I've started adding more to-do lists + links, see above.
* Itemized list of cache maintenance & improvements:
** Added an external documentation utility that uses Python's mkdocs to make a neat little web server that serves web pages from markdown files.
** Added a code map for debugging and infrastructure purposes. These can be found in the documentation utility.
*** To be slowly updated as I trace through the code more thoroughly.
** Added a mini-issue/bug-list within the external documentation tool.
*** Want to get the other caching members to start updating that so we can host it via github-pages as a live resource.
** Helped @Sam write a guide for using EMADE on Google Cloud by being the test dummy.
** Performed code review for the new member's merges (and reverted them, because I made a mistake).
** Merged in didLaunch.py, GTMOEP.py, launchGTMOEP.py to keep up with the master branch.
** Updated some input XML files to work with caching.
** Added several in-line documentation strings to the bulk of the caching feature- see data.py.
*See the bulk PR [https://github.gatech.edu/emade/emade/pull/106].

== August 19th, 2019 ==
* First meeting back from summer break, now a third semester student.
* Teams are not split into several smaller sub-teams for reorganization purposes.
* Now part of the ADF subteam.
** '''General Purpose''': Improve/replace the current ADF use in EMADE with something that's more improved.
** What's the end goal for the semester?
** How do we achieve that goal?
** What should we produce at the end of the semester?
* Meetings to be established at some point.
* Do some research figure out what ADFs can/can't do/should do.

== August 23rd, 2019 ==
* First weekly thursday meeting.
** Trying to figure out what to do for the next week.
** Some questions to think about:
*** What should we ultimately try to produce? What have we thought of that was already a stretch goal?
*** What steps should we take to achieve that goal? What are our milestones?
*Mostly, what we should focus on ADF research; we're all new to this, so we should take the time to understand EMADE's ADF system and the purpose of ADFs in general.
'''Action Item'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Do some research on ADFs and implementations.
|
|08/23/2019
|
|08/26/2019
|-
|Learn about EMADE's ADF system.
|
|08/23/2019
|
|08/26/2019
|}

== August 26th, 2019 ==
* Scrums & adding meeting notes to the subteam wiki
** Subteam wiki notes to the current date [https://vip.gatech.edu/wiki/index.php/Automatically_Defined_Functions here].
* Started taking notes on ADF process.
* Read through EMADE's ADF system.
** Initial thoughts: doesn't seem to make much use of ADFs.
** ADFs are attached to individuals as part of their primitive list, so they're crossed over just w/ them too.
** Majority of ADF code exists in gtMOEP.py.
** For some reason, there are only ever 3 ADFs at a time.
** Notes [https://hastebin.com/noguxurole.cs here].

== August 29th, 2019 ==
* Sharing ADF knowledge, generating our gameplan for the semester.
** Notes [https://hastebin.com/noguxurole.cs here].
** Added accumulated knowledge to subteam wiki page.
'''Action Item'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Help correct the benchmark dataset XMLs to run correctly. 
|
|08/29/2019
|
|09/12/2019
|-
|Add a ADF size parameter that denotes the # of ADFs allowable.
|
|08/29/2019
|
|09/12/2019
|}

== September 9th, 2019 ==
* Do research into apriori algorithms (fundamental).
** mlxtend framework, found [http://rasbt.github.io/mlxtend/#examples here] seems like a good candidate.
** It provides a clean API for doing apriori passes over; it can utilize pandas dataframes to calculate the support for every combination of itemsets an individual might have.
*Help set up a testing infrastructure, for the sake of reliability.
**Need GCP/Amazon credits to mess with.
**Basically, what I did last semester, so shouldn't be too hard to set up for the four of us once we've gotten the credits (unless we use Amazon, which I'm not familiar with in the slightest.
*Read apriori data mining papers.
**Some helpful links:
**https://www.hackerearth.com/blog/developers/beginners-tutorial-apriori-algorithm-data-mining-r-implementation/
**https://www3.cs.stonybrook.edu/~cse634/lecture_notes/07apriori.pdf
*PR combined with Aaron's and committed to working tree, see [https://github.gatech.edu/jye72/emade/commit/2bef2d0bb6bfb3f578b3e5cfef652fd9dd3001f6].
'''Action Item'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read apriori papers, notes, to learn how to apply to our use case. 
|
|09/09/2019
|
|09/12/2019
|-
|Play with mlxtend; come up with a small demo.
|
|09/09/2019
|
|09/12/2019
|}

== September 12th, 2019 ==
* Created a mini-mlxtend demo based off the examples given.
** Can calculate supports for a pandas dataframe, of which can be easily filtered through for sets that:
*** are of a certain length (to limit the height of our ADFs).
*** are of a parent-child relationship (needs to iterate through the individual for this, however).
*** anything you could possibly imagine.
** It could be useful if we decide to go this route; however, based on our discussions, it seems less and less likely.
** We're still fleshing out the algorithm we're going to actually use. There were problems with our initial idea, which was to apply apriori to a tree to build larger ADFs.
** This doesn't work, as it doesn't maintain the tree structure or care about parent child relationships. 
** Planning on continuing to play with mlxtend and see what I can make happen; API is nice, but also very small and rather limited to what you can do with the pandas dataframe.
* Need a new plan for our algorithm to build our ADFs. 
'''Action Item'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Help brainstorm a way to retain tree parent-child relationships in ADF generation. 
|
|09/12/2019
|
|09/23/2019
|-
|Continue to mess with mlxtend.
|
|09/09/2019
|
|09/12/2019
|}

== September 23rd, 2019 ==
* Came up with a new algorithm, courtesy of Aaron & Joel.
** Basically, we go for a simpler solution (based on limited time).
** We create small ADFs of at most height 2.
** This is done by computing a pass of apriori on the bottom of the tree, calculating supports for the pairs of nodes there.
** This helps us create a small tree. Over time, we can grow this tree by finding relationships to the ADF and appending a node as we go.
** ADFs don't need to mutate like actual individuals.
* Not much use for mlxtend after all.
** mlxtend isn't required for a simple frequency loop over the items at the bottom of the tree.
** We can just do it manually, after all.
* We just need to fill out Joel's stubs for said algorithm, mostly. 
* Got GCP & Amazon credits, but GCP credits tied to Jason's account.
** So, we can go for the easy route and try Amazon's infrastructure. 
'''Action Item'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Help implement the new algorithm; flesh out the details as we go. 
|
|09/23/2019
|
|
|-
|Set up Amazon's EC2 for testing infrastructure.
|
|09/23/2019
|
|09/30/2019
|}

== September 30th, 2019 ==
* It's been one week since I made my Amazon account for AWS credits, but currently still do not have them.
** Followed up with Michael (Jurado), and apparently he received his credits by signing with the provided link.
** I've done the same, but haven't received any credits at all.
** This is alright though, as I have more knowledge of using Google Cloud Platform instead; planning on meeting with Jason to figure this out.
* A-priori officially abandoned.
** No more need for my mlxtend work; short script found on this branch [https://github.gatech.edu/jye72/emade/tree/ben/mlxtend_demo here], just in case.
** Helpful mlxtend docs [http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/ here].

== October 3rd, 2019 ==
* GCP status;
** I've been working with Jason (as the guinea pig) to get the GCP cluster operable. It's at least SSH'able at the moment.
** Some helpful notes for anyone planning on using the GCP cluster in the future:
*** You can use either the GCP web console or the GCP cloud shell utility locally to copy EMADE onto the cluster.
*** Note that the web console gives you very limited space; you may or may not be able to clone EMADE (w/ git-lfs) at times.
*** The bucket (think of it like a mounted hard disk on the cluster) for our team is `gcp://emade-vip-benson`.
*** To copy onto the GCP cluster: `gcp-util cp -r your-filename-here gcp://emade-vip-bucket`.
*** After this, it'll take around an hour to copy onto the GCP cluster; this is due to the number of datasets that are uploaded as well.
*** There is no way at the moment that allows you to easily modify your fork up in the bucket besides pushing the entire thing back up.
'''Action Item'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|_contract_adfs implementation & PR 
|not started
|09/23/2019
|
|10/17/2019
|-
|Set up Amazon's EC2 for testing infrastructure.
|replaced w/ GCP
|09/23/2019
|
|09/30/2019
|-
|Help setup GCP cluster
|partially; SSH-able
|09/30/2019
|
|11/24/2019
|-
|Help work on the midterm presentation slides.
|no started
|09/23/2019
|
|10/17/2019
|}
== October 7th, 2019 ==
* _contract_adfs
** Told to implement _contract_adfs a couple days ago; started to do so today.
** The purpose of _contract_adfs is to find ADF nodes within the given individual and replace them with the ADF node contents.
** A rough sketch of the algorithm that's required:
*** Go through each ADF in ADFs.
*** Get the root name of the ADF.
*** Given the pset mapping, we can find the primitive that the root name of the ADF represents within the primitive pool (because that's where we put our ADFs).
*** Make a copy of the new inidividual.
*** Modify the new individual such that the node that represent the ADF is replaced with the ADF found within the pset mapping.
*** Remove the old individual, re-insert at the same index of that individual.
*The format of the the new data structure we've invented is rather confusing.
**Here's the low-down; ADFs are of the form (executable function, flat input type list, return type, root node, children node, function string).
**The children nodes field of the ADF is represented by the tuple (indiv_index, node_index, child1_index, child2_index, ..., childn_index).

== October 10th, 2019 ==
* Still working on _contract_adfs.
** Here's a [https://github.gatech.edu/jye72/emade/commit/760c4b67acad7e063a933adc14b78cea0bec75eb commit] that's been accepted into the repo that contains the start of the algorithm mentioned on October 7th, 2019.
** There are a couple problems with the algorithm thus far:
*** I'm still not 100% sure as to the mapping. This compiles, but doubtful that it works.
*** Re-inserting & removing the individual that has the ADF in it changes the indices of the other individuals that are in the population, which might end up messing with how the rest of the function works post return.
*** Indices will shift as the nodes are removed, as above, but the way we find all ADFs might occur even if they are the leaves of the individual.
**It's not been tested yet, as the rest of the ADF infrastructure is actually relatively unstable, but the fork at least builds.

== October 17th, 2019 ==
* Finished working on _contract_adfs.
** Here's the [https://github.gatech.edu/jye72/emade/commit/760c4b67acad7e063a933adc14b78cea0bec75eb commit]; it implements the algorithm and hopefully works as intended.
** I've at least been told it looks right; so, I'll be leaving it as marked complete until I'm told it no longer works.
** It's not at all portable either; if the format that we have changes, everything changes.
*Finished working on midterm presentation slides.
**Find our midterm presentations (& later our final presentations) [https://drive.google.com/drive/folders/1KYg8yE7XwSo3Jofx2GdsVt3U4l8Da4sJ here].
'''Action Item'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|_contract_adfs implementation & PR 
|finished
|09/23/2019
|
|10/17/2019
|-
|Set up Amazon's EC2 for testing infrastructure.
|replaced w/ GCP
|09/23/2019
|
|09/30/2019
|-
|Help setup GCP cluster
|partially; SSH-able
|09/30/2019
|
|11/24/2019
|-
|Help work on the midterm presentation slides.
|finished
|09/23/2019
|
|10/17/2019
|}

== October 21st, 2019 ==
* Midterm presentations!
** Find our midterm presentations (& later our final presentations) [https://drive.google.com/drive/folders/1KYg8yE7XwSo3Jofx2GdsVt3U4l8Da4sJ here].

== October 28th, 2019 ==
* First meeting post midterms, getting new members.
* Issue with our code persists; Gabe working on trying a seed on F-Priori on his PC.
** Still no word on GCP progress; I push up to the bucket on occasion just to keep it up to date when we get close to having a working GCP run.
*Trying to establish what we're going to finish in the next half of this semester.
**What to do about the bugs with our current implementation?
**What will we be satisfied with at the end of the semester?
**How are we going to show that the ADF implementation is better/worse than what exists now?
**All questions to answer in the next couple weeks.

== October 31st, 2019 ==
*Merged in the rest of the PRs that we had remaining, e.g [https://github.gatech.edu/jye72/emade/commit/49098dfd8773b91dd3bf8e23cccb8494f090f3ea this] seeding fix.
**We modified the original titanic seeding file to insert ADFs into individuals in the population.
**Seems to work, as our database is at least populated when running `seeding_from_file.py` from GPFramework.

== November 4th, 2019 ==
* Catching people up & meeting. 
** Going to pull in viz team's github from [https://github.gatech.edu/adachille3/emade-viz here].
*** Fix viz team's repo to fit our new entry from the database; see the short fix [https://github.gatech.edu/jye72/emade/commit/54c5ae80a2a503554f49de9d0c5c4e61fb960d55 here].
*** The link is actually adding the viz repo as a submodule to the fork; the actual change is in that repo, under Gabe's name (we used his credentials).
** The idea is that to fulfill the benchmarks that we decided to do on the 31st, we can leverage viz's tools to make some good looking graphs.
** Said benchmarks are:
*** convergence speed (overall, and maybe generation completion times)
*** final individual fitness values (& maybe their sizes, e.g if we replaced ADFs with singular nodes how complex is the remaining tree?
'''Action Item'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Help setup GCP cluster
|partially; SSH-able
|09/30/2019
|
|11/24/2019
|-
|Fix viz team's repo to fit our new entry from the database.
|finished
|11/4/2019
|
|11/4/2019
|-
|Use viz repo to produce said benchmarks
|not started
|11/4/2019
|
|
|}

== November 11th, 2019 ==
* Trying to use viz repo to test how we would generate graphs.
** The current way that the viz repo works is it fetches your database URL based on web cookies in `app.py`. 
** (It's a flask app; run with `flask run`)
** If we run on GCP, we need to run the viz repo remotely to an exposed port on GCP (which Jason will need to do for me).
** If we run locally, we don't need to do that at all.
** Either way, you can hard-code the address to whatever you'd like.
** One possible thing to do; add a new text box that lets you choose what the database URL is that the viz tool will try to use instead of what's available via cookies.
'''Action Item'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Help setup GCP cluster
|partially; SSH-able
|09/30/2019
|
|11/24/2019
|-
|Fix viz team's repo to fit our new entry from the database.
|finished
|11/4/2019
|
|11/4/2019
|-
|Use viz repo to produce said benchmarks
|not started
|11/4/2019
|
|11/25/2019
|-
|Update viz repo to allow you to choose a database URL.
|not started
|11/11/2019
|
|11/25/2019
|}
== November 14th, 2019 ==
* EMADE GCP bucket problems.
** Trying to get the GCP bucket updated with our new EMADE fork contents (which gets updated rather frequently now).
** Looks like the GCP bucket is unable to remove its contents.
*** My attempts to remove data, e.g <code>gsutil -m rm [gs://emade-vip-benson/** gs://emade-vip-benson/**].</code>
*** Doesn't work, some theories:
**** The command just tells the bucket to schedule a deletion, and it isn't going to do so immediately.
**** It's not actually deleting anything, maybe the path is wrong (but I don't really think it is).
***The bucket is out of space for now.
**Got the bucket fixed by having Jason manually go into my bucket and delete everything in it.
**I can now push again, but don't know if the problem will show up again as we continue to update.
*Idea: Add a script that allows you to push up to the GCP repo as efficiently as possible (e.g just push modified source files).
**Not really a great idea, since pushing the entire repo up to the bucket takes ~an hour because of the datasets.
**We could just have the datasets be stored in some other place in the cluster, then you just push up only the EMADE source needed to make a build.

== November 18th, 2019 ==
* Planning on meeting early on the 25th to hash things out before break.
** Need to have a benchmark run and a run of our fork, see the performance differences via viz, put graphs on the final presentation.
** Seeding problems are still problems, since the last week.
** `None index not subscriptable` also a problem; can't evaluate our individuals with ADFs in them.
*** A guess: might be because the `_contract_adfs` I wrote was not really working. 
**** It might be messing with the indices in such a way that it tries to index with a null value; rework the individual replacement w/ something else.
**** Remove the multiple ADF replacement also.

== November 23rd, 2019 ==
* Despite (now hypothetical) code freeze on the 25th:
** Meeting on Monday, Tues. of this week to figure out what to actually presentation on the 2nd.
** ADFs still not being evaluted correctly.
*** Better ADFs though? They aren't super simple `adf1()`s or what not, and they do get into the primitive set.
**Apparently, we're almost ready for a real GCP run.
***Jason is still working on it.
***Basically confirmed we don't actually have a GCP run as of yet.
***Don't know how much we would benefit from a GCP run now, as our ADFs don't actually work right now (can't get evaluated in an individual).

== November 25th, 2019 ==
* Backups plans for if we don't end up running on the cluster at all:
** Running on my own PC; I at least have MariaDB setup on my PC, which ended up taking the entire day to install.
** Tested a way for us to generate a bunch of data on several computers;
*** We could have multiple people do a run up to some # of generations on their PCs, dump all the data individually 
**** <code>mysqldump -u username -p database_name > data-dump.sql</code>
***And you can easily concatenate them (sort of, some issues if the tables already exist, as you need to add additional `--no-create-info` to mysqldump in consecutive appending.
****`cat data-dump1.sql data-dump2.sql ... data-dumpn.sql > data-dump-concat.sql` 
***After which you can just load that into your database, like so:
****`mysql -u username -p database_name < data-dump-concat.sql`
***A couple problems with this approach:
****Doesn't really seem to make any sense to mix generations around.
****IDs are the same, so you end up having conflicts between different databases.
**On the other hand, GCP seems almost ready to run at this point. We've decided to try it at some point, but I'm told our alternatives now are PACE-ICE (not really an option) and our own PCs. 
***Aaron, Gabe, (maybe) Joel, Aryender, Ruari have working versions of EMADE to do runs on, so I most likely don't need to rebuild EMADE on my own PC.
***I'll just get a bunch of data dumps to generate graphs from viz and use them for our final presentation.

== December 2nd, 2019 ==
* Final presentations due and presented.
** See [[here]].
'''Action Item'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Help setup GCP cluster
|partially; SSH-able
|09/30/2019
|
|11/24/2019
|-
|Fix viz team's repo to fit our new entry from the database.
|finished
|11/4/2019
|
|11/4/2019
|-
|Use viz repo to produce said benchmarks
|finished
|11/4/2019
|
|11/25/2019
|-
|Update viz repo to allow you to choose a database URL.
|not started
|11/11/2019
|
|
|}