== Team Member Info ==
Team Member: David Zhang

Email: dzhang351@gatech.edu

Interests: Machine Learning, Optimization, Tennis

== October 20, 2021 ==
'''Team Meeting Notes'''
Seeding: <code>python3 src/GPFramework/seeding_from_file.py templates/input_titanic.xml test.txt</code>
Change reuse to 1 if using seeding and create new db to populate data

== October 13, 2021 ==
'''Team Meeting Notes'''

Today we had a work session with EMADE with subteams

Goal: be able to run <code>mysql -h hostname -u username -D database name -p</code>, connect master and worker programs

Errors
* I had an error in Master.err saying "pymysql.err.OperationalError: (1045, "Access denied for user 'root'@'localhost' (using password: YES)")"
** to address this error, I reset the password using "mysqladmin -u root password NEWPASSWORD"
* Tree missing valid primitive for data type
** I used the command <code>grep -rl "Tree missing valid primitive for data type" "./"</code> which searches the error in all subdirectories of emade. From this, I found that the following 3 files contained the error message which I used for debugging.

.//build/lib/GPFramework/gtMOEP.py

.//worker5549.out

.//src/GPFramework/gtMOEP.py


After running
<code>python3 src/GPFramework/launchGTMOEP.py templates/input_titanic.xml</code>, 
the following tables appear within the titanic db in mysql and I am able to access the data successfully

+-------------------+
| Tables_in_titanic |
+-------------------+
| history           |
| individuals       |
| paretofront       |
| statistics        | 

I ran the query <code>select * from individuals natural join paretofront;</code> to retrieve all the individuals that were pareto optimal and then plotted my pareto front in matplotlib. Below is my initial result using emade (default params and input file). Some next steps include normalizing to include FNR and FPR rather than FN and FP and to edit the GP implementation and titanic data splitter to match the ML data splits so that a proper comparison can be made.

[[files/dzhang351/prelim_paretofront_emade.png|50x50px]]

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get master output in mysql db and be able to run <code>mysql -h hostname -u username -D database name -p</code>
|Complete
|October 13, 2021
|October 20, 2021
|October 15
|-
|Connect worker nodes as a team
|In progress
|October 13, 2021
|October 20, 2021
|
|-
|Edit GP using emade framework
|In progress
|October 13, 2021
|October 20, 2021
|October 19, 2021
|-
|}

== October 6, 2021 ==
'''Team Meeting Notes'''

Introduction to Emade

Emade (Evolutionary Multi-objective Algorithm Designe Engine)
* Combines multi-objective evolutionary search with high-level primitives
* Evaluations much more expense with automated ml (due to higher level primitives)
* Emade uses a MySql server behind the scenes
* Emade installation and setup
** Need to download installed git-lfs, clone git repo
** run setup.py to install

Emade input file
* Used for EMADE configuration
* python, mysql connection information (one master connection; all others are workers)
* workers per host specifies how many evaluations run in parallel (keep this relatively low at 2-3 since it is resource intensive)

- Headless chicken crossover: injects much randomness; second parent is randomly created

- Default selection uses selNsga2 with binary tournament

Connecting as worker
* add -w flag (signals that this is not a master process)
* make sure MySql accepts remote connections
* connecting: Mysql -h hostname -u username -p
* set reuse flag to 1 if need to pickup on last state

Assignment
* Run Emade as a group; one person 
* Run for substantial number of generations
* Make plot of non-dominated frontier and compare with ML, MOGP
* Show successful trees; plots of AUC, evaluation time (visualize results)

Installation process
* Used homebrew to install git lfs (git, conda already installed)
* cloned emade repository and installed python dependencies via conda and pip
* created empty mysql database using <code>create database titanic</code>
* updated input_titanic.xml with python and mysql configurations and ran <code>python3 src/GPFramework/launchGTMOEP.py templates/input_titanic.xml</code>

I ran in to an issue with mysql permissions not working (denied access to root user@localhost) and I found this stack overflow to be really helpful.

https://stackoverflow.com/questions/9695362/macosx-homebrew-mysql-root-password/59958898

For conda installation issues, I found this github post to be helpful.
https://github.com/jrobchin/Computer-Vision-Basics-with-Python-Keras-and-OpenCV/issues/1

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install Emade
|Complete
|September 29, 2021
|October 6, 2021
|October 11, 2021
|-
|Install Mysql
|Complete
|September 29, 2021
|October 6, 2021
|October 11, 2021
|-
|Review Mysql, EMADE concepts
|Complete
|September 29, 2021
|October 6, 2021
|October 6, 2021
|-
|}


== September 29, 2021 ==
'''Team Meeting Notes'''

This week, we had class presentations for ML vs MOGP comparison on the titanic dataset

* Selecting from the HOF may preserve pareto individuals
* Adding terminals can help improve performance greatly (instead of simply comparing)
* One hot encoding for categorical variables that have no relationship
* Using HOF to save pareto optimal solutions may be helpful (many groups used this)
* Different selection functions teams used
** SPEA2
** NSGAII
** selRoulette
** custom selection tournament
** pairing NSGA-II, selTournamentDCD
* For non-strongly type GP, we can use an activation function (like tanh) or simply check if value > 0 or not to determine result
* MOGP more diverse than ML solutions; teams also had better performance for MOGP compared to ML (lower AUC)

ML vs MOGP comparison

*Built in sklearn methods made it very easy to optimize and find a solution for ML
*GP constrained by primitives and terminals, whereas ML algorithms are constrained by architecture
*Tree size limit for GP (<91) makes limits complexity of algorithm compared to ML algorithms like MLP
*Both ML and GP algorithms dependent on loss functions
*ML results seemed to be more consistent between runs (GP mating and mutation resulted in highly different final individuals and GP sometimes gets “stuck” or shows increasing loss over time)


Some questions
* Why did GP solutions often have 0 FP or 0 FN while ML solutions almost never had those (more equal distribution of FP, FNs for the most part). Does the evaluation have to to with this?
* What is * represent when placed inside a param in python?
** Use * to represent a variable number of parameters being passed in

'''Subteam Notes'''

This week we did not split up into subteams but worked on reviewing the GP models invidually to try to improve the results that we had from last week given the feedback.

Main takeaways from individual learning
* one hot encoding instead of mapping 1,2,3...etc to categorical variables (this is because adding numbers will sometimes wrongly assign weight to certain values and the difference between different categories will be encoded unequally)
* trying new selection (like selNSGAII over selLexicase) and different mating/mutations for MOGP and utilizing the HOF can greatly improve performance of GP

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review GP for understanding
|In Progress
|September 29, 2021
|October 6, 2021
|October 5, 2021
|-
|Rework GP to include some suggestions to try and get better performance
|In Progress
|September 29, 2021
|October 6, 2021
|October 5, 2021
|}

== September 22, 2021 ==
'''Team Meeting Notes'''

This week we were assigned the same titanic dataset ML problem but now we are allowed to use multi-objective genetic programming (MOGP) to solve the problem. The goal is to obtain the same non-dominated pareto front and to compare the AUC of the ML model results with that of the MOGP.

For next week: Continue with the titanic dataset but use GP
* Can use strongly or loosely typed GP
* Can't use default algorithms in deep (mu+lambda...etc), but can use mutation, selection...etc (need to write the genetic loop)
* Comparison of Pareto front between ML co-dominant set and that of GP
* Try different operators, mutations...etc (note: selTourn is not multi objective)
* As a group, submit a csv file for predictions for each Pareto optimal point

'''Subteam Notes'''

Met as a subteam on Saturday 9/25 to go over GP implementation for titanic dataset. We had a bit of trouble and did not get much progress. Our main issues included how to force boolean output and what evaluation function to use.

For my individual contribution, I coded most of the genetic programming part of the assignment including the evaluation function, terminals and primitives used, genetic operators, and the genetic programming loop. I also created the slides for 2-4, and 8-11 and presented the slides for our genetic programming implementation and ML vs MOGP comparison.

The following are some of the GP problems that I faced and some of the solutions/notes that I took down while debugging

GP Problems 

* Bloat control tree size >90 
** Set max tree size for mate/mutation or add penalizing factor into eval function
** Can also set another objective to minimize instead of adding error to original objectives
** Add penalizing factor in eval function 
** Also speed much faster when tree size reduced 
* Keeping boolean output (used strongly typed); could alternatively use loosely typed but force a boolean output 
* Couldn’t add terminals for some reason (adding terminals would greatly improve result since we could only compare floats between columns and not to a predetermined constant)
* Making sure one solution is not completely FP or FN (multi-objective keeps converging to a non-optimal solution) 
** Penalize more for having high FNR, FPR or 0 FP, FN
** add squared penalizing term FN^2 and FP^2 instead of simply FP, FN in order to encourage more equal distribution of FP and FNs 
** Also single-objective could work in this problem but we cannot create pareto fronts required for the assignment

Eval Function used for multi-objectives

    def evaluation_func_multi(individual, x_train, y_train, pset):
        func = gp.compile(expr=individual, pset=pset)
        args = [x_train[cols[i]] for i in range(9)]
        predictions = func(*args) 
        confusion = confusion_matrix(y_train, predictions)
        FN = confusion[1,0]
        FP = confusion[0,1]
        positives = np.sum(confusion, axis=1)[0]
        negatives = np.sum(confusion, axis=1)[1]
        #prevent very unequal FN, FP results
        if FN >= positives or FP > negatives:
            return (1000000, 1000000)
        e1 = FN**2 + len(individual) * 20   #add regularization term to error to prevent bloating
        e2 = FP**2 + len(individual) * 20   #(could alternatively add separate objective to minimize)
        return (e1, e2)

* As shown above, I used an eval function that squared the FN and FP results so that it would penalize more uneven results (high FNR or FPR). I also added an error term for the tree size to prevent bloating and prefer the simpler model.
* As Dr. Zutty mentioned in class, another way to penalize tree size is to simply include a third objective to minimize in the evaluation function instead of adding the error terms for tree size to both objectives.

Pareto Front Comparison (ml first, then mogp)

[[files/dzhang351/ml_auc.png|50x50px]]
[[files/dzhang351/mogp_auc.png|50x50px]]

One reason that the mogp_auc was significantly higher than that of the ml_auc for us was that we did not add any constant terminals. This means that we only compared the columns against each other with some transformation (like arithmetic operators), which would often not be sufficient. Also, some things to test to improve performance include trying a different selection instead of selLexicase like selNSGAII. Also, the evaluation function enforcing more equal FN and FP may have been good practically but could have also hurt the AUC since it can eliminate possibly better individuals that are not even (in terms of FN and FP) but still performant.

Interesting findings 

* Sometimes the evaluation results increase over time (This could be as a result of a bad mutation)
* Compared to ML, GP seemed to have more varied results (possibly by including mutations and keeping a large population), whereas ML optimizes at each time step/iteration. We also set a low regularization param which may contribute to this result.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet up with subteam to work on titanic lab
|In Progress
|September 22, 2021
|September 29, 2021
|September 25, 2021
|-
|Finish titanic problem using GP
|In Progress
|September 22, 2021
|September 29, 2021
|September 29, 2021
|-
|Work on presentation for ML/GP
|In progress
|September 22, 2021
|September 29, 2021
|September 27, 2021
|-
|}

== September 15, 2021 ==
'''Team Meeting Notes'''

Assigned group for bootcamp and Titanic assignment to create co-dominant models within group
Subteam group (Austin Peng, Nikhil Vangala, Jordan Stampfli (later replaced with Leul Wubete)

Lecture 4: Machine Learning Intro
* Preprocessing usually done on all the data before training
** handle NAN values by dropping them or imputing with average
** encode strings to numerical values
* Metrics to use
** False negative rate (FNR)
** False positive rate (FPR)
* Test, Train data both come from "train.csv" dataset (since we need the labels)
* Some things to try:
** Balance data set (50/50 split of train test for the labels)
** Find a way to encode categorical variables better
** Need to make sure random state is the same for all group members and same partitions for training/testing

'''Subteam Notes'''

* Met up 9/18 to work on creating co-dominant classifiers for the Titanic problem. 
* For preprocessing, we made the following changes
** Dropped ticket and cabin
** Replace NA values with the mean value for the column
** Filtered "age" and "fare" into ranges of values (ie: Under 11, 11-18, 18-22...etc)
** Replaced the categorical variables of "sex" and "embarked" with coded values
** Extracted titles from name and mapped them to ints
*** <Code>titles_map = {'Mr':1, 'Miss':2, 'Mrs':3, 'Master':4, 'Rare':5}</Code>

Findings

* We set the train test split to 0.20 initially and a random state of 10 so that the splits would be the same among group members. We were able to get good results, but because the train test split was 0.20, it made it harder to find co-dominant models since there were less data points in testing so the variation in results was higher, so we decided to go with a 0.33 train test split just for the purposes of creating easier co-dominant models. This resulted in a lower accuracy though, since less data is being trained on so there was still a tradeoff.
* For the MLP model, the default params had it so that there was only one hidden layer with 100 nodes (essentially perceptron)
* After testing multiple runs, it seemed that 2 hidden layers performed the best (adding more hidden layers resulted in diminishing or worse returns) and around (30-60 nodes per layer worked well). In the end, I decided on (64,32) for the hidden layer parameter
* For the learning rate and regularization; since we found out that sometimes 200 epochs did not converge to the answer, I increased the learning rate start to 0.01 from 0.001 and alpha (l2 penalty) to 0.001 from 0.0001. This means that the algorithm should converge faster and also generally create a simpler model due to the increased regularization.
* I kept the default solver ('adam') and activation function ('relu') although I did try testing the stochastic gradient descent ('sgd') solver and sigmoid as the activation function. My reasoning for testing sigmoid as an activation is because it is good for predicting probability as an output. But since the results were not better than the defaults, I kept the sklearn default params for solver and activation function.
* The final models that we ended up using and their false negative and false positive scores are displayed in the table below. For reference, our models all scored around 80-85% in overall accuracy.

The model that I ended up going with using for my co-dominant model was the Gaussian Naive Bayes classifier. I also contributed to the MLP tuning process and worked with Nikhil to find the pareto-optimal fronts. Since MLP was comparatively good compared to the other models, we had to "adversely tune" some parameters to make sure that MLP did not dominate another algorithm (like Gaussian Naive Bayes)


Table of Scores

{| class="wikitable"
!Subteam member
!Model
!False Negatives
!False Positives
|-
|David
|Gaussian Naive Bayes
|40
|21
|-
|Nikhil
|MLP
|30
|29
|-
|Jordan
|Random Forest
|30
|30
|-
|Austin
|Logistic Regression
|31
|25
|-
|}


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Exchange contact info for subteam
|Completed
|September 15, 2021
|September 22, 2021
|September 18, 2021
|-
|Meet up with subteam to work on creating the model for the Titanic dataset
|Completed
|September 15, 2021
|September 22, 2021
|September 18, 2021
|-
|Submit predictions for co-dominant model
|In progress
|September 15, 2021
|September 22, 2021
|September 18, 2021
|-
|}

== September 8, 2021 ==
'''Team Meeting Notes'''

Lecture 3: Multiple Objectives

Main idea: There is often more than one objective to minimize/maximize. How can our algorithm take into account multiple objectives?

* Metrics for algorithms: accuracy, computational speed, scalability, reliability
* Gene pool: set of genomes to be evaluated during the current generation
* Genome: genotypic description of an individual
** GA = set of values
** GP = tree structure (parse tree)
* Search space: Set of all possible genomes. For GP, set of all possible algorithms
* Evaluation: associates a genome/individual with a set of scores. phenotype is the expression of a genotype
** Confusion matrix: 2x2 grid with TP, FP, TN, FN results
** Sensitivity (or recall, true positivity rate): TP/(TP + FN)
** Specificity (true negative rate): TN/(TN + FP)
** False negative rate: 1-TPR
** False positive rate: 1-TNR
** Precision (PPV): TP / (TP + FP)
** Negative predictive value (NPV): TN / (TN + FN)
** Accuracy: (TP + TN) / (P + N)

Pareto Optimality: An individual is pareto optimal if there i s no other individual in the population that outperforms the individual on all objectives
* The set of all pareto optimal individuals is called the "pareto frontier"
* We want to drive selection by favoring Pareto individuals while also maintaining diversity by giving all individuals some probability of mating

Nondominated Sorting Genetic Algorithm II (NGSA II)
* Separate population into nondomination ranks (Rank I => pareto optimal; Rank II => pareto optimal after removing rank I individuals; Rank n => pareto optimal after removing rank n-1 individuals)
* Individuals selected through a binary tournament
** Lower pareto ranks beat higher pareto ranks
** Ties on the same front are broken by higher crowding distance (summation of normalized euclidean distances to all points within the front)

Strength Pareto Evolutionary Algorithm 2 (SPEA2)
* Each individual is given a strength S (how many others in the population it dominates)
* Each individual is given a rank R (sum of S's of individuals that dominate it)
** for ties, distance to the kth nearest neighbor (sigma^k) is calculated and fitness of R+1/(sigma^k+2) obtained

'''Lab 2 Multi-Objectives'''

Objective: Create a GP algorithm that optimizes multiple objectives. In this case, we want to minimize the mean squared error (MSE) and the tree size.

The objective is applicable in the real world since we often want to minimize MSE (commonly a loss function in regression) while also utilizing the simplest model possible (to reduce overfitting and generalize better) which is taken care of by the min tree size objective.

Notes:
* Set seed to have reproducible results
* Changed problem to more challenging base function compared to lab 2 part 1
* Define pareto dominance function which returns true if an individual pareto dominates another (is better on both objectives)
* Utilize DEAP's eaMuPlusLambda() function which takes in two extra params: mu (number of individuals to select for the next generation), lambda (number of children to produce at each generation). We set mu=50, lambda=100 in our case

The pareto front AUC is 4.886 for the base case and the objective is have as small of an AUC as possible. This means that the non-dominated individuals ("Hall of fame" individuals) are all optimized to have min MSE and tree size.

To try to reduce the AUC as much as possible I tried the following hyperparameter tuning and the results are as follows.

{| class="wikitable"
!Hyperparameter
!Original value
!New value
!Change in AUC
|-
|MUTPB
|0.2
|0.1
|4.89 -> 5.52
|-
|NGEN
|50
|100
|4.89 -> 5.12
|-
|MU
|50
|40
|4.89 -> 2.34
|-
|}

From the hyperparameter tuning, it seems that increasing then number of generations and decreasing the mutation probability does not help with the results; however, decreasing the mu slightly from 50-40 decreased the AUC by more than half. This could be because having less individuals per generation eliminates more individuals quicker, thereby decreasing the tree size and making the function simpler. Interestingly, the best individual fitness is higher (which is worse since we are trying to minimize) but since the tree sizes are so small, the AUC is still much lower.
[[files/dzhang351/lower_AUC.png|thumb|100x100px]]
From the AUC graph above, we can see that the MSE is not particularly great, but compared to the previous AUC, the second objective (tree size) was greatly reduced which resulted in a reduction of nearly 100%. This means that the in terms of minimizing the squared errors, my updated model was relatively the same, but because of the reduction in complexity of the model and the fact that we are using multiple objectives, my model performed better as shown in a reduction in the AUC.

DEAP GP information
* using PrimitiveSetTyped() allows for type constraining the input and output of a particular primitive
* a Terminal is a predefined constant and we can generate terminals by a function (ephemeral constant)
* tree sizes can be bloated over time in a GP problem; to limit tree size we can set a max size param in our mate and mutate methods to reduce bloating

My self-graded notebook:
[[files/dzhang351/self-grade-notebook.png|thumb|100x100px]]

Note: I did not include subteam results as we have not split into subteams yet.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review Class Notes
|Completed
|September 8, 2021
|September 14, 2021
|September 15, 2021
|-
|Self grade wiki notebook
|Completed
|September 8, 2021
|September 14, 2021
|September 15, 2021
|-
|Finish Lab 2
|Completed
|September 8, 2021
|September 14, 2021
|September 15, 2021
|-
|}

== September 1, 2021 ==
'''Team Meeting Notes'''

Lecture 2: Genetic Programming

Main idea: instead of taking an individual and evaluating its objective score, the individual is the function itself.

Tree Representation
* Nodes are called primitives and represent functions
* Leaves are called terminals and represent parameters
** Input is a particular type of terminal
* Function is evaluated upward from leaves to the root
* Tree converted to a lisp preordered parse tree (operators followed by inputs)
* Crossover in tree-based GP => exchanging subtrees at a particular node
* Mutations
** inserting node or subtree
** deleting node or subtree
** changing node or subtree

Evaluating a tree: Feed a number of points into the function to get outputs and compare with the ground truth function. Can use MSE or other error function for evaluation.

Idea behind EMADE: Add useful primitives that can optimize genetic programs. We can even use other machine learning algorithms as primitives (making genetic programming potentially very expressive)

'''Lab 2 Until Multi-Objective GP'''

For genetic programming, we will use "gp.PrimitiveTree" instead of a list since an individual is represented with a tree-like structure (parse tree)

The two new primitives I added were the power and sin functions.

<code>pset.addPrimitive(np.power, arity=2)</code>

<code>pset.addPrimitive(np.sin, arity=1)</code>

My reasoning for adding power is because power can easily generate large numbers without having to use many nodes. I chose the sin function because it is expressive in terms of generating cyclical behavior.

The mutation function that I added is below

<code>toolbox.register("mutate", gp.mutShrink)</code>

I chose to use gp.mutShrink because of the idea of Occam's razor (the simplest solution that performs well should be preferred over a more complicated one that performs similarly)

After trying to run the GP loop, I had some issues with overflow (from both power and later testing exponentials using np.power and np.exp respectively).

[[files/dzhang351/overflow_error.png]]

Lab 2 Debugging notes: One error that I had was with the mutShrink method parameters. I initially assumed that the parameters would be the same as mutUniform, however mutShrink only takes in the individual as a parameter and does not need expression or pset. Removing those parameters resolved the syntax errors, however the overflow errors were still present resulting in NAN values in the summary outputs. Np.exp resulted in overflow while np.power resulted in invalid data types (due to complex numbers from negative fractional powers).

Observations:
After repeating sin(x), tan(x) terminals, one interesting observation is the variance in fitness outcome
# Best individual is add(add(x, multiply(x, multiply(multiply(x, x), x))), multiply(add(multiply(x, x), x), x)), (1.1041911805293318e-16,)
# Best individual is add(multiply(tan(x), tan(tan(sin(sin(x))))), tan(tan(sin(x)))), (0.07116831096996691,)
# Best individual is add(multiply(tan(x), tan(x)), tan(x)), (0.21309774609405807,)

* It seems that the simplest model (3) performed the worst while the most complicated model (1) performed the best in terms of minimizing the objective
* Also, the difference between the min and max values are very high and the minimum value seems to flatline after a few iterations
* After adding another mutate method of "mutateEphemeral", the variation seemed to be lower in results (consistently getting c * e-16 results)
    toolbox.register("mutate", gp.mutEphemeral, mode="all")
* Finally the last item I tested was the number of iterations on variance.
** My hypothesis was that as number of iterations increased, the variance in optimal fitness between runs would decrease. I increased the number of runs to 100 (previously 40) and when I ran the GP multiple times, I realized that the variance was similar. It seems that the GP would converge on a solution and get "stuck" performing substantially worse on some rounds, while performing well on others. For example the output of GP min fitness would often switch between (2) and (1) from above which is a substantial difference.

An example graph of one particular run of GP is shown below
[[files/dzhang351/lab2p1.png|thumb|100x100px]]
As shown in the graph, we can see that the values converge very quickly to a solution. This is good in a sense that we are able to find a good solution very quickly that minimizes fitness, but also could be a problem in some cases. For example on some particular runs, I found that the solution converges to a very suboptimal one (ie: one run produces 1e-16 error while another run produces 0.0026 error) and when it does converge, it doesn't seem to be able to converge to another "more optimal" result. This may be due to the fact that mating through crossovers is not introducing enough genetic diversity after a solutions is found. One potential solution is possibly adding more randomness in crossovers or introducing mutations more when our population has seemed to converge in order to introduce more genetic diversity. 

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review Class Notes
|Completed
|September 1, 2021
|September 8, 2021
|September 4, 2021
|-
|Complete Lab 2 until Multi Objective
|Completed
|September 1, 2021
|September 8, 2021
|September 7, 2021
|-
|Resolve overflow issues for Lab 2
|Completed
|September 5, 2021
|September 8, 2021
|September 7, 2021
|-
|}

== August 25, 2021 ==

'''Team Meeting Notes'''

Lecture 1: Genetic Algorithm Introduction

Algorithm idea: each generation is created through mating/mutation of individuals in the previous population. Through numerous operations of this process, the fitness of the population will be optimized.

Keywords and Concepts

* Individual vs Population (population composed of individuals) 
* Objective: performance measure of an individual (goal is to maximize objective) 
* Fitness: relative measure of performance in comparison to other individuals 
* Selection: represents “survival of the fittest” 
* Fitness Proportionate: the greater fitness value, the higher probability of being selected for mating 
* Tournament: several tournaments among individuals determine selection 
* Mate/Crossover: represents mating between individuals 
** ie: Single point, double point crossover
* Mutation: introduces random modifications (to maintain diversity) 
** ie: Bit flip 

Genetic Algorithm Steps

# Randomly initialize population 
# Determine fitness 
# Repeat until fitness is good enough
## Select parent from population
## Perform mating/crossover
## Mutation
## Determine new fitness

*Types of problem that are good for genetic programming: problems with highly discontinuous, large search spaces 

'''Lab 1'''

One Max Problem

Objective: The population is a string of bits and we are trying to maximize the number of 1 bits within the population.
To maximize a single objective using DEAP, we can use the following line of code

<code> creator.create("FitnessMax", base.Fitness, weights=(1.0,)) </code>

Conversely, in order to minimize a single objective, we can set the weight tuple to (-1.0,) and to create a multi-objective problem, we can set the tuple to (1.0, 1.0)

Some settings used for the one max problem include
* using 2 points crossover function
* mutation with probability 0.05
* tournament selection with tournament size 3 (we use tournament selection because it tends to preserve more diverse traits than simple competition against the entire population
* running the algorithm for 40 generations on a population of 300

Summary

After running the final output 5 times, I got a score of 100 3 times, and 99 2 times. This means that for the most part, my algorithm converged to the best possible population with its optimal population but not always since we only run the algorithm for 40 generations. 

The following is my one-max output graph
[[files/dzhang351/onemax_output.png|thumb|100x100px]]
As shown from the one max output, since this is a maximization problem, after each generation the score increases for the min, max, and average until it reaches nearly max score.

N-Queens Problem

* Objective: we are minimizing the number of queen conflicts on a chess board where conflicts only occur on the diagonals in this problem.
* A permutation is used to define our sample space where each value represents the column of the queen on row i
* We apply partially matched crossover for mating (instead of 2 point)
* For mutation, we cannot directly alter the values, but we instead shuffle indices (in order to preserve board constraints)

Custom Mutate Function (swapping 2 indices)

    #swaps two indices instead of one
    def mutate2(individual, indpb):
        size = len(individual)
        for i in range(size-1):
            if random.random() < indpb:
                swap_indx1 = random.randint(0, size - 2)
                swap_indx2 = random.randint(0, size - 2)
                if swap_indx1 >= i:
                    swap_indx1 += 1
                if swap_indx2 >= i:
                    swap_indx2 += 1
                individual[i], individual[swap_indx1] = \
                    individual[swap_indx1], individual[i]
                individual[i+1], individual[swap_indx2] = \
                    individual[swap_indx2], individual[i+1]
        return individual,

After applying the custom mutate function that swaps more indices, it appears to perform relatively the same as the original mutate function (this is because mutate doesn't happen often and only swapping 2 indices is not a major change from the original)

The following is my n-queens convergence graph
[[files/dzhang351/n-queens_output.png|thumb|100x100px]]

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review Class Notes
|Completed
|August 25, 2021
|September 1, 2021
|August 26, 2021
|-
|Install Jupyter notebook and dependencies
|Completed
|August 25, 2021
|September 1, 2021
|August 28, 2021
|-
|Complete Lab 1
|Completed
|August 25, 2021
|September 1, 2021
|August 31, 2021
|-
|}
