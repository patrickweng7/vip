[[files/GeorgiaTechBuzz.jpg|thumb|231x231px|
bzzzzzzzz
]]
== Team Member ==
Team Member: David Neil Daniell

Email: dndaniell20@gatech.edu
Cell Phone; 678-713-0095

Interests: Soccer, Exercising, Python, Game AI, Genetic Algorithms

== April 19, 2021 ==

==== Individual Notes: ====
*So, for this week, I continued to focus on researching more indicators that are not volume based.
*I tried to stray away from indicators that were moving average based.
*One of the remaining issues that we found in our results is that we struggle a little bit with stocks that are downtrending
**Which makes since because it's harder to make a profit on bearish stocks
**Stocks that are bullish can be bought and sold at more random times.
*For this week, I focused on finding technical indicators that would aid the buying and selling of stocks on a downtrend.
*This would particularly be useful for helping the performance with VZ, a downtrending stock that performed relatively low against the random buy and sell individuals.
**VZ is actually the only stock that performed worse compared to the randomly seeded individuals.
*From my research, one indicator that helps with more downtrending stocks is DMI or Directional Movement Index.
**Similar to Aroon, DMI is an indicator that find the strength and direction of a price movement.
**The intention of the indicator is to reduce false signals, which is prone with Aroon.
**DMI uses three lines, a negative DM and a positive DM line and the ADX line which is the average directional index.
***ADX shows momentum as opposed to direction (strength of a trend).
**With the positive and negative DM lines, the spread also indicates the strength of the current trend.
**Positive over negative, an uptrend is present
**Negative over positive, a downtrend is present.
**Crossover of the two lines indicates that a trend may be reversing.
*The following formula is used to calculate DMI
**DMI positive = ((sum_through_period(current_high - prev_high) + Directional movement) / Average true range) * 100
**DMI negative = ((sum_through_period(current_low - prev_low) - Directional movement) / Average true range) * 100
**important note: since DMI is looking backwards, the trend may not continue to go a certain way upon entry or exit.


==== Subteam Notes (Stocks): ====
*This week during our subteam meeting, we discussed the results of the emade run with the new TI's and eval functions
*The run made a promising individual that outperformed the random buy/sell individual distribution on all stocks
**This is the individual:
***Learner(MyBollingerBand(ARG0, 2, 61, falseBool), LearnerType('DECISIONTREE_REGRESSION', None), EnsembleType('SINGLE', None))
**Essentially, it uses bollinger bands and a decision tree learner to predict buy and sell points.
*We also discussed increasing the window size of the data it looks at (currently at 30 days)
*We decided that we'd try to increase the window size from 30 to 40 days
**We might increase it further with the inclusion of a bigger dataset.
**The aforementioned run was done with a window size of 40 and uses the same evaluation functions as run prior (overall profit percentage, average profit per transaction, profit percentage variance, and the CDF)
**Promising results overall. We now prepare for the final presentation.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Presentation Prep
|Started
|April 19th, 2021
|N/A
|N/A
|-
|Look into finding more technical indicators that aren't volume based, but not Stochastic RSI, AROON
|Done
|April 12th, 2021
|N/A
|April 19th, 2021
|}


== April 12, 2021 ==

==== Individual Notes: ====
*So, for this week, I focused on researching more indicators that are not volume based.
*I tried to stray away from indicators that were moving average based.
*One of the remaining issues that we found in our results is that we struggle a little bit with stocks that are downtrending or neutral oscillating
**Which makes since because it's harder to make a profit on bearish or neutral stocks
**Stocks that are bullish can be bought and sold at more random times.
*For this week, I focused on finding technical indicators that would aid the buying and selling of stocks on a downtrend.
*This would particularly be useful for helping the performance with VZ, a downtrending stock that performed relatively low against the random buy and sell individuals.
**VZ is actually the only stock that performed worse compared to the randomly seeded individuals.
*From my research, one indicator that helps with more downtrending stocks is Aroon.
*To put it into simple terms, Arron is an indicator that is used to identify trend reversals in the stock price as well as a trend strength indicator.
*As opposed to moving averages, Aroon uses time as an aspect, where the indicator will measure the amount of time between highs and lows over a period of time.
*Similar to a MACD indicator, it uses to lines and a crossover to indicate buy/sell signals.
**The Aroon Up line indicates the strength of an uptrend while the Aroon down line indicates the strength of a downtrend
**Aroon Up is usually green while aroon down is usually red.
*Additionally, there are numbers that are associated with each line.
**The lines move between values 0-100.
***A line reading of above 50 indicates that a high or low was seen within the last 12 periods (if your period is 25)
***A line reading of below 50 indicates that a high or low was seen within the last 13 periods
**Crossover usually means a trend change (which is good to determine safe buy points).
*The formula for Aroon Up/Down is as follows:
**Aroon Up = ((N - Periods since N Period high)/ N) * 100
**Aroon Down = ((N - Periods since N Period Low)/ N) * 100
**N is the period of time that you choose to use
*One important note is that Aroon indicators lack the predictive nature of other indicators since it solely looks backwards at previous price highs and lows.

==== Subteam Notes (Stocks): ====
*This week during our subteam meeting, we discussed a lot of things in preparation for another run of EMADE
*We further discussed some improvements including
**Adding another dataset to our folds to evaluated individuals' performance on larger datasets
***The reasoning or logic behind this is that it would give a baseline to see how EMADE performs on data that isn't strictly the same data from the paper
***Essentially, we started to branch out from the paper.
**With the incorporation of new datasets, we decided to talk about new potential stocks to add.
***Some ideas include:
****S%P 500, XLP, ETFs in the range of 2010 to 2019
*Additionally, Abhiram made some visualizations to compare an individual with a Monte-Carlo random distribution, so that we can see how we perform relative to random trading
*We also discussed adding some more technical indicators that I researched (Fibonacci, Stochastic RSI, Beta, Aroon, VWMA, VWAP)
*The addition of a CDF evaluation function
*We also discussed running EMADE separately on each stock.
**The thinking behind this suggestions is that the optimal individual would be different for different stocks.
*Abhiram ran an experiment EMADE run for a few generations on just AAPL and VZ data with this theory in mind
**He found that good individuals correlated to good performance on other stocks

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look into finding more technical indicators that aren't volume based, but not Stochastic RSI
|Done
|April 5th, 2021
|N/A
|April 12th, 2021
|-
|Look into finding more technical indicators that aren't volume based, but not Stochastic RSI, AROON
|Started
|April 12th, 2021
|N/A
|N/A
|}


== April 5, 2021 ==

==== Individual Notes: ====
*So, for this week, I focused on researching more indicators that are not volume based.
*I tried to stray away from indicators that were moving average based.
*One of the remaining issues that we found in our results is that we struggle a little bit with stocks that are downtrending or neutral oscillating
**Which makes since because it's harder to make a profit on bearish or neutral stocks
**Stocks that are bullish can be bought and sold at more random times.
*For this week, I focused on finding technical indicators that would aid the buying and selling of stocks on a more neutral trend.
*From my research, one indicator that helps with more choppy/sideways trends is a Stochastic RSI indicator
**The difference between RSI and Stochastic RSI is that:
**Stochastic RSI moves relatively fast from overbought to oversold in comparison to RSI.
**It isn't a replacement to RSI, and it actually should be used in conjunction with other indicators for it to be truly affective.
**Additionally, Stochastic RSI is deemed a more sensitive indicator that simply means that the actual RSI is high in regards to it's more recent readings.
*The formula for Stochastic RSI is as follows:
** (RSI - min(RSI)) / (max(RSI) - min(RSI))
***RSI is where the current RSI reads at the time of calculation
***The min RSI is the smallest RSI reading over the last N periods
***The max RSI is the largest RSI reading over the last N periods
*Because of it's sensitivity, Stochastic RSI tends to be useful for more choppy stocks that may or may not be neutral trending

==== Subteam Notes (Stocks): ====
*This week during our subteam meeting, we continued to onboard first semester students
*Rishi and Abhiram finished the the integration of all the TA-lib methods that were to replace the same TA's that we implemented individually
**There are some small issues with optimization and seeding that need to be done before running EMADE with it.
*During the Subteam Meeting on Thursday, we continued to discuss new evaluation functions that we could use.
**An interesting suggestion was to implement something that calculates how close our buy and sell signals are to the nearest local max and min points
*Additionally, with Max's random trading implementation, we found that our algorithm doesn't do too well compared to random trading.
*Abhiram came up with the potential solution:
**Instead of using a overall profit percentage function that we use to evaluate individuals, we can compute the z-score of individuals’ profit percentage so we can normalize it relative to random trading.
***This will help because a given overall profit percentage differs on a stock to stock basis, so with this method, we can have a more unbiased way to compare stocks in emade.
**We also started another EMADE run this week.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look into finding more technical indicators that aren't volume based, but not Stochastic RSI
|Started
|April 5th, 2021
|N/A
|N/A
|-
|Look into finding more technical indicators that aren't volume based
|Done
|March 22nd, 2021
|N/A
|April 5th, 2021
|}

== March 22, 2021 ==

==== Individual Notes: ====
*So, for this week, I focused on researching more volume based indicators such as VWMA.
**Not to be confused with VWAP, which is an intraday trading indicator.
*VWMA is another useful volume based indicator that takes the historical price movement and the volume overtime into account as an indicator
**It stands for Volume Weighted Moving Average
*In addition to accounting for end of the day closing prices, it also takes in the trading volume for the day.
*To put it simply, it's very similar to a moving average
*Calculating the VWMA of a stock is pretty simple
**You choose a period to work from (N), and that's the historical data that we account for.
**The formula for VWMA is as follows:
***sum_for_N_days(Price(N) * Volume(N)) / sum_for_N_days(Volume(N))
**Here's the application for the indicator
**Most people use a shorter period VWMA and a longer period VWMA to check for crossovers, similar to MACD
**By doing this crossover, a trader can identify good entry and exit positions for a specific stock.
***For example, if the stock price is currently uptrending and the shorter and longer-period VMWAs crossover each other, the trader is signaled to sell the stock.
****The same could be said about a stock that is currently in a downtrend.
**For the most part, this indicator is actually better than a simple moving average.

==== Subteam Notes (Stocks): ====
*This week during our subteam meeting we were onboarding the new students
*We primarily made a presentation that reviews the basics of EMADE, and what exactly the stocks team is doing when it comes to coding on the team
*We also discussed the research paper that we're using this semester.
**With that, we decided to task them with reading the paper before our team meeting on Thursday
*We also discussed what Dr. Zutty meant by Monte-Carlo simulations to compare individuals
**He then came into the meeting later to clarify what he meant.
**Kartik and Max will run an experiment with this implementation later this week
*During the Subteam Meeting on thursday, Max ran an experiment with this "monte carlo implementation", and it achieved some interesting results
*Rishi and Abhiram continued to prepare the rest of the TA-Lib indicators
**We also discussed the addition of some new indicators

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look into more volume based indicators such as VWMA
|Done
|March 15th, 2021
|N/A
|March 22nd, 2021
|-
|Look into finding more technical indicators that aren't volume based
|Started
|March 22nd, 2021
|N/A
|N/A
|}

== March 15, 2021 ==

==== Individual Notes: ====
*So after the midterm presentation, we as a group shifted our team dynamic as more new members came in.
**Since our group doubled in size, we decided to split up the team into subteams so that people always have an area to work on.
*My subteam is the Research and Technical Indicators subteam
**For the remaining portion of the semester, I'll be researching Technical Indicators and ideas to improve our performance.
**This week I researched volume  indicators, specifically VWAP, VWMA
*Here is what I found for VWAP
**VWAP is an intraday trading indicator, which allows traders to determine buy and sell points during intraday trading.
**It stands for Volume Weighted Average Price
**It's useful for day trading for the most part, but might allow us to be more dynamic with our buy and sell signals for short time intervals.
**Here is the way it works.
***VWAP is calculated by adding up all the money traded for every transaction during the market day, which comes out to be the price multiplied by the number of shares traded and then dividing by the number of total shares traded during the day.
***Here is the formula for VWAP
****sum(price*volume) / sum(volume)
***The use of VWAP is a trend confirmation tool.
****For instance, when a trader sees that the price is above the VWAP line, then that might indicate that they want to initiate a long position.
****On the other hand, if the price is below the VWAP line, then it might indicate that a trader might want to sell the stock now and buy it back at a lower price.
***VWAP is a lagging indicator, meaning that the lag increases throughout the day due to the inclusion of more data.
***Furthermore, keep in mind that it's based on single day historical data.
***I'll do more research on VWMA next week.

==== Subteam Notes (Stocks): ====
*Since our midterm presentation our team has grown dramatically.
*With the increase in team size by over two times, it's important to make sure that the newer people are onboarded well.
*We discussed what we needed to do for newer members, and decided that we should make a small presentation to help onboard.
*We also discussed the use a Monte Carlo algorithm to compare to our EMADE individuals and how they stack up against a control group essentially
*The "monte carlo algorithm" will randomly decide on buy or sell decisions, and compare profit to that of the EMADE individual (should correlate to the stock price trend)
*We decided that this method had too high of a variance, and that we should instead compare it with a buy-and-hold scenario
*We also discussed how we could potentially change the objective functions that we use or add more to our evaluation
*We discussed potentially adding:
**The number of transactions
***However, we don't know how to use this because we can't really maximize or minimize it
**Mean absolute average
***Doesn't correlate well to profit percentage though
**Average profit received per transaction
***We'll maximize this
**Profit variance per transaction
***We'll minimize this
**Lastly, (overall profit percentage) - (profit from Buy-and-hold)


==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look into more volume based indicators such as VWMA
|Started
|March 15th, 2021
|N/A
|N/A
|}

== February 22, 2021 ==

==== Individual Notes: ====
*So this week, I primarily focused on implementing a concept of exponential smoothing in python
*The method is pretty simple.
**[[files/ExponentialSmoothingCode.png|thumb|841x841px]]The parameters are a series of data organized by time or a time series.
**You initialize the smoothed list by adding the first timestep of x<sub>t</sub> to s<sub>t</sub>
**After initializing "smoothed" list, you can loop through each time step in x<sub>t</sub> such that:
***each timestep in s<sub>t</sub> is overwritten by the equation:
****s<sub>t</sub> = αx<sub>t</sub> + (1 - alpha)s<sub>t-1</sub>
***After the pseudocode was written, Abhiram went ahead and implemented it, and I checked over it and made sure it functioned.
***The code for exponential smoothing can be found in the figure to the right

==== Subteam Notes (Stocks): ====
* In our subteam meeting, Abhiram discussed some improvements to the PLR code that was written not too long ago.
* Our team implemented some new technical indicators including:
** BIAS, DeltaSMA, DeltaBIAS, DeltaMACD, DeltaSTOCH, DeltaWILLR, DeltaRSI, BiasEMA, and DeltaEMA
* In addition to these primitives, we are also working on some volume based technical indicators.
* Additionally, we found that our price data was inconsistent to the paper.
** This is due to the fact that the data we gathered, AAPL for example, was post-stock split. So, it's prices had been adjusted due to that.
* We found more consistent data via AlphaVantage to fix our problem with datasets
* Max found a Python library that has a bunch of technical indicators implemented, so it could be useful to use in the future.
* For the coming weeks, we decided to have the following goals:
** Integrate Exponential smoothing and Trading Point Decision
** Finalize the PLR code that we've already written
** Look more into the GA threshold algorithm used to estimate profit via buying and selling points
*** The main issue that I'll be investigating is how they implement their main evolutionary cycle
** Finalize everything such that we can run everything through EMADE to see how we do.
** Look into implementing indicators such as inflow and outflow volumes
*** This will require a special type of dataset, so I will be researching the feasibility of finding this dataset.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Begin coding Exponential Smoothing
|Done
|February 15th, 2021
|N/A
|February 22nd, 2021
|-
|Look into finding datasets for inflow and outflow of stocks
|Not Started
|February 22nd, 2021
|N/A
|N/A
|-
|Look into how the paper implements their GA evolution cycle
|Not Started
|Febraury 22nd, 2021
|N/A
|N/A
|}

== February 15, 2021 ==

==== Individual Notes: ====
*So this week, I focused on researching Exponential Smoothing and tried to figure out what it is and what it is used for.
** I'll also need to figure out how it is used in relation to the model in our paper.
* I did a quick search on wikipedia to get an idea and here's what I found.
* Exponential Smoothing is a smoothing technique used on time series data
** Unlike a moving average smoothing technique where the data over time is weighted equally, exponential smoothing uses a smoothing factor that essentially gives a decreasing weight over that period or series of time.[[files/Exponential Smoothing Formula.png|thumb|350x350px]]Here is the formula for exponential smoothing on the left:
** I'll go over what each of the symbols mean in the formula:
*** x<sub>t</sub> represents a piece of data at timestep "t". This is the raw data that is inputed into the exponential smoothing function
*** s<sub>t</sub> represents a piece of ''smoothed'' data at timestep "t". s<sub>t-1</sub> obviously represents the data at the previous timestep
*** α represents the smoothing factor which is a value 0 <= α <= 1.
**** a smoothing factor of 0 gives us a simple moving average smoothing
****[[files/Example of exponential Smoothing.png|thumb|600x600px]]a smoothing factor of 1 just gives us the original graph.
**** Therefore, we can derive:
***** A higher smoothing factor gives bias to more recent data
***** A lower smoothing factor has a greater smoothing effect and does not respond to more recent changes in data as much
**** With this being a variable part of exponential smoothing, I thought that this would be a good thing to put into emade per stock
***** With GA's we can maximize the best smoothing factor per stock forecast
***** I figured that most stock forecasts would perform different based on smoothing factor

==== Subteam Notes (Stocks): ====
* During our subteam meeting, we discussed the data that I had found on Yahoo Finance.
* We also discussed Abhiram's concept of the PLR model
** We found that there were a few errors that needed to be fixed, but with another pair of eyes on it, it should be fine.
* I explained the concept of exponential smoothing to the group
** The next action item is to create an exponential smoothing method in python that we can use.
** I'll be working on that task with Abhiram.
* After code review, the PLR code was able to successfully function properly.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Begin coding Exponential Smoothing
|Not Started
|February 15, 2021
|N/A
|N/A
|-
|Look into Exponential Smoothing
|Done
|February 8th, 2021
|N/A
|February 15th, 2021
|}

== February 8, 2021 ==

==== Self Evaluation: ====
[[files/David Daniell Self Eval.pdf|thumb|Self Evaluation]]
* Click here

==== Individual Notes: ====
* For this week, I went ahead and looked into the paper that we were using to try to find the datasets that they used.
** If we can use the exact same datasets, we might be able to mimic results which is a good baseline for us to be at.
** You can find this paper in the link below:
*** https://www.sciencedirect.com/science/article/pii/S1568494611000937
** After looking through the paper, I found the data that we'll be passing into the dynamic threshold model, which will determine the buy and sell signals that it will produce.
** It seems that the authors of the paper decided to focus on specific stocks as opposed to a general market analysis.
*** They specifically organized the stocks into 3 different trends:
**** Bullish
***** Indicates a long term upwards trend
**** Bearish
***** Indicates a long term downwards trend
**** Neutral
***** Indicates a sideways trend that does not move up or down too much
*** In addition to this structure, they also decided to bring in stocks from the Taiwan Stock market as well as the NYSE
**** American Stocks:
***** AAPL - labeled as Bullish long term
***** BA - labeled as Neutral long term
***** VZ - labeled as Bearish long term
**** [[files/FindingHistoricalStockData.png|thumb|600x600px]]Taiwan Stocks
***** AUO
***** EPISTAR
***** UMC
*** After discussing with Abhiram, we decided to only focus on American Stocks.
** For the American stocks, they chose to use set dates of 01/02/2008 to 12/30/2008 for each stock's dataset.
** To find this data, I went to Yahoo Finance and looked at historic data.[[files/HistoricalStockData.png|thumb|888x888px]]
*** You can see a screenshot of where to find historic data in the figures to the right

==== Subteam Notes (Stocks): ====
* During our subteam meeting, we solidified the paper that we'd be using. That paper was posted earlier in my individual section of the notebook.
* As a result of finding our baseline paper, we decided that we'd focus on developing a PLR (Piecewise Linear Representation)
** This PLR will essentially produce the trading signals that represent price points to buy and sell specific stocks
* In addition to that aspect of the paper, we also decided to look into Exponential Smoothing
** This will be my task for the next week.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Find the Dataset used in the paper
|Done
|February 1st, 2021
|N/A
|February 8th, 2021
|-
|Look into Exponential Smoothing
|Done
|February 8th, 2021
|N/A
|N/A
|}


== November 28, 2020 ==
==== Individual Notes: ====
* So this week, Tanishq, Kinnera, and I got together to discuss the unit tests and how we were going to divvy them up.
** Looking Abhirams sample code for the normalization unit tests, we figured we would have to do a couple of things.
*** First test to see if the primitive/technical indicator is added.[[files/Unit Tests for primitives.png|thumb]]
*** Secondly, test the shape of the output of each technical indicator to see if the math is done correctly.
*** OR have a sample output that's calculated by hand and then input the numbers necessary within that TI's function to see if the values match.
** So far, we've tested the output shape of almost all the existing technical indicators without any issues.

==== Subteam Notes (Stocks): ====
* During our subteam meeting, we just discussed the final presentation and what needs to be done for it.
** Since we were on break for a majority of the time, we did not get too much done between the last meeting and this one.
** We briefly discussed the progress on technical indicators. The first semesters should be done any time now.
** We also discussed ways that we could improve our results that do not take from the paper that we chose at the beginning of the semester.
* Here are some things we discussed.
** We could play the the granularity of the data.
*** Does the data structured by the minute, hours, few hours, days, etc
** We could try to increase the date range of the test data to larger than 5 years.
*** More test data, the better
** We could improve upon our current genetic labeling
*** We can play with the frequency that we make trades
*** Look into other oracles used in literature
*** Ways we can prevent the models from bottoming out during learning
** Lastly, we can run Stream to Feature function on normalized data to help even out outliers.
* Other than these things, we still need to look into new literature that we can use as a pseudo benchmark. These are the things that we'll be working on next semester.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create unit tests for the existing TI's
|In Progress/Mid-way through
|November 17, 2020
|N/A
|N/A
|}


== November 17, 2020 ==
==== Individual Notes: ====
* Between the 10th and the 24th, Tanishq and I found time to test if we can add the primitives successfully to EMADE. 
** Our code worked and we we able to add them successfully on a run.
** During this time we had to wait for the first semesters to get done with their primitives.
* There were no issues with the code we wrote. So, there's not much to document.

==== Subteam Notes (Stocks): ====
* During our subteam meeting, we talked about the issue with the main paper more.
** We found that there were inconsistencies within the paper.
*** An example can be shown to the left.
**** Essentially, there was a point where the MAT, Market Price, and trend indicator did not match up properly.
**** This makes the paper that we're currently using impossible to mimic, and essentially, we can no longer use it as our baseline.
** For the technical indicator team (first semesters), they still were not finished, but there's no issue since the EMADE team has to figure out the research paper issues.
** Since progress is currently difficult, we decided that we could create unit tests for all the technical indicators.
*** It would be useful since all the unit tests are run as their added to the EMADE registry as primitives.
** Our task is to try to complete some of them while the TI's are being made still.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create unit tests for the existing TI's
|Started
|November 17, 2020
|N/A
|N/A
|-
|Test to see if TI's are successfully added to EMADE with a test run
|Finished
|November 10, 2020
|N/A
|November 24, 2020
|}


== November 10, 2020 ==
==== Individual Notes: ====
* This week it was Tanishq and I's job to work on creating the functionality that adds new primitives to EMADE.
* The process was relatively simple.
** There's a register function within GPFramework of EMADE that allows you to add new primitives that EMADE will use.
*** This register function has a few different parameters.
**** The name of the primitive that you're adding as a string
**** Unit test (the name of the unit test function) as a string
**** The name of the primitive (specifically the name of the primitive function) that you're adding
**** Valid data types that are passed in
**** and the relationship that the data has with EMADE
*****[[files/CodeSnippetAddingPrimitives.png|thumb|600x600px]]This is the important part. We needed to make sure that we added this parameter as Stream to Feature as opposed to Stream to Stream or Feature to Feature
***** Once you fill in all the necessary parameters. The only thing needed is to call the register function for all the technical indicators as the primitives
***** Here is a snippet of code explaining what I mean.

==== Subteam Notes (Stocks): ====
* During our subteam meeting, the progress being made on the technical indicators, functionality to put the technical indicators into EMADE as primitives, and updates on current work.
** The new members on the team still needed to finish their technical indicators/are still getting used to the code base.
*** They said that they'd have them all done in a couple of weeks.
** Tanishq and I talked about adding some of the pre-existing technical indicators to EMADE and how it works.
** The bigger issue that the EMADE team brought up is that theirs something fishy with the paper that we're using. The results weren't matching up nearly as close as they should be.
*** So, the EMADE team was tasked to look into fixing some bugs currently present in the project as well as look into the paper more.
*** If the paper shows to be inconsistent with the work shown on it. We will have to go back to the drawing board in a few ways
*** We also talked about normalizing the data to help improve results.
**** Someone from the EMADE team will work on that while the first semesters and Tanishq and I test the functionality of our current work with an EMADE run.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create the functionality to add technical indicators to EMADE
|Finished
|November 3, 2020
|N/A
|November 10, 2020
|-
|Test to see if TI's are successfully added to EMADE with a test run
|Started
|November 10, 2020
|N/A
|N/A
|}

== November 3, 2020 ==
==== Individual Notes: ====
* For the last week following the presentation, the time was spent getting new members up to speed on the papers and the EMADE team fixing potential bugs.
* I helped with some EMADE runs, and started looking into putting the technical indicators that were already made into EMADE successfully.
* The majority of the work was talked about with the subteam.

==== Subteam Notes (Stocks): ====
* During our subteam meeting, we talked about needed to add our technical indicators to EMADE.
** We also talked about the current research paper that we're basing our model off of with the new members.
*** They talked about their opinions on the paper and how they understood it.
** From there, it was the first semesters' jobs to add in extra technical indicators.
*** Once we have these technical indicators, we can successfully add them to EMADE.
**** Adding them into EMADE was Tanishq and I's job for the mean time.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create the functionality to add technical indicators to EMADE
|Started
|November 3, 2020
|N/A
|N/A
|}

== October 18, 2020 ==
==== Individual Notes: ====
[[files/Best ind results.png|thumb]]
* This week was spent finalizing the presentation from last week and deciding who will take what in the presentation, speaking-wise.
* As for development, there was none this week really.
* We spent a majority of the time working with what we have and trying to do some test runs using emade.
* As a whole, I had to fix some bugs in my personal code that allows emade to run, but got it fixed within no time.

==== Subteam Notes (Stocks): ====
* During our subteam meeting, we talked about the results of our emade runs.
** The results were as follows:
*** The figure to the left indicates that the best individual produced by emade generated a profit of 2.3 percent.
*** However, the actual next day results using those by and sell points in time generated a negative profit of -11.7 percent.
*** Since this actual loss in profit was so large, we're thinking that there must be something wrong with the code to produce something this drastic
**** [[files/Truth graph.png|thumb]]We'll work on fixing this after the presentation.
*** The truth graph of the next day trend score is below the first figure to the left.
** The final thing on the agenda was to decide who takes which slides when speaking for the presentation.
*** I ended up sticking with just the CEFLANN introduction slide.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Work on the presentation
|Finished
|October 12, 2020
|N/A
|October 18, 2020
|-
|Practice presenting research on CEFLANN
|Finished
|October 12, 2020
|N/A
|October 18, 2020
|}


== October 12, 2020 ==
==== Individual Notes: ====
* For the most part, this period of time was spent finalizing research on the CEFLANN model and working on the presentation.
* [[files/MidtermPres.png|thumb]]It was my job to focus on the CEFLANN section of presentation
** In the presentation, I talk about the issues with researching FLANN or CEFLANN due to the inconsistent abbreviations and the conflicting names with the nearest neighbor algorithm.
** Additionally, I explain why it was computationally efficient, which is due to the fact it doesn't use hidden layers in its architecture.
** It also uses an ELM which speeds up the learning process ''significantly.''
*** Studies report that it's a 100 times faster.
** I also mention in the presentation that our paper uses an CEFLANN model and compares it to other common classifiers.
*** This aspect of the paper is the main reason that we decided to try to implement a CEFLANN and try to mimic the results of the paper.
**** This decision includes that the paper also uses technical indicators as features to the model.

==== Subteam Notes (Stocks): ====
* For the subteam meeting, we mainly focused on completing the presentation. 
** We once again discussed the viability of making our CEFLANN model from scratch.
***Although it would be difficult to implement, the paper yields promising results, so we still are considering it as an option.
**** However, since I'm not an expert on EMADE and how difficult it would be to implement. The decision as to if we're going to continue with pursuing implementing an ELM and CEFLANN from scratch is still in the air.
** We also discussed what the new members would do when they joined the team.
*** For the most part, we talked about having them do technical indicators that we will eventually be putting into emade.
** We also talked about how we can put those technical indicators into EMADE. This is what I would be researching along with Tanishq.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Work on the presentation
|Ongoing
|October 12, 2020
|N/A
|N/A
|-
|Practice presenting research on CEFLANN
|Ongoing
|October 12, 2020
|N/A
|N/A
|}

== September 28, 2020 ==
==== Individual Notes: ====
* For the weekend, it was my task to do research on the artificial neural network model, CEFLANN
* Before researching CEFLANN, I thought it would be important to try to understand the difference between FLANN and CEFLANN
** From my research, I couldn't find any differences or any articles linking to the differences between CEFLANN and FLANN
*** A FLANN model is inherently "computationally efficient" because it lack hidden layers, and the "CE" in "CEFLANN" stands for computationally efficient
**** This fact leads me to believe that the two acronyms are synonymous.[[files/FLANN model structure.png|thumb]]
*** With that in mind, I'll go into what I learned about CEFLANN
**** In comparison to MLP's, FLANN or CEFLANN models don't have any hidden layers.
**** In the figure to the right, you can see the FLANN's single layer structure.
**** Since they don't have any hidden layers, that leads them to be more "computationally efficient" and having a high convergence rate.
**** [https://ieeexplore.ieee.org/document/6508166 One of the papers] that I read compared the efficiency of convergence between an MLP and a FLANN model
***** The MLP took 100 times longer to simulate in comparison to the FLANN Model.
****** The MLP model took ~100 seconds to simulate while the FLANN model took ~1-2 seconds to simulate.
**** Each of the papers, I found listed a step by step process as to how they developed their model, which is useful in our case.
**** Even for the paper that we chose to use for our baseline has a step by step process as to how they developed the model with the use of technical analysis and technical indicators.
**** You can find the step by step process in this [https://www.sciencedirect.com/science/article/pii/S2405918815300179 paper].
*** [[files/Paper that describes the use of an ELM.png|thumb]]As for the ELM, I found that the ELM is actually used to train the CEFLANN model instead of using a traditional back propagation technique.
*** In addition to the research done to understand FLANN and CEFLANN, I also looked into sources we could use for implementation.
**** I could not find any existing libraries that have a professional CEFLANN/FLANN implementation built out in python.
**** However, I found a github repo that has one built out in python.
**** We could try to use this and change it up as we see fit, or we could try to build one completely from scratch.
**** Here is a link to the repo

==== Subteam Notes (Stocks): ====
* For the subteam meeting, we talked about the research that I did for the most part. 
** We discussed the viability of making our CEFLANN model from scratch. 
*** However, since I'm not an expert on EMADE and how difficult it would be to implement. The decision as to if we're going to continue with pursuing implementing an ELM and CEFLANN from scratch is still in the air.
** We did not discuss the repo that I have in my individual notes. However, we will discuss it on Thursday.
** For the most part, the technical indicators team is busy implementing, so Tanishq and I will continue to research CEFLANN's further while we have time.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Document research on the CEFLANN
|Ongoing
|September 24, 2020
|N/A
|N/A
|-
|Discuss finding and decide whether or not it's feasible
|Ongoing
|September 24, 2020
|N/A
|N/A
|}


== September 24, 2020 ==
==== Individual Notes: ====
* This week I had to set up EMADE and get everything running on Colab.
** Since I already had EMADE set up on my laptop, I decided to update my repo and checkout the Stocks-specific branch.
*** Rishi had already set up a branch that we can edit and what not without affecting the main branch.
** From there, I had to set up the mySQL database that was connected to the AWS server.
*** To do this step, I deleted the other connections that I wasn't using.
*** Additionally, I had to go in and edit the input_titanic.xml file to specify the database and table I was connecting to.
** Once I had set up the mySQL and AWS server connection, I could try to set up the EMADE process within Colab.
** Abirahm had already finished a set-up file that I could use and edit a few things to have it work for me as well.
** So, all I really needed to do was to specify worker or master process and run the notebook.
** We did encounter an issue with deap; however, all we needed to do was pull a specific version of it to fix the problem.
**[[files/Command to run EMADE in Colab.png|thumb]]Once we fixed this issue, I was able to run the titanic dataset with EMADE on Colab.
*** I tried both worker and master processes and had success with both.

==== Subteam Notes (Stocks): ====
* For this subteam meeting, we decided to talk about implementation that aligns with the paper, and tasking specific parts of the implementation.
* Since setting up EMADE on Colab was relatively easy we decided to not really touch on it.
* One of the issues that came up was the fact that we don't have any information on the CEFLANN model that the paper used.
** In addition to that fact, we also didn't know too much about the ELM model that they use too.
* The issue with this fact is that we found that there wasn't any libraries that has an available CEFLANN or ELM implementation
** Because there's not an official library, we have to either:
*** Implement both models themselves
*** Or find an existing repo that we can use
** So far, we didn't have either of those worked out, meaning we needed to do more research on the model itself.
* As of this meeting, Rishi did a little bit of research. Enough to realize that there's not a library available.
* Additionally, though, it was hard to find any documentation on how to implement a CEFLANN model, or what a CEFLANN model even is in the first place.
* This issue leads to me being tasked to do research on CEFLANN and how to implement it.
* I was tasked to try to find some information on the model by the next Monday before our Main meeting at 5pm.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Document research on the CEFLANN
|Started
|September 24, 2020
|N/A
|N/A
|-
|Discuss finding and decide whether or not it's feasible
|Started
|September 24, 2020
|N/A
|N/A
|-
|Set up EMADE on Colab
|Done
|September 17, 2020
|N/A
|September 16, 2020
|-
|Set up SQL database connected to AWS server
|Done
|September 17, 2020
|N/A
|September 16, 2020
|-
|Run titanic successfully with no issues on Colab
|Done
|September 17, 2020
|N/A
|September 16, 2020
|}


== September 17, 2020 ==
==== Individual Notes: ====
* Individual notes are sparse for this week because we decided to hold off on doing anything until we chose a specific paper to work off of.
* However, we also needed to wait on the EMADE team to give us feed back on what is feasible so that we can narrow down the papers to choose from.
** I'm a part of the Research team, so I didn't have too many tasks to do.

==== Subteam Notes (Stocks): ====
* During our subteam meeting, we talked about the exact timelines or time intervals we'd work with.
** We decided to move to predicting closing day times since all the major papers that we read worked with this timeline.
*** It would ultimately help us set up a baseline for our project sooner.
** In addition to that, we also talked about trying to switch to a classification type of problem as opposed to a regression type of problem.
*** Instead of predicting the exact price of a stock using technical indicators and previous price data, we'd ultimately switch to a "buy" or "sell" classification problem.
**** Our reasoning behind this change in our goals was that we already wanted to automate stock trading regardless of what our model returned.
**** So, if we used a regression model to predict the price, we still wanted to automate the buying or selling portion.
***** If we used a regression model, we'd still have to develop an algorithm that used this predicted data to determine whether or not to buy or sell the stock.
**** Therefore, we thought it would be simpler to use a classification type of model that would handle determining whether to buy or sell a stock.
***** It simplifies the roadmap to get to our overall goal to a certain degree.
** This subteam meeting was also when we needed to decide on a paper to focus on so that we can use their methodology to produce a baseline model.
*** After hearing back from the EMADE team, we decided to use Colab to run EMADE on our future model.
**** PACE just presents extra complexity that we can't afford to deal with throughout the semester
**** Colab seems to be more flexible, so progress would probably be faster using it.
*** Additionally, we finalized the paper that we were going to use to produce a baseline model for our project.
**** Here is the paper: [https://doi.org/10.1016/j.jfds.2016.03.002 A hybrid stock trading framework integrating technical analysis with machine learning techniques]
***** This model uses a CEFLANN model and an ELM to train that model.
***** The CEFLANN is super light weight (hence, the "computationally efficient" CE part)
***** It provides a fair number of models that it compares results to.
****** Which is very good because a lot of our papers don't compare baseline results.
***** It uses technical indicators just like we also needed.
***** It's a classification model, so it returns a signal to buy, sell, or hold a specific stock.
***** It also provides a nice step by step methodology that we can vaguely follow to produce a baseline.
*** We decided that we'd try to mimic their results first using their methodology as a baseline.
*** Once we have a baseline, we can start integrating that model into EMADE to further improve it.
*** The technical indicators team will start trying to replicate this model while the rest of us need to set up EMADE on Colab.
**** Once we have EMADE set up on Colab using AWS servers, we can try to run titanic to see if it works properly.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up EMADE on Colab
|Started
|September 17, 2020
|N/A
|N/A
|-
|Set up SQL database connected to AWS server
|Started
|September 17, 2020
|N/A
|N/A
|-
|Run titanic successfully with no issues on Colab
|Started
|September 17, 2020
|N/A
|N/A
|-
|Discuss and choose the most promising paper to work off of
|Done
|September 3, 2020
|N/A
|September 17, 2020
|-
|Discuss future roadmap based on EMADE team findings
|Done
|September 3, 2020
|N/A
|September 17, 2020
|}


== September 10, 2020 ==
[[files/David Daniell Self Graded.docx|thumb]]

==== Individual Notes: ====
* This week we researched different ML papers pertaining to stock price prediction.
** Our task was to find 4 relevant papers, and read 2 of them in depth. Here were my findings.
* [http://www.ijesi.org/papers/Vol(7)i10/Version-2/D0710022933.pdf Use of Multiple Linear Regression Models to Predict Stock Price]
** This paper focused on predicting closing day prices using multiple linear regression models.
** Here are some key takeaways:
*** The project showed promising results giving an RMSE of 1.14.
*** Some closing notes said that the project could be further improved with the implementation of Genetic Programming.
**** This could be useful since we're using EMADE to evolve whatever we put into it.
* [https://ieeexplore.ieee.org/document/7415179 Stock price prediction using linear regression based on sentiment analysis]
** This second paper only used one linear regression model; however, the focus of the paper included exploring how public sentiment affects the accuracy and performance of the stock price predicition.
** They talk about other papers that emphasize the potential improvements of existing models using public sentiment.
** Our team never thought of using public sentiment, so I thought it would be a decent idea to at least explore it.
** Some key takeaways are:
*** The team found decent baseline results with an accuracy of ~67%.
*** Showed that public sentiment was a useful predictor.
* Stock Price Prediction Using Machine Learning
** This paper focused on using different regression models to predict closing day prices.
** In comparison to the other papers, this paper was not as useful; however, there was one important aspect that was useful to use.
** Some key takeaways are:
*** The post mortem suggestions for improving the current project highlighted the importance of training data to improve accuracy.
*** They emphasized that having a huge amount of training data (previous stock prices) was important.
*** This aspect was important to us because we haven't talked about the specifics of training data or how we're going to get our data.
* With this in mind, the last paper I looked at actually combined two of the suggestions of the last two papers that I looked at.
* [https://www.aclweb.org/anthology/W19-6403.pdf Automated Stock Price Prediction Using Machine Learning]
** Uses several different models to predict closing day prices. However, there are some key takeaways.
*** This team decided to also just use public sentiment and previous price data BUT
**** The amount of data that they used per stock was massive
**** They used over 10 years of previous price data per stock.
*** This feat combined with the use of public sentiment granted them an accuracy of ~82%
*** Additionally, they mentioned that the project could be improved (accuracy wise) by incorporating technical indicators (good for us).
*** And they also mentioned that the grouping or batching of training data was important. Choosing how you group your data timeline-wise could lead to performance increases.
** Overall this paper was useful for us because it provides a good raw baseline accuracy (82%) and shows that the amount of training data that you have really has an effect on the performance.

==== Subteam Notes (Stocks): ====
* During our Research team meeting, we all talked about the different research papers that we looked at.
** Here is a doc of all the papers we read.
*** [https://docs.google.com/document/d/1sG8iOknJ61Sry5qHomU9_ACwK7osv4rF-4h15YL7xtw/edit Papers]
** For my papers specifically, we talked about the complications of using public sentiment.
*** We'd have to potentially use NLP while webscraping from various sources.
*** We could potentially work with the NLP team.
*** However, it would be more time efficient to add on the use of public sentiment after we get a successful baseline running.
*** If we have time at the end of the semester, we could potentially add it on seeking further improvement.
** We also went back to the discussion of whats the best beneficial time interval to work with.
*** We decided to stay with the short interval (i.e. hourly, every 5 mins); however, we probably will change to a longer interval later on (i.e. daily)
**** Based on our research, working shorter intervals seem to prove to be a more difficult task because of the crazy volatility.
** At the end, we decided that next week would include us choosing a paper to work off of to produce a baseline.
*** Using their methodologies, we will be able to add on and change their project as we need to achieve our goals. 
** We also needed to wait on the EMADE team to update us on how to go about future work with EMADE.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Discuss findings on papers.
|Done
|September 3, 2020
|N/A
|September 10, 2020
|-
|Find and read 4 research papers on topic of stock price prediction
|Done
|September 3, 2020
|N/A
|September 10, 2020
|-
|Discuss and choose the most promising paper to work off of
|Started
|September 3, 2020
|N/A
|N/A
|-
|Discuss future roadmap based on EMADE team findings
|Started
|September 3, 2020
|
|N/A
|}

== September 3, 2020 ==
==== Individual Notes: ====
* On Monday, we had a quick meeting to decide how we're going about researching.
* The research team was tasked with researching different technical indicators that could be useful to us.
* My task specifically was to research the technical indicator category, Oscillators, which usually take into account of the change or trend of the graph across an interval of time.
** I ended up doing some research on a few oscillators including:
*** MACD
*** RSI
*** Stochastic Oscillator
** RSI findings:
*** The relative strength index (RSI) is a momentum indicator or oscillator indicator used in technical analysis that measures the magnitude of recent price changes to evaluate overbought or oversold conditions in the price of a stock or other asset.
*** You can calculate the RSI of a stock by using this two step process.
**** The formula for step one is to the right. As you can see, we use the average gain and losses to generate a percentage.
**** The formula for step two is below step one. What's different in this calculation is that we take into account of the previous and current average gains and losses to generate a percentage.
**** Key takeaways for RSI:[[files/RSI calculation.png|thumb]]
***** The RSI percentage will rise as the number and size of positive closes increase.
***** On the other hand, the RSI percentage will fall as the number and size of losses increase.
***** RSI provides information about bullish and bearish price momentum in different stocks, which is useful for our project.
***** RSI provides useful information about the trends of a stock.
***** Generally, an asset is considered overbought given an RSI above 70%, and an asset is considered oversold given an RSI below 30%.[[files/Step 2.png|thumb]]
** MACD findings:
*** Moving Average Convergence Divergence (MACD) is another momentum indicator or oscillator indicator that shows the relationship between two moving averages of an asset's price. 
*** Calculating the MACD of a stock is simple. All that you need to do is subtract the 26 week period EMA (Exponential Moving Average) from the 12 week period EMA.
**** Key takeaways for MACD:
***** By calculating the MACD, a nine-day EMA of the MACD called the "signal line" is generated, which can function as a signal to buy or sell.
***** It's also used as signal to check if a market is overbought or oversold (like RSI).
***** It also helps investors understand whether the bullish or bearish movement in a stock's price is strengthening or weakening (trend analysis).
** Stochastic Oscillator findings:
*** To keep the research done on Stochastic Oscillators brief (because I feel that it's not particularly useful in our case due to our stock choices), I'll give the highlights.
*** Key takeaways for Stochastic Oscillators:
**** A stochastic oscillator is a momentum indicator like RSI that compares a particular closing price of an asset to a range of that asset's prices over a period of time (chosen).
**** It signals whether an asset is overbought or oversold like RSI.
**** The main difference between RSI and a stochastic oscillator is that a stochastic oscillator works better for stocks that consistently oscillate between two price points.
**** On the other hand, RSI works better with stocks that have definitive trends instead of volatile peaks and valleys that stay in between two price points.

==== Subteam Notes (Stocks): ====
* During Research's official team meeting time (now Thursday's 5:30) we discussed our technical indicators in detail.
* We presented our notes on the different types of technical indicators.
** We also decided which ones are more useful for ML applications. Most of them (besides Stochastic oscillators due to our stock choice) ended up being useful.
* We discussed that it would be useful for us to research the methodologies of projects with similar goals like price prediction.
** Each member should fine some published papers on ML in finance or stock price prediction to come up with ideas on how we should implement in EMADE.
*** 3-4 papers each, read about 2 of them in depth.
*** How to read research papers:
**** Read abstract, if interesting move to Results
**** Read results, if interesting, move to Methods
**** Read Methods if the first two provide promising and relevant information to our project.
* We also discussed potentially changing to a classification based project so that we don't have to worry about developing a separate algorithm that tells us to buy or sell a stock based on the current price point.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Technical Indicator research
|Done
|August 31, 2020
|N/A
|September 3, 2020
|-
|Discuss the results of that research to define what indicators are important
|Done
|September 3, 2020
|N/A
|September 3, 2020
|-
|Find ML research papers on stock price prediction
|Not Started
|September 3, 2020
|N/A
|
|}


== August 31, 2020 ==
==== Individual Notes: ====
* No individual notes for this week as we were still figuring out our road map for the semester. And I joined late.

==== Subteam Notes (Stocks AKA STONKS): ====
* We officially figured out our direction that we're going to take.
** With the of EMADE, we want to use regression to predict the stock prices
*** These stock prices are predicted on a short time interval. We figured that predicting the hourly price would be harder to do, but if we could predict prices at that interval, we would be able to do larger intervals with ease.
*** This part of the project is up for debate though. We might change our direction after reading through theoretical papers on the topic.
*** In addition to this action item, we all decided to split the group into 3 teams.
**** Research
**** EMADE
**** Technical Indicators
*** Research is responsible for handling all the ML research needed for the project in addition to finding cutting-edge methods to help achieve our goal.
*** EMADE is responsible for integrating our regression model into EMADE so we can evolve it.
*** Technical indicators is responsible for integrating the necessary technical indicators into our regression model that we will eventually evolve with EMADE.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Organize the team, divide and conquer
|Done
|August 31, 2020
|N/A
|August 31, 2020
|-
|Decide on which sub-sub-team to join (Research)
|Done
|August 31, 2020
|N/A
|August 31, 2020
|}


== April 18, 2020 ==
==== Individual Notes: ====
* UPDATE: I had a teammate look into the code I had for my model and try to get something running.
* She ended up finding out that the data augmentation and preprocessing that I had in my original model was causing the issue.
* So, I went back to see if I could run my model on my computer, and sure enough. I was working.[[files/NewModelDropout.png|thumb|450x450px|This is the new architecture with the inclusion of dropout.]]
** After running the baseline 3 Block VGG model for 200 epochs for completeness, I found that the overall test accuracy came out to be around 73.4 percent.
*** Keep in mind, that this model did not have any dropout, data augmentation or batch normalization like the paper did.
*** However, I went ahead and tried to test my luck by adding in dropout.
**At this point the model had completely changed. The figure to the left shows the new model architecture to reflect the inclusion of increasing dropout.
*** As you can see, dropout is performed after each section/pair of Conv2D layers as well as the fully connected layers.
*** Dropout is when you randomly ignore or drop a node or neuron during the forward pass or back propagation of the neural network.
**** The idea is that this will help prevent overfitting and ultimately increase your accuracy.
*** The results were much better with this inclusion.
*** After training for 200 epochs for completeness, the overall test accuracy rose to 84.5%.
**** Which is significant considering the only aspect that I added was dropout.
****[[files/VGGvariantResults.png|thumb|400x400px|This graph shows the results of the training with the inclusion of increasing dropout.]]You can see the results to the right.
**** Although the results were great with this simple inclusion, the paper noted that the results increase even more (marginally) with the inclusion of:
***** Data augmentation
***** Batch normalization
**** Which is convenient because ezCGP is working on including data augmentation into the new framework.
**** With data augmentation and preprocessing the paper reported that after 400 epochs the test accuracy was 88.42%
***** These results show that data augmentation as well as dropout can lead to significant improvement to baseline architectures.
*** Although I would want to test out the evolution of this model in ezCGP, we were using the old framework 1.0, and the old framework did not support dropout as a primitive.
**** Nor did I have time to debug any issues what may arise as I'm trying to get something to run.

==== Subteam Notes (ezCGP): ====
* Final presentations!
** Everyone is working on/finished their slide.
** For the first semester's we ended up getting one of our models working with ezCGP, and the test accuracy did improve significantly!
*** I believe the final test accuracy was around 85%. The original baseline was around 64%.
*** Please refer to the team presentation slides for exact numbers.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Become more familiar with CGP and how it is used with NNs
|Done
|April 6, 2020
|N/A
|April 9, 2020
|-
|Set up VM and Linux OS for development
|Done
|April 6, 2020
|N/A
|April 7, 2020
|-
|Set up Pace and ezCGP
|Done
|April 6, 2020
|N/A
|April 12, 2020
|-
|Find Dataset to train
|Done
|April 6, 2020
|N/A
|April 13, 2020
|-
|Find man-made model and run it with the dataset
|Done
|April 6, 2020
|N/A
|April 18, 2020
|-
|Evolve and compare said model with ezCGP
|Not Possible
|April 6, 2020
|N/A
|N/A
|-
|Finish slide for final presentation!
|Done
|April 18, 2020
|N/A
|April 18, 2020
|}


== April 14-15, 2020 ==
==== Individual Notes ====
[[files/VGGARCH.png|thumb|390x390px|This is an example of a VGG architecture]]
* '''Meeting with subteam: ezCGP'''
** Today we had another impromptu meeting to prepare ourselves for next Monday.
** After going through all of the issues the other day, I decided to ask some team members what they thought.
*** One person suggested that I try implementing another model. So, I went out to find another model to eventually evolve.
**** The model I chose was similar to a VGG model.
**** Here is the layout of a typical VGG model. There are several variants of a VGG architecture, so I won't go into full detail.
***** However, this is a breakdown of the model I chose.
****** The first layer is a convolutional layer.
****** The second layer is the second convolutional layer.
****** We then pass it through a max pooling layer
****** After max pooling, the third layer is the 3rd convolutional layer
****** The fourth layer is the 4th convolutional layer
****** We then place through max pooling again.
****** The fifth layer is the 5th convolutional layer.
****** The sixth layer is the 6th convolutional layer.[[files/VGG Variant.png|thumb|600x600px|This is a python transcription of the VGG variant that I went with to replace the AlexNet model.]]
****** We then pass it through max pooling for the 3rd time.
****** From here, the last layer is our output layer that we pair with a softmax activation function to squeeze everything between 1 and 0.
***** To compare this model to the AlexNet, it's actually a lot more simple, and it uses just convolutional layers for the most part.
****** A key difference between this model and the AlexNet was that the AlexNet transitions into a fully connected layer while the VGG variant model does not.
**** Here is a Python transcription of the VGG variant model.
**** So, after successfully pulling in the new model. I tried training it on the cifar-10 dataset. However, I ran into the same GPU error.
**** I was pretty frustrated at this point, because I was switching back and forth from different Tensorflow versions to try to find a model that would work. 
**** At this point, another teammate suggested that I try to run the model on PACE and maybe there was a compatibility error with my computer.
**** So, I began trying to set up an environment separate from the environment that I would use for ezCGP within pace to try to run my model successfully.
**** This new endeavor came with its own problems. I tried adding a new environment; however, PACE would not let me because I had "run out of storage."
**** As a result, I had to completely wipe my PACE account of all the folders that it had previously. 
**** This meant that I had to completely wipe my clone of ezCGP and the environment that I set up, and I had to completely reset everything up
**** ''Just'' to set up another environment in addition to my ezCGP environment.
**** Finally, after wiping everything and setting up the ezCGP environment again, I was finally able to set up another environment called ''modelTestEnv'' for my model.
**** After struggling to download all the packages that I needed to run my model due to anaconda errors, I had gathered all the dependencies that I would need to run my model.
**** I now had to set up the .pbs script that coordinates with the python file during its runtime.
***** After setting up my .pbs file, I tried running my model, but it would result in an immediate termination.
***** The logical idea would be to check the .out file, which I did; however, the .out file that returns errors and epoch information was blank.
***** The command that returns .out file outputs "cat <file name>.out" returned a blank line.
***** I tried to use vim to look into and actual file; however, this caused an error where I wouldn't be able to exit the file. (I used all of the possible vim commands to exit)
***** At this point, I gave up as I had spent at least 2 days troubleshooting PACE, and I had a lot of huge assignments due within the next 4 days. 

==== Subteam Notes (ezCGP): ====
* None for this section.
** I'll try to elaborate on subteam notes every time there is a VIP meeting with my subteam.
** As a whole, we needed to:
*** Find an image based dataset to train
*** A model that takes an image based dataset as an input.
*** Train this model and record its accuracy after a certain number of epochs
*** Try to put this model through ezCGP to evolve it and potentially out perform the man-made model.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Become more familiar with CGP and how it is used with NNs
|Done
|April 6, 2020
|N/A
|April 9, 2020
|-
|Set up VM and Linux OS for development
|Done
|April 6, 2020
|N/A
|April 7, 2020
|-
|Set up Pace and ezCGP
|Done
|April 6, 2020
|N/A
|April 12, 2020
|-
|Find Dataset to train
|Done
|April 6, 2020
|N/A
|April 13, 2020
|-
|Find man-made model and run it with the dataset
|Started
|April 6, 2020
|N/A
|N/A
|-
|Evolve and compare said model with ezCGP
|Not Started
|April 6, 2020
|N/A
|N/A
|}


== April 13, 2020 ==
==== Individual Notes ====
* '''Meeting with subteam: ezCGP'''
** Today, during our meeting, now that we had set up PACE and ezCGP. Sam, who is in charge of the first semester students, wanted everyone to find a model during the time we had during the meeting.
** During our meeting, we could ask any questions regarding to finding a paper, model, or dataset to help us progress.
** This is where my stream of problems began:
***[[files/AlexNetImage.png|thumb|700x700px|This figure details the exact model composition of an AlexNet model that I used in the beginning.]]Finding a model was not bad. The model that I chose was an AlexNet. There had been several papers on this model, and the model had produced reliable results.
**** So, I went ahead and found a medium article that helped me figure out the structure of the model.
***** Look at the figure to the right to see the exact composition.
***** Here is a breakdown of the model below:
****** The first layer is the 1st Convolutional layer
****** Pass it through a max pooling layer
****** The second layer is the 2nd Convolutional Layer
****** Pass it through another max pooling layer
****** The third layer is the 3rd Convolutional Layer
****** The fourth layer is the 4th Convolutional Layer
****** The seventh layer is the 5th Convolutional Layer
****** Pass it through another max pooling layer
****** Now, we pass it to a fully connected layer (flatten)[[files/AlexNetPythonImage.png|thumb|500x500px|This is the model transcribed in Python]]
****** The fifth layer is the 1st Fully Connected Layer
****** We add dropout to prevent overfitting of data
****** The sixth layer the 2nd Fully Connected Layer
****** Add dropout again
****** The seventh layer is the 3rd Fully Connected Layer
****** Add drop out again.
****** Finally, the eighth layer is the output layer, and we combine it with a softmax activation function to bring everything between 1 and 0.
****** The figure to the left is the Python Code that describes this model.
**** At this point, I had found a good basis for my model. All I needed to do was take the cifar-10 dataset that I had chosen and run the model to record it's accuracy and training results.
***** This is where I had problems.
***** To start, I had major issues with 'pip' installing tensorflow 2.0 onto my computer, which was needed for the code base that I had.
****** To solve this issue, I had to completely uninstall Python 3.7 and pip from my computer and remove it from my environment variables.
****** Once I uninstalled it, I reinstalled Python 3.7.4 and pip, and I successfully installed tensorflow and all of it's packages.
***** From there, I tried running my model to see if my code would work. However, I was still having issues with Tensorflow.
****** First off, Tensorflow 2.0 defaults to using your CPU because there are extra dependencies needed to use your GPU.
******* However, upon running the program, I had run into this error.
******** "Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2"
********* Meaning my CPU is not defaultly compatible with Tensorflow
******* To bypass this issue, I thought to use my GPU instead of my CPU; however, I ran into more issues regarding that.
******* My program couldn't even recognize my GPU due to the fact that Tensorflow couldn't find specific .dll files
******* I looked into this and found that in order to use your GPU with Tensorflow 2.0, there were several dependencies that Tensorflow required. You need:
******** Up-to-date Nvidia video drivers
******** The Nvidia CUDA toolkit that needs to be version 10.1
******** CUPTI, which comes with the CUDA toolkit
******** The CUDA, cuDNN SDK (>= 7.6)
******* All of these requires a specific GPU to even be compatible with this software.
******* I thought my GPU drivers were up to date, and I knew I had a compatible GPU (I have a GTX 1070) in the first place, so why was I getting errors?
******* I realized I was missing the CUDA toolkit and the cuDNN SDK, which provides those .dll files that Tensorflow couldn't find.
******* So, I downloaded the correct CUDA toolkit and the cuDNN SDK and tried again. However, I was met with another warning that inevitably made me rethink my strategy for the time being.
******** "Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only  Relying on driver to perform ptx compilation. This message will be only logged once."
********Everything seemed to run fine with this message; however, I learned that this message usually results in your program getting hung up on something and eventually time out.
*******I tried researching this problem, and a lot of people seem to encounter it; however, Tensorflow currently does not have a solution
******So, I took time to talk to other members about the issues to see if they had any thoughts.

==== Subteam Notes (ezCGP): ====
* None for this section.
** I'll try to elaborate on subteam notes every time there is a VIP meeting with my subteam.
** As a whole, we needed to:
*** Find an image based dataset to train
*** A model that takes an image based dataset as an input.
*** Train this model and record its accuracy after a certain number of epochs
*** Try to put this model through ezCGP to evolve it and potentially out perform the man-made model.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Become more familiar with CGP and how it is used with NNs
|Done
|April 6, 2020
|N/A
|April 9, 2020
|-
|Set up VM and Linux OS for development
|Done
|April 6, 2020
|N/A
|April 7, 2020
|-
|Set up Pace and ezCGP
|Done
|April 6, 2020
|N/A
|April 12, 2020
|-
|Find Dataset to train
|Done
|April 6, 2020
|N/A
|April 13, 2020
|-
|Find man-made model and run it with the dataset
|Started
|April 6, 2020
|N/A
|N/A
|-
|Evolve and compare said model with ezCGP
|Not Started
|April 6, 2020
|N/A
|N/A
|}


== April 12, 2020 ==
==== Individual Notes ====
* '''Meeting with subteam: ezCGP'''
** During the subteam meeting the week prior, we were shown how to set up a VM and set up Pace.
*** Although the presentation went without errors, I still encounter several errors that were specific to myself and my computer.
****[[files/PaceSetup.png|thumb|Step by step guide explaining the set up commands to setup PACE vis GitBash]]When setting up a Linux virtual machine to develop on, there are settings that you want to adjust that help with testing/development.
***** You want around 8gb of ram set aside for the VM.
***** around 4 cores of your CPU set aside.
***** 25gb of storage set aside. (That's mostly to hold the OS)
***** Max out your vram that you set aside. I believe I set aside 8gb because I had a dedicated GPU.
**** However, I couldn't set aside more than 1 core for my VM, nor could I actually and install a 64-bit version of Linux.
***** I looked into the issue, and found that the issue was caused by a setting that I need to change in my system BIOS.
****** Going into the BIOS you need to properly enable CPU Virtualization that allows you to actually run a VM efficiently.
******* By enabling that setting, I was successfully able to install a 64-bit Linux version and correctly manage all of my VM settings.
**** Next, I had to set up pace.
***** Setting up PACE wasn't terrible because ezCGP already has a set up documentation on how to set-up ezCGP on PACE
***** Here is the list of commands that allows you to access PACE and install an environment to run ezCGP
***** I had trouble installing the correct packages to my conda environment, so I tried to set them up manually. However, this decision lead to a major mistake later on.

==== Subteam Notes (ezCGP): ====
* None for this section.
** I'll try to elaborate on subteam notes every time there is a VIP meeting with my subteam.
** As a whole, we needed to:
*** Find an image based dataset to train
*** A model that takes an image based dataset as an input.
*** Train this model and record its accuracy after a certain number of epochs
*** Try to put this model through ezCGP to evolve it and potentially out perform the man-made model.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Become more familiar with CGP and how it is used with NNs
|Done
|April 6, 2020
|N/A
|April 9, 2020
|-
|Set up VM and Linux OS for development
|Done
|April 6, 2020
|N/A
|April 7, 2020
|-
|Set up Pace and ezCGP
|Done
|April 6, 2020
|N/A
|April 12, 2020
|-
|Find Dataset to train
|Started
|April 6, 2020
|N/A
|N/A
|-
|Find man-made model
|Not Started
|April 6, 2020
|N/A
|N/A
|-
|Evolve and compare said model with ezCGP
|Not Started
|April 6, 2020
|N/A
|N/A
|}


== April 7, 2020 ==
==== Individual Notes ====
* '''Meeting with subteam: ezCGP'''
** Attended the introductory lecture to CGP and Deep Learning
*** CGP has a few core aspects that make it different from GP:
****GP:
*****Uses generic tree structure to house its primitives
*****Does not allow for access to multiple primitives
****CGP:
*****Uses a DAG to organize its primitives a input data
*****Allows for data to access multiple primitives
*****This will be used to evolve our model [[files/IntroToCNNs.png|thumb|800x800px|This is an example of a CNN, broken down into its core components]]
***CNNs
****Have the typical multi-layered NN component
****as well as another network that takes the original input and convolutes it so that we get everything that is essential to classifying
***Deep Learning and CGP:
****If you think about it, we can categorize the different steps done when running a NN into 3 separate blocks
*****Data augmentation
*****Data pre-processing
*****Model training and validation
****These blocks are what make up an individual in GP
****We will be performing our evolutionary functions on these blocks
**Set-up VM to host a linux OS
***used for development
**Set up PACE
***used for running evolutionary cycles and testing ezCGP
***Each generation is output into separate files

==== Subteam Notes (ezCGP): ====
* First semester students have a task from Jason
* We must find a dataset that is different from Cifar-10
** Can be found on kaggle maybe?
** The dataset needs to be image related for architecture reasons
** Once we find a dataset, we need to find a man-made model that would be used to run this dataset.
** Once we've run the trained the model and recorded results, we need to try to evolve and outperform that model with ezCGP
** Once we've evolved it using ezCGP, we compare the results and see how the evolved version did better than the man-made model.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Become more familiar with CGP and how it is used with NNs
|Midway
|April 6, 2020
|N/A
|N/A
|-
|Set up VM and Linux OS for development
|Done
|April 6, 2020
|N/A
|April 7, 2020
|-
|Set up Pace and ezCGP
|Not Started
|April 6, 2020
|N/A
|N/A
|-
|Find Dataset to train
|Not Started
|April 6, 2020
|N/A
|N/A
|-
|Find man-made model
|Not Started
|April 6, 2020
|N/A
|N/A
|-
|Evolve and compare said model with ezCGP
|Not Started
|April 6, 2020
|N/A
|N/A
|}


== March 31, 2020 ==
==== Individual Notes ====
* '''Meeting with subteam: ezCGP'''
** Attended the introductory lecture to Neural Networks
*** NN have a few core components regarding Deep Learning:
****[[files/NNintroduction.png|thumb|700x700px|A deep learning NN with multiple layers]]They have multiple layers
***** Input layer
****** contains the artifact that you are classifying
******* For our case, it's pixels of an image.
***** Hidden layers
****** use activation functions from layer to layer
***** Output layer or the classifying layer
****** Uses softmax to squeeze everything between 0 and 1.
****** The output is a series of percentages that dictate the likelihood of what the input data is classified as.
**** Weights inbetween layers
***** Allows us to quantifiably distinguish the importance of connections. 
**** Loss function:
***** number of correct guesses/total guesses
**** Back propagation 
***** Using the loss function, we update the weights from back to front using something similar to gradient descent
***** Allows us to update layer importance based on how wrong we were
***** Back prop allows us to improve our NN over time.
**** Separating your Data:
***** You want to roughly separate your data into these proportions:
****** 60/70% training
****** 20/15% testing
****** 20/15% validation
***** Cross Validation
****** Changing/swapping your training, testing, and confirmation data over time
******* Allows for more variety within your data
******* Takes a longer amount of time to run through your NN

==== Subteam Notes (ezCGP): ====
* This is the start of the subteam notes section for my subteam ezCGP
* I know that this team focuses on evolving models used for CV on datasets such as:
** Cifar-10
*** contains 10 image classifications
** Cifar-100
*** contains 100 image classifications

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get familiar with NNs
|Done
|March 30, 2020
|N/A
|April 1, 2020
|-
|Become familiar with the subteam and learn what our goal is.
|Done
|March 30, 2020
|N/A
|April 1, 2020
|}

== February 19, 2020 ==
==== Individual Notes ====
* '''Setting up EMADE'''
** Clone the EMADE repository with git/git lfs
** Need to solve issues with tensorflow package dependencies?
** MySQL is already set up.
** Try to set up everything with pip3.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up emade/install req. packages
|Started
|February 19, 2020
|N/A
|March 7, 2020
|-
|Get entire team set up with EMADE
|Started
|February 19, 2020
|N/A
|March 7, 2020
|}

== February 10, 2020 ==
==== '''Team Notes (Continuation of February 5th entry)''' ====
* '''Multi-Objective Genetic Programming Lab'''
** '''RESULTS'''
*** [[files/BestInd.png|thumb]]When comparing the results from the ML lab to the GA lab results, it turns out that our Multi-Objective Genetic Programming outperformed the ML models.
*** Our best individual for our MOGP gave us approximately an 83 percent accuracy rating on the Titanic predictions.
**** In comparison, the best ML model gave us around 79 percent accuracy rating.
*** The best individual tree can be seen in the figure to the right.
*** Here is a written version: less(norm_div(Sex, norm_sub(0.6666666666666666, 0.0)), norm_sub(norm_reciprocal(Pclass), norm_pow(0.5, 0.0)))
*** The respective fitness for that individual is (0.01348314606741573, 0.15730337078651685)
**** The first entry being percentage of FP's while the second defines the percentage of FN's
**** The area under the curve for the resulting pareto front is: .03248
*** [[files/Pasted image 0 (1).png|thumb]]On the other hand, the best ML model had a FP and FN rating of
**** ≈(.0134, .1573) respectively
**** The best tree had a FP and FN rating of:
**** ≈(.13, .34) respectively
**** Here is the ML model pareto front (below the 
**** In conclusion, we can say that the best individual created by our multi-objective genetic algorithm out performed the best classifier by a relatively wide margin.
**** In addition, we can also argue that we can perform better predictions with a simpler tree 
***** The best individual is a tree with a depth of 3.
**** Because this tree is so simple to evaluate, the required processing power to evaluate the tree is much smaller than the required processing power to perform the predictions with the ML models.

* As a whole, this experience was enlightening as I did not think that we could out perform the ML models on such a considerable scale.
** The difference in accuracy was about 4 percent, which is considerable.
[[files/Images;ld.png|thumb]]

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create Presentation
|Done
|February 5, 2020
|N/A
|February 9, 2020
|-
|Compare ML lab and GA research results
|Done
|February 5, 2020
|N/A
|Febraury 9, 2020
|}

== February 5, 2020 ==

==== '''Individual Notes:''' ====
* '''Multi-Objective Genetic Programming Lab:'''
**Multi-Objective programming is genetic programming an evolutionary loop, where you are minimizing or maximizing more than one objective
**To give an example, our two objectives in our lab are False Positives and False Negatives
***In our evolutionary loop, we are trying to minimize these two objectives with every generation
**[[files/Min-max feature scaling.png|thumb|250x250px]]For this lab, I had to help design how to evaluate the fitness of every tree/individual
**After running the dataset through the tree, the tree produces a prediction for every individual in the dataset.
**The implementation is in the team notes below.
**I also helped design the normalization function that we used to evaluate the data in a meaningful way.
**Instead of using a min-max feature scaling that makes our data prey to outliers and odd normalization's
**We used something similar to a standard score normalization where we use the mean and standard deviation.

* [[files/Standardscore.png|thumb|250x250px]]
** This allows us to account for outliers more effectively.

==== '''Team Project Notes:''' ====
* '''Multi-Objective Genetic Programming Lab:'''
** For this lab, we were supposed to create predictions using the data that we used in the last lab, but instead of using machine learning models to predict survival results of the titanic, we use multi-objective genetic programming to simulate the same results
** To do this task, we would first need to decide how to represent the primitives and terminals in our trees.
** After meeting and discussing among ourselves, we decided that the following would be our primitives that we would use to construct our trees.
***[[files/Imagesadfkaslk;f.png|thumb|600x600px]]Primitives:
**** sigmoid function, which takes one parameter
**** addition (normalized), which takes two parameter
**** subtraction (normalized), which takes two parameter
**** multiplication (normalized), which takes two parameter
**** division (normalized), which takes two parameter
**** power (normalized), which takes two parameter
**** reciprocal function (normalized), which takes one parameter
**** if/else conditionals, which takes three parameters
*** Terminal nodes: will be the features or columns of data within train_data
** Since we already cleaned our data in the machine learning lab, our objectives after deciding and creating our primitives were:
*** Create a way to evaluate the tree per individual or row in the dataset
**** The evaluation function should result in the result of 1 or 0 denoting if the individual lived or died.
*** Per row in the dataset, evaluate the tree
***[[files/EvalFPFN.png|thumb]]Create a way to evaluate the fitness of that individual (the tree in this case) that's based on the amount of FP's and FN's in the predicted dataset.
**'''Tree/Individual Evaluation'''
*** For the tree evaluation, this is the approach that we took:
**** We first created two lists:
***** One that would contain the predicted results of the people who survived or died.
****** These results would be rounded to ensure we get a number that is 1 or 0. 
***** One containing the actual results of the people who survived or died. 
**** After initializing those lists, we ran the individual or tree through each row of data in the training set, appending the results to the predicted list. 
**** After producing the predicted list, we created a new list by subtracting the each element in the predicted list from each element using the actual list.
***** Looping through each value in this list, we calculated and counted the amount of truths, false positives, and false negatives.
******[[files/FitnessEval.png|thumb]]If the value in the list was 1, it was a FP 
****** If the value in the list was -1, it was a FN 
****** If the value in the list was 0, it was a correct prediction.[[files/Pasted image 0.png|thumb]]
*** Fitness evaluation:
**** Multi-objective: minimizing the amount of FP's and FN's 
**** Since this lab is about using multi-objective genetic programming to predict the results of the Titanic dataset, we tried to minimize the amount of FP and FN's over time 
** '''Evolutionary Loop:'''
*** This minimization of FP's and FN's occurred through 200 generations in an evolutionary loop. 
*** The mating probability was 0.5 while the mutation probability was 0.1 
==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Decide on the primitives that we're going to use in the lab
|Done
|February 5, 2020
|N/A
|February 2, 2020
|-
|Create our fitness evaluation for our evolutionary loop
|Done
|February 5, 2020
|N/A
|Febraury 4, 2020
|-
|Compare the individuals using a pareto front
|Done
|February 5, 2020
|N/A
|Febraury 4, 2020
|-
|Find pareto dominant front for our trees
|Done
|February 5, 2020
|N/A
|February 9, 2020
|}

== January 28, 2020 ==

==== '''Individual Notes:''' ====
* '''Machine Learning Lab:'''
** Our goal is to take a set of data, clean it up, run a model on this data that essentially allows us to predict an outcome based on the previous training data 
** Specifically, the dataset that we're using is the Titanic passenger data.
*** For this lab, we are predicting whether or not a passenger survives based on key features about the passenger.
**** These features can be defining characteristics about the passenger such as: Age, Class (like first, economy, etc), Fare, Sex, Number of siblings, spouse/single, etc. 
*** For this lab, we will be choosing the features that we believe have influence over a passengers survival rate, cleaning the data of blank/NA values, and choosing a model to use to predict the passenger's survival. 
*** At the end, we will create a pareto front to describe the performance of each model and to explain how each model performs in predicting a passengers survival. 
*** We will be using a concrete test dataset to gauge a model's accuracy. 
*** For this lab, I sampled 4 different models including:[[files/Pfront.png|thumb|400x400px]]
**** A random tree classifier  
**** A ridge classifier 
**** A gaussian process classifier  
**** A passive aggressive classifier 
*** The results as to how each model perform are as follows:
**** The ridge classifier and random tree classifier performed the best with around 11% FP's and 25% FN's. 
**** The passive aggressive classifier performed at a mid-tier level with around 13% FP's and 34% FN's 
**** The gaussian process classifier performed the absolute worst with around 19% FP's and over 52% FN's 
**** The other three models (SVM, Decision Tree, and Naive Bayes) are included for comparison purposes. 
==== '''Team Notes:''' ====
* Machine learning lab
** For our plan as a team, we decided to first figure out which features were important and potentially had a correlation to a passenger's survival or death.
*** After our meeting we determined that the important features were:
**** Class + Fare (combined)
**** Sex
**** Age
**** #Number of Parents + Siblings (Which combines/is derived from two existing features)
** As a team, we were to clean the dataset that included our desired features
** And then, we were supposed to use different models to that use the training data set to predict a passenger's survival.
** After using different models to predict the passenger's survival outcome individually, we plot them on a pareto front to establish which model worked best to accurately predict a passengers survival outcome.
==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Clean the data set according to our preferred features
|Done
|January 31, 2020
|N/A
|February 2, 2020
|-
|Use a few different models on the training data
|Done
|January 31, 2020
|N/A
|Febraury 4, 2020
|-
|Compare the performance of each using a pareto front
|Done
|January 31, 2020
|N/A
|Febraury 4, 2020
|-
|Compare performances with the rest of the team
|Started
|January 31, 2020
|N/A
|
|}

== January 15, 2020 ==

==== '''Individual Notes:''' ====
* '''Genetic Programming as a concept:'''
** At a high level, instead of taking the individual through an evaluator to obtain objective scores. You use the individual as an function itself. 
** For example, you have input data. You put that data through an individual to get your output data. 
** This individual function can be represented as a tree. 
** Branch roots/Nodes are called primitives and represent functions. The  
** The leaves (input)  are called terminals and act as parameters to the root function. 
** The input can be thought of as a particular type of terminal. 
** The output is produced at the root of the tree. The entire tree is ran from the bottom up. 
** Converting the tree into an array is similar to how depth first search runs. 
** While defining the function mathematically (f(x) = _____) from the tree is from the bottom up. 

* '''Lab Notes:'''
** '''Multi-Objective GP'''
*** After visualization our objective space and pareto front, the area under the curve or AUC measures how well our population did with the multiple objectives.
*** The smaller the AUC, the better.
*** An control individual is used when measuring the pareto dominance of a individual. 
**** We're able to map out which individuals get dominated by the control and which individual's dominate the control.
*** Pareto Dominance is used to sort a population by comparing to a given (random?) individual.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Go through the lecture 2 notes, revisit GP
|Done
|January 15, 2020
|N/A
|January 19, 2020
|-
|Visit and look over the DEAP documentation
|Done
|January 15, 2020
|N/A
|January 21, 2020
|-
|Go through and complete the more detailed lab.
|Done
|January 15, 2020
|N/A
|January, 20, 2020
|}


== January 8, 2020 ==

==== '''Individual Notes:''' ====
* '''GA's as a concept:''' , 
* Initialize population
* Evaluate the objectives of each individual
* Choose a selection process like Fitness Proportionate (probabilistic) or Tournament style (True random)
* Perform crossovers/matings and mutations to the chromosomes of the selected individuals.
* Repeat the process until the best individual's objective is "good enough".
* Perhaps our application of for processing of cancerous cells would be a good application.

==== '''Key words of GA's:''' ====
* Individual: a single candidate in the population with properties such as DNA or a chromosome that can be altered.
* Population: group of individuals whose properties will be altered in the GA.
* Objective: a value used to characterize the current state of an individual of which you are trying to maximize or minimize.
** Usually, the goal is to increase an individual's objective through the GA/evolutionary algorithm.
* Fitness: a relative comparison of an individual's objective to other individuals' objectives
** You ask: How well does the individual accomplish a task relative to the rest of the population?
*** This gives you a fitness eval.
* Evaluation: a function that computes the objective of an individual.
* Selection: a selection simulates a ‘survival of the fittest’/natural selection scenario
** It gives preference to better, stronger individuals allowing them to pass on their genes.
** '''Types of selection:'''
*** Fitness Proportionate (probabilistic): the greater the fitness value of an individual, the higher the probability of being selected for mating
*** Tournament (true random): randomly select individuals for the tournament. Fittest individual wins and are selected for mating.
* Mate/Crossover: Represents mating between individuals to create children with new chromosomes made up of the parents' genes.
* Mutate: introduces random modifications to individual's chromosomes; 
** Introduces diversity and helps avoid local maxima.
'''PYTHON 3 notes:'''
* zip(iter1, iter2) returns an iterable object that has the parameters' indexes paired up into tuples. So: { (iter1[0], iter2[0]), (iter1[1], iter2[1]) }  (Needs to be converted to a list or set)
* map(function, iterable) performs the parameter "function" to every given index in the iterable. This returns an iterable object. (Needs to be converted to a list or set)
* "del" keyword can be used to delete objects. In Python, everything is an object. Therefore, the "del" keyword can also be used to delete variables, lists, and even parts of a list.

==== '''Action Items:''' ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Familiarize myself with the VIP wiki.
|Completed
|January 8, 2020
|N/A
|January 9, 2020
|-
|Go over lecture slides detailing GA's conceptually
|Completed 
|January 8, 2020
|N/A
|January 9, 2020
|-
|Go through the explanatory lab.
|Completed
|January 8, 2020
|N/A
|January 11, 2020
|-
|Go through and complete the more detailed lab.
|Completed
|January 8, 2020
|N/A
|January 12, 2020
|-
|Complete this page.
|Completed
|January 8, 2020
|N/A
|January 15, 2020
|}