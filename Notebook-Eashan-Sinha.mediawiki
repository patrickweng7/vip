= General Information =

'''Team Member''': Eashan Sinha

'''Email''': esinha6@gatech.edu

'''Cell Phone''': (404) 637-7910

'''Interests''': Artificial Intelligence/Machine Learning, Data Science, Basketball, Trying new Foods, Percussion

= Fall 2021 (Bootcamp) =

== Week 1 (8/25 - 8/31) ==

=== Lecture Notes (8/25) ===
'''Genetic Algorithm'''

At any given time, there are multiple solutions to a problem
* '''Individual''' is a specific candidate in the population
* '''Population''' is a group of individuals
* '''Objective''' is a way to characterize individuals I'm trying to maximize or minimize
* '''Fitness''' is relative comparison to other individuals
* '''Evaluation''' spits out the DNA (computes the objective of the individual)
* '''Selection''' = 'survival of the fittest'
** fitness proportionate = greater fitness = greater probability of being selected for mating
** tournament = I can pick any number of individuals and whoever has the higher fitness moves to the next round and winners are selected for mating
** You're not directly tied to your fitness but your fitness will still affect the probability of you being selected
** But you can combine the two above (which is how it's set up in emade)
* '''Mate/Crossover''' is when we take two individuals (or more) and exchange DNA between them (mating)
* '''Mutation''' is when we make a small change to an individual -- purpose is to maintain diversity
** We make small changes to a child (after mating)

'''Algorithm'''

# Randomly initialize a population
# Evaluate the population to get the objective (determine fitness)
# Repeat‚Ä¶
#* Select parents
#* Crossover on parents creating population
#* Mutation of population
#* Determine fitness of population
:: ... until best individual is good enough

=== Lab 1 - Genetic Algorithms with DEAP ===

==== ''Lab Setup'' ====
* Saved 'Lab 1 - Genetic Algorithms with DEAP' as .ipynb file (''Lab 1 - Genetic Algorithms with DEAP.ipynb'')
* Downloaded and installed Anaconda for MacOS
* Launched '''JupyterLab''' using Anaconda Navigator
* Imported Lab .ipynb file into JupyterLab
* Opened a new Terminal window and installed '''deap''' using <code>$ pip install deap</code>

==== ''Lab Notes'' ====
'''OneMax Problem'''
* Simple genetic algorithm problem-- '''Objective''': find a bit string containing all 1s with a set length
* Using DEAP Python Library to solve
* '''Deap Overview''': https://deap.readthedocs.io/en/master/overview.html
* we use DEAP's Toolbox to define functions available to our GA
** Attribute Generator: "attr_bool"
*** Randomly generates a boolean represented by either 0 or 1
** Structure Initializer: "individual"
*** Generates an individual and initializes each with a list of 100 booleans --> bit string length of 100
** Structure Initializer: "population"
*** Defines population-- list of individuals
* Defined evaluation function for fitness objective
** Returns sum of Boolean integers of an individual; more 1s = higher fitness score (max score = 100)
** Sum is returned as a tuple to match previously defined fitness objective
* Defined algorithm's genetic operators
** '''Four Functions''':
*** "evaluate": evaluation function previously defined
*** "mate": two-point crossover function
*** "mutate": flipping a bit w/ independent probability of flipping each bit = 0.05
*** "select": tournament selection of 3 individuals
* Defined <code>main</code> '''genetic algorithm'''
** Started off by initializing population (n) to 300
** Evaluated population by mapping evaluation function to population. Then we assigned each individual their respective fitness value.
* '''Evolution'''
** Began Evolutionary Process
*** Created evolutionary loop to set algorithm to run for 40 generations
** Added '''selection''': used tournament selection on population and made a list of an exact copy of the selected individuals --> ensures offspring are a separate instance
** Performed '''crossover''' and '''mutation''' in the offspring
*** Mate two individuals w/ 50% probability and mutate an individual with 20% probability. <code>delete</code> statements invalidate the fitness of mated/mutated offspring
** Re-evaluate modified offspring & Replace old pop. with offspring
** Define and print stats for population
*** Gathered all fitnesses in one list and print stats

'''N Queens'''
* Started by importing necessary modules
* Then we defined the individual classes, the fitness objective, and functions using DEAP's toolbox
* Ran our evolutionary loop for 100 generations
* After 100 generations, our global minimum of 0 was still not reached

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|August 25, 2021
|September 1, 2021
|August 30, 2019
|-
|Join Team Slack
|Completed
|August 25, 2021
|September 1, 2021
|August 25, 2021
|-
|Complete "Lab 1 - Genetic Algorithms with DEAP" in JupyterLab
|Complete
|August 25, 2021
|September 1, 2021
|September 1, 2021
|}

== Week 2 (9/1 - 9/7) ==

=== Lecture Notes (9/1) ===
'''Genetic Programming'''

* Last week, we focused on Genetic Algorithms and how we want to arrive at a evolved population-based solution
* Instead of utilizing a function '''evaluator''' on an individual to obtain objective scores, we want to run the [input] data through the individual and use the individual as the function itself
** Then we want the evaluator to be run on the Output Data
* We represent Genetic Programming as a '''Tree Structure'''
** The '''Nodes''' of the tree are called ''primitives'' and represent functions
** The '''Leaves''' are called ''terminals'' and represent parameters
*** The input can be thought of as a terminal
*** The output is produced at the root of the tree
** If our input was '''x''', then our output would be '''f(x)'''

'''How is the Tree Stored?'''

* The tree is converted  into a '''lisp preordered parse tree'''
** The operator would be followed by inputs
* The tree for f(X) = 3 * 4 + 1 can be written as: [+,*,3,4,1]
* We do a depth-first traversal in this pre-ordered parse tree

'''Crossover in GP'''
* Basically exchanging subtrees
* Start by randomly picking a point in each tree
* These points and everything below create subtrees
* The subtrees are exchanged to produce children
* We take what's left of parent 1 and what's left of parent 2 and swap and create 2 child algorithms out of that

'''Mutation in GP'''
* Mutation can involve‚Ä¶
** Inserting a node or subtree
** Deleting a node or subtree
** Changing a node
* When we delete a node or subtree, we can perform a shrink operation to fill up that tree
* Any change we make locally to a tree, that is called a '''mutation'''
* Example: Symbolic Regression
** Using simple primitives, use genetic programming to evolve a solution to y = sin(x)
** Primitives include: +, *, -, /
** Terminals include ''integers'' and X
* We solve this using a Calc 1 Concept of '''Taylor Series'''
** Taylor Series for sin(x)

'''Evaluating a Tree'''
* We can feed a number of input points into the function to get outputs
** ùëã=[0..2ùúã]
* We can measure error between outputs and truth
* Other Primitives could make this a lot easier
** Power()
** Factorial()
** Sin()
** Cos()
** Tan()
** Summation()
** Pi

=== Lab 2 - Genetic Programming and Multi-Objective Optimization ===

==== ''Lab Setup'' ====
* Saved 'Lab 2 - Genetic Programming and Multi-Objective Optimization' as .ipynb file (''Lab 2 - Genetic Programming and Multi-Objective Optimization.ipynb'')
* Downloaded and installed Anaconda for MacOS
* Launched '''JupyterLab''' using Anaconda Navigator
* Imported Lab .ipynb file into JupyterLab

==== ''Lab Notes'' ====
'''Symbolic Regression'''

* Lab focuses on '''genetic programming'''
* Imported necessary libraries for GP
* Created fitness and individual classes
* We'll be representing our individuals in a tree structure
* Initialized a primitive set and added all the primitives our trees can use
* Added two primitives:
<pre>pset.addPrimitive(np.divide, arity=2)
pset.addPrimitive(np.mod, arity=2)</pre>
* Defined evaluation function <code>evalSymbReg</code>
* Registered our genetic operators using <code>toolbox.register()</code>
* Notes for '''Lab 2''': https://drive.google.com/file/d/1lHZMtajC7a2EHqinNs6WXW1Khj-_/view?usp=sharing

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|September 1, 2021
|September 8, 2021
|September 5, 2021
|-
|Review Lecture Notes
|Completed
|September 1, 2021
|September 8, 2021
|September 5, 2021
|-
|Complete '''Part 1''' of "Lab 2 - Genetic Programming and Multi-Objective Optimization" in JupyterLab
|In Progress
|September 1, 2021
|September 8, 2021
|September 8, 2021
|}

== Week 3 (9/8 - 9/14) ==

=== Lecture Notes (9/1) ===
* We first rated ourselves out of five on our '''Python''' and our '''Machine Learning''' knowledge and familiarity.
* I gave myself a 4 for Python and a 2 for Machine Learning

'''Multiple Objectives -- The MO in MOGA and MOGP'''

* Conducted class survey of what we look for in a mate:
** Personality
** Intelligence
** Humility
* Everyone has different objectives and preferences for what objectives they would want their partners to have
* However, an algorithm particularly looks for '''fitness''' in its mates
* We will focus on the translation of the vector of scores from evaluation into a fitness value
* Gene pool is the set of genome to be evaluated during the current generation
** Genome
*** Genotypic description of an individuals
*** DNA
*** GA = set of values
*** GP = tree structure, string
** Search Space
*** Set of all possible genome
*** For Automated Algorithm Design
*** Set of all possible algorithms
*** How big is the search space?
*** Why is this important for algorithm design?
* The Evaluation of a Genome associates a genome/individual (set of parameters for GA or string for GP) with a set of scores
** TP = True Positive
** FP = False Positive
* '''Objectives''' = Set of measurements each genome (or individual)  is scored against
* Objective Space ‚Äì Set of objectives 
* Evaluation ‚Äì Maps an genome/individual
** From a location in search space to a location in objective space

'''Classification Measures'''

* '''Confusion Matrix'''
** 2 x 2 table --> predicted positives & predicted negatives x actual positives & actual negatives
* We‚Äôre going to stay in a '''binary world''' in this lecture
* '''Binary Classification''': It can only be 1 or 0, one thing or the other, positive or negative, as shown above
* '''Multi Classification''': It can be in a set of things (the desired object can be one of many things)
* Apple example = The goal is to retrieve all apples but the algorithm looks at only red objects so we get the above confusion matrix
'''True Positive Rate (Sensitivity / Recall)''': The number of true positives / number of positives = '''Hit Rate'''
'''True Negative Rate (Specificity)''': True negatives/actual negatives
* The stepped line is called the '''Pareto Frontier'''
* '''Minimization Measures'''
** We want to push that '''Pareto Frontier''' down to the origin as much as possible
** Apple Example: We can check to see if our rows are correct by summing TP + FN = AP and FP + TN = AN
* '''Other Measures''':
** Precision or Positive Predictive Value
** False Discovery Rate
** Negative Predictive Value
** Accuracy

'''Objective Space'''

* We don‚Äôt have to be limited to accuracy based on algorithm purposes
* There are a lot of other things that can tell us the quality of our algorithm
* Each individual is evaluated using objective functions
** MSE
** Cost
** Complexity
** TPR
** FPR

'''Pareto Optimality'''

* An individual is '''Pareto optimal''' if there is no other individual in the population that outperforms the individual on '''ALL''' objectives
* The set of all Pareto individuals is known as the Pareto frontier
* These individuals represent unique contributions
* We want to drive selection by favoring Pareto individuals
** But maintain diversity by giving all individuals some probability of mating
* '''Pareto Optimal''' solutions are usually ones that we‚Äôre trying to reach but nondominating points are those that we have already reached

'''Nondominated Sorting Genetic Algorithm II (NSGA II)'''

* Population is separated into nondomination ranks
* Individuals are selected using a binary tournament
* Lower Pareto ranks beat higher Pareto ranks
* Ties on the same front are broken by crowding distance 
** We have four total ranks here
** The best rank is 0
* Now we want to randomly select individuals based on a binary tournament in which two points are compared to each other.
* Tiebreakers are through '''crowding distance'''.
* A more normalized distribution with each point being relatively equally distanced from each other is more desirable than a front with points that are more distanced from each other
* A measure of how ‚Äòalone‚Äô a point is in a distribution shows whether it has a high crowding distance‚Äì if it is more alone, and far away from other points on its front, then it has a higher crowding distance and we choose that point as the winner of the tiebreaker because we want to explore the areas near it and see if we can achieve a better point from there

'''Strength Pareto Evolutionary Algorithm 2 (SPEA2)'''
* Each individual is given a strength S
** S is how many others in the population it dominates
* Each individual receives a rank R
** R is the sum of S‚Äôs of the individuals that dominate it
** Pareto individuals are nondominated and receive an R of 0
* A distance to the kth nearest neighbor (ùõîk) is calculated and a fitness of R + 1/(ùõîk + 2) is obtained
* We want to favor things that have a further distance. We want them to have a lesser fitness

=== Lab 2 - Genetic Programming and Multi-Objective Optimization ===

==== ''Lab Setup'' ====
* Lab already set up from previous part of lab

==== ''Lab Notes'' ====
'''Multi Objective Genetic Programming'''

* Lab focuses on '''Multi Objective Genetic Programming'''
* Imported necessary libraries for GP
* Notes for Part 2: https://drive.google.com/file/d/1UKU52KrPMSAjsz1OMSHfHXQ82QfSiR7s/view?usp=sharing

==== Self Grading ====
https://drive.google.com/file/d/1eqabI0rtKpuu59RfS-RfCVAUg5WPc67E/view?usp=sharing

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|September 8, 2021
|September 15, 2021
|September 14, 2021
|-
|Review Lecture Notes
|Completed
|September 8, 2021
|September 15, 2021
|September 15, 2021
|-
|Complete '''Self Grading Rubric'''
|Completed
|September 8, 2021
|September 15, 2021
|September 15, 2021
|-
|Complete '''Part 2''' of "Lab 2 - Genetic Programming and Multi-Objective Optimization" in JupyterLab
|In Progress
|September 8, 2021
|September 15, 2021
|September 15, 2021
|}

== Week 4 (9/15 - 9/21) ==

=== Lecture Notes (9/15) ===

==== Intro to Machine Learning ====
* Went over '''SciKit-Learn''' and its documentation
* Looked at Kaggle for examples of ML done on Titanic Dataset to predict survivors
* Assigned to subteams:
* My teammates:
** Pranav Pusarla
** Jessi Chen
** Leul Wubete
** Elan Grossman
* Introduced to '''scikit-learn''': https://scikit-learn.org/stable/index.html
* We will be using skcikit-learn to import many Machine Learning algorithms/models that we can use to analyze our Titanic Disaster and more accurately predict survival.
* We went through some of the source code in the Jupyter Notebook for the Titanic Project
* We were tasked with doing the preprocessing of the data 

=== Group Project -- Titanic- Machine Learning from Disaster ===

==== ''Project Setup'' ====
* Project setup same way as previous labs

==== ''Sub Team Notes (9/19)'' ====
* Met with all team members at the '''Inspire Atlanta''' Apartments lobby
* We all first started going through the provided python notebook
* Then we started going over the current preprocessing methods that existed in the notebook
* We then decided to one-hot encode the 'Embarked' and 'Sex' columns in order to represent each category by 1s and 0s
** Using <code>pd.get_dummies(...)</code>
* Furthermore, we removed the 'Name', 'Ticket', and 'Cabin' columns using <code>test_data.drop(...)</code>
* After preprocessing our data, we each chose an algorithm (ML) to get a Pareto Optimal Front in which each of our algorithms were codominant
* '''My Contribution'''
** Utilized the '''MLPClassifier''', which uses an Multi-Layer Perceptron Algorithm that trains using Backpropogation
** Tweaked the classifier to obtain optimal results
** Obtained results that outperformed some of the other algorithms by team members
** Worked to improve other algorithms to continue to obtain a pareto optimal front with all algorithms being utilized

==== Algorithm Results ====
{| class="wikitable"
!Name
!Algorithm
!False Positives
!False Negatives
|-
|Elan
|Random Forest Classifier
|25
|29
|-
|Eashan
|Neural Network
|21
|31
|-
|Pranav
|Gaussian Process Classifier
|19
|35
|-
|Jessi
|Gradient Descent
|7
|79
|-
|Leul
|Support Vector Classifier
|5
|93
|}

==== Our Pareto Front ====
https://drive.google.com/file/d/1lLVaUdBj782mNr_BzCkZT0j36cDt1RX6/view?usp=sharing

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|September 15, 2021
|September 22, 2021
|September 20, 2021
|-
|Review Lecture Notes
|Completed
|September 15, 2021
|September 22, 2021
|September 22, 2021
|-
|Team Meeting 1
|Completed
|September 15, 2021
|September 22, 2021
|September 19, 2021
|-
|Complete Data Processing and Pareto Front using Titanic Dataset
|Completed
|September 15, 2021
|September 22, 2021
|September 19, 2021
|}

== Week 5 (9/22 - 9/28) ==

=== Lecture Notes (9/22) ===

* Went over action items for Presentation next week (9/29)

'''Presentation Guidelines'''

* Make sure '''title slide''' has clear title, contributors, and date of presentation
* Make sure '''Graphs''' have:
** Clear title
** Axis labels
** Readable Font size
** Pareto Front lines go in the right direction for min v. max problems
* We were shown an example of a pareto-optimal calculation
* Shown example of AUC Calculation
* Shown example of graphing
* Must use Page Numbers
* Technical presentation must be stand-alone and include text
** Having image-based presentation isn't really necessary
* Text is important
* '''Include a Take-Away point to summarize what we want our audience to take away from the slides'''

=== Group Project -- Titanic- Machine Learning from Disaster ===

==== ''Project Setup'' ====
* Project setup same way as previous labs

==== ''Sub Team Notes (9/24)'' ====

* We met multiple times to complete the '''Titanic: Machine Learning from Disaster''' project
* We were tasked with developing the genetic algorithm using DEAP
* We reused much of the code from the prior week, especially in preprocessing the data
* We then decided to one-hot encode the 'Embarked' and 'Sex' columns in order to represent each category by 1s and 0s
** Using <code>pd.get_dummies(...)</code>
* Furthermore, we removed the 'Name', 'Ticket', and 'Cabin' columns using <code>test_data.drop(...)</code>
* We also replaced the missing 'Age' and 'Fare' values, which were null, with their mean.
* We then replaced the missing 'Embarked' values with the mode of 'Embarked'.
* We were then eventually able to successfully construct an evolutionary algorithm using our custom selection tournament method (in which we selected the dominant individual as the survivor), node replacement mutation method, and 1-point cross-over mating function
* We were able to use hyperparameter optimization, which helped lower our algorithm's AUC compared to the ML algorithm's AUC
* '''My Contribution'''
* My main contribution was to help optimize our data preparation.
** We ended up further replacing null values in 'Age' and 'Fare' with their respective means
** We also replaced the null 'Embarked' with its mode
* I also presented the slide on data preprocessing during our presentation
* Here's a link to our sub-team project page: 
** https://github.gatech.edu/emade/emade/wiki/Bootcamp-Subteam-1
* Here is our Genetic Algorithm Pareto Optimal Front with the respective AUC in comparison to the ML front:
https://drive.google.com/file/d/1lHZMtajC7a2EHqiAshQnNs6WXW1Khj-_/view?usp=sharing

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|September 22, 2021
|September 29, 2021
|September 26, 2021
|-
|Review Lecture Notes
|Completed
|September 22, 2021
|September 29, 2021
|September 28, 2021
|-
|Team Meeting 1
|Completed
|September 22, 2021
|September 29, 2021
|September 24, 2021
|-
|Team Meeting 2
|Completed
|September 22, 2021
|September 29, 2021
|September 25, 2021
|-
|Team Meeting 3
|Completed
|September 22, 2021
|September 29, 2021
|September 27, 2021
|-
|Complete '''Predicting Titanic Survivors''' Assignment and Presentation
|In Progress
|September 22, 2021
|September 29, 2021
|September 28, 2021
|}

== Week 6 (9/29 - 10/05) ==

=== Lecture Notes (9/29) ===

* Viewed Titanic Machine Learning from Disaster presentations from all groups
* Make sure to submit peer evals by the end of next week.
* Some teams used NSGA-II as their classification algorithm, but it doesn't work out of the box
* There were some very interesting presentations and 

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|September 29, 2021
|October 6, 2021
|October 5, 2021
|-
|Review Lecture Notes
|Completed
|September 29, 2021
|October 6, 2021
|October 5, 2021
|-
|Finish up Lab Notebook
|In Progress
|September 29, 2021
|October 6, 2021
|October 5, 2021
|}

== Week 7 (10/06 - 10/12) ==

=== Lecture Notes (10/06) ===

* Discussed Emade and action items for the main project due in three weeks.
* Learned about Emade and how it relates to what we've been doing thus far.

'''Introduction to Emade'''

We‚Äôll have a multi-week project regarding EMADE
* Monday, 10/25 will be presentations for the full titanic dataset project comparing ML to MOGP to EMADE

Things we should include in our notebook due next week:
* evidence of personal contributions
* lecture/meeting notes
* team contributions
* individual contributions
** the data prepossessing slides

'''What is EMADE?'''

Now we‚Äôre moving into automated machine learning, not just ML
* primitives are now gonna be the ML functions and hyperparameters that will go with that
* Our inputs are no longer the datasets
* The nodes are going to be the ML algorithms
* Evaluations are very expensive w mL algorithms

EMADE is the Evolutionary Multi-objective Algorithm Design Engine.
* It combines a multi-objective evolutionary search with high-level primitives to automate the process of designing machine learning algorithms.

What we have to do/have done:

* Configure a mysql server on your machine.
* Download and install git-lfs.
* Cloned the emade repository.
* Run the setup module to install the package.
** Jason copied a conda environment file into this reference repo if we want to try that: 
<pre>$ conda env create ‚Äìf emade.yml</pre>

Running EMADE

* To start a run of EMADE we will navigate to the top level directory and run
<code>python src/GPFramework/launchGTMOEP.py templates/input_titanic.xml</code>
* '''input_titanic.xml''' is our input file
* What is it though?
** The input file is an xml document that configures all the moving parts in EMADE, we will step through it now.
*** XML is just HTML but tags can basically be anything.
** The first block is for configuring Python.
** EMADE automatically detects cluster management software for gridengine and SLURM. For our purposes, you only need to worry about the local python command. This should be the command to run python where all of EMADE python dependencies are installed. 
*** If we have our virtual environment activated, or if they are installed at the base installation, this can just be python.

Input File: Database Configuration
* The next block is for configuring a MySQL connection.
* Username and password are for MySQL, the user should have full permissions on the database specified.
** We will have to make the server, user, and database outside of EMADE, through MySQL.

Input File: Datasets
* EMADE can run across multiple datasets, the titanic example uses only one.
* The data is preprocessed into gzipped csv files. (Example on next slide).
* It is cross folded 5 times.
** This will create 5 Monte Carlo trials that algorithms can be scored with.
* Each train and test file create a DataPair object in EMADE.
* These files are prepared with the titanic_data_splitter.py in datasets/titanic/
* Each row corresponds to an instance (person), each column is a feature, the final column is the truth data.
** EMADE reserves the last column for fitting models (train data) and scoring (test data).
* This uses a vectorizer and splits the training dataset
* nump.loadtext can load in the gvim file
* In EMADE, the truth data is the final column of the dataset

Input File: Objectives
* Next block is for objectives
* Columns of database are going to be the names
* Weight specifies if it should be minimized (-1.0) or maximized (1.0)
* The <evaluationFunction> specifies the name of a method in src/GPFramework/evalFunctions.py
* Achievable and goal are used for steering the optimization, lower and upper are used for bounding.

Input File: Some More Parameters
* Next block is for parameters
* Evaluation specifies where evaluation functions specified in the objectives section live, and how much memory each worker is allowed to use before marking an individual as ‚Äúfatal‚Äù
* If any worker holds more than 30% memory, then it kills that worker to prevent the computer from crashing.
* Default workers is 5, but we should drop that down to 2 to see what our computer can handle
* <workersPerHost> specifies how many evaluations to run in parallel.
** EMADE is resource intensive, keep this number low on a laptop! (2-3).

Input File: Evolution Parameters
* Next block is for the evolution parameters
* Evolution parameters essentially control the various 'magic constants' or hyperparameters that affect the evolutionary process.
* What is Headless Chicken?

Connecting a Worker Process to a Peer
* We use the <code>‚Äìw</code> flag along with your peer‚Äôs server info in the dbconfig in order to allow your computer to act as a worker process for your peer‚Äôs master process.
* the <code>‚Äìw</code> says don‚Äôt run the master process‚Äì only run the evaluations
** Only runs is MySQL connection works
 $ python src/GPFramework/launchGTMOEP.py templates/input_titanic.xml -w
* Make sure that our dbconfig in the input file specifies their IP address and not localhost.

Understanding EMADE output
* The best place to look at the outputs of EMADE are in the MySQL databases.
* Connect to a MySQL server from command line:
 $ mysql -h hostname -u username -p
** We should be prompted with a password
* We can then select a database using: <code>use database;</code> in MySQL
* Some helpful queries:
 Select * from individuals;
 Select * from individuals where `FullDataSet False Negatives` is not null;
 Select * from individuals join paretofront on individuals.hash=paretofront.hash where paretofront.generation=(select max(generation) from paretofront);
* Individuals will complete after giving EMADE some time
* Whoever is running the SQL database
** the third SQL query (bolded) says every

EMADE Structure
* We‚Äôll have to make our own database/tables
* '''src/GPFramework''' is the main body of code
* '''gtMOEP.py''' is the main EMADE engine, most of the evolutionary loop is in here, including the evaluation method
* '''gp_framework_helper.py''' is where the primitive set is built for EMADE, this function points to where the primitives live, such as:
** methods.py
** signal_methods.py
** spatial_methods.py
* '''data.py''' provisions the DataPair object that is passed from primitive to primitive.
* datasets/ is where some test datasets live.
* templates/ is where the input files live.

OUR ASSIGNMENT:
* Run EMADE as a group. '''1 person should have the sql server set up and act as the master process''', the rest should connect their workers.
** Two or more people can be connecting the workers.
* Run for a substantial number of generations

=== Individual Notes ===

* Made sure I had MySQL and Git ready and installed
* Everything was working with MySQL and Git
* Refreshed my memory with MySQL by looking over some queries written in my CS4400 (Database Systems) class

Guide to getting set up on Emade:
https://github.gatech.edu/emade/emade#emade

'''Installing Emade'''

1. Installed Git LFS using <code>$ git lfs install</code>

2. Ran git config 
 $ --global credential.helper cache

3. Cloned the Emade repository locally using
 $ git clone https://github.gatech.edu/emade/emade

That is where I left off because Emade took a while to clone

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|October 6, 2021
|October 12, 2021
|October 8, 2021
|-
|Review Lecture Notes
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Download/Install Emade
|Complete
|October 6, 2021
|October 12, 2021
|October 10, 2021
|-
|Get Emade Set Up Locally
|Complete
|October 6, 2021
|October 12, 2021
|October 12, 2021
|}

== Week 8 (10/13 - 10/19) ==

=== Lecture Notes (10/13) ===

* Discussed Emade and action items for the main project due in three weeks
* Learned about Emade and how it relates to what we've been doing thus far.

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|October 6, 2021
|October 12, 2021
|October 8, 2021
|-
|Review Lecture Notes
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Download/Install Emade
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Finish up Lab Notebook
|Complete
|October 6, 2021
|October 8, 2021
|October 8, 2021
|}

== Week 9 (10/20 - 10/25) ==

=== Lecture Notes (10/20) ===

* Discussed Emade and action items for the main project due in three weeks
* Learned about Emade and how it relates to what we've been doing thus far.

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|October 6, 2021
|October 12, 2021
|October 8, 2021
|-
|Review Lecture Notes
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Download/Install Emade
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Finish up Lab Notebook
|Complete
|October 6, 2021
|October 8, 2021
|October 8, 2021
|}

== Week 10 (10/25 - 10/31) ==

=== Lecture Notes (10/25) ===

* Final Presentation Day

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|October 6, 2021
|October 12, 2021
|October 8, 2021
|-
|Review Lecture Notes
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Download/Install Emade
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Finish up Lab Notebook
|Complete
|October 6, 2021
|October 8, 2021
|October 8, 2021
|}

== Week 11 (11/01 - 11/07) ==

=== Lecture Notes (11/01) ===

* Final Presentation Day

=== Subteam Notes (11/01) ===

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|October 6, 2021
|October 12, 2021
|October 8, 2021
|-
|Review Lecture Notes
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Download/Install Emade
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Finish up Lab Notebook
|Complete
|October 6, 2021
|October 8, 2021
|October 8, 2021
|}

== Week 12 (11/08 - 11/14) ==

=== Lecture Notes (11/08) ===

* Final Presentation Day

=== Subteam Notes (11/10) ===

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|October 6, 2021
|October 12, 2021
|October 8, 2021
|-
|Review Lecture Notes
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Download/Install Emade
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Finish up Lab Notebook
|Complete
|October 6, 2021
|October 8, 2021
|October 8, 2021
|}

== Week 13 (11/15 - 11/21) ==

=== Lecture Notes (11/15) ===

* Final Presentation Day

=== Subteam Notes (11/17) ===

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|October 6, 2021
|October 12, 2021
|October 8, 2021
|-
|Review Lecture Notes
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Download/Install Emade
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Finish up Lab Notebook
|Complete
|October 6, 2021
|October 8, 2021
|October 8, 2021
|}

== Week 13 (11/22 - 11/28) ==

=== Lecture Notes (11/22) ===

* Final Presentation Day

=== Subteam Notes (N/A) ===

* There was no subteam meeting this week due to Thanksgiving Break

=== Individual Notes ===


=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|October 6, 2021
|October 12, 2021
|October 8, 2021
|-
|Review Lecture Notes
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Download/Install Emade
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Finish up Lab Notebook
|Complete
|October 6, 2021
|October 8, 2021
|October 8, 2021
|}

== Week 14 (11/29 - 12/05) ==

=== Lecture Notes (11/22) ===

* Weekly scrum meeting

=== Subteam Notes (12/01) ===

* 

=== Individual Notes ===


=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|October 6, 2021
|October 12, 2021
|October 8, 2021
|-
|Review Lecture Notes
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Download/Install Emade
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Finish up Lab Notebook
|Complete
|October 6, 2021
|October 8, 2021
|October 8, 2021
|}

== Week 15 (12/06 - 12/10) ==

=== Lecture Notes (12/06) ===

* Weekly scrum meeting

=== Subteam Notes (12/08) ===

* 

=== Individual Notes ===


=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|October 6, 2021
|October 12, 2021
|October 8, 2021
|-
|Review Lecture Notes
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Download/Install Emade
|In Progress
|October 6, 2021
|October 12, 2021
|
|-
|Finish up Lab Notebook
|Complete
|October 6, 2021
|October 8, 2021
|October 8, 2021
|}