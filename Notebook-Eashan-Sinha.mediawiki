= General Information =

'''Team Member''': Eashan Sinha

'''Email''': esinha6@gatech.edu

'''Cell Phone''': (404) 637-7910

'''Interests''': Artificial Intelligence/Machine Learning, Data Science, Basketball, Trying new Foods, Percussion

= Fall 2021 (Bootcamp) =

== Week 1 (8/25 - 8/31) ==

=== Lecture Notes (8/25) ===
'''Genetic Algorithm'''

At any given time, there are multiple solutions to a problem
* '''Individual''' is a specific candidate in the population
* '''Population''' is a group of individuals
* '''Objective''' is a way to characterize individuals I'm trying to maximize or minimize
* '''Fitness''' is relative comparison to other individuals
* '''Evaluation''' spits out the DNA (computes the objective of the individual)
* '''Selection''' = 'survival of the fittest'
** fitness proportionate = greater fitness = greater probability of being selected for mating
** tournament = I can pick any number of individuals and whoever has the higher fitness moves to the next round and winners are selected for mating
** You're not directly tied to your fitness but your fitness will still affect the probability of you being selected
** But you can combine the two above (which is how it's set up in emade)
* '''Mate/Crossover''' is when we take two individuals (or more) and exchange DNA between them (mating)
* '''Mutation''' is when we make a small change to an individual -- purpose is to maintain diversity
** We make small changes to a child (after mating)

'''Algorithm'''

# Randomly initialize a population
# Evaluate the population to get the objective (determine fitness)
# Repeat‚Ä¶
#* Select parents
#* Crossover on parents creating population
#* Mutation of population
#* Determine fitness of population
:: ... until best individual is good enough

=== Lab 1 - Genetic Algorithms with DEAP ===

==== ''Lab Setup'' ====
* Saved 'Lab 1 - Genetic Algorithms with DEAP' as .ipynb file (''Lab 1 - Genetic Algorithms with DEAP.ipynb'')
* Downloaded and installed Anaconda for MacOS
* Launched '''JupyterLab''' using Anaconda Navigator
* Imported Lab .ipynb file into JupyterLab
* Opened a new Terminal window and installed '''deap''' using <code>$ pip install deap</code>

==== ''Lab Notes'' ====
'''OneMax Problem'''
* Simple genetic algorithm problem-- '''Objective''': find a bit string containing all 1s with a set length
* Using DEAP Python Library to solve
* '''Deap Overview''': https://deap.readthedocs.io/en/master/overview.html
* we use DEAP's Toolbox to define functions available to our GA
** Attribute Generator: "attr_bool"
*** Randomly generates a boolean represented by either 0 or 1
** Structure Initializer: "individual"
*** Generates an individual and initializes each with a list of 100 booleans --> bit string length of 100
** Structure Initializer: "population"
*** Defines population-- list of individuals
* Defined evaluation function for fitness objective
** Returns sum of Boolean integers of an individual; more 1s = higher fitness score (max score = 100)
** Sum is returned as a tuple to match previously defined fitness objective
* Defined algorithm's genetic operators
** '''Four Functions''':
*** "evaluate": evaluation function previously defined
*** "mate": two-point crossover function
*** "mutate": flipping a bit w/ independent probability of flipping each bit = 0.05
*** "select": tournament selection of 3 individuals
* Defined <code>main</code> '''genetic algorithm'''
** Started off by initializing population (n) to 300
** Evaluated population by mapping evaluation function to population. Then we assigned each individual their respective fitness value.
* '''Evolution'''
** Began Evolutionary Process
*** Created evolutionary loop to set algorithm to run for 40 generations
** Added '''selection''': used tournament selection on population and made a list of an exact copy of the selected individuals --> ensures offspring are a separate instance
** Performed '''crossover''' and '''mutation''' in the offspring
*** Mate two individuals w/ 50% probability and mutate an individual with 20% probability. <code>delete</code> statements invalidate the fitness of mated/mutated offspring
** Re-evaluate modified offspring & Replace old pop. with offspring
** Define and print stats for population
*** Gathered all fitnesses in one list and print stats

'''N Queens'''
* Started by importing necessary modules
* Then we defined the individual classes, the fitness objective, and functions using DEAP's toolbox
* Ran our evolutionary loop for 100 generations
* After 100 generations, our global minimum of 0 was still not reached

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|August 25, 2021
|September 1, 2021
|August 30, 2019
|-
|Join Team Slack
|Completed
|August 25, 2021
|September 1, 2021
|August 25, 2021
|-
|Complete "Lab 1 - Genetic Algorithms with DEAP" in JupyterLab
|Complete
|August 25, 2021
|September 1, 2021
|September 1, 2021
|}

== Week 2 (9/1 - 9/7) ==

=== Lecture Notes (9/1) ===
'''Genetic Programming'''

* Last week, we focused on Genetic Algorithms and how we want to arrive at a evolved population-based solution
* Instead of utilizing a function '''evaluator''' on an individual to obtain objective scores, we want to run the [input] data through the individual and use the individual as the function itself
** Then we want the evaluator to be run on the Output Data
* We represent Genetic Programming as a '''Tree Structure'''
** The '''Nodes''' of the tree are called ''primitives'' and represent functions
** The '''Leaves''' are called ''terminals'' and represent parameters
*** The input can be thought of as a terminal
*** The output is produced at the root of the tree
** If our input was '''x''', then our output would be '''f(x)'''

'''How is the Tree Stored?'''

* The tree is converted  into a '''lisp preordered parse tree'''
** The operator would be followed by inputs
* The tree for f(X) = 3 * 4 + 1 can be written as: [+,*,3,4,1]
* We do a depth-first traversal in this pre-ordered parse tree

'''Crossover in GP'''
* Basically exchanging subtrees
* Start by randomly picking a point in each tree
* These points and everything below create subtrees
* The subtrees are exchanged to produce children
* We take what's left of parent 1 and what's left of parent 2 and swap and create 2 child algorithms out of that

'''Mutation in GP'''
* Mutation can involve‚Ä¶
** Inserting a node or subtree
** Deleting a node or subtree
** Changing a node
* When we delete a node or subtree, we can perform a shrink operation to fill up that tree
* Any change we make locally to a tree, that is called a '''mutation'''
* Example: Symbolic Regression
** Using simple primitives, use genetic programming to evolve a solution to y = sin(x)
** Primitives include: +, *, -, /
** Terminals include ''integers'' and X
* We solve this using a Calc 1 Concept of '''Taylor Series'''
** Taylor Series for sin(x)

'''Evaluating a Tree'''
* We can feed a number of input points into the function to get outputs
** ùëã=[0..2ùúã]
* We can measure error between outputs and truth
* Other Primitives could make this a lot easier
** Power()
** Factorial()
** Sin()
** Cos()
** Tan()
** Summation()
** Pi

=== Lab 2 - Genetic Programming and Multi-Objective Optimization ===

==== ''Lab Setup'' ====
* Saved 'Lab 2 - Genetic Programming and Multi-Objective Optimization' as .ipynb file (''Lab 2 - Genetic Programming and Multi-Objective Optimization.ipynb'')
* Downloaded and installed Anaconda for MacOS
* Launched '''JupyterLab''' using Anaconda Navigator
* Imported Lab .ipynb file into JupyterLab

==== ''Lab Notes'' ====
'''Symbolic Regression'''

* Lab focuses on '''genetic programming'''
* Imported necessary libraries for GP
* Created fitness and individual classes
* We'll be representing our individuals in a tree structure
* Initialized a primitive set and added all the primitives our trees can use
* Added two primitives:
<pre>pset.addPrimitive(np.divide, arity=2)
pset.addPrimitive(np.mod, arity=2)</pre>
* Defined evaluation function <code>evalSymbReg</code>
* Registered our genetic operators using <code>toolbox.register()</code>
* Notes for '''Lab 2''': https://drive.google.com/file/d/1lHZMtajC7a2EHqinNs6WXW1Khj-_/view?usp=sharing

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|September 1, 2021
|September 8, 2021
|September 5, 2021
|-
|Review Lecture Notes
|Completed
|September 1, 2021
|September 8, 2021
|September 5, 2021
|-
|Complete '''Part 1''' of "Lab 2 - Genetic Programming and Multi-Objective Optimization" in JupyterLab
|In Progress
|September 1, 2021
|September 8, 2021
|September 8, 2021
|}

== Week 3 (9/8 - 9/14) ==

=== Lecture Notes (9/1) ===
* We first rated ourselves out of five on our '''Python''' and our '''Machine Learning''' knowledge and familiarity.
* I gave myself a 4 for Python and a 2 for Machine Learning

'''Multiple Objectives -- The MO in MOGA and MOGP'''

* Conducted class survey of what we look for in a mate:
** Personality
** Intelligence
** Humility
* Everyone has different objectives and preferences for what objectives they would want their partners to have
* However, an algorithm particularly looks for '''fitness''' in its mates
* We will focus on the translation of the vector of scores from evaluation into a fitness value
* Gene pool is the set of genome to be evaluated during the current generation
** Genome
*** Genotypic description of an individuals
*** DNA
*** GA = set of values
*** GP = tree structure, string
** Search Space
*** Set of all possible genome
*** For Automated Algorithm Design
*** Set of all possible algorithms
*** How big is the search space?
*** Why is this important for algorithm design?
* The Evaluation of a Genome associates a genome/individual (set of parameters for GA or string for GP) with a set of scores
** TP = True Positive
** FP = False Positive
* '''Objectives''' = Set of measurements each genome (or individual)  is scored against
* Objective Space ‚Äì Set of objectives 
* Evaluation ‚Äì Maps an genome/individual
** From a location in search space to a location in objective space

'''Classification Measures'''

* '''Confusion Matrix'''
** 2 x 2 table --> predicted positives & predicted negatives x actual positives & actual negatives
* We‚Äôre going to stay in a '''binary world''' in this lecture
* '''Binary Classification''': It can only be 1 or 0, one thing or the other, positive or negative, as shown above
* '''Multi Classification''': It can be in a set of things (the desired object can be one of many things)
* Apple example = The goal is to retrieve all apples but the algorithm looks at only red objects so we get the above confusion matrix
'''True Positive Rate (Sensitivity / Recall)''': The number of true positives / number of positives = '''Hit Rate'''
'''True Negative Rate (Specificity)''': True negatives/actual negatives
* The stepped line is called the '''Pareto Frontier'''
* '''Minimization Measures'''
** We want to push that '''Pareto Frontier''' down to the origin as much as possible
** Apple Example: We can check to see if our rows are correct by summing TP + FN = AP and FP + TN = AN
* '''Other Measures''':
** Precision or Positive Predictive Value
** False Discovery Rate
** Negative Predictive Value
** Accuracy

'''Objective Space'''

* We don‚Äôt have to be limited to accuracy based on algorithm purposes
* There are a lot of other things that can tell us the quality of our algorithm
* Each individual is evaluated using objective functions
** MSE
** Cost
** Complexity
** TPR
** FPR

'''Pareto Optimality'''

* An individual is '''Pareto optimal''' if there is no other individual in the population that outperforms the individual on '''ALL''' objectives
* The set of all Pareto individuals is known as the Pareto frontier
* These individuals represent unique contributions
* We want to drive selection by favoring Pareto individuals
** But maintain diversity by giving all individuals some probability of mating
* '''Pareto Optimal''' solutions are usually ones that we‚Äôre trying to reach but nondominating points are those that we have already reached

'''Nondominated Sorting Genetic Algorithm II (NSGA II)'''

* Population is separated into nondomination ranks
* Individuals are selected using a binary tournament
* Lower Pareto ranks beat higher Pareto ranks
* Ties on the same front are broken by crowding distance 
** We have four total ranks here
** The best rank is 0
* Now we want to randomly select individuals based on a binary tournament in which two points are compared to each other.
* Tiebreakers are through '''crowding distance'''.
* A more normalized distribution with each point being relatively equally distanced from each other is more desirable than a front with points that are more distanced from each other
* A measure of how ‚Äòalone‚Äô a point is in a distribution shows whether it has a high crowding distance‚Äì if it is more alone, and far away from other points on its front, then it has a higher crowding distance and we choose that point as the winner of the tiebreaker because we want to explore the areas near it and see if we can achieve a better point from there

'''Strength Pareto Evolutionary Algorithm 2 (SPEA2)'''
* Each individual is given a strength S
** S is how many others in the population it dominates
* Each individual receives a rank R
** R is the sum of S‚Äôs of the individuals that dominate it
** Pareto individuals are nondominated and receive an R of 0
* A distance to the kth nearest neighbor (ùõîk) is calculated and a fitness of R + 1/(ùõîk + 2) is obtained
* We want to favor things that have a further distance. We want them to have a lesser fitness

=== Lab 2 - Genetic Programming and Multi-Objective Optimization ===

==== ''Lab Setup'' ====
* Lab already set up from previous part of lab

==== ''Lab Notes'' ====
'''Multi Objective Genetic Programming'''

* Lab focuses on '''Multi Objective Genetic Programming'''
* Imported necessary libraries for GP
* Notes for Part 2: https://drive.google.com/file/d/1UKU52KrPMSAjsz1OMSHfHXQ82QfSiR7s/view?usp=sharing

==== Self Grading ====
https://drive.google.com/file/d/1eqabI0rtKpuu59RfS-RfCVAUg5WPc67E/view?usp=sharing

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|September 8, 2021
|September 15, 2021
|September 14, 2021
|-
|Review Lecture Notes
|Completed
|September 8, 2021
|September 15, 2021
|September 15, 2021
|-
|Complete '''Self Grading Rubric'''
|Completed
|September 8, 2021
|September 15, 2021
|September 15, 2021
|-
|Complete '''Part 2''' of "Lab 2 - Genetic Programming and Multi-Objective Optimization" in JupyterLab
|In Progress
|September 8, 2021
|September 15, 2021
|September 15, 2021
|}

== Week 4 (9/15 - 9/21) ==

=== Lecture Notes (9/15) ===

==== Intro to Machine Learning ====
* Went over '''SciKit-Learn''' and its documentation
* Looked at Kaggle for examples of ML done on Titanic Dataset to predict survivors
* Assigned to subteams:
* My teammates:
** Pranav Pusarla
** Jessi Chen
** Leul Wubete
** Elan Grossman
* Introduced to '''scikit-learn''': https://scikit-learn.org/stable/index.html
* We will be using skcikit-learn to import many Machine Learning algorithms/models that we can use to analyze our Titanic Disaster and more accurately predict survival.
* We went through some of the source code in the Jupyter Notebook for the Titanic Project
* We were tasked with doing the preprocessing of the data 

=== Group Project -- Titanic- Machine Learning from Disaster ===

==== ''Project Setup'' ====
* Project setup same way as previous labs

==== ''Sub Team Notes (9/19)'' ====
* Met with all team members at the '''Inspire Atlanta''' Apartments lobby
* We all first started going through the provided python notebook
* Then we started going over the current preprocessing methods that existed in the notebook
* We then decided to one-hot encode the 'Embarked' and 'Sex' columns in order to represent each category by 1s and 0s
** Using <code>pd.get_dummies(...)</code>
* Furthermore, we removed the 'Name', 'Ticket', and 'Cabin' columns using <code>test_data.drop(...)</code>
* After preprocessing our data, we each chose an algorithm (ML) to get a Pareto Optimal Front in which each of our algorithms were codominant
* '''My Contribution'''
** Utilized the '''MLPClassifier''', which uses an Multi-Layer Perceptron Algorithm that trains using Backpropogation
** Tweaked the classifier to obtain optimal results
** Obtained results that outperformed some of the other algorithms by team members
** Worked to improve other algorithms to continue to obtain a pareto optimal front with all algorithms being utilized

==== Algorithm Results ====
{| class="wikitable"
!Name
!Algorithm
!False Positives
!False Negatives
|-
|Elan
|Random Forest Classifier
|25
|29
|-
|Eashan
|Neural Network
|21
|31
|-
|Pranav
|Gaussian Process Classifier
|19
|35
|-
|Jessi
|Gradient Descent
|7
|79
|-
|Leul
|Support Vector Classifier
|5
|93
|}

==== Our Pareto Front ====
https://drive.google.com/file/d/1lLVaUdBj782mNr_BzCkZT0j36cDt1RX6/view?usp=sharing

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|September 15, 2021
|September 22, 2021
|September 20, 2021
|-
|Review Lecture Notes
|Completed
|September 15, 2021
|September 22, 2021
|September 22, 2021
|-
|Team Meeting 1
|Completed
|September 15, 2021
|September 22, 2021
|September 19, 2021
|-
|Complete Data Processing and Pareto Front using Titanic Dataset
|Completed
|September 15, 2021
|September 22, 2021
|September 19, 2021
|}

== Week 5 (9/22 - 9/28) ==

=== Lecture Notes (9/22) ===

* Went over action items for Presentation next week (9/29)

'''Presentation Guidelines'''

* Make sure '''title slide''' has clear title, contributors, and date of presentation
* Make sure '''Graphs''' have:
** Clear title
** Axis labels
** Readable Font size
** Pareto Front lines go in the right direction for min v. max problems
* We were shown an example of a pareto-optimal calculation
* Shown example of AUC Calculation
* Shown example of graphing
* Must use Page Numbers
* Technical presentation must be stand-alone and include text
** Having image-based presentation isn't really necessary
* Text is important
* '''Include a Take-Away point to summarize what we want our audience to take away from the slides'''

=== Group Project -- Titanic- Machine Learning from Disaster ===

==== ''Project Setup'' ====
* Project setup same way as previous labs

==== ''Sub Team Notes (9/24)'' ====

* We met multiple times to complete the '''Titanic: Machine Learning from Disaster''' project
* We were tasked with developing the genetic algorithm using DEAP
* We reused much of the code from the prior week, especially in preprocessing the data
* We then decided to one-hot encode the 'Embarked' and 'Sex' columns in order to represent each category by 1s and 0s
** Using <code>pd.get_dummies(...)</code>
* Furthermore, we removed the 'Name', 'Ticket', and 'Cabin' columns using <code>test_data.drop(...)</code>
* We also replaced the missing 'Age' and 'Fare' values, which were null, with their mean.
* We then replaced the missing 'Embarked' values with the mode of 'Embarked'.
* We were then eventually able to successfully construct an evolutionary algorithm using our custom selection tournament method (in which we selected the dominant individual as the survivor), node replacement mutation method, and 1-point cross-over mating function
* We were able to use hyperparameter optimization, which helped lower our algorithm's AUC compared to the ML algorithm's AUC
* '''My Contribution'''
* My main contribution was to help optimize our data preparation.
** We ended up further replacing null values in 'Age' and 'Fare' with their respective means
** We also replaced the null 'Embarked' with its mode
* I also presented the slide on data preprocessing during our presentation
* Here's a link to our sub-team project page: 
** https://github.gatech.edu/emade/emade/wiki/Bootcamp-Subteam-1
* Here is our Genetic Algorithm Pareto Optimal Front with the respective AUC in comparison to the ML front:
https://drive.google.com/file/d/1lHZMtajC7a2EHqiAshQnNs6WXW1Khj-_/view?usp=sharing

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|September 22, 2021
|September 29, 2021
|September 26, 2021
|-
|Review Lecture Notes
|Completed
|September 22, 2021
|September 29, 2021
|September 28, 2021
|-
|Team Meeting 1
|Completed
|September 22, 2021
|September 29, 2021
|September 24, 2021
|-
|Team Meeting 2
|Completed
|September 22, 2021
|September 29, 2021
|September 25, 2021
|-
|Team Meeting 3
|Completed
|September 22, 2021
|September 29, 2021
|September 27, 2021
|-
|Complete '''Predicting Titanic Survivors''' Assignment and Presentation
|In Progress
|September 22, 2021
|September 29, 2021
|September 28, 2021
|}

== Week 6 (9/29 - 10/05) ==

=== Lecture Notes (9/29) ===

* Viewed Titanic Machine Learning from Disaster presentations from all groups
* Make sure to submit peer evals by the end of next week.
* Some teams used NSGA-II as their classification algorithm, but it doesn't work out of the box
* There were some very interesting presentations and 

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|September 29, 2021
|October 6, 2021
|October 5, 2021
|-
|Review Lecture Notes
|Completed
|September 29, 2021
|October 6, 2021
|October 5, 2021
|-
|Finish up Lab Notebook
|In Progress
|September 29, 2021
|October 6, 2021
|October 5, 2021
|}

== Week 7 (10/06 - 10/12) ==

=== Lecture Notes (10/06) ===

* Discussed Emade and action items for the main project due in three weeks.
* Learned about Emade and how it relates to what we've been doing thus far.

'''Introduction to Emade'''

We‚Äôll have a multi-week project regarding EMADE
* Monday, 10/25 will be presentations for the full titanic dataset project comparing ML to MOGP to EMADE

Things we should include in our notebook due next week:
* evidence of personal contributions
* lecture/meeting notes
* team contributions
* individual contributions
** the data prepossessing slides

'''What is EMADE?'''

Now we‚Äôre moving into automated machine learning, not just ML
* primitives are now gonna be the ML functions and hyperparameters that will go with that
* Our inputs are no longer the datasets
* The nodes are going to be the ML algorithms
* Evaluations are very expensive w mL algorithms

EMADE is the Evolutionary Multi-objective Algorithm Design Engine.
* It combines a multi-objective evolutionary search with high-level primitives to automate the process of designing machine learning algorithms.

What we have to do/have done:

* Configure a mysql server on your machine.
* Download and install git-lfs.
* Cloned the emade repository.
* Run the setup module to install the package.
** Jason copied a conda environment file into this reference repo if we want to try that: 
<pre>$ conda env create ‚Äìf emade.yml</pre>

Running EMADE

* To start a run of EMADE we will navigate to the top level directory and run
<code>python src/GPFramework/launchGTMOEP.py templates/input_titanic.xml</code>
* '''input_titanic.xml''' is our input file
* What is it though?
** The input file is an xml document that configures all the moving parts in EMADE, we will step through it now.
*** XML is just HTML but tags can basically be anything.
** The first block is for configuring Python.
** EMADE automatically detects cluster management software for gridengine and SLURM. For our purposes, you only need to worry about the local python command. This should be the command to run python where all of EMADE python dependencies are installed. 
*** If we have our virtual environment activated, or if they are installed at the base installation, this can just be python.

Input File: Database Configuration
* The next block is for configuring a MySQL connection.
* Username and password are for MySQL, the user should have full permissions on the database specified.
** We will have to make the server, user, and database outside of EMADE, through MySQL.

Input File: Datasets
* EMADE can run across multiple datasets, the titanic example uses only one.
* The data is preprocessed into gzipped csv files. (Example on next slide).
* It is cross folded 5 times.
** This will create 5 Monte Carlo trials that algorithms can be scored with.
* Each train and test file create a DataPair object in EMADE.
* These files are prepared with the titanic_data_splitter.py in datasets/titanic/
* Each row corresponds to an instance (person), each column is a feature, the final column is the truth data.
** EMADE reserves the last column for fitting models (train data) and scoring (test data).
* This uses a vectorizer and splits the training dataset
* nump.loadtext can load in the gvim file
* In EMADE, the truth data is the final column of the dataset

Input File: Objectives
* Next block is for objectives
* Columns of database are going to be the names
* Weight specifies if it should be minimized (-1.0) or maximized (1.0)
* The <evaluationFunction> specifies the name of a method in src/GPFramework/evalFunctions.py
* Achievable and goal are used for steering the optimization, lower and upper are used for bounding.

Input File: Some More Parameters
* Next block is for parameters
* Evaluation specifies where evaluation functions specified in the objectives section live, and how much memory each worker is allowed to use before marking an individual as ‚Äúfatal‚Äù
* If any worker holds more than 30% memory, then it kills that worker to prevent the computer from crashing.
* Default workers is 5, but we should drop that down to 2 to see what our computer can handle
* <workersPerHost> specifies how many evaluations to run in parallel.
** EMADE is resource intensive, keep this number low on a laptop! (2-3).

Input File: Evolution Parameters
* Next block is for the evolution parameters
* Evolution parameters essentially control the various 'magic constants' or hyperparameters that affect the evolutionary process.
* What is Headless Chicken?

Connecting a Worker Process to a Peer
* We use the <code>‚Äìw</code> flag along with your peer‚Äôs server info in the dbconfig in order to allow your computer to act as a worker process for your peer‚Äôs master process.
* the <code>‚Äìw</code> says don‚Äôt run the master process‚Äì only run the evaluations
** Only runs is MySQL connection works
 $ python src/GPFramework/launchGTMOEP.py templates/input_titanic.xml -w
* Make sure that our dbconfig in the input file specifies their IP address and not localhost.

Understanding EMADE output
* The best place to look at the outputs of EMADE are in the MySQL databases.
* Connect to a MySQL server from command line:
 $ mysql -h hostname -u username -p
** We should be prompted with a password
* We can then select a database using: <code>use database;</code> in MySQL
* Some helpful queries:
 Select * from individuals;
 Select * from individuals where `FullDataSet False Negatives` is not null;
 Select * from individuals join paretofront on individuals.hash=paretofront.hash where paretofront.generation=(select max(generation) from paretofront);
* Individuals will complete after giving EMADE some time
* Whoever is running the SQL database
** the third SQL query (bolded) says every

EMADE Structure
* We‚Äôll have to make our own database/tables
* '''src/GPFramework''' is the main body of code
* '''gtMOEP.py''' is the main EMADE engine, most of the evolutionary loop is in here, including the evaluation method
* '''gp_framework_helper.py''' is where the primitive set is built for EMADE, this function points to where the primitives live, such as:
** methods.py
** signal_methods.py
** spatial_methods.py
* '''data.py''' provisions the DataPair object that is passed from primitive to primitive.
* datasets/ is where some test datasets live.
* templates/ is where the input files live.

OUR ASSIGNMENT:
* Run EMADE as a group. '''1 person should have the sql server set up and act as the master process''', the rest should connect their workers.
** Two or more people can be connecting the workers.
* Run for a substantial number of generations

=== Individual Notes ===

* Made sure I had MySQL and Git ready and installed
* Everything was working with MySQL and Git
* Refreshed my memory with MySQL by looking over some queries written in my CS4400 (Database Systems) class

Guide to getting set up on Emade:
https://github.gatech.edu/emade/emade#emade

'''Installing Emade'''

1. Installed Git LFS using <code>$ git lfs install</code>

2. Ran git config 
 $ --global credential.helper cache

3. Cloned the Emade repository locally using
 $ git clone https://github.gatech.edu/emade/emade

That is where I left off because Emade took a while to clone

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|October 6, 2021
|October 12, 2021
|October 8, 2021
|-
|Review Lecture Notes
|In Progress
|October 6, 2021
|October 12, 2021
|October 10, 2021
|-
|Download/Install Emade
|Complete
|October 6, 2021
|October 12, 2021
|October 10, 2021
|-
|Get Emade Set Up Locally
|Complete
|October 6, 2021
|October 12, 2021
|October 12, 2021
|}

== Week 8 (10/13 - 10/19) ==

=== Lecture Notes (10/13) ===

* There was no lecture today
* It was just a work session with our subteams
* We worked on getting Emade installed and running
* We also worked on getting a host for the MySQL server
* Elan set up the server and port-forwarded it to us

=== Individual Notes ===
'''Continued Setup of EMADE'''

* Ran <code>conda install opencv</code> in my local EMADE folder
* Ran the following to install the necessary packages:
 $ conda install numpy pandas tensorflow keras scipy psutil lxml matplotlib PyWavelets sqlalchemy networkx cython scikit-image mysqlclient pymysql scikit-learn
 $ pip install xgboost lmfit multiprocess hmmlearn deap opencv-python
* Logged in to Elan's server using:
 $ mysql -uguest -pemademade -h71.204.44.212 -P6603
* Elan was the master as the server was hosted on his computer; however, he often experienced crashes due to the size of EMADE
* Connection was successful although I ran into some issues, which Elan and I will figure out together
* May eventually need to try connecting using:
 $ mysql -h hostname -u username -D database -p
* Configuring <code>input_titanic.xml</code> file:
```
 <dbConfig>
        <server>127.0.0.1:6603</server>
        <username>guest</username>
        <password>emademade</password>
        <database>titanic</database>
        <reuse>1</reuse>
 </dbConfig>
```
* We also worked on figuring out how to query out '''Pareto Front''':
 select pf.generation, age, `FullDataSet False Positives`, `FullDataSet False Negatives`
 from individuals ind join paretofront pf on pf.hash = ind.hash
 where `FullDataSet False Negatives` is not NULL;

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Notes into Notebook
|Complete
|October 13, 2021
|October 19, 2021
|October 18, 2021
|-
|Get Emade running and log into Elan's MySQL server
|Complete
|October 6, 2021
|October 21, 2021
|October 19, 2021
|-
|Lay Out Titanic Presentation Slides
|Complete
|October 13, 2021
|October 22, 2021
|October 19, 2021
|}

== Week 9 (10/20 - 10/25) ==

=== Lecture Notes (10/20) ===

* There was no lecture today
* Today was mainly used as a day to really get EMADE running and some trial runs in for some experiments on the titanic dataset
* We wanted to make sure everyone was able to run EMADE as a worker (aside from Elan) so that we could get some results to present next Monday
* Many of the Automated Algorithm Design alumni helped me out with my troubles
* Randomly, when I tried running <code>$ bash reinstall.sh</code>, I was facing some errors, so the alumni helped me resolve
* By the end of the working session, our subteam created a task list for the upcoming week before the presentation:
** Ensure everyone is able to connect to the database
** Finalize the data preprocessing code
** Have everyone be able to process some worker (or master) runs on EMADE
** Modify XML file to achieve varying results
** Generate new Pareto Front
** Finish Presentation Slides
** Rehearse Presentation

=== Subteam Meeting 1 (10/22) ===
* At this point, most of us are able to connect to the master through SQL
* We are also able to run EMADE correctly for the most part
* To process a run:
 $ python src/GPFramework/launchGTMOEP.py -w templates/input_titanic.xml
* One problem we faced was that EMADE kept returning inf values
** So we edited the data preprocessing files and finalized them
** '''testPreprocessed.csv.gz''' and '''trainPreprocessed.csv.gz'''
* We made sure all the default files worked with '''input_titanic.xml''' and with the titanic database

=== Subteam Meeting 2 (10/24) ===
* We worked on ensuring we achieved a low AUC with our updated XML
* After changing the mate and mutate functions, we achieved an AUC (Area under curve) of 0.131, which was lower than what we were reaching in our previous dataset
* We achieved this after 115 generations
* Here is a picture of our Pareto front compared to the default algorithm:

[[files/esinha6/W9_Group1_vs_Default_ParetoFront|height=100px]]

* We achieved the following AUC's for from EMADE, MOGP, and Machine Learning:
** Machine Learning AUC: 0.3201224
** MOGP AUC: 0.1798859
** EMADE AUC: 0.1311417
* '''Our EMADE algorithm beat MOGP and Machine Learning!!!'''
* We then spent time finishing the slides and rehearsing our presentation.
* Here is a link to our presentation: https://docs.google.com/presentation/d/1YIiVWW3tQe2RZSRNIsomlLb2mUbaaj1bZN-rHt9KRb0/edit?usp=sharing

=== Individual Notes and Contributions ===
* I was having a little bit of trouble getting EMADE running as some packages actually didn't install properly
* The error was a <code>Module not found</code> error

[[files/esinha6/W9_Package_Conflicts|height=100px]]

* I tried creating a conda environment for Python 3.7 and reinstalled and it worked!
* I realized that messing with my global environment will often cause many issues
* Worked on finalizing the '''testPreprocessed.csv.gz''' and '''trainPreprocessed.csv.gz''' files
* A problem we faced was that we would achieve a:
```
 ValueError: could not convert string to float: 'Embarked_S'
```
* We removed the last column, because we actually wanted the "Survived" column to be the last, and then it ended up working
* During the second subteam meeting, I made a lot of changes to our XML file
* I duplicated '''input_titanic.xml''' to '''emade_titanic.xml'''
** I changed the database in emade_titanic.xml to be emade_titanic, because that's the database we are going to use
```
 <dbConfig>
        <server>71.204.44.212:6603</server>
        <username>guest</username>
        <password>emademade</password>
        <database>emade_titanic</database>
        <reuse>1</reuse>
 </dbConfig>
 <datasets>
        <dataset>
            <name>FullDataSet</name>
            <type>featuredata</type>
            <MonteCarlo>
                <trial>
                    <trainFilename>datasets/emade_titanic/trainPreprocessed.csv.gz</trainFilename>
                    <testFilename>datasets/emade_titanic/testPreprocessed.csv.gz</testFilename>
                </trial>
            </MonteCarlo>
        </dataset>
 </datasets>
```
* I later then duplicated '''emade_titanic.xml''' to '''super_titanic.xml'''
** Changed database to super_titanic
** We worked on changing some of the mating and mutation functions by increasing and decreasing the probabilities of some
** We wanted to experiment to see whether certain functions' probabilities would yield better results
* I researched some of the crossover methods, especially Headless chicken to see maybe what changing some of the parameters may actually do.
* I looked at a paper on EMADE written by Dr. Zutty, which gave so much information on crossovers and how they work.
** Essentially, "Headless chicken crossover is a single-point crossover with one parent being a randomly generated tree. Headless chicken ephemeral crossover is our ephemeral-only crossover operating with one parent being a randomly generated tree." (Zutty)

[[files/esinha6/W9_Ephemeral_crossover|height=100px]]

** I also looked into some of the different mutation methods EMADE leverages, including insertion, ephemeral, node replacement, uniform, and shrink.
** Some more about the headless chicken experiment: The success of crossover was tested by comparing the traditional crossover with random crossover (crossing the individual over with random string) ‚Äì There is no crossover without the idea of crossover, do not call headless chicken a chicken, although it has many chicken features.
* We finally decided on making some changes to some of the mate/mutation functions used in our '''super_titanic.xml''' file:
** Doubled Headless Chicken from 0.10 to 0.20
** Changed crossover to 0.40
** We set all the other mutation methods to 0.025 after increasing our Headless chicken probability

Mate/mutation changes in '''super_titanic.xml''':
```
 <matings>
            <mating>
                <name>crossover</name>
                <probability>0.40</probability>
            </mating>
            <mating>
                <name>crossoverEphemeral</name>
                <probability>0.40</probability>
            </mating>
            <mating>
                <name>headlessChicken</name>
                <probability>0.20</probability>
            </mating>
            <mating>
                <name>headlessChickenEphemeral</name>
                <probability>0.20</probability>
            </mating>
        </matings>
‚Äã
        <mutations>
            <mutation>
                <name>insert</name>
                <probability>0.025</probability>
            </mutation>
            <mutation>
                <name>insert modify</name>
                <probability>0.05</probability>
            </mutation>
            <mutation>
                <name>ephemeral</name>
                <probability>0.25</probability>
            </mutation>
            <mutation>
                <name>node replace</name>
                <probability>0.025</probability>
            </mutation>
            <mutation>
                <name>uniform</name>
                <probability>0.025</probability>
            </mutation>
            <mutation>
                <name>shrink</name>
                <probability>0.025</probability>
            </mutation>
 </mutations>
```
* Another one of my main contributions was setting up the presentation slide deck
* I wrote out my slides for the data preprocessing slides and had them be looked over by the members in my group
* I added some additional reasoning for preprocessing the data the way we did
** One improvement I can make is simply finding a way to use the names of the passengers as useful data to analyze
* I rehearsed my part of the presentation multiple times to ensure I didn't exceed my 1.5 - 2 minute time slot.
** I wanted to ensure the other members of my team had enough time to speak on their topics adequately

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Notes into Notebook
|Completed
|October 20, 2021
|October 25, 2021
|October 25, 2021
|-
|Finalize Data Preprocessing
|Complete
|October 20, 2021
|October 23, 2021
|October 23, 2021
|-
|Generate New Pareto Front
|Complete
|October 20, 2021
|October 25, 2021
|October 24, 2021
|-
|Finalize Slides
|Complete
|October 20, 2021
|October 25, 2021
|October 24, 2021
|}

== Week 10 (10/25 - 10/31) ==

=== Lecture Notes (10/25) ===

==== FINAL PRESENTATION DAY ====
* Link to notes on presentations: https://docs.google.com/presentation/d/1YIiZSRNIsomlLb2mUaj1bZN-rHt9KRb0/edit?usp=sharing
* I had to leave at 6:30 for my CS2110 Lab Section
* We were given very strong feedback on our presentation and we learned there were numerous things we could have done to improve
** One thing I could have done was potentially extracted useful data out of the "Ticket" and "Cabin" columns, as we had just dropped those columns with the assumption that there could be noi meaningful information gained from analysis of that data
* We were given the task to submit our team preferences prior to the following Monday, November 1, 2021

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Presentation Notes into Notebook
|Completed
|October 25, 2021
|November 1, 2021
|October 30, 2021
|-
|Rank Subteams Based on Preference on Canvas
|Complete
|October 25, 2021
|November 1, 2021
|November 1, 2021
|}

== Week 11 (11/01 - 11/07) ==

=== Lecture Notes (11/01) ===

* Introduction to new teams
* We were assigned to our new subteams.
** I was assigned to Image Processing
* Dr. Zutty led the weekly scrum and all subteams gave their weekly updates

'''About Image Processing'''
* I learned that the goal of IP is to help improve EMADE's image processing algorithms
* Goal is to improve the way emade handles image processing in datasets
** We will use the chest x-ray dataset
* images having disease or not having a disease
* the dataset had some images with multiple images
** so that every image is only connected to just one type of disease
* splitting subteams into 4 different changes:
** change selection methods (nsga-iii)
*** understanding how lexicase work
*** fully implement it
*** research new selection methods
**** Can we get better performing individuals w lxicase over nsga-ii
** mating and mutation methods (GP)
*** Creating new mating and mutation methods
*** reading literature
** hyper-features
*** find features that work well together
*** look at literature, our own individuals
*** how to break primitives up
*** we have to implement and test primitives
*** Teaches us a lot about how emade works
** Data preparation (environment infrastructure)
*** Running EMADE
*** Squashing bugs
*** A lot of bug stuff
*** Shows us how primitives operate
*** data preprocessing
** Data + NN Evaluation

To Do:
* get PACE-ICEset up and do an emade run on it
** There's a PACE guide on the Wiki and on the internet
** Clone Aryaan's EMADE fork locally --> branch is 'Image-Processing(nn-vip);
** Run EMADE on PACE-ICE
*** Environment on PACE-ICE
*** Set up a SQL database
** Read CheXNet Papers
** Read Previous VIP Papers

=== Subteam Notes (11/03) ===
* Heidi is working on the hyperfeatures
* She's working on the pre-processing
* New data will be multi-class, not multilabel

WHAT WE WANT TO DO
* We want to improve Emade's Image Processing Ability

=== Individual Notes ===
* I submitted my first preference to be for the team '''Image Processing''' and that was the team that I got as my subteam
* I wasn't able to get PACE working

[[files/esinha6/W11_PACE_Issues|height=100px]]

* We realized that PACE was actually under maintenance and that we had to wait till Saturday (11/06) for PACE to be back up again
* I read most of the CheXNet Paper "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning"
* Researched about PACE and how I can use it to run EMADE

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|November 1, 2021
|November 7, 2021
|November 6, 2021
|-
|Set Up PACE
|In Progress
|November 1, 2021
|November 7, 2021
|
|}

== Week 12 (11/08 - 11/14) ==

=== Lecture Notes (11/08) ===

* Weekly scrum to kick off the meeting
* Our team Image Processing kicked off the scrum meeting
* We mentioned our main objectives for the remaining of the semester

=== Subteam Notes (11/10) ===
* Today was more of a workday
* Went over a little bit about how EMADE and the Image Processing Subgroup works
* Spent most of the time trying to set up PACE but still wasn't able to fully get it set up
* We don't use the master branch
** We use CacheV2
*** There is no Deep Learning (Neural Networks) in CacheV2 right now
** There is a branch with subteams
** Then is another branch called nn-vip (Neural Architecture Search)
*** We use a lot of NAS's work to do what we do
** Within nn-vip, there is Image-Processing(nn-vip) sub-branch
*** There are separate branches from here for the subteams within our Image Processing team
** Anything with '_____methods.py', those are
* EMADE.py is the main script which does all the GP things
* launchEMADE.py will launch jobs on the PACE cluster, if we're on PACE.
** It does other launch things as well
*** Breakdown of our branch: Master -> CacheV2 -> sub teams -> nn-vip -> Image-Processing(nn-vip)
* To run EMADE, we have to seed our database with NN Learners, so that it evolves the individuals so that those individuals can mate and mutate
** There is a seeding file: seeding_from_file.py [xml file] seedfile
** Shared directory: /storage/home/hpaceice1/shared-classes/materials/vip/AAD/ip_group
* datasets:
** train
** test
** val <-- train on here

=== Individual Notes ===
* I spent most of the time still trying to figure out how to set up PACE
* I also spent a significant amount of time looking at some of the various crossover methods for mate/mutate
* I wanted to choose a method that yielded more targeted results to see if our AUC would decrease
* I started off by looking at Crossover Simulated Binary, which modifies in-place the input individuals

 defcxSimulatedBinary(ind1, ind2, eta):
    """Executes a simulated binary crossover that modify in-place the input
    individuals. The simulated binary crossover expects :term:`sequence`
    individuals of floating point numbers.
    :param ind1: The first individual participating in the crossover.
    :param ind2: The second individual participating in the crossover.
    :param eta: Crowding degree of the crossover. A high eta will produce
                children resembling to their parents, while a small eta will
                produce solutions much more different.
    :returns: A tuple of two individuals.
    This function uses the :func:`~random.random` function from the python base
    :mod:`random` module.
    """
    for i, (x1, x2) in enumerate(zip(ind1, ind2)):
        rand=random.random()
        if rand<=0.5:
            beta=2.*rand
        else:
            beta=1./(2.*(1.-rand))
        beta**=1./(eta+1.)
        ind1[i] =0.5*(((1+beta) *x1) +((1-beta) *x2))
        ind2[i] =0.5*(((1-beta) *x1) +((1+beta) *x2))
    return ind1, ind2

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|November 8, 2021
|November 15, 2021
|November 15, 2021
|-
|Set Up PACE
|In Progress
|November 8, 2021
|November 15, 2021
|
|-
|Find and read about ONE Mate/Mutate Methods
|Complete
|November 8, 2021
|November 15, 2021
|November 15, 2021
|-
|Experiment with new mate/mutate methods
|In Progress
|November 8, 2021
|November 15, 2021
|
|}

== Week 13 (11/15 - 11/21) ==

=== Lecture Notes (11/15) ===

* Started off with weekly scrum discussing how each team is planning to move forward to a code freeze for the final presentation
* Most new students were still trying to solve issues with PACE
* The rest of the day was a work day
* Aryaan and I spent the majority of the time looking over the mate/mutate functions and what things I could potentially experiment with

=== Subteam Notes (11/17) ===

* I spent the majority of the meeting trying to understand why I was not able to ssh into my node
* When I would run: <code>mysql -u esinha6 -p</code>, I would prompted for my password and I would put it in but I would always achieve the same error:
 ERROR 2002 (HY000): Can't connect to local MySQL server through socket 'scratch/db/mysqldb.sock'
* This was the issue that I had been trying to fix but wasn't able to
* I made sure I was ssh'ing into the correct node but I would always achieve the same error
* Rohan, Elan, and Maxim helped me a lot but we plan to figure everything out hopefully by next week 

=== Individual Notes ===

* I looked over a lot of the various mate/mutate functions with Aryaan and saw what he has already implemented versus what I can potentially implement to run tests on
* He showed me the XML that he was still working on to configure
* I was still struggling to get PACE working as I was following the instructions on the guide but still wasn't able to fully set it up
* One improvement in the future that I can make is to maybe edit that new PACE setup guide so that it is updated and really allows new students, especially in Image Processing to be able to set up PACE fast and get straight into work
* Although I was able to log into PACE and clone the right repo onto PACE, I still wasn't able to log into the node that I would allocate
* I wanted to log into that node so that I could then run EMADE and perform some experiments

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|November 15, 2021
|November 22, 2021
|November 22, 2021
|-
|Set Up PACE
|In Progress
|November 8, 2021
|November 22, 2021
|
|-
|Experiment with new mate/mutate methods
|In Progress
|November 8, 2021
|November 22, 2021
|
|}

== Week 14 (11/22 - 11/28) ==

=== Lecture Notes (11/22) ===

* Started off with weekly scrum
* Image Processing Sub team kicked off the scrum
* Used today as a workday to finalize an idea for the presentation on 12/10
* Rohan and I worked on solving our PACE issues until about 7:30PM today
* We were still unable to connect to the node so we were working through it with Max for a while

=== Subteam Notes (N/A) ===

* There was no subteam meeting this week due to Thanksgiving Break!!

=== Individual Notes ===
* I was still facing the same error with PACE
* I wasn't able to ssh into the node as I would keep getting the following error:
 ERROR 2002 (HY000): Can't connect to local MySQL server through socket 'scratch/db/mysqldb.sock' (111)
* We thought there might have been something wrong with my port number colliding with someone else's who was also using PACE.
* Here is my .my.cnf file with the PORT number listed
```
 [mysqld]
 datadir=scratch/db
 socket=mysqldb.sock
 user=esinha6
 symbolic-links=0
 # uncomment the following line if you use a port and change the port number to the one you use
 port=3391

 [mysqld_safe]
 log-error=mysqldb.log
 pid-file=mysqldb.pid
 [mysql]
 socket=scratch/db/mysqldb.sock
```
* There didn't seem to be much wrong
* I still wasn't receiving permission to SSH into the node

[[files/esinha6/W14_PACE_Permission_Denied|height=100px]]

* We decided that we would debug again later because we kept going in loops trying to figure out what was wrong with our inability to SSH into the node

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|November 22, 2021
|November 29, 2021
|November 28, 2021
|-
|Set Up PACE
|In Progress
|November 8, 2021
|December 5, 2021
|
|-
|Run EMADE on PACE using new XML File configuration
|In Progress
|November 22, 2021
|December 10, 2021
|
|}

== Week 15 (11/29 - 12/05) ==

=== Lecture Notes (11/29) ===

* Weekly scrum meeting
* Image Processing Kicked off
* Used today mainly as a workday to be able to get everything running and finalized in terms of presentation
* Finally able to get EMADE running on PACE!
** Check '''Individual Notes''' below for how I did it

=== Subteam Notes (12/01) ===

* We first ensured everyone had tasks assigned for the week
* I was assigned to work with Aryaan, Elan, and Rohan on the mate/mutate functions to see if we can extract useful information from experiments with different mutate functions

=== Individual Notes ===

* I was able to figure out all my PACE issues and process a run of EMADE

==== Setting Up PACE and Running EMADE on PACE ====

* Forked Aaryan's repository
** I forked the wrong one earlier so make sure to double check that you have the right repo forked
* Transferred the EMADE fork onto PACE using:
```
 $ scp -r [directory of local emade fork] esinha6@pace-ice.pace.gatech.edu
```
* Create and place the '''.my.cnf''' file in the Username folder on PACE.
** Either create it locally then scp to pace or create it using <code>vim</code> while on PACE
* Here is my '''.my.cnf''':
```
 [mysqld]
 datadir=scratch/db
 socket=mysqldb.sock
 user=esinha6
 symbolic-links=0
 # uncomment the following line if you use a port and change the port number to the one you use
 port=3391

 [mysqld_safe]
 log-error=mysqldb.log
 pid-file=mysqldb.pid
 [mysql]
 socket=scratch/db/mysqldb.sock
```
Then I ran:
```
 $ mysql_install_db --datadir=$HOME/scratch/db
```
and
```
 $ /usr/bin/mysqld_safe --datadir='/storage/home/hpaceice1/esinha6/scratch/db'
```
* ssh into your node using:
```
 $ ssh esinha6@atl1-1-02-012-5-l
```
* While in the node, run:
```
 $ mysql -u root
```
* Then we want to create a username and password so run the following SQL commands:
```
 DELETE FROM mysql.user WHERE user='';
 GRANT ALL PRIVILEGES ON *.* TO 'esinha6'@'%' IDENTIFIED BY 'PASSWORD' WITH GRANT OPTION;
 FLUSH PRIVILEGES;
```
* Make sure to replace esinha6 and PASSWORD with your corresponding username and desired password
* Log into the MySQL shell with:
```
 $ mysql -u esinha6 -p
```
* Then type in your password
* Then you want to create your database while in the MySQL shell:
```
 CREATE DATABASE mating_runs;
```
* Then we want to navigate back to our '''emade''' folder in PACE and run the following to set up PACE Anaconda:
** <code> $ module load anaconda3/2020.02</code>
** <code> $ conda activate /storage/home/hpaceice1/shared-classes/materials/vip/AAD/ip_group/envs/</code>
** Make sure '''setup.py''' is already installed in the shared environment, if not, run <code>python setup.py install</code>
** Then run:
```
 $ python src/GPFramework/seeding_from_file.py templates/input_chestxray.xml seeding_test_cv_all_empty
```
* Then we want to create our '''pbsmysql.pbs''' file that looks like this:
** Tip: ASK how to put PACE in VSCode-- it makes creating, deleting, moving files so much easier than through the terminal
```
 #!/bin/bash
 #PBS -N mysqldb
 
 #PBS -q pace-ice
 
 #PBS -l nodes=1:ppn=1
 
 #PBS -l walltime=08:00:00
 
 #PBS -M esinha6@gatech.edu
 mysqld_safe --datadir='/storage/home/hpaceice1/esinha6/scratch/db'
```
* Then we want to created our '''launchEMADE.pbs''' file:
```
 #PBS -N emade-chest
 #PBS -l nodes=1:ppn=4
 #PBS -l pmem=4gb
 #PBS -l walltime=8:00:00
 #PBS -q pace-ice-gpu
 #PBS -o emade-chest.out
 
 cd ~/emade
 echo "Started on `/bin/hostname`"  # prints the name of the node job started on
 export CC=gcc
 module load anaconda3/2020.02
 conda activate /storage/home/hpaceice1/shared-classes/materials/vip/AAD/ip_group/envs
 echo "STARTING EMADE"
 # The next line seeds the run if you're starting from scratch. if you're resuming a run then comment it out.
 python src/GPFramework/seeding_from_file.py templates/input_chestxray.xml seeding_test_cv_all_empty
 # This line launches EMADE
 python src/GPFramework/launchEMADE.py templates/input_chestxray.xml
```
* So now we open our MySQL server:
```
 $ qsub pbsmyql.pbs
```
* To '''process a run of EMADE on PACE''', we just run
```
 $ qsub launchEMADE.pbs
```
* The above script will create EMADE_master.e##### and EMADE_master.o##### files after running for a while. Make sure to check for any errors and outputs.


=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|November 29, 2021
|December 6, 2021
|December 5, 2021
|-
|Set Up PACE
|Complete
|November 8, 2021
|December 5, 2021
|December 2, 2021
|-
|Run EMADE on PACE using new XML File configuration
|Complete
|November 22, 2021
|December 510 2021
|December 2, 2021
|}

== Week 16 (12/06 - 12/12) ==

=== Lecture Notes (12/06) ===

* Weekly scrum meeting
* Image Processing used the time as a work day to ensure we were all on track for a code freeze and finalization of presentation slides

=== Subteam Notes (12/08) ===

* Mainly a work day
* Aryaan, Rohan, Elan, and I were designated to be the mate/mutation crew to see what results we could find by changing some of the probabilities and functions used in mate/mutations
* Aryaan sent us the XML file configuration and we immediately got to work by running some trials with our data
* The lowest ROC AUC was about 0.36 with about 36 million parameters generated

=== Final Presentations (12/10) ===

* Image Processing presented our presentation and received very good feedback from Dr. Zutty and Dr. Rohling
* One thing the mate/mutation crew and I could have done was cite the creators of the various functions we used and/or change the functions to fit our desired needs
* We could have also been more specific about our results and also more targeted in the functions we used

=== Individual Notes ===

* We spent a lot of time on trials and on the presentation slides for the final presentation
* Here is a link to the XML file configuration: https://drive.google.com/file/d/1WNAr95nann3ChyjI2l1iqozEDK4aNbSa/view?usp=sharing
* Here are the probabilities that we changed and added in to mate/mutate:
```
 <matings>
            <mating>
                <name>crossover</name>
                <probability>0.20</probability>
            </mating>
            <mating>
                <name>crossoverEphemeral</name>
                <probability>0.20</probability>
            </mating>
            <mating>
                <name>headlessChicken</name>
                <probability>0.10</probability>
            </mating>
            <mating>
                <name>headlessChickenEphemeral</name>
                <probability>0.20</probability>
            </mating>
            <mating>
                <name>cxTwoPoint</name>
                <probability>0.00</probability>
            </mating>
            <mating>
                <name>cxUniform</name>
                <probability>0.30</probability>
            </mating>
            <mating>
                <name>partiallyMatched</name>
                <probability>0.00</probability>
            </mating>
            <mating>
                <name>orderedCx</name>
                <probability>0.00</probability>
            </mating>
 </matings>
‚Äã 
 <mutations>
            <mutation>
                <name>insert</name>
                <probability>0.05</probability>
            </mutation>
            <mutation>
                <name>insert modify</name>
                <probability>0.10</probability>
            </mutation>
            <mutation>
                <name>ephemeral</name>
                <probability>0.25</probability>
            </mutation>
            <mutation>
                <name>node replace</name>
                <probability>0.05</probability>
            </mutation>
            <mutation>
                <name>uniform</name>
                <probability>0.05</probability>
            </mutation>
            <mutation>
                <name>shrink</name>
                <probability>0.05</probability>
            </mutation>
 </mutations>
```
* The most notable additions were '''cxTwoPoint''' and '''cxUniform'''
* I then processed trial runs of EMADE
* I wanted to query my results for one of the trials so I used:
```
 select `FullDataSet ROC AUC`, `FullDataSet Num Parameters`
 from individuals ind join paretofront pf on pf.hash=ind.hash
 where pf.generation = (select max(generation) from paretofront);
```
* After querying, I achieved the following results:
```
 +---------------------+----------------------------+
 | FullDataSet ROC AUC | FullDataSet Num Parameters |
 +---------------------+----------------------------+
 |            0.484691 |                   12046600 |
 |            0.455363 |                   24087300 |
 |            0.499883 |                         30 |
 |             0.48709 |                    1395180 |
 |            0.485118 |                   12042400 |
 |            0.465182 |                   12048000 |
 |            0.489069 |                        287 |
 +---------------------+----------------------------+
```
* My lowest ROC AUC was 0.455363, which was substantially higher than Aryaan's lowest of ~0.36
* The number of parameters generated for that was 24087300, which was the most number of parameters generated for any of my individuals
* We learned that often, the greater number of generated parameters correlates to a lower ROC AUC
* However, we want to figure out how to reduce the number of generated parameters while simultaneously lowering the ROC AUC as much as possible.
* Here is an example of an individual:
```
 NNLearner(((((((((ARG0)))))))), FlattenLayer4d(adf_4(adf_11(adf_18(adf_2(adf_12(adf_4(adf_2(InputLayer)))))))), ((((falseBool)))), 
 ((((((((RMSpropOptimizer)))))))))\nadf_2: DenseLayer4dim(DenseLayerUnit256, defaultActivation, InputLayer)\nadf_4: 
 Conv2DLayer(Conv2DFilterUnit48, reluActivation, Conv2DKernelSize5, ARG0)\nadf_11: DenseLayer4dim((DenseLayerUnit32), (seluActivation), 
 DenseLayer4dim(DenseLayerUnit32, eluActivation, InputLayer))\nadf_12: MaxPoolingLayer2D(Conv2DKernelSize5, ARG0)\nadf_18: 
 DenseLayer4dim((DenseLayerUnit32), (tanhActivation), Conv2DLayer(Conv2DFilterUnit16, tanhActivation, Conv2DKernelSize5, 
 ARG0))|0.407442|36166300
```
* That particular individual generated 36166300 parameters and had an ROC AUC of 0.407442, which is much lower than what I achieved
* We can represent that individual as a tree
* In our presentation slides, we were able to visualize a tree using https://github.com/cwhaley112/TreeVis
* Here is a link to Image Processing's final presentation slides: https://docs.google.com/presentation/d/1c6c51KhAIRJMMRo2mzNDDFsWm_MkFgXmNa3j7aLppdk/edit?usp=sharing

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add Meeting Lecture Notes into Notebook
|Completed
|December 6, 2021
|December 12, 2021
|December 12, 2021
|-
|Run trials of EMADE on PACE
|Complete
|December 6, 2021
|December 10, 2021
|December 8, 2021
|-
|Query Results to see different individuals
|Complete
|December 6, 2021
|December 10, 2021
|December 8, 2021
|-
|Finish up Lab Notebook
|Complete
|December 6, 2021
|December 12, 2021
|December 12, 2021
|}