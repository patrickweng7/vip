== Team Member ==

Team Member: Johnny Maroun

Email: gmaroun3@gatech.edu

Cell Phone: 404-906-1139

Interests: Machine Learning, Python, Basketball, Golf

== Week 1: January 7, 2019 ==
'''Team Meeting Notes:'''
* Stock team dissolved and I joined the Deep Learning Sub-Team.
'''Deep Learning Sub-Team Notes:'''
* Discussed the overall outline of the framework with Rodd
* Discussed the technical characteristics of the Cartesian Genetic Programming approach
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Clone the ezCGP and peruse the codebase
|Completed (Looked at main top-level code)
|January 7, 2019
|January 14, 2019
|January 14, 2019
|-
|Join the Deep Learning slack channel and become GitHub contributor
|Completed 
|January 7, 2019
|January 14, 2019
|January 10, 2019
|}

== Week 2: January 14, 2019 ==
'''Team Meeting Notes:'''
* Plan to delegate tasks now and in the coming weeks split into sub-groups within Deep.
'''Deep Learning Sub-Team Notes:'''
* '''Cartesian Genetic Programming Notes:'''
* Original application for circuits on a 2-D grid
* Represents any DAG
* Each node is a tuple of genes, one gene specifying the function the node applies and remaining expressing where it takes input
* Takes input from linear predecessor
* Skip: Method for avoiding duplicate eval’s. Each offspring node compared for equivalence. Given same fitness as parent if equivalent. Penalty is search stuck in local optima
* Accumulate: Enters a cycle of repeated mutation until individual worth eval is created. Evaluates F(n+1) continuously
* Single: Mutates genes randomly until exactly 1 active is mutated
* Reorder: Assigning node new locations of genes without changing node behavior. Random Serialization algorithm
* DAG: Random different ordering methods to avoid creating a graph that cycles
* Arity: Number of arguments/functions the node takes in
* '''Codebase Notes:'''
* main.py: Creates a universe and appends a soln to the final_pop array
* universe.py: Class for unique “state of the world”. Dynamically select which block to evolve. Also method to initialize population with individuals
* individual.py: Individual genome composed of smaller genomes. Bring blocks together. Has eval, fitness, mutate, and mate methods 
* blocks.py: Various methods for creating, evolving blocks of genes
* genome.py: Instantiates arg list and input node list, builds weights and random inputs/arguments. Other funcs to actually build the genome
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Successfully become a contributor on the Symbolic Reg branch of ezCGP
|Completed 
|January 14, 2019
|January 15, 2019
|January 15, 2019
|-
|Further familiarization with the code
|Partially Completed 
|January 14, 2019
|January 20, 2019
|January 20, 2019
|}

== Week 3: January 21, 2019 ==
'''Team Meeting Notes:'''
* Joined the Tensorflow integration team
'''Tensorflow Notes:'''
* 60,000 images of fashion MNIST
* The images are 28x28 NumPy arrays, with pixel values ranging between 0 and 255. The labels are an array of integers, ranging from 0 to 9.
* Normalize the raw image data for preprocessing to feed into NN (/255.0)
* tf.keras models are optimized to make predictions on a batch, or collection, of examples at once
* mnist = tf.keras.datasets.mnist
* (x_train, y_train), (x_test, y_test) = mnist.load_data()
* x_train, x_test = x_train / 255.0, x_test / 255.0
* Tensor: Multi-dimensional matrix
* Graph: Provides information on nodes and metadata of specific individual operations
'''ezCGP Notes:'''
* main.py uses problem.py to establish the problem at hand
* in universe.py, creates a "unique state of the world" with collection of individuals
* individual.py creates a block or instance of a "genome" with a certain order of input functions est. in problem
* block.py are partial-genome components that reference mate and mutate methods that culminate and are passed up to individual
* operators.py creates the primitive operations defined in the problem space
* right now operators are aggregated in NumPy context, we are aiming at transferring framework to work with Tensorflow
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Familiarize myself with Tensorflow
|Completed (Basic Level)
|January 21, 2019
|January 28, 2019
|January 28, 2019
|-
|Better understand the block vs. genome architecture
|Completed 
|January 21, 2019
|January 28, 2019
|January 25, 2019
|}

== Week 4: January 28, 2019 ==
'''Team Meeting Notes:'''
* Start integrating an individual in the TF context
'''ezCGP Notes:'''
* problem_mnist.py needs tf add operator
* evaluator of individual needs to be changed
* changed skeleton block to tf.Tensor rather than np.ndarray
* Tester.py works with creating individual, bypassing universe creation
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with the team to start low-level TF approach
|Completed (Basic Level)
|January 28, 2019
|January 31, 2019
|February 2, 2019
|-
|Rewrite the problem.py to incorporate mnist problem
|Completed partially (working on operators)
|January 28, 2019
|January 31, 2019
|February 3, 2019
|}

== Week 5: February 4, 2019 ==
'''Team Meeting Notes:'''
* Work on adding TensorFlow primitives to blocks.py
'''ezCGP Notes:'''
* There is an issue with how we're feeding in our input to the evaluate function
* Probably due to tensor being in different format that conventional NumPy array
* Tensor being fed in was 2-D, while the input took a 1-D flattened out array
* A few convolutional functions erroring out, most likely due to the data not being sent in stream/batches but rather in one go
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with the team to work out bugs with Individual/Blocks
|Completed
|February 4, 2019
|February 9, 2019
|February 7, 2019
|-
|Run the working neural net and graph loss values
|Completed partially 
|February 4, 2019
|February 8, 2019
|February 8, 2019
|}

== Week 6: February 11, 2019 ==
'''Team Meeting Notes:'''
* Meet Thursday/help desk
'''ezCGP Notes:'''
* Individual was being set to "dead" upon the throwing of an exception stating that the key in our feed_dict is None.
* Fixed the bug and then ran into issue due to inconsistent matrix dimensions between operations.
* Fixed this error by flattening matrix at the final preprocessing step before adding the final dense layer for the output predictions.
'''Links to contribution:'''
* https://github.com/ezCGP/ezCGP/commit/59338cb52937a65c4874dbd1b7392e1cabfbe5ce
* Above commit to access the batches within the tensorblock_eval method
* https://github.com/ezCGP/ezCGP/commit/c1eb8963e22aa5d69a4bd27a403c02d048a9197a
* Altered batch training so the individual now successfully fits to the training data from having a training accuracy of 25% after 20 epochs, the training accuracy is now > 99% in 5 epochs.
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with the team to work out bugs with batch size
|Completed
|February 11, 2019
|February 14, 2019
|February 14, 2019
|-
|Run the converging neural net and graph new loss values
|Completed 
|February 11, 2019
|February 16, 2019
|February 14, 2019
|}

== Week 7: February 18, 2019 ==
'''Team Meeting Notes:'''
* Work on coming full circle with the main.py to tensor functionality
'''ezCGP Notes:'''
* Fixed the error caused when the batch_size is not a divisor of the number of examples. Batch_size can be adjusted in the skeleton_block 
* There needs to be more cohesion and modularity within the user's ability to manipulate parameters
* Individual fitness should be function of testing set, not training set
* Partitioned off percentage of data for validation set, allowed the individual tensor_eval to run off this benchmark
'''Links to contribution:'''
* https://github.com/ezCGP/ezCGP/commit/4aeaeca1b8e592a9eaffa4f4b9005b89e24d974e
* Above link worked on adding validation data labels to be passed externally into eval_function
* Still not the prettiest in terms of modularity/pretty code in general
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with the team to work out further bugs with batch_size
|Completed
|February 18, 2019
|February 23, 2019
|February 22, 2019
|-
|Make sure deep-copying the individual does not give a threading error
|Completed 
|February 18, 2019
|February 25, 2019
|February 24, 2019
|}

== Week 8: February 25, 2019 ==
'''Team Meeting Notes:'''
* General exploration of TensorFlow functions/API
'''ezCGP Notes:'''
* tf.graph_util.import_graph_def: Imports the graph from graph_def into the current default Graph
* Returns a list of Operation and/or Tensor objects from the imported graph
* tf.contrib.graph_editor.copy: Simply copies a subgraph, might be better than deep-copy currently exploring
* Returns a tuple (sgv, info) where: sgv is the transformed subgraph view; info is an instance of TransformerInfo containing information about the transform
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with the team to make sure main.py is converging properly
|Completed
|February 25, 2019
|February 29, 2019
|February 29, 2019
|-
|Uncomment more tensor operators to see the decrease in fitness score. 
|Completed
|February 25, 2019
|February 29, 2019
|February 30, 2019
|}

== Week 9: March 4, 2019 ==
'''Team Meeting Notes:'''
* Obtain hard results on MNIST and CiFAR-10 for presentation
'''ezCGP Notes:'''
* Mid-semester presentation notes:
** Skeleton for the Deep Presentation
** Deep learning, genetic programming, and cartesian high level definitions
** High level structure of exCGP specifically with DAG (directed acyclic graph)
** Described the overall goal to utilize the CGP to evolutionarily optimize network architectures for a general case
** Saw MNIST and CiFAR results against benchmarks
** General improvements to be made and conclusion for the midterm
* MNIST: Best individual 
** Validation accuracy: 99.1%
** F1-score: 99.08%
* CiFAR-10: Best individual 
** Validation accuracy: 80.2%
** F1-score: 79.52%
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with the team Thursday to crank out presentation details
|Completed
|March 4, 2019
|March 9, 2019
|March 9, 2019
|-
|Run individually on 2 datasets with batching 
|Completed
|March 4, 2019
|March 9, 2019
|March , 2019
|}

== Week 10: March 11, 2019 ==
'''Team Meeting Notes:'''
* Presentation next week
'''ezCGP Notes:'''
* Presentation: https://docs.google.com/presentation/d/1UmH5CSSlO2NPVMmBe_Kqo3sie2gQs4Sm2fldg9JC6lE/edit#slide=id.p
* AdamOptimizer:
** Popular technique used most explicitly to perform optimization on the training model of a Deep Neural Network
** Uses the Adam (Adaptive Moment Estimation) algorithm which computes individual learning rates for diff. parameters. Name derives from using the “first and second moments” of gradient to adapt the learning rate for each weight of a NN (moment being the expected value of a random variable raised to some power)
** Uses the Adam (Adaptive Moment Estimation) algorithm which computes individual learning rates for diff. parameters. Name derives from using the “first and second moments” of gradient to adapt the learning rate for each weight of a NN (moment being the expected value of a random variable raised to some power)
* Batch_Normalization:
** Technique for improving performance/independence of hidden layers in a DNN. Implementation is normalizing the input to layer by adjusting and scaling the activations of the previous layer
** Minimizes covariate shift (common problem in train/test input distribution variance) by normalizing the outputs of hidden layers
* More complex primitives used- Res_Block & Conv_Block
* Max pooling- Reduce dimension space by taking the max of predefined subregions\
* channel- output filter of a hidden layer
* zero padding- padding input volume with zeros
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with the team on Thursday to finish the midterm presentation
|Completed
|March 11, 2019
|March 14, 2019
|March 14, 2019
|-
|Pick slides and review material to present in the best way 
|Completed
|March 11, 2019
|March 14, 2019
|March 14, 2019
|}

== Week 11: March 18, 2019 ==
'''Team Meeting Notes:'''
* Spring Break- No meeting
* Read an interesting paper on CGP/Deep Learning
'''ezCGP Notes:'''
* CGP can represent any DAG
* Skip: Method for avoiding duplicate eval’s. Each offspring node compared for equivalence. Given same fitness as parent if equivalent. 
* Penalty is search stuck in local optima
* Eval's:
** Accumulate: Enters a cycle of repeated mutation until individual worth eval is created. Evaluates F(n+1) continuously
** Single: Mutates genes randomly until exactly 1 active is mutated
** Reorder: Assigning node new locations of genes without changing node behavior. Random Serialization algorithm
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Find a good article and gather some interesting insight 
|Completed
|March 18, 2019
|March 20, 2019
|March 23, 2019
|-
|Formulate a plan for dividing the team into smaller sections 
|Completed
|March 18, 2019
|March 20, 2019
|March 20, 2019
|}

== Week 12: March 25, 2019 ==
'''Team Meeting Notes:'''
* We are going to split teams for the rest of the semester: A & B
'''ezCGP Notes:'''
* Team A will focus on overall development/network architecture and usability of the framework (Ani, Gib, Michael P., Jinghau)
* Team B will focus on the regression problem for exCGP and creating higher level primitives for the networks (Myself, Sam, Michael J.)
* Recruited 4 new team members to Deep and coordinated to catch them up to speed on the CGP underpinnings
'''Dataset Options Moving Forward:'''
* Housing dataset from Kaggle (predict price from relative area): https://www.kaggle.com/gabriellima/house-sales-in-king-county-usa
* Stock dataset, 40 years of GE data from Stock Subteam I was on last semester
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet on Thursday to flesh out detailed goals for Team B/regroup after break
|Completed
|March 25, 2019
|March 29, 2019
|March 29, 2019
|-
|Figure out a good regression dataset -housing, stock 
|Completed
|March 25, 2019
|March 29, 2019
|March 29, 2019
|}

== Week 13: April 1, 2019 ==
'''Team Meeting Notes:'''
* First Team B functionality meeting this week
'''ezCGP Notes:'''
* Refactor problem.py for housing dataset preprocessing
** Dropped features: drop_features = ["id","date", "price", "zipcode", "yr_renovated"]
** split_and_normalize() with 70% training, 15% testing and 15% validation across gen's
** Libraries: SkLearn StandardScaler(), mean_absolute_error(), transform(), etc...
* Branch out from symbolic regression branch
** Created branch talebi-tubbies for Team B regression: https://github.com/ezCGP/ezCGP/tree/talebi-tubbies
* Comment out all but dense layer and redefine scoring functions
'''Links to contribution:'''
* https://github.com/ezCGP/ezCGP/commit/6fd73aaab123a2c3a4346a93ed26b04bc81cde74
* https://github.com/ezCGP/ezCGP/commit/414291b56e2b2ebf7ffc0b0a4b61476c9db0d5f5
* Above commits are from Sam and don't include my name, but he will confirm that him, Michael J. and I contributed to choosing the Kaggle dataset and data preprocessing
* We also looked at reconstructing data import to one-shot instead of batching with some of the bigger datasets, and regression scoring functions such as Average Percent Change and Mean Absolute Error (main 2)
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with the team to refactor problem.py and blocks.py
|Completed
|April 1, 2019
|April 4, 2019
|April 4, 2019
|-
|Brainstorm other datasets for next week's integreation 
|Completed
|April 1, 2019
|April 4, 2019
|April 6, 2019
|}

== Week 14: April 8, 2019 ==
'''Team Meeting Notes:'''
* Decide on boundaries/parameters and set a goal for Team B
'''ezCGP Notes:'''
* Discussed the infeasibility of incorporating historic stock data for regression at this point in the semester
* We decided to constrain our layers for testing purposes to be dense layers specifically for now, conv layer and other present non-trivial issues
* Created a flag for signifying regression in the main code, the overall goal of ezCGP is usability and that has to span holistically from the user standpoint
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with the team to discuss regression strategy
|Completed
|April 8, 2019
|April 11, 2019
|April 11, 2019
|-
|Collaborate on overall integration of regression/main branches. 
|Completed
|April 8, 2019
|April 11, 2019
|April 11, 2019
|}

== Week 15: April 15, 2019 ==
'''Team Meeting Notes:'''
* Team B highlighted 3 main areas for improvement/addition for final presentation material
'''ezCGP Notes:'''
* We need to revamp results_analysis.py to include accuracy over generations, and relabel axes (our scoring metrics) for regression
* We normalized our data/results but need to rescale in order to compare against Kaggle benchmark
* We need to run the housing dataset/make sure the results are performing and create a Pareto Optimality Graph
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run the housing dataset and construct result in a Pareto front
|Completed
|April 15, 2019
|April 20, 2019
|April 20, 2019
|-
|Meet with the team on Thursday to benchmark against Kaggle 
|Completed
|April 15, 2019
|April 18, 2019
|April 18, 2019
|}

== Week 16: April 22, 2019 ==
'''Team Meeting Notes:'''
* Final Presentation push
* Presentation: https://docs.google.com/presentation/d/10t_-9TvkV_GwWpHTPa6wG7tWbhio_NAjP8u-f-bfWqE/edit#slide=id.g5829e82f7b_0_19
'''ezCGP Notes:'''
* Need to gather data for Housing and construct a pareto graph over the generations
* Compare against Kaggle benchmark
* Meet and gather all the data for a slide deck
'''Links to contribution:'''
* https://github.com/ezCGP/ezCGP/commit/8dce27868c555ccded7509f6d939f0aef03887ee
* https://github.com/ezCGP/ezCGP/commit/e5b1a75c175c998aaf2ce231dc6540abff7c6fc7
* https://github.com/ezCGP/ezCGP/commit/1964d34066dd5075d7455e3adf376b82e7260cef
* ** Mainly changes over the past week for results_analysis, creating Pareto front for Housing dataset **
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add preprocessing and Pareto Front to Deep Pres.
|Completed
|April 18, 2019
|April 22, 2019
|April 21, 2019
|-
|Meet with the team on Thursday to benchmark against Kaggle 
|Completed
|April 18, 2019
|April 22, 2019
|April 21, 2019
|}

== Semester Conclusion & Why I Deserve an A ==
'''Itemized List of Goals Reached:'''
* Coming into the semester, I had worked with Machine Learning both through EMADE and through other courses, but lacked real Deep Learning development specifically

* Throughout the 16 weeks, I have read through research papers about DL, dived into Rodd's codebase with the team and implemented primitives/analysis and overall development of ezCGP
* By learning though doing and actively contributing to the framework, I would argue that my work here has been the best of the 3 semesters
* I have been at office hours numerous times and have consistently met with my team every Thursday for 1.5 hrs+ and on Sunday's when need be, every single week
* Shown by my links here in the Wiki Notebook, I have pushed changes to both the tensorflow-nn and the talebi-tubbies branches
* Since joining Team B after the post-midterm split, I helped in creating the new branch, picking out the dataset, preprocessing the data, and runnning analyses/creating a pareto front for the Regression Problem
'''Notes on mid-semester feedback:'''
* '''Verbatim Feedback:''' "You are keeping a great notebook, especially with linking to your contributions. Keep up the excellent work. Your team notes that you are hard working, but could put a bit more effort into your independent work and be more communicative during meetings."
* As shown on this very page, I have been keeping up to pace with the quality of my notebook, if not better since the feedback
* As per my team contributions notes, I have made it a point to let my presence/contributions to the overall team known a lot better, maintained going to 100% of classes and meetings, and continued to advance the ezCGP framework on my own
* I have done all I can to be a team member, take and give advice from my team, and provide a lot of cohesive completeness to my group
* Since we split into smaller sub-teams I feel that my independent work has been more augmented and is clearer to see, as there were only 3 of us (Myself, Michael J. and Samuel Zhang) to vouch