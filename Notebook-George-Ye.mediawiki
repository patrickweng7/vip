== Team Member ==
[[files/GYImage.png|thumb]]

Team Member: George Ye

Email: gye31@gatech.edu

Cell Phone: 678-365-1412

Interests: Machine Learning, Basketball, Video Games

== December 10, 2021 ==

Final Presentation Notes:

* Image Processing:
** Data preprocessing with such as horizontal shifts, sheering, etc.
** ROCU AUC and num parameters for eval functions
** NSGA2 still performed better than NSGA3 due to number of objectives
** NSGA2 still performed better than lexicase due to fewer training cases and or smaller population size
** Explored semantic operators such as semantic cross and semantic mutation
** Best semantic crossover individual only performed a little better than seeded individual
** New geometric cross over operators
** Hyperfeatures are multiple primitives that work well together and combine them into a single primitive

* Market/Stock Analysis
** Using primitives like technical indicators
** Statistical correlation between different indictators and individuals in EMADE
** Multiple trial runs with different number of objectives
** argmin and argmax surprisingly performed very well
** Many individuals used argmax
** Paper focused on individuals performed best in certain objectives
** Beating the previous paper from midterm on all stocks besides Johnson and Johnson stock
** Cumulative distribution based on average of random trading versus best individual average profit versus paper average profit

* Neural Architecture Search
** Increasing model complexities for neural networks
** Eventually competitive with SOTA architectures
** Modifications to mating and mutation without seeded runs
** Midterm runs were used as benchmarks
** Introduced modules
** Easier implementaiton vs resnet/vgg architectures
** If module has been trained, load old weights
** Cool tracking in sql database
** Modules in theory are unbounded

* Modularity
** Modularity and reusbaility in EMADE
** Select combinations based on their frequency
** Infrastructure update from tuples to classes
** Increasing complexity of ARLs by increasing the tree depths
** Experimental setup based on titanic dataset
** Comparing no ARLs, old ARL architecture and new ARL architecture
** New ARL architecture with a lot more generations could perform better than old arl
** In general though, lower AUC with some ARL implementation versus no ARLs.

== December 6, 2021 ==

'''Team Meeting Notes:'''
* Getting ready for final presentation

'''Subteam Meeting Notes:'''

* Subdividing work for final presentation
* Intro to NLP, QA, BiDAF, primitive development, experiment procedure, region of interest, future work

'''Personal Work:'''

* Get scaling with multiple pareto fronts to scale properly with outliers. Some code I used to play around with the scale factor for both the x and y axis:

```
scale_factor_y = 0.3
scale_factor_x = 0.18

xmin, xmax = plt.xlim()
ymin, ymax = plt.ylim()

plt.xlim(xmin * scale_factor_x, xmax * scale_factor_x)
plt.ylim(ymin * scale_factor_y, ymax * scale_factor_y)
```

* Pace run running through 190 generations but it was only having the bad individual:
```
WindowKaiser(findClusterLSR(LowpassFilterGaussian(ARG0, TriState.STREAM_TO_FEATURES, Axis.FULL, -9, trueBool, 0.1, 4.419257980631761), passTriState(TriState.STREAM_TO_STREAM), greaterThan(0.1, 10.0)), passTriState(passTriState(TriState.STREAM_TO_FEATURES)), passAxis(passAxis(Axis.AXIS_2)), myFloatIntMult(passFloat(0.1), myIntDiv(50, 64)), myAnd(myOr(trueBool, falseBool), notEqual(10.0, -2.3921047463883305)))
```
Trying to figure out the reason because the run is seeded with NNLearner2 individual
* Figured out a potential error for the run while working with Steven:
```
        if len(input_layers) == 2 and data_pair.get_datatype()=='textdata':
            model().fit(x=x, y=y, validation_data=validiation_data_final, batch_size=batch_size, epochs=epochs)
```
* Initially epochs=epochs was not a parameter. After adding in more epochs, made it so that NNLearner2 would never actually evaluate. I looked in the database and out files and the seeded individual is always in progress. Potential reasoning is because of low disk space when running on pace.

* Working on presentation slides for primitive BiDaf modeling layer and development challenges with keras


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|EMADE run for experimental design
|Complete
|December 6, 2021
|December 10, 2021
|December 10, 2021
|-
|Primitive Modeling layer and development issues
|Complete
|December 6, 2021
|December 10, 2021
|December 10, 2021
|-
|}

== November 29, 2021 ==

'''Team Meeting Notes:'''

* Image Processing: Primitives should not be writing disk. Not at code freeze yet
* Modularity: Continue with runs. Upward spike AUC error found
* NLP: Naive solution seeded compared to EMADE running without seeded
* NAS: Not at code freeze yet. Working on getting everything merged.
* Teams need to get to code freeze point to start experiment runs.


'''Subteam Meeting Notes:'''

* Beginning to start experimental runs for final presentation
* Change dataset to be regression problem for word start index rather than character start index 

'''Personal Work:'''

* Pace EMADE runs were done in 8 hour increments. Evaluation function were num params for complexity and mse for accuacy.
* PACE emade run getting a weird, wrong individual. Potential error is probably because not seeded with NNLearner2 individual: 
```
WindowKaiser(findClusterLSR(LowpassFilterGaussian(ARG0, TriState.STREAM_TO_FEATURES, Axis.FULL, -9, trueBool, 0.1, 4.419257980631761), passTriState(TriState.STREAM_TO_STREAM), greaterThan(0.1, 10.0)), passTriState(passTriState(TriState.STREAM_TO_FEATURES)), passAxis(passAxis(Axis.AXIS_2)), myFloatIntMult(passFloat(0.1), myIntDiv(50, 64)), myAnd(myOr(trueBool, falseBool), notEqual(10.0, -2.3921047463883305)))
```

* MSE was the worst possible error and max number of params. But it did compile.
* Correct workflow for pace:
** qsub mysql database. If this is the first run, clear out the database. (Drop and then create database)
** qsub seeding file with naive solution.
** qsub emade run
** Note: making sure node server is correct for emade run and mysql database

* New PACE emade run with 6 valid individuals using seeded individual: 
```
Pareto Individual 0 after gen 49 is NNLearner2(ARG0, ARG1, OutputLayer(LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, AttentionLayer(EmbeddingLayer(16, ARG0, heWeights, InputLayer()), EmbeddingLayer(16, ARG1, glorotNormalWeights, InputLayer())))), 100, passOptimizer(AdamOptimizer))(241243.0, 2257025.0) Age 1.0
Pareto Individual 1 after gen 49 is NNLearner2(ARG0, ARG1, OutputLayer(AttentionLayer(EmbeddingLayer(16, ARG0, heWeights, InputLayer()), EmbeddingLayer(16, ARG1, glorotNormalWeights, InputLayer()))), 100, passOptimizer(passOptimizer(AdamOptimizer)))(242279.0, 2246401.0) Age 1.0
Pareto Individual 2 after gen 49 is NNLearner2(ARG0, ARG1, OutputLayer(GRULayer(6, reluActivation, 32, trueBool, falseBool, AttentionLayer(EmbeddingLayer(16, ARG0, passWeightInitializer(randomUniformWeights), InputLayer()), EmbeddingLayer(16, ARG1, glorotNormalWeights, InputLayer())))), myIntSub(10, 1), AdamOptimizer)(251242.0, 2240877.0) Age 1.0
Pareto Individual 3 after gen 49 is NNLearner2(ARG0, ARG1, OutputLayer(ConcatenateLayer2(InputLayer(), LSTMLayer(88, softmaxActivation, 7, falseBool, falseBool, EmbeddingLayer(16, ARG0, passWeightInitializer(glorotUniformWeights), InputLayer())))), myIntSub(10, 1), AdamOptimizer)(280517.0, 1157449.0) Age 1.0
Pareto Individual 4 after gen 49 is NNLearner(ARG0, OutputLayer(InputLayer()), 1, FtrlOptimizer)(384797.0, 401.0) Age 1.0
Pareto Individual 5 after gen 49 is NNLearner(ARG0, OutputLayer(OutputLayer(DenseLayer(falseBool, sigmoidActivation, 255, InputLayer()))), myIntAdd(6, 55), AdamaxOptimizer)(384798.0, 3.0) Age 1.0
Pareto Individual 6 after gen 49 is NNLearner(ARG0, OutputLayer(DenseLayer(falseBool, eluActivation, 1, InputLayer())), 1, RMSpropOptimizer)(392514.0, 1.0) Age 1.0
```
* Finally getting NNLearners
* eval functions is (mse, num_params)

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Full PACE run on SQUAD dataset
|Complete
|November 29, 2021
|December 6, 2021
|December 6, 2021
|-
|Continue previous pace run to up to 24 hours
|Complete
|November 29, 2021
|December 6, 2021
|December 6, 2021
|-
|}

== November 22, 2021 ==

'''Team Meeting Notes:'''
* Output layer is too difficult to implement, so we have changed to a regression problem rather than classification.
* NNLearner2 to for squad datasets

'''Subteam Meeting Notes:'''
* Thanksgiving break

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|}

== November 15, 2021 ==

'''Team Meeting Notes:'''
* Prep work for hackathon. Main focus will be on getting NNLearner2 to work

'''Subteam Meeting Notes:'''
* One team focused on getting NNLearner to work
* Another team focused on getting bidirectional attention layer to work

'''Personal Work:'''
* Karthik and I worked on getting the attention flow layer to work. There were two main issues:
** In the megamerge step, their was a function to convert a list of 1d tensors into a 2d tensor using tf.convert_to_tensor. Function was not working correctly. Instead, we converted each 1d tensor into a 2d tensor and then concatenated all together.
```
# T*2d megamerged matrix G
megamerged_matrix = [[0]*8*self.d]*self.t

# merge columns of embedded_context, c2q, and q2c
for i in range(self.t):
    megamerged_matrix[i] = tf.concat([embedded_context[i], c2q[i], c2q[i]*embedded_context[i], q2c[i]*embedded_context[i]], 0)
        
for i in range(len(megamerged_matrix)):
    megamerged_matrix[i] = tf.expand_dims(megamerged_matrix[i], 0)
    
# convert into (T,8d) tensor
return tf.concat(megamerged_matrix, axis = 0)
```
** Second issue was that the layer could not take varying batch sizes (only worked for batch size of 1). To fix this, we can return a None batch size, which represents a varying batch size. Or return the match size based on the input. We also did some better code design by abstracting the call method with a helper that does the work of getting the similarity matrix, calculating c2q and q2c, and doing the merge.
```
def call_helper(self, embedded_context, embedded_query):
    # similarity_matrix: (T, J)
    similarity_matrix = self.make_similarity_matrix(embedded_context, embedded_query)
            
    # Context --> Query Attention. U_hat = Attended query matrix for all context words, size (T,2d)
    c2q = self.contextToQueryAttention(embedded_query, similarity_matrix)

    # Query --> Context Attention, size (T,2d)
    q2c = self.queryToContextAttention(embedded_context, similarity_matrix)

    megamerged = self.megamergeStep(embedded_context, c2q, q2c)
    return megamerged

if input[0].shape[0] is None:
    megamerged = self.call_helper(embedded_context, embedded_query)
    megamerged = tf.expand_dims(megamerged, 0)
    final_result_matrix = tf.compat.v1.placeholder_with_default(megamerged, [None, self.t, self.d * 8])
    return final_result_matrix
else:
    batch_size = input[0].shape[0]
    batches = []
    for i in range(batch_size):
        megamerged = self.call_helper(embedded_context, embedded_query)
        batches.append(megamerged)
    for i in range(len(batches)):
        batches[i] = tf.expand_dims(batches[i], 0)
                

    return tf.concat(batches, axis = 0)

```

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Hackathon project of getting Bidirectional attention flow to work
|Complete
|November 15, 2021
|November 22, 2021
|November 22, 2021
|-
|}

== November 8, 2021 ==

'''Team Meeting Notes:'''
* Runs works on one data pair
* Finish resolving issues of big merge

'''Subteam Meeting Notes:'''
* Splitting up into NNLearner with 2 Data pair team, integration team, and output layer team
* For the integration team, set up pace to run squad dataset with standalone tree evaluator

'''Personal Work:'''
* Continued working with Shiyi and Karthik on Attention -> Modeling -> Output testing and making sure the inputs and outputs match.
* A lot of the code had errors when compiling:
** LSTM requires a 3D array with the first parameters representing batch size. To accomplish this we needed to expand out 2D dataset
** When creating the similarity matrix, the context column and query column has mismatch dimensions for the input. Hence, element wise multiplication wasn't working. Here is the fix:
```
context_column = tf.expand_dims(context_column, axis = -1)
query_column = tf.expand_dims(query_column, axis = -1)
```
** The output layer required the output from the modeling layer and additional matrix that needed to be computed to calculate both the start and end index in the context. We decided that the additional matrix would be calculated in the modeling layer:
```
# Now returns a tuple of matrices M1 and M2 - Why? Check: https://towardsdatascience.com/modeling-and-output-layers-in-bidaf-an-illustrated-guide-with-minions-f2e101a10d83
return tf.reshape(lstm_1_output, [lstm_1_output.shape[1], lstm_1_output.shape[0]]), tf.reshape(lstm_2_output, [lstm_2_output.shape[1], lstm_2_output.shape[0]])
```
* Currently running into errors with the standalone tree evaluator, going to be switching to just running with the Amazon Dataset.
** Also having errors, going to try to run locally

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Getting successful pace stand alone tree evaluator on Amazon Dataset
|In Progress
|November 8, 2021
|November 15, 2021
|November 15, 2021
|-
|Fixing modeling, attention, and output layers
|Complete
|November 8, 2021
|November 15, 2021
|November 15, 2021
|-
|}

== November 1, 2021 ==

'''Team Meeting Notes:'''
* Introducing new memebers to NLP
* Exchanging contact info, slack info, and intro presentations
* Unit tests for new primitives for BiDAF

'''Subteam Meeting Notes:'''
* Pace setup for new members
* Split into groups working on output layer, attention layer (hard), and modeling layer
* NNLearner intro for new members

'''Personal Work:'''
* Custom modeling layer code for BiDaf:

```
class BidafModeling(Layer):
    def __init__(self, out_dim, activation = 'tanh', dropout = 0.2, regularizer = 0, **kwargs):
        super(BidafModeling, self).__init__(kwargs)
        if regularizer == 0:
            regularizer = None
        else:
            regularizer = l2(10**(-1*regularizer))
        self.lstm_1 = Bidirectional(LSTM(units = out_dim, activation=activation, kernel_regularizer=regularizer, dropout=dropout))
        self.lstm_2 = Bidirectional(LSTM(units = out_dim, activation=activation, kernel_regularizer=regularizer))

    
    def call(self, inputs):
        inputs = tf.reshape(inputs, [inputs.shape[1], inputs.shape[0], 1])
        lstm_1_output = self.lstm_1(inputs)
        lstm_1_output = tf.expand_dims(lstm_1_output, -1)
        lstm_2_output = self.lstm_2(lstm_1_output)
        return tf.reshape(lstm_2_output, [lstm_2_output.shape[1], lstm_2_output.shape[0]])
```


* Custom EMADE function for layer as primitive:

```
def BidafModelingFunc(output_dim, layerlist, activation = 'tanh', regularizer = 0, dropout=0.2):
    """
    Create Bidaf Modeling layer
    """
    layerlist.append(BidafModeling(output_dim, activation=activation, regularizer=regularizer, dropout=dropout))
    return layerlist
```


* Helped Karthik and Shiyi with debugging and connecting the modeling layer, attention layer, and output layer


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Custom layer for Bidaf Modeling layer
|Complete
|November 1, 2021
|November 8, 2021
|November 8, 2021
|-
|Emade function for using the custom layer as a primitive
|Complete
|November 1, 2021
|November 8, 2021
|November 8, 2021
|-
|}


== October 25, 2021 ==

Super nice and interesting presentations.

Presentations:
* Bootcamp 1:
** Data preprocessing: One hot encoding for sex, dropping columns, etc.
** Comparing machine learning algorithms such as SVM to genetic programming and EMADE
** Evolutionary Design for genetic algorithms: FPR, FNR, OnePointLeaf crossover, Custom selection tournament
** Normal emade parameters
** EMADE performed better than genetic and ML AUC.
* NAS:
** NNLearner for neural architecture, Learner just for various ML algos.
** Minimize accuracy errors and minimize num parameters
** Changes in emade include new set of datapairs, generating more adfs, change inputs to adf
** Creating a counter for the number of layers
* Bootcamp 2:
** EMADE had more complex primiteves and therefore had shorter trees than MOGP.
** Generations were running super slowly
** Comparing ML vs MOGP vs EMADE: emade performed the best
* Image Processing:
** 30 Generations
** Objectives: Precision-Recall AUC and number of parameters
** Semantic Operator results: issues in generating non-seeded valid neural network architectures. 
** Extending work on hyper features
** Implementing and comparing new selection methods
* Bootcamp 3:
** Dropped columns and one hot encoding
** Gender_embarked: takes into account both gender and embarked and set to 0, 1 based on likelihood of survival from plotting gender and embarked port
** Eval methods were FNR and FPR
** ML > MOGP > EMADE
** EMADE could not run for many generations
* Stocks
** ML algorithm in the indivudal uses historical data and technical indicators to predict future trigger gains
** Primitive analysis with technical indicator vs learner with confusion matrix
** Profit percentage, average profit per transaction, CDF of profit, variance of profit per transaction
** Comparing to TS Fuzzy model paper
** End goal is to get research paper, setting up for controlled experiments
* Bootcamp 4:
** MLP, Decision Tree, SVM, XGBoost
** Strongly typed GP
** Logical and mathematical operations as primitives
** CXOnePoint as mate and mutUniform as mutate
** MOGP > EMADE > ML in terms of AUC with pareto front
** MOGP has most individuals in pareto front, then EMADE, then ML
* Modularity:
** Exploring building blocks that can help with the genetic process
** ARLs: Adaptive representation through learning
** Introducing reusability and hopefully dynamically evolve these sections
** evaluation -> selection -> genetic operations -> arls -> evaluation
** Increasing complexity of ARLs by increasing the tree depth
** Stock runs with ARLs


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finishing up presentation notes
|Complete
|October 25, 2021
|October 25, 2021
|November 1, 2021
|-
|}

== October 18, 2021 ==

'''Team Meeting Notes:'''
* Midterm presentations on 10/25
* Some pace stuff to set runs at:
** 1 master - 1 core
** numhosts - how many worker jobs
** num cpu per worker (multiplicative with numhosts)
** runtime in hours
** For example, 3 num hosts, 4 num cpu per worker, and 8 hour run = 3 x 4 x 8 = 96 cpu hours

'''Subteam Meeting Notes:'''
* Prepping for presentation
** Literature review slides (3 papers + visualizations)
** Dataset prep
** Current EMADE status runs on QA
** New eval methods
** Future plans

'''Personal Work:'''
* Working on BiDAF extension model with BERT and Highway Network
* BERT is a pretrained model for embedding
* Highway network optimize networks and increasing their depth using non-linear transforms called a transform gate and carry gate to create an "information highway"
* https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15848195.pdf


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Working on presentation slides (paper on BiDAF extension)
|Complete
|October 18, 2021
|October 25, 2021
|October 25, 2021
|-
|}


== October 11, 2021 ==

'''Team Meeting Notes:'''
* Fall break, no in person meeting
* Hackathon on 10/16

'''Subteam Meeting Notes:'''
* 304 emade branch that needs to be merged with nn-vip
* A lot of merge conflicts...
* Finding seeds based on state of the art models. Examples include BiDAF, BERT, XLNET, etc.
* Continuing this work on the hackathon date (10/16)


'''Personal Work:'''
* Created individual seeds for the baseline EMADE run
* https://docs.google.com/document/d/1id8NqEuLTB7ds_75bMjUKTYzc1aBUD6-0TaOfPCsW5c/edit
 
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read through state of the art models and create string seeds
|Complete
|October 11, 2021
|October 18, 2021
|October 18, 2021
|-
|Attend hackathon
|Complete
|October 16, 2021
|October 16, 2021
|October 16, 2021
|-
|}


== October 4, 2021 ==

'''Team Meeting Notes:'''
* There was feature/implementation already coded for handling multiple data pairs
* Getting the infrastructure set up by starting with the input xml for the QA SQuAD dataset
* Hackathon for midterm presentation on 10/16 from 1pm-5pm

'''Subteam Meeting Notes:'''
* Look into PACE-ICE guide setup that Cameron made
* Sure way to test for emade and pace ice setup is to test with the Amazon dataset
* Can also use the single individual evaluator in emade
* Subdivide into 2 main tasks:
** Look through the difference between nn-vip branch and cache v2 branch to know what differences will need to be implemented or merged
** Look through SQuAD dataset for potential individuals to seed for emade runs. Looking through individuals that scored high on the leaderboards for SQuAD dataset would be a good place to start. Also visualizing these individuals would be nice


'''Personal Work:'''
* Planning to start these this weekend:
* Follow Cameron PACE-ICE setup to test EMADE and PACE with Amazon dataset
* Looking for high scoring individuals to use as seeds for emade runs and visualize the individuals.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Watch Cameron PACE-ICE Guide
|Complete
|October 6, 2021
|October 11, 2021
|October 11, 2021
|-
|Look for high scoring individuals for potential seeds
|Complete
|October 6, 2021
|October 11, 2021
|October 11, 2021
|-
|}


== September 27, 2021 ==

'''Team Meeting Notes:'''
* Team switch: From Stocks (non-paper) subteam to NLP
* NLP Subteam meeting times: Wednesday at 2pm


'''Subteam Meeting Notes:'''
* Meeting on Wednesdays at 2pm. Also Monday meetings as well
* Intro to QA and NLP subteam
* Things that need to be implemented: 
** XML for SQuAD dataset
** NNLearner overloading
** New fork for code


'''Personal Work:'''
* Looked through Devan's quick run-through of QA:
** Asking questions and retrieving answers through natural language queries
** MAP (Mean Average Precision) and MRR (Mean Reciprocal Rank) are metrics used to evaluate QA systems
** Little confused on the model structure (Character Embedding layer, word embedding layer, etc.)
* SQuAD dataset is a reading comprehension dataset based on a set of Wikipedia articles


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read through Q/A presentation
|Complete
|September 27, 2021
|October 4, 2021
|October 4, 2021
|-
|Look through SQuAD dataset
|Complete
|September 27, 2021
|October 4, 2021
|October 4, 2021
|-
|}

== September 20, 2021 ==
'''Team Meeting Notes:'''
* Image processing based stock analysis is a pretty cool idea
** Technically could engineer unlimited amount of features
** Initial rules article seems improbable since no rules are used in addition to EMADE
** Difficulty finding articles specifically related to stock generalization

'''Subteam Meeting Notes:'''

* Rules article does not seem significant since it does not add rules as an additional parameter
* Difficulty getting direction as senior members are busy with stock research paper

'''Personal Work:'''

* Took notes on https://ieeexplore.ieee.org/abstract/document/4598507 (An empirical study of Genetic Programming generated trading rules in computerized stock trading service system)
** Trades based on rules generated
** Rules follow a tree based structure
** Comparison of GP tree based algorithm to MACD indicator and buy/hold strategy
** Data used was 30 companies from the Dow Jones Industrial Average (DJIA).
** Rules are used but they are basically the same thing as individuals generated from EMADE.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meeting with Dr. Zutty to figure out Stock Subteam
|In Progress
|September 26, 2021
|September 28, 2021
|September 28, 2021
|-
|}


== September 13, 2021 ==
'''Team Meeting Notes:'''
* Focus on literature review of different stock articles
** Do not just pick a random article: Reputable vs code base
** Reputable articles have a lot of citations, however oftentimes do not have a Github or code base available.
** Less reputable articles such as medium articles can still provide good insight and implementation detail since they usually have python code.

'''Subteam Meeting Notes:'''
* Narrowing focus of research idea to stock generalization and fundamental analysis
** Stock generalization can be described as having an individual that can be proficient in inherently different stocks (or different sectors).
** Previous stock work has shown an individual perform well on a single stock but then underperform greatly on a different stock
** Fundamental analysis in contrast with technical analysis for picking stocks. Also consider combining the technical and fundamental techniques.
** Some issues that can arise: Fundamental data is greatly less than technical analysis data since usually based on quarterly balance sheets.

'''Personal Work'''
* Set up Pace-Ice with Emade
* Looking into the 2 areas of focus: stock generalization and fundamental analysis.
** Article focused on fundamental analysis: https://ir.lib.uwo.ca/cgi/viewcontent.cgi?article=8423&context=etd

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up Pace-Ice with Emade
|Complete
|September 13, 2021
|September 20, 2021
|September 20, 2021
|-
|Literature review on fundamental analysis and stock generalization
|Complete
|September 13, 2021
|September 20, 2021
|September 20, 2021
|-
|}




== September 6, 2021 ==
'''Team Meeting Notes:'''
* No team meeting due to Labor Day

'''Personal Work'''
* Review brainstorming session for stocks team. Some of the things covered:
** Expanding data set to include more years (2008 - 2020). Also focusing more on input data rather than hyperparameters
** Adding fundamental analysis 
** Read paper: Algorithmic Financial Trading with Deep Convolutional Neural Networks: Time Series to Image Conversion Approach
*** Unique approach focused on image processing with the CNN-TA model using a 2D Convolutional Neural Network
*** Based on image processing involving at least 15 different technical indicators. 
*** From the images of time series data, compare different strategies to the train model results. Example of a common strategy is Buy & Hold.

Self-grade VIP notebook (trouble uploading image, just going to describe the columns):
* Name & contact info: 5/5
* Teammate names and contact info easy to find: 3/5
* Organization: 5/5
* Updated at least weekly: 10/10
* Main meeting notes: 4/5
* Sub-teamsâ€™ efforts: 9/10
* To-do items: clarity, easy to find: 5/5
* To-do list consistency (weekly or more): 9/10
* To-dos & cancellations checked & dated: 4/5
* Level of detail: personal work & accomplishments: 14/15
* References (internal, external): 9/10
* Useful resource for the team: 13/15
* Total: 90

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete self-eval for notebook with rubric
|Complete
|September 6, 2021
|September 13, 2021
|September 13, 2021
|-
|}

== August 30, 2021 ==
'''Team Meeting Notes:'''
* Team rankings decided

'''Personal Work'''
* Personal rankings
** Stocks: 1
** NLP: 2
** Modularity: 3
** Interpretability: 4

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Find out subteam meeting schedule
|Complete
|August 30, 2021
|September 6, 2021
|September 6, 2021
|-
|}

== August 23, 2021 ==
'''Team Meeting Notes:'''
* Moving from previous VIP wiki to GitHub wiki
* GitHub wiki can be cloned and pulled locally for editing
* Background on old teams such as Modularity, Stocks, EZGCP, NLP
* Brainstorm for new teams:
** Covid
** Genetic Fundamentals
** Image Processing
** Data Science Automation

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Brainstorm subteam ideas
|Completed
|August 23, 2021
|August 30, 2021
|August 30, 2021
|-
|Rank subteams (4)
|Completed
|August 23, 2021
|August 30, 2021
|August 30, 2021
|-
|}

== April 30, 2021 ==
'''Final Presentation Notes:'''
* Stocks group:
** Implemented many new technical indicators as features for their stocks. 
** Utilized many statistical charts and calculations for their analysis
** Team subdivided into 3 main groups: research, statisical analysis, EMADE
* EZGCP group:
** Focuses on using DAGs instead of EMADE tree structure
** Ways for improvements include longer runs (past 12 gen)
** Analysis on which layers were used most and implement them in EZGCP first
* NLP group:
** Shoutout to Cameron for his tree visualization tool
** Amazed that the training set was only 5% of the data yet they tested on over 400000 data points
** Ways for improvement include analyzing a specific individual and why the characteristics of that individual may have allowed it to perform well

Great job to all groups and good work on the presentation modularity team!

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Notebook by May 1, 2021
|Completed
|April 30, 2021
|May 1, 2021
|May 1, 2021
|-
|}

== April 26, 2021 ==
'''Team Meeting Notes:'''
* Weekly scrum meeting:
* Final presentation prep. Modularity team will be comparing Titanic and MNIST datasets based on a baseline run, previous ARL implementation, and current ARL implementation.
* Work on putting together presentation slides as well as practice presentation.
* Hypervolume may not work properly (3 objectives). We will be using 2 objectives instead based on cohen and f1 accurarcy.

'''Sub Team Notes:'''
* Previous runs to find best objectives and seeding files were considered exploratory runs. Complete experimental runs with new seeding file and objective functions.
* Everyone should contribute to presentation slides. Contributions include what you did, analyzed, future work, etc.

'''Personal Work:'''
* Worked with team on Full EMADE run with new seeding file and new objective functions resulted in 316 valid individuals as compared to previous runs which had less than 10 valid individuals.
* Analyzed specific individual in seeding file on why it performed well:

[[files/Learner22.png]]

* Analysis: The scalar multiply primitive greatly increases all pixel values that are non black while leaving pixels that have black values or 0 unchanged. Then, the values of the pixels are summer together. The model is then better able to distinguish different black to white contrasts such as the number 1 compared to the number 8 and have a higher accuracy in classification
* Talked with Gabe on ideas for possible improvements for seeding file: Obtaining more diverse individuals, using same objective functions, looking into optimal number of seeds to use, giving definite reason as to why to include a specific seeded individual. Added slide to presentation.



'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finish presentation slide for seed intro, seed examples, and improvements to seed
|Completed
|April 14, 2021
|April 15, 2021
|April 15, 2021
|-
|Complete experiment run 1, 2 using new objective functions and seed file
|Completed 
|April 26, 2021
|April 27, 2021
|April 27, 2021
|-
|Complete experiment run 3, 4 using new objective functions and seed file
|Completed 
|April 28, 2021
|April 29, 2021
|April 29, 2021
|-
|Analyze reasoning for sample seeds
|Completed 
|April 29, 2021
|April 30, 2021
|April 30, 2021
|-
|Finish up notebook by Saturday
|Completed 
|April 26, 2021
|May 1, 2021
|May 1, 2021
|-
|}

== April 19, 2021 ==
'''Team Meeting Notes:'''
* Weekly scrum meeting:
* Getting setup for final presentation on April 30. Team prepares to present findings based on current runs on MNIST as well as using the new ARL architecture being developed
* Looking into different objectives other than F1 and accuracy. Dr. Zutty mentioned other examples such as precision vs. recall, analyzing multi objective space based on confusion matrix.

'''Sub Team Notes:'''
* Issues related to not getting enough valid individuals: No tradeoff space between the different objectives. Hard to optimize because only 1 fitness value able to use
* Seeding file: With a better seeding file, a better run can be made.

'''Personal Work:'''
* Tasking for this week is to build another seeding file based on the 9 valid individuals from the previous week. Analyze to see if the seeding file produces more valid individuals.
* Tasked to work with Shiyi and Rishit to combine seeding files and run EMADE. Send Gabe successful seeding file
* Valid individuals using the new seeding file was over 200, which is much better than the old seeding file which often got less than 10 valid individuals. Here was the seeding file we used:

    Learner(ToUint8(ThresholdBinaryInverseMask(ARG0, 2, 3, 100.0), passTriState(1), passQuadState(3)), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(ToUint8(ThresholdBinaryInverseMask(ARG0, 2, 3, 10.0), passTriState(1), passQuadState(3)), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(ToUint8(ThresholdBinaryInverseMask(ARG0, 2, 3, 10.0), passTriState(1), 3), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(ToUint8(ThresholdBinaryInverseMask(ARG0, 2, 3, 1.0), passTriState(1), passQuadState(3)), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(ToUint8(ScalarMultiply(ARG0, passTriState(passTriState(passTriState(1))), passQuadState(passQuadState(passQuadState(passQuadState(3)))), passFloat(myFloatAdd(10.0, 10.0))), passTriState(2), passQuadState(3)), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(ToUint8(ScalarMultiply(ARG0, passTriState(passTriState(1)), passQuadState(passQuadState(passQuadState(passQuadState(3)))), passFloat(myFloatAdd(10.0, 10.0))), passTriState(2), passQuadState(3)), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(ToUint8(ARG0, passTriState(2), passQuadState(passQuadState(passQuadState(1)))), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(ToUint8(ARG0, passTriState(2), passQuadState(passQuadState(3))), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(ToUint8(ARG0, passTriState(2), passQuadState(3)), ModifyLearnerList(learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None), [4, -5]))
    Learner(ToUint8(ARG0, passTriState(2), passQuadState(3)), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(MySum(ToUint8(ARG0, passTriState(2), passQuadState(myAnd(greaterThan(myFloatSub(myFloatMult(100.0, 1.0), myFloatDiv(100.0, 0.1)), 
    myFloatAdd(myFloatDiv(-4.741846291961977, 0.01), myFloatAdd(0.1, 1.564728841380587))), equal(ifThenElseFloat(greaterThanEqual(0.19380559630370886, 0.661611502441847), 
    ifThenElseFloat(trueBool, 100.0, 10.0), myFloatMult(10.0, 1.0)), myFloatMult(myFloatAdd(10.0, 0.1), passFloat(1.0)))))), 2, myIntAdd(lessThanOrEqual(-2.5879193132975575, 
    0.01), passTriState(0))), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(MySum(ToUint8(ARG0, passTriState(2), passQuadState(3)), 2, myIntAdd(lessThanOrEqual(-2.5879193132975575, 100.0), passTriState(0))), learnerType('KNN', {'K': 3, 
    'weights': 0}, 'SINGLE', None))
    Learner(MySum(ToUint8(ARG0, passTriState(2), passQuadState(3)), 2, myIntAdd(lessThanOrEqual(-2.5879193132975575, 0.01), passTriState(0))), learnerType('KNN', {'K': 3, 
    'weights': 0}, 'SINGLE', None))
    Learner(MySum(ScalarMultiply(ARG0, passTriState(passTriState(2)), passQuadState(passQuadState(passQuadState(passQuadState(passQuadState(passQuadState(3)))))), 
    passFloat(myFloatSub(-4.529968443741202, 4.571234818546902))), 2, 1), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(MySum(ScalarMultiply(ARG0, passTriState(passTriState(2)), passQuadState(passQuadState(passQuadState(passQuadState(3)))), passFloat(myFloatSub(2.6188999083341127, 
    10.0))), 2, 1), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(MySum(ScalarMultiply(ARG0, passTriState(passTriState(2)), passQuadState(passQuadState(passQuadState(passQuadState(3)))), passFloat(myFloatSub(-4.529968443741202, 
    10.0))), 2, notEqual(myFloatAdd(myFloatDiv(0.1, 1.0), myIntToFloat(3)), myIntToFloat(myIntDiv(7, 65)))), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(MySum(ScalarMultiply(ARG0, passTriState(passTriState(2)), passQuadState(passQuadState(passQuadState(passQuadState(3)))), passFloat(myFloatSub(-4.529968443741202, 
    10.0))), 2, notEqual(1.0, myIntToFloat(myIntDiv(7, 65)))), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(MySum(ScalarMultiply(ARG0, passTriState(passTriState(2)), passQuadState(passQuadState(passQuadState(passQuadState(3)))), 
    passFloat(myFloatSub(-4.5161830325958805, 10.0))), 2, 1), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(MySum(ScalarMultiply(ARG0, passTriState(passTriState(2)), passQuadState(passQuadState(passQuadState(passQuadState(0)))), passFloat(myFloatSub(4.4154845028433485, 
    10.0))), 2, 1), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(MySum(ScalarMultiply(ARG0, passTriState(passTriState(2)), passQuadState(passQuadState(passQuadState(passQuadState(0)))), passFloat(myFloatSub(0.7816936070337128, 
    10.0))), 2, 1), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(MySum(ScalarMultiply(ARG0, passTriState(passTriState(2)), passQuadState(passQuadState(passQuadState(passQuadState(0)))), 
    passFloat(myFloatSub(-4.5161830325958805, 10.0))), 2, 1), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(MySum(ScalarMultiply(ARG0, passTriState(passTriState(2)), passQuadState(passQuadState(passQuadState(passQuadState(0)))), 
    passFloat(myFloatSub(-4.5161830325958805, -1.3398941928711094))), 2, 1), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(MySum(MySum(ScalarMultiply(ARG0, passTriState(passTriState(2)), passQuadState(passQuadState(passQuadState(passQuadState(0)))), myFloatMult(3.4759895791027127, 
    100.0)), 1, 1), 2, myIntDiv(1, 10)), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(MySum(MorphCloseRect(ARG0, passTriState(passTriState(2)), passQuadState(passQuadState(3)), 4, notEqual(0.8740984530290694, 0.01)), 2, 
    myIntDiv(myOr(lessThan(myFloatDiv(1.0, 10.0), myFloatDiv(0.01, 1.0)), trueBool), 10)), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(HighpassFourierShift(ARG0, 2, 3, 100), ModifyLearnerList(learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None), [4, -5]))
    Learner(Cv2Sqrt(MySum(ScalarMultiply(ARG0, passTriState(passTriState(2)), passQuadState(passQuadState(passQuadState(passQuadState(0)))), 
    passFloat(myFloatSub(-4.5161830325958805, -1.3398941928711094))), 2, 1), passTriState(passTriState(2)), passQuadState(passQuadState(1))), learnerType('KNN', {'K': 3, 
    'weights': 0}, 'SINGLE', None))
    Learner(Cv2Sqrt(ARG0, passTriState(passTriState(2)), passQuadState(passQuadState(passQuadState(passQuadState(passQuadState(passQuadState(passQuadState(0)))))))), 
    learnerType('KNN', {'K': 3, 'weights': 0}, 'BAGGED', None))
    Learner(Cv2Sqrt(ARG0, passTriState(passTriState(2)), passQuadState(passQuadState(passQuadState(1)))), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(Cv2Sqrt(ARG0, passTriState(passTriState(2)), passQuadState(passQuadState(1))), learnerType('SVM', {'C': 1.0, 'kernel': 0}, 'ADABOOST', {'n_estimators': 50, 
    'learning_rate': 1.0}))
    Learner(Cv2Sqrt(ARG0, passTriState(passTriState(2)), passQuadState(passQuadState(1))), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))
    Learner(Cv2Sqrt(ARG0, passTriState(passTriState(2)), passQuadState(1)), learnerType('KNN', {'K': 3, 'weights': 0}, 'SINGLE', None))

* This will be the seeding file used in experimental runs.
* Analysis: Many of the individuals are very similar in their main models and primitives. Often only differ by a single parameter or an additional simple primitive.
* Question: What is a good number for the number of individuals (i.e. can't put infinitely number of seeds)


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Able to produce more valid individuals with better seeding file from my previous run? (Successful)
|Completed
|April 14, 2021
|April 15, 2021
|April 15, 2021
|-
|Run EMADE with combined seeding file with Rishit and Shiyi
|Completed 
|April 24, 2021
|April 25, 2021
|April 25, 2021
|-
|}

== April 12, 2021 ==
'''Team Meeting Notes:'''
* Weekly scrum meeting:
* Currently we do not have a tradeoff space for pareto frontier because both objectives have the same values
* Looking for ways to implement past statistics lecture with the individuals from the EMADE runs
* Not many individuals created with the best runs

'''Sub Team Notes:'''
* Run EMADE and analyze individuals.
* In templates, there is a reuse parameter. When set to 0, it assumes an empty database. When set to 1, it will use where the last run ended as a starting point
* Best to do a continuous run, since stopping and then starting introduces extra randomness.
* Worker processes will continue to run until all individuals in the database have been evaluated.
* To check for EMADE completion, check that all individuals in the database have a status of EVALUATED or WAITING_FOR_MASTER
* Later generations can take longer since they are more valid individuals later on than in the beginning

'''Personal Work:'''
* Performed another exploratory run using my laptop as master process and 2 colab instances as worker processes. 
* I tried stopping and then continuing a EMADE run. It led to much more individuals as well as an optimization value associated with different individuals (0 or 1).
* This above run is not the correct pratice. Hence, I ran another continuous run with the same setup as before
* Results from correct EMADE run:

[[files/Mnist_test_gy1.PNG]]

* Failed to get more than 9 valid individuals. This run only obtained 6 valid individuals
* Questions confused on still:
* I have several individuals at the end of the EMADE run who are WAITING_FOR_MASTER. Are they completed?



'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Try to get more than 9 valid individuals after complete EMADE run (Unsuccessful)
|Completed
|April 17, 2021
|April 18, 2021
|April 18, 2021
|-
|Restarted run after the master process crashed. (Results are not accurate)
|Completed 
|April 14, 2021
|April 14, 2021
|April 14, 2021
|-
|}

== April 5, 2021 ==
'''Team Meeting Notes:'''
* Statistics lecture from Dr. Zutty
* Common statistics review: Mean, Variance, Standard Deviation
* Hypothesis Testing: Compute the probability of observing some sample at least as extreme. Based on P(sample | hypothesis). Called a p-value
* Threshold to reject the hypothesis is based on some alpha. Common value for alpha = 0.05
* False positive = Type II error = Beta. Statistical power of a hypothesis test is the probability of not making Type II error, or 1 - Beta
* t-statistic: How far we are from the mean by standard deviation. Compute using t-table
* Usually better to use a 2-tailed t test for statistical analysis
* Can compute p-value from t-statistic with python scipy function

'''Sub Team Notes:'''
* Database for emade runs hosted on AWS.
* Seeding database with current individuals known to run well
* General pipeline for EMADE runs: Create a new schema for a newrun in the AWS database. With an empty database, seed the database. Initiate a master process. Add as many worker processes needed. Note: There should only be ONE master process running at a time.
* Use autoclicker script to ensure that google colab does not detect inactivity
* Better to have the master process running on a local machine and worker processes running on the cloud.

'''Personal Work:'''

* Error from previous week with reinstall.sh is fixed with the following lines:
    !sudo apt install dos2unix
    !find . -type f -print0 | xargs -0 dos2unix

* During my run with EMADE, not entirely why EMADE stopped before completing. Received warnings concerning:
    SAWarning: Coercing Subquery object into a select() for use in IN(); please pass a select() construct explicitly
    RuntimeWarning: invalid value encountered in double_scalars
    individuals[j].fitness.values[i]

* Warning does not stop EMADE run

* Run exploratory run to reach gen 49. I completed a run with 2 alt colab instances and my laptop as master process. Results and MYSQL query to obtain valid individuals shown below:

[[files/Mnist_test_gy.PNG]]



'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|EMADE run again, check to see when the warnings occur. Look for potential fix (No fix for warning)
|Completed
|April 7, 2021
|April 8, 2021
|April 8, 2021
|-
|Reach max generation, currently hardcoded at 49. 9 valid individuals
|Completed 
|April 8, 2021
|April 9, 2021
|April 9, 2021
|-
|}

== March 29, 2021 ==
'''Team Meeting Notes:'''
* Assignment of main teams.
* Assigned to modularity team
* Monday meetings are in a scrum sense. Each team will give weekly reports and updates as well as report any issues they are having.

'''Sub Team Notes:'''
* Introduction to current team members on Modularity as well as new students
* Brief introduction of ARLs and what the modularity team aims to prove
* Tasking: Read over various articles as to which much of the modularity research is based on.
* Outlook: Modularity team will be focused on performing runs on MNIST dataset.

'''Personal Work:'''

Article from Modularity Meeting: Discovery of Subroutines in Genetic Programming

* ARL stands for adaptive representation in learning
* Identify useful blocks of code that appear as a result of genetic operations. An informed, or heuristic technique, was employed in the selection of what could be useful blocks of code.
* Create a number of individuals that you know are good to replace low fitness individuals (seeding).

Article from Modularity Meeting: An Analysis of Automatic Subroutine Discovery in Genetic Programming

* Compares different heuristics for code selection and test their effectivness with ARLs
* Proposes and analyses a new heuristic, the Saliency
* Two extensions to ARL: diffusion of the new subroutines through mutation and the MaxFit technique.

Issues: 
* Running into issues with bash reinstall.sh with modularity colab notebook



'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Pull the latest branch for mnist runs
|Completed
|March 31, 2021
|March 31, 2021
|March 31, 2021
|-
|Create and upload emade-cloud folder to google drive for google colab runs
|Completed 
|March 31, 2021
|March 31, 2021
|March 31, 2021
|-
|Run EMADE installation and seeding files. Begin EMADE run.
|Completed 
|March 31, 2021
|April 4, 2021
|April 4, 2021
|-
|}

== March 22, 2021 ==
'''Team Meeting Notes:'''
* Bootcamp student presentations on running EMADE.
* Bootcamp subteams had varying results when running emade, difficult to use in the beginning.
* Returning students gave presentations over their specific topics. I was particularly interested in stocks, followed by modularity, NLP, and ezgcp.
* EZGCP team is dag based version of EMADE (tree)
* Modularity team is an additional step before another generation to seed in good individuals
* Stocks team is looking to predict price trends (difficult) using emade


'''Sub Team Notes:'''
* Great job to everyone on the team, really good presentation. Good luck on your main teams!
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete team selection form 
|Completed
|March 22, 2021
|March 29, 2021
|March 29, 2021
|}

== March 17, 2021 ==
'''Team Meeting Notes:'''
* Debugging errors for EMADE runs on titanic dataset.
* Met in breakout room with subteam to figure out remote connection to master database
* Designated Nishant to run master process 
* Key points to ensure that remote connection to MySQL database includes: granting user permissions, allow for connections in MySQL settings, connect to GT VPN, add firewall rules for port 3306 
* Emade run with workers process saw a increase in generation counts (i.e. running more efficiently)

'''Sub Team Notes:'''
* Confused about objective functions in EMADE as they were based on FP and FR rather than FPR and FNR.
* Asked in general chat and Anish provided guidance into looking into objective function code base as well as the FPR function in emade.
* As a team discussed and edited the code within emade to produce FPR and FNR rates for the pareto front.
* Group decided to run emade overnight to see how many generations we could get (got to gen 26)

'''Personal Work:'''
* I worked on designing the slideshow presentation, compiling our previous presentations into a more succinct manner so that we could also talk more about EMADE and our design choices there.
* Running into issues plotting the pareto front due to indices mismatch. I wrote the code to fix the pareto front.

Code for fixing pareto front connections: 

    ind_sort = fitness_1.argsort()
    fitness_1 = fitness_1[ind_sort]
    fitness_2 = fitness_2[ind_sort]

FPR Edit:

[[files/FPR_edit.png|alt=FPR]]

FNR Edit:

[[files/FNR_edit.png|alt=FNG]]


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run at least 20 generations of EMADE
|Completed
|March 17, 2021
|March 18, 2021
|March 18, 2021
|-
|Analyze max generation pareto front and plot with AUC
|Completed 
|March 18, 2021
|March 20, 2021
|March 20, 2021
|-
|}

== March 10, 2021 ==
'''Team Meeting Notes:'''
* For titanic EMADE, there is titanic folder in the templates folder 
* Editing input file with dbConfig with the right IP address and MySQL username and passwords 
* Input file performs 5 fold monte carlo trail over the test and train data and averages all the results
* Evaluation function tag uses whichever method specified from the evalFunctions.py file
* Worker processes will allow all group members to work as a cluster and increase the efficiency of EMADE.
* The command for the worker process is: python src/GPFramework/launchGTMOEP.py templates/input_titanic.xml -w
* The IP address for the DBconfig needs to match whoever is the host for the master process

'''Sub Team Notes:'''
* Running emade has led to many problems. 
* Connection to the localhost works 
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Peer Evaluation
|Completed
|March 1, 2021
|March 5, 2021
|March 5, 2021
|}







== March 3, 2021 ==
'''Team Meeting Notes:'''
* Continuing presentations for ML vs. MOGP on titanic problem 
* Brief introduction to EMADE

'''Sub Team Notes:'''
* Installation of EMADE:
** Problems with opencv. Fixes included installing a clean version of anaconda. 
** Conda usually does a good job correcting dependencies, but there were some issues with dependencies that required additional installs.
** Correct MySQL server version. Installed GUI version.
** Installed scoop using Windows powershell
*Links to official installation for anaconda, MySQL, Scoop
**https://www.anaconda.com/products/individual
**https://dev.mysql.com/downloads/mysql/ (version 8.0.22 worked for subteam 2)
**<nowiki>https://scoop.sh/</nowiki>
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Peer Evaluation
|Completed
|March 1, 2021
|March 5, 2021
|March 5, 2021
|}



== February 24, 2021 ==
'''Team Meeting Notes:'''
* Group presentations for ML vs. MOGP on titanic problem. 
* Work on installing EMADE for next presentation

'''Sub Team Notes:'''
* Good work to everyone on Subteam 2 for a good slideshow setup and presentation.
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Peer Evaluation
|Completed
|March 1, 2021
|March 5, 2021
|March 5, 2021
|-
|Install EMADE
|Completed 
|February 24, 2021
|March 3, 2021
|March 3, 2021
|-
|Finish up notebook for midterm check
|Completed
|February 24, 2021
|March 3, 2021
|March 1, 2021
|}

== February 17, 2021 ==
'''Team Meeting Notes:'''
* Use genetic programming on titanic survivor problem and compare results to ML models
'''Sub Team Notes:'''
* Preprocess data is the same as the data used for ML to keep it consistent.
* Functions of our primitive tree take in 7 inputs for each feature
* Evaluation function is multi objective based on FPR and FNR
* Deap Functions used:
** Select Method: selSPEA2 works well with multiple objectives
** Mate: cxOnePoint
** Mutate: mutUniform
** Population of 300 was used and an evolutionary loop of 40 generations was run, with 0.5 probability of mating and 0.2 probability of mutation
** HOF (Hall of Fame) for pareto optimal set
[[files/MOGP Pareto Front.png|alt=MOGP Pareto Front|none|thumb|MOGP Pareto Front]]

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete group csv file for MOGP
|Completed
|February 17, 2021
|February 24, 2021
|February 23, 2021
|-
|Prep for group presentation
|Completed 
|February 17, 2021
|February 24, 2021
|February 24, 2021
|}






== February 10, 2021 ==
'''Team Meeting Notes:'''
* Titanic Survivor problem introduction 
* Kaggle Data set and competition problem 
* Walk through ipynb file on basic preprocessing and running machine learning models 
* Assign subteams and team problem: Run several machine learning models. Submit predictions.csv and find a way for all models to product pareto dominant solutions based on FPR and FNR 
'''Sub Team notes:'''

Preprocess data:
* Fill NaN age values by imputing the "titles" from the "name" column and using the median age of the specific titles. Then drop the "titles" column after
* One hot encoding for "gender" as to not give an advantage to a gender
* Fill NaN "fare" values by imputing the median fares based on the "Pclass". Drop not needed columns after
* Drop "cabin", "embarked", "ticket", and "name" columns
For my model classifier, I used a neural network.

NN Structure (All layers fully connected):
* 1 input layer with 7 nodes for each feature
* 2 hidden layers. First hidden layer uses 12 nodes. Second hidden layer uses 6 nodes. Number of nodes chosen after running some tests as seen below.
* 1 output layer with one node for the classification (0 or 1)
Testing code on initial structure: Recording accuracy vs average of 3 trials for specific node count
    from keras.models import Sequential
    from keras.layers import Dense
    acc = []
    nodes = []
    # i represents number of nodes
    for i in range(5, 40):
        total = 0
        # Compute average over 3 trials
        for j in range(3):  
            model = Sequential()
            model.add(Dense(i, input_dim=7, activation='sigmoid'))
            # second hidden layer has half the nodes as the first hidden layer
            model.add(Dense(i//2, activation='sigmoid'))
            model.add(Dense(1, activation='sigmoid'))
            model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
            model.fit(X_train.values, y_train.values, epochs=100)
            accuracy = model.evaluate(x = X_test.values, y = y_test.values, return_dict = True)
            total += (accuracy["accuracy"])
        acc.append(total / 3)
        nodes.append(i)
Visualization of accuracy vs node count:
[[files/Visualization of Accuracy vs Node Count.png|alt=Visualization of Accuracy vs Node Count|none|thumb|Visualization of Accuracy vs Node Count]]
Using the graph, I decided on a 12 node count. This would mean the first hidden layer has 12 nodes and the second hidden layer has 6 nodes.

Here is the code for running for a neural network on the structure described above:
    from keras.models import Sequential
    from keras.layers import Dense
    model = Sequential()
    model.add(Dense(12, input_dim=7, activation='sigmoid'))
    model.add(Dense(12//2, activation='sigmoid'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.fit(X_train.values, y_train.values, epochs=100)
    accuracy = model.evaluate(x =  X_test.values, y = y_test.values, return_dict = True)
    print(accuracy)
    {'loss': 0.39511963725090027, 'accuracy': 0.826815664768219}

Here is the results of the model on the X test split:
    Confusion Matrix
    [[104  13]
    [ 18  44]]
    True Negatives 104
    False Positives 13
    False Negatives 18
    True Positives 44
    FPR: 0.22807017543859648
    FNR: 0.14754098360655737

'''Action Items:'''
{| class="wikitable"