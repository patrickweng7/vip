= Team Member =
Team Member: Haritaa Mourougassamy

Email: haritaam@gatech.edu

Interests: Microbiology, Foreign Language, Music

= Spring 2021 =

== January 20, 2021 ==

=== '''Lecture Notes''' ===
* Genetic algorithms essentially follow the process of evolution
*# Mating/mutation of individuals from previous population
*# Individuals' fitness is evaluated (could it be better?...natural selection)
*# If fitness is not good enough, crossover/mutate and change; evaluate next generation once again
*# Once the fitness cannot get any better, algorithm stops (no more mutations)
* '''Individuals''': represented as DNA, so can be mutated or work with genetic algorithms
* '''Population''': a group! Never looking for just one solution, but for the entire group
* '''Objective''': can be seen as a "score on a test"
* '''Fitness''': not concrete, relative comparison to the rest of the population
** Relating it back to the test score analogy above, this would be considered the "grade curve"
* '''Evaluation''': (missed explanation for this keyword)
* '''Selection''': "survival of the fittest"; fittest individuals have the highest probability of passing genes on
* '''Fitness proportionate''': parents are chosen through random selection, all objectives are normalized, then a random selection from the scores
** Using the test analogy: raw scores are divided by the sum of a vector of all the scores (What percent of the total score did an individual make up? This determines their probability of being selected for mating)
* '''Tournament''': Determine desired number of individuals, then whoever has a better objective score out of all the individuals selected wins (gets selected for mating)
* Think of chromosomes/strands of DNA as a list; genes are individual elements in that list
* '''Single point crossover''': using two lists of same length, pick a random index and swap everything after that index
** Everything before chosen index pulled from first list, everything after chosen index pulled from second list
** Can do vice versa, creates two children
* '''Double point crossover''': Select two indices, alternate between first and second list across these swap points
* '''Mutation''': change a ''single'' gene of a parent
* Genetic algorithm for a population (rather than an individual):
*# Randomly initialize and determine fitness of population 
*# Repeat until best individual is good enough:
*## Select parents from population
*## Perform crossover on parents to create a new (?) population
*## Perform mutation of population
*## Determine fitness of population
* '''One max problem''': goal is to get a list with all 1's (1 is a desired gene --> boolean)
** Evaluator: the ''sum'' of the elements of the list (determines the objective score)

=== Action Items ===
{| class="wikitable"
!'''Task'''
!'''Status'''
!'''Date Assigned'''
!Due Date
!'''Date Completed'''
|-
|Set up notebook
|Completed
|1/20/21
|1/27/21
|1/26/21
|-
|Join Slack and Piazza
|Completed
|1/20/21
|1/27/21
|1/26/21
|-
|Install Jupyter Notebook
|Completed
|1/20/21
|1/27/21
|1/20/21
|-
|Install DEAP
|Completed
|1/20/21
|1/27/21
|1/27/21
|-
|Jupyter Notebook Tutorial
|Completed
|1/20/21
|1/27/21
|1/27/21
|-
|One Max Problem
|Completed
|1/20/21
|1/27/21
|1/27/21
|-
|No Queens Problem
|Completed
|1/20/21
|1/27/21
|1/27/21
|}

== January 27, 2021 ==

=== Lecture Notes ===
* Consider the individual as a function (fed input data, will return output data) rather than using a function evaluator to find the individual’s objective score
** '''Objective score''': sum of individual elements from output data
* Representing programs as tree structure
** '''Primitives:''' nodes; represent functions
** '''Terminals:''' leaves; represent input parameters
** Flow in through bottom of tree, output at the root of tree (but function is not written in that order)
* '''Lisp preordered parse tree:''' operator followed by inputs (write it from the root down)
** Preorder traversal to find parse tree
** Find the operator first, followed by its two inputs, fill in the inputs recursively (see example below)[[files/Screen Shot 2021-02-17 at 11.26.26 PM.png|none|thumb|f(x) = (3*4)+1]]
*** Begin at root: [+, ''input 1'', ''input 2''] --> [+, *, ''input 3'', ''input 4'', 1] --> [+, *, 3, 4, 1]
* '''Crossover:''' select a random point in each tree, then swap subtrees
** <u>Subtrees:</u> selected points and everything below them
** As you go through evolution, use safe division to avoid issues like a "divide by 0"
* '''Mutation:''' insert, delete, or change a node
* Evaluating a tree: feed a number of input points into the function to get outputs, run f(x), measure error between outputs and true values

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Compile lecture notes into notebook
|Completed
|1/27/21
|2/3/21
|1/27/21
|-
|Lab #2, Part 1
|Completed
|1/27/21
|2/3/21
|2/3/21
|}

== February 3, 2021 ==

=== Lecture Notes ===
* Two algorithms may have their own unique benefits; our goal is to find a way to combine them into a single algorithm with the best of both worlds
** Like mating! What's attractive in a mate? --> What's attractive in an algorithm? (i.e. speed, memory, accuracy, etc.)
* '''Gene pool:''' set of genomes to be evaluated during the current generation
** <u>Genome:</u> genotypic description; ''GA:'' set of values; ''GP:'' tree structure, string
** <u>Search space:</u> set of all possible genomes (in automated algorithm design, it becomes the set of all possible algorithms)
*** Near infinite! It can be as big as you want it to be... We can't possibly evaluate all possible algorithms, so we use the genetic algorithm concept to explore the search space
* '''Objective space:''' evaluation of a genome associates individual with a set of scores (set of all objectives)
** <u>True positive:</u> how often we identify the desired object
** <u>False positive:</u> how often we identify something else as the desired object
* '''Objectives:''' set of measurements each genome is compared against (can be thought of as the phenotypes)
* '''Evaluation:''' takes us from a search space to an objective space (genotypic --> phenotypic)
* '''Type I Error''': actual positive, predicted negative (FN)
* '''Type II Error''': actual negative, predicted positive (FP)
* Depending on what we are trying to accomplish, we will choose algorithms that push max/min measures closer to either end
** '''Maximization measures:''' perfect algorithm will reside at the value (1,1)...bigger is better! (maximize things classified correctly
*** <u>Sensitivity/True Positive Rate (TPR)</u>: TPR = TP / P = '''TP / (TP + FN)''' (AKA hit rate or recall)
*** <u>Specificity/True Negative Rate (TNR)</u>: TNR = TN / N = '''TN / (TN + FP)'''
** '''Minimization measures:''' perfect algorithm will reside at the value (0,0)...smaller is better! (minimize things classified incorrectly) 
*** <u>False Negative Rate (FNR):</u> FNR = FN / P = '''FN / (TP + FN)''' = 1 - TPR
*** <u>Fallout/False Positive Rate (FPR):</u> FPR = FP / N = '''TN / (FP + TN)''' = 1- TNR = 1- SPC[[files/Screen Shot 2021-02-18 at 12.04.01 AM.png|center|thumb|163x163px|maximization measures]][[files/Screen Shot 2021-02-18 at 12.32.09 AM.png|center|thumb|172x172px|minimization measures ]]
* Other measures:
** <u>Precision/Positive Predictive Value (PPV)</u>: PPV = '''TP / (TP + FP)''' (maximization measure)
** <u>False Discovery Rate (FDR):</u> FDR = '''FP / (TP + FP)''' = 1 - PPV (minimization measure)
** <u>Negative Predictive Value (NPV):</u> NPV = '''TN / (TN + FN)''' (maximization measure)
** <u>Accuracy (ACC)</u>: ACC = (TP + TN) / (P + N) = '''(TP + TN) / (TP + FP + FN + TN)''' (maximization measure)
* '''Pareto optimal:''' no other individual in the population can outperform this individual on ALL objectives 
** <u>Pareto frontier:</u> set of all pareto individuals, each representing unique contributions
*** Want to drive algorithm selection by favoring pareto individuals but maintain diversity by giving all individuals some probabiity of mating
** Draw a box from some point to the origin; if any other point lies within that box, then that individual isn't pareto optimal (dominated on both the x and y axis by some other point)
*** In this case, only comparing two features, which is why it's acceptable to use the x and y axis
* '''Nondominated sorting genetic algorithm II (NSGA II):''' population is sorted into nondomination ranks (pareto ranks)
** <u>Pareto ranks:</u> find the pareto optimal set, then pretend those no longer exist. Not including those, find the ''next'' pareto optimal set. Then eliminate those and use the remaining, so on and so forth (like that one combinatorics graph algorithm). 
*** If smaller is better,m you've essentially created a ranking system of the best individuals
** Individuals are then selected by binary tournament (randomly pull two individuals out of the population and make them compete)
** Lower pareto ranks beat higher pareto ranks (i.e. rank 1 beats rank 3)
** Ties on the same rank are broken by <u>crowding distance:</u> summation of normalized euclidean distances to all points within the front
*** The higher crowding distance wins (means all individuals on that particular rank are further away, in a more unexplored area of the objective space)
* '''Strength Pareto Evolutionary Algorithm 2 (SPEA 2):''' 
** Each individual is given a strength S (how many others in population it dominates, look up and to the right to see how many are in its little box)
** Each individual receives a rank R (sum of strengths of the individuals that dominate it)
*** Since pareto individuals are nondominated, they receive a rank of 0
** A distance to the kth nearest neighbor (σ^k) is calculated and a <u>fitness</u> of R+1/(σ^k + 2) is obtained to use as a tie breaker if necessary 

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Compile lecture notes into notebook
|Completed
|2/3/21
|2/10/21
|2/3/21
|-
|VIP Self Graded Rubric
|Completed
|2/3/21
|2/10/21
|
|-
|Lab #2, Part 2
|Completed
|2/3/21
|2/10/21
|
|}

=== Self-Graded Notebook Rubric ===
{| class="wikitable"
| colspan="4" |Haritaa Mourougassamy
|
| colspan="21" |Automated Algorithm Design
|
| colspan="9" |1
|-
| colspan="4" |Student Name
|
| colspan="21" |VIP Team
|
| colspan="9" |Semester
|-
| colspan="2" |'''VIP Notebook Grading Rubric'''
| colspan="12" |Poor
| colspan="12" |Intermediate
| colspan="10" |Exemplary
|-
| rowspan="5" |Notebook Maintenance

(25)
|Name & contact info
| colspan="6" |0
| colspan="6" |1
| colspan="5" |2
| colspan="7" |3
| colspan="6" |4
| colspan="4" |'''5'''
|-
|Teammate names and contact info easy to find
| colspan="6" |0
| colspan="6" |1
| colspan="5" |2
| colspan="7" |3
| colspan="6" |4
| colspan="4" |'''5'''
|-
|Neat, legible, additional pages secured
| colspan="6" |0
| colspan="6" |1
| colspan="5" |2
| colspan="7" |3
| colspan="6" |4
| colspan="4" |'''5'''
|-
|Organization
| colspan="6" |0
| colspan="6" |1
| colspan="5" |2
| colspan="7" |3
| colspan="6" |4
| colspan="4" |'''5'''
|-
|Updated at least weekly
| colspan="6" |0
| colspan="6" |1
| colspan="5" |2
| colspan="7" |3
| colspan="6" |4
| colspan="4" |'''5'''
|-
| rowspan="2" |Meeting notes

(15)
|Group topics
| colspan="6" |0
| colspan="6" |1
| colspan="5" |2
| colspan="7" |3
| colspan="6" |4
| colspan="4" |'''5'''
|-
|Other individuals
| colspan="5" |0
| colspan="3" |1
| colspan="3" |2
| colspan="3" |3
| colspan="2" |4
| colspan="5" |5
| colspan="2" |'''6'''
| colspan="3" |7
| colspan="3" |8
| colspan="3" |9
| colspan="2" |10
|-
| rowspan="5" |Personal work & accomplishments

(35)
|To-do items:

clarity, easy to find
| colspan="6" |0
| colspan="6" |1
| colspan="5" |2
| colspan="7" |3
| colspan="6" |4
| colspan="4" |'''5'''
|-
|To-do list consistency (weekly or more)
| colspan="5" |0
| colspan="3" |1
| colspan="3" |2
| colspan="3" |3
| colspan="2" |4
| colspan="5" |5
| colspan="2" |'''6'''
| colspan="3" |7
| colspan="3" |8
| colspan="3" |9
| colspan="2" |10
|-
|To-dos & cancellations

checked & dated
| colspan="6" |0
| colspan="6" |1
| colspan="5" |2
| colspan="7" |3
| colspan="6" |'''4'''
| colspan="4" |5
|-
| rowspan="2" |Level of detail:

personal work & accomplishments
| colspan="12" |
| colspan="12" |Includes ideas, progress and results. 
| colspan="10" |Includes ideas, progress, results + '''explanations, justifications & reflections.'''
|-
|0
| colspan="3" |1
| colspan="3" |2
| colspan="2" |3
|4
| colspan="3" |5
| colspan="2" |'''6'''
| colspan="3" |7
| colspan="2" |8
| colspan="2" |9
| colspan="3" |10
| colspan="2" |11
|12
| colspan="3" |13
| colspan="2" |14
|15
|-
| rowspan="3" |Useful resource 

(25)
|References

(internal, external)
| colspan="5" |0
| colspan="3" |1
| colspan="3" |2
| colspan="3" |3
| colspan="2" |4
| colspan="5" |5
| colspan="2" |6
| colspan="3" |'''7'''
| colspan="3" |8
| colspan="3" |9
| colspan="2" |10
|-
| rowspan="2" |Useful resource for the team
| colspan="12" |
| colspan="12" |Useful: Someone familiar with the project would find sufficient answers.
| colspan="10" |Excellent: Useful to future group members; someone familiar with the field could follow the work.
|-
|0
| colspan="3" |1
| colspan="3" |2
| colspan="2" |3
|4
| colspan="3" |5
| colspan="2" |6
| colspan="3" |7
| colspan="2" |8
| colspan="2" |9
| colspan="3" |'''10'''
| colspan="2" |11
|12
| colspan="3" |13
| colspan="2" |14
|15
|-
|Comments:
|Column totals:
| colspan="12" |0
| colspan="12" |0
| colspan="10" |93
|-
| colspan="21" |
| colspan="5" |Total out of 100:
| colspan="10" |93
|}

== February 10, 2021 ==

=== Team Meeting Notes ===
Bootcamp Subteam 2, Team Leader: Karthik Subramanian
* Kaggle: machine learning challenge website
* Using Titanic Machine Learning from Disaster dataset
** Task: given a set of features (each set represents a passenger), predict whether each passenger survives or not
** Positive case: survivor
* Steps to take as a group:
*# Decide on a common set of features to process out of raw data
*# Decide on a fold of your test data test
*# For each person on the team: need a pareto dominant algorithm on false positive, false negative rates
*# All solutions should be codominant
* Assignment on canvas to upload individual testing fiels
* '''Fold:''' pick a certain percentage of the data to fit your algorithms, use the remaining percentage to score
** Once you have a pareto optimization you're satisfied with, run it against the test.csv
* Resources:
** Sample notebook on github respository
** Play with several different models and their hyperparameters
** pandas.pydata.org for pandas documentation
* Presenting on Feb 24: will talk about models created in this assignment

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Compile lecture notes into notebook
|Completed
|2/10/21
|2/17/21
|2/10/21
|-
|Find a time to meet with group
|Completed
|2/10/21
|2/17/21
|
|-
|Create kaggle account
|Completed
|2/10/21
|2/17/21
|
|}

== February 13, 2021 ==

=== Team Meeting Notes ===
Link to my personal notebook: https://colab.research.google.com/drive/1XzR0e26n-W91rTgx3bbvE08_SwRKE95-#scrollTo=KVRho4c6mcfr
* What is machine learning? Two versions: supervised and unsupervised
** This is supervised: we have the answer, we just have to get the model to learn it
** ML is basically asking the computer, given data, how to find the function that maps the data to some sort of output
* Downloaded needed CSV files, created individual copies of the colab notebook, put everything in our own personal folders in our drive and adjusted directories in the code
* Preprocessing: a way to clean your data; data won’t be perfect in the real world
** Handle missing values (NaN)
** Could fill in missing values using some function “full na?”
** Think about the features that matter most: what are the important features?
* Train[“name of column”]
* Train.shape gives us size of table
* Axis = 0 means row and 1 means column
* Implace means you don’t have to reassign to that variable (storage efficient)
* Take a look at old notebooks on kaggle to see what kinds of things they did (and get exposed to different types of coding terminology): <nowiki>https://www.kaggle.com/c/titanic/code</nowiki>
** <nowiki>https://www.kaggle.com/sufyansadiq/titanic-survival-predictions</nowiki>
** <nowiki>https://www.kaggle.com/magicard/titanic-survivors-data-analyze</nowiki>
** <nowiki>https://www.kaggle.com/vahidehdashti/preprocessing-titanic</nowiki>
** <nowiki>https://www.kaggle.com/vinothan/titanic-model-with-90-accuracy</nowiki>
* Outlier detection to prevent inaccurate predictions from skewed data
* Instead of filling in na age values with mean, use median as a countermeasure against skewed data from the outliers
* train.groupby(“Embarked”)[“Survived”].count()
* Based on the graph, not sure if there’s a reason more people from S embarked survived (might be a case of overfitting)
** George’s suggestion: try one algorithm with embarked dropped and one without it dropped (might
* Get the titles from names (names that are rare have higher precedence, names that aren’t as rare have lower precedence)
* Instead of using the general median to fill in ages, try filling in NaN in ages based on title groupings (i.e. NaN for Mr., use median of known Mr.)

* Find a way to quantity males and females into numbers because data models love numbers! But don’t want to say a 1 is “above” a 0
** Have a column called isMale and a column for isFemale
** Use the get_dummies function
** https://pbpython.com/categorical-encoding.html
* tomorrow: finalize the pre-processing pipeline and start the train_test_split
** we will also have to do similar on the test.csv file (do preprocessing on test file on our own before tomorrow)
* FPR and FNR calculate these values for the model you use; put the values you are getting for these 2 metrics in the slack

==== Reference Links ====
* https://www.py4e.com/book.php
* https://ijsret.com/wp-content/uploads/2020/07/IJSRET_V6_issue3_481.pdf
* https://towardsdatascience.com/simple-example-using-boruta-feature-selection-in-python-8b96925d5d7a
* <nowiki>https://www.researchgate.net/profile/Neytullah_Acun/publication/324909545_A_Comparative_Study_on_Machine_Learning_Techniques_Using_Titanic_Dataset/links/5b199ad40f7e9b68b428acf4/A-Comparative-Study-on-Machine-Learning-Techniques-Using-Titanic-Dataset.pdf</nowiki>
* https://realpython.com/pandas-groupby/
* Mathplot lib to get stuff to graph different groups and data from the csv
** <nowiki>https://stackoverflow.com/questions/59204445/how-to-do-i-groupby-count-and-then-plot-a-bar-chart-in-pandas</nowiki>
* https://datascience.stackexchange.com/questions/17769/how-to-fill-missing-value-based-on-other-columns-in-pandas-dataframe

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Compile meeting notes into notebook
|Completed
|2/13/21
|2/17/21
|2/14/21
|-
|Update colab notebook
|Completed
|2/13/21
|2/14/21
|2/14/21
|}

== February 14, 2021 ==
* Combined preprocessing logic into one general function and apply to both test and train .csv
* Write functions for imputing missing values
* Split train and test code to use in machine learning models
* Pick machine learning models
** Haritaa: Random Forest Classifier
** Harris: xgboost
** Karthik: Logistic Regression
** Nishant: kNearestNeighbors
** Prahlad: SVC
** George: Neural Nets

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Compile meeting notes into notebook
|Completed
|2/14/21
|2/17/21
|2/14/21
|-
|Try different parameters for RFC
|Completed
|2/14/21
|2/17/21
|2/15/21
|-
|Calculate FPR, FNR (send in slack)
|Completed
|2/14/21
|2/17/21
|2/15/21
|-
|Create pareto frontier of all models
|Completed
|2/14/21
|2/17/21
|2/15/21
|-
|Export results to .csv
|Completed
|2/14/21
|2/17/21
|2/17/21
|}

== February 17, 2021 ==

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Compile lecture notes into notebook
|Completed
|2/17/21
|2/24/21
|2/17/21
|-
|Find time to meet with group
|Completed
|2/17/21
|2/24/21
|2/18/21
|}

== February 20, 2021 ==
* Create a population and individual
* Decided to use distance from origin to metrics (FPR, FNR) to calculate fitness
* Used Dr. Zutty's github resources and Lab 2 resources to write a GL algorithm
* Created empty slideshow for presentation, will go over during next meeting

=== Action Items ===
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Compile meeting notes into notebook
|Completed
|2/14/21
|2/17/21
|2/14/21
|-
|Update slideshow with information
|Completed
|2/14/21
|2/17/21
|2/15/21
|-
|Update colab notebook
|Completed
|
|
|
|-
|Fiddle around with numbers and 
methods in GL algorithm
|Completed
|
|
|
|}

== February 23, 2021 ==
* Updated colabs based on any number tweaking we did individually 
* Debugged together so everyone's code could run 
* Compared AUCs from various generations; most successful was 40 generations
* Exported pareto frontier graph and metrics 

== February 24, 2021 ==

== March 3, 2021 ==