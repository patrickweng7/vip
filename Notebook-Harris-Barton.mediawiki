=== Harris Barton ===
* Email: Hbarton7@gatech.edu
* Phone: 850-687-9318
* Subteam: Image Processing
* 2 semesters in VIP AAD
* 3rd year CS Major concentrating in Theory and Intelligence.
* Interests: Machine Learning, Software Engineering, Digital Signal Processing, BJJ (No-Gi)
= Fall 2021 =  
== December 6th, 2021 (Final Presentation Day) == 
'''Meeting Notes\Accomplishments:'''
* Our code is now frozen and we are doing experiments. 
* Austin and I are using this meeting as a work session to do some experiments.
* The comparisons that the selection method people will be doing are baseline vs NSGA-III, baseline vs Lexciase, and lexicase vs NSGA-III.
'''subteam meeting notes:'''
* Preparing for presentation and doing experiments
* I am running experiments and am going to document PACE-ICE errors below.
* Completed a run of NSGA3 as a part of the experiments we were running. Austin, Dhruv, and I all ran NSGA-III to try to at least see if we could get some sort of satistical significance. 
 Lowest AUC: 0.377054
 Num Params: 192680000
* Completed a run of lexicase as apart of the experiments as well. 
 Lowest AUC: 0.348809
 Num Params: 12056200
* Neither of these ROC-AUC values compared nicely with that of NSGA2 run, which had a nice individual of ROC AUC: 0.218952.
'''Encountered errors and their fixes:'''
* '''Error:''' 
  Traceback (most recent call last):
  File "src/GPFramework/launchEMADE.py", line 273, in <module>
    schema_doc = etree.parse(os.path.join('templates', 'inputSchema.xsd'))
  File "src/lxml/etree.pyx", line 3521, in lxml.etree.parse
  File "src/lxml/parser.pxi", line 1859, in lxml.etree._parseDocument
  File "src/lxml/parser.pxi", line 1885, in lxml.etree._parseDocumentFromURL
  File "src/lxml/parser.pxi", line 1789, in lxml.etree._parseDocFromFile
  File "src/lxml/parser.pxi", line 1177, in lxml.etree._BaseParser._parseDocFromFile
  File "src/lxml/parser.pxi", line 615, in lxml.etree._ParserContext._handleParseResultDoc
  File "src/lxml/parser.pxi", line 725, in lxml.etree._handleParseResult
  File "src/lxml/parser.pxi", line 652, in lxml.etree._raiseParseError
  OSError: Error reading file 'templates/inputSchema.xsd': failed to load external entity "templates/inputSchema.xsd"
* '''Fix:'''
 Had to go in and add inputSchema.xsd. I accidentally removed it when I was trying to get below quota.
* '''Error:'''
 :44
 211208 18:35:36  InnoDB: Unable to open the first data file
 InnoDB: Error in opening ./ibdata1
 211208 18:35:36  InnoDB: Operating system error number 11 in a file operation.
 InnoDB: Error number 11 means 'Resource temporarily unavailable'.
 InnoDB: Some operating system error numbers are described at
 InnoDB: http://dev.mysql.com/doc/refman/5.5/en/operating-system-error-codes.html
 211208 18:35:36 InnoDB: Could not open or create data files.
 211208 18:35:36 InnoDB: If you tried to add new data files, and it failed here,
 211208 18:35:36 InnoDB: you should now edit innodb_data_file_path in my.cnf back
 211208 18:35:36 InnoDB: to what it was, and remove the new ibdata files InnoDB created
 211208 18:35:36 InnoDB: in this failed attempt. InnoDB only wrote those files full of
 211208 18:35:36 InnoDB: zeros, but did not yet use them in any way. But be careful: do not
 211208 18:35:36 InnoDB: remove old data files which contain your precious data!
 211208 18:35:36 [ERROR] Plugin 'InnoDB' init function returned error.
 211208 18:35:36 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
 211208 18:35:36 [ERROR] mysqld: Can't lock aria control file '/storage/home/hpaceice1/hbarton7/scratch/db/aria_log_control' for exclusive use, error: 
 11. Will retry for 30 seconds
 211208 18:36:07 [ERROR] mysqld: Got error 'Could not get an exclusive lock; file is probably in use by another process' when trying to use aria control 
 file '/storage/home/hpaceice1/hbarton7/scratch/db/aria_log_control'
 211208 18:36:07 [ERROR] Plugin 'Aria' init function returned error.
 211208 18:36:07 [ERROR] Plugin 'Aria' registration as a STORAGE ENGINE failed.
 211208 18:36:07 [Note] Plugin 'FEEDBACK' is disabled.
 211208 18:36:07 [ERROR] Unknown/unsupported storage engine: InnoDB
 211208 18:36:07 [ERROR] Aborting
 211208 18:36:07 [Note] /usr/libexec/mysqld: Shutdown complete
 211208 18:36:07 mysqld_safe mysqld from pid file /storage/home/hpaceice1/hbarton7/scratch/db/mysqldb.pid ended
*'''Fix:'''
 I had a SQL instance running, all I had to do was kill it. 
'''Some Presentation Notes:'''
* We had some interesting questions asked during the presentation, namely that we were not looking deep enough into the performance of our algorithm on specific classes. 
* The ROC-AUC value in combination with the Num_Params objective doesn't really tell us how our algorithm is doing on classifying specific diseases. Suppose we had an ROC-AUC of 0.5, at surface level, this seems like we are essentially doing a coin flip for each disease in prediction, but this might not be the case in that it might be doing a lot better at classifying certain diseases over some others. 
* So essentially, we were not really looking deep enough into what is going on behind the hood. Rather, we spent too much of our time trying to get things to work and spit out ROC-AUC values and less time really trying to understand how our algorithm is working. 
* We strayed away from what research in the sense that we kind of tried to present results positively, rather than specify the dangling threads that we have and be open about what we don't know and what we need to know and do better in future work.

'''Action Items:'''
{|class = "wikitable"
!Assignment
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|- 
|Meet with Subteam
|Complete
|December 6th, 2021
|December 8th, 2021
|December 8th and 9th, 2021
|-
|Code freeze
|Complete
|November 29th, 2021
|December 6th, 2021
|December 6th, 2021
|-
|Run Experiments lexicase
|Complete
|December 6th, 2021
|December 10th, 2021
|December 10th, 2021
|-
|Work on experiments NSGA3
|Complete
|December 6th, 2021
|December 10th, 2021
|December 9th, 2021
|-
|Do Presentation
|Complete
|December 6th, 2021
|December 10th, 2021
|December 10th, 2021
|}
== November 29th, 2021 == 
'''Meeting Notes\Accomplishments:'''
* Austin and I are using this meeting as a work session to try and get the lexicase from EMADE working. Currently we are having individuals not evaluating. 
* The comparisons that the selection method people will be doing are baseline vs NSGA-III, baseline vs Lexciase, and lexicase vs NSGA-III.
'''Subteam Meeting Notes:'''
* I am trying to do some lexicase runs myself, but I am not able to do the reinstall command because I am getting permission denied. 
* Thus, I am attempting to download the environment that we are using locally so that I can run PACE without having to do ./reinstall inside of the shared environment.
* We are waiting to be able to leverage the changes of the NAS team and use the new environment with TF2.6
'''To-DO/Accomplishments:'''
* Austin and I are trying to look into the statistics of the last couple of runs that he did and try to understand why the individuals are not being fully evaluated. 
* It doesn't seem to be a lexicase issue, because some other people were getting similar errors. It could be due to the upgrades of tensorflow and the fact we have not prepared the new environment.
* I was diagnosed with Staph infection on December 1st, and ended up feeling pretty bad starting on Thursday and until Sunday. So I did not do  what I wanted to. For now, we are going to simply do experiments with the current implementation of lexicase in EMADE and see if it shows any improvements over the baseline.
'''Action Items:'''
{|class = "wikitable"
!Assignment
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|- 
|Meet with Subteam
|Complete
|November 29th, 2021
|December 1st, 2021
|December 6th, 2021
|-
|Download Environment locally with yml file
|Complete
|November 29th, 2021
|December 6th, 2021
|December 6th, 2021
|- 
|Look into the specifics of the run and see why we are getting error string individuals.
|Complete (kind of)
|November 29th, 2021
|December 6th, 2021
|December 1st, 2021
|}
== November 22nd, 2021 (Thanksgiving Break) == 
'''Meeting Notes\Accomplishments:'''
* Austin and I have implemented lexicase methods from DEAP inside of EMADE and did test runs. So now all we have to do is do some experiments on the two lexicases and NSGA-III.
* Austin will send me the stats of the runs we did and I will insert them here when I have then. As of now, it seems as if we are getting one individual and not changing in 200 generations so we need to look into that.
* This week is thanksgiving break so there is not much in the agenda for the upcoming. We are not meeting this week and we are pretty close to code freeze stage. 
* On the selection method side, we need to look into that issue with the runs and see if this is some sort of code bug, and that is about it. Once this bug is fixed, all we need to do are experiments.
* The fixing and looking into this bug will most likely be done next week.
* I still do not have a great understanding of the sort of lexicase that Dr.Zutty is discussing and how to implement it. We do not have a ton of time, so I think that we will stick to using the current lexicase implemented and EMADE and see if we can also use the epsilon lexicase that DEAP has. 
'''Action Items:'''
{|class = "wikitable"
!Assignment
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|- 
|Meet with Subteam
|N/A (thanksgiving break)
|
|
|
|-
|Discuss lexicase with Dr.Zutty
|Complete
|November 22nd, 2021
|November 29th, 2021
|November 29th, 2021
|}
== November 15th, 2021 == 
'''Meeting Notes:'''
* We have a very clear goal in sight for the final presentation. Namely, we want to see if we can improve EMADES image processing capabilities. As a baseline, we are going to see how EMADE can currently handle the chest x-ray dataset and then see how our separate changes can improve the performance, characterized by ROC and F1-score. Additionally, if we see that we can improve performance with one of the changes separately or multiple, then we will begin investigating combinations of these changes, maybe lexicase and some sort of cross over methods, if they both seem to be improving performance separately. 
'''Subteam Meeting Notes''':
* I was not able to make this subteam meeting. 
* However, due to time constraints, I will move forward with simply implementing DEAPS version of lexicase. Hopefully, we will have this done in the hackathon on Sunday and we can then proceed to do experiments comparing the different lexicases and NSGA-III and their impact on the performance.
* Other than that, Max is working on getting a new conda environment ready for Tensorflow 2.6, which we will create documentation for when it is finished. 
* The epsilon lexicase paper did not seem to be very clear on how to implement it to me, but it turns out that DEAP has epsilon lexicase implemented! 
* So we might as well try and do some runs with epsilon lexicase and just see for ourselves if it might have some decent results hopefully comparable or better than the current EMADE implementation of lexicase. 
* This week, the goal is to get DEAPS version of lexicase in EMADE smoothly so that I can do experiments with it in the week of and week after thanksgiving break. If time permits, might as well run tests with their normal lexicase and their epsilon lexicase. 
* We want to be able to do experiment with various values of epsilon and see if we can tune that parameter so that we can pick the best epsilon value to be able to compare with the rest. 
* I did find another paper on lexicase and epsilon lexicase that seems to have a comprehensive explanation of both, so I read this paper and took some minimal notes. I did not read this paper and take notes at the level of detail which I wanted to, due to the fact that we did have some issues when implementing the DEAP version of epsilon lexicase.
'''Paper Notes:'''
{|class = "wikitable"
!Assignment
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|- 
|Meet with Subteam
|Completed
|November 15th, 2021
|November 17th, 2021
|November 22nd, 2021
|- 
|Implement Lexicase from DEAP --> epsilon lexicase and auto epsilon lexicase possibly?
|Completed
|November 15th, 2021
|November 14th, 2021
|November 22nd, 2021
|}
== November 8th, 2021 ==
'''Meeting Notes:'''
* Jason and I discussed the difference between the current EMADE implementation of lexicase and the one that might work better for our task. 
* To be honest, I don't feel like I quite grasp it in the context of our problem, so I am going to read some more papers on lexicase and hopefully the sort of thing he is discussing will be more clear to me.
* Specifically, how to get these test cases in the context of our dataset and use lexicase to be able to select individuals that are performing really well on specific cases and not so well on others and not fall into the issue of selecting individuals that do okay in aggregate over some of these special case individuals.
'''Subteam Meeting Notes''':
* We used this session as kind of a work session. Austin is working with me on lexicase implementation and literature search and we used this time attempting to resolve some issues involving him getting on PACE. 
* I also found 2 more papers which I intend to read before the meeting next week which are implementations of lexicase. One is something called epsilon lexicase, which is implemented in DEAP. Sadly, DEAPS link to the paper they used to implement epsilon lexicase does not work so I am not sure if this is that one or not. 
'''Paper Notes:'''
* '''Link to Paper:''' https://dl.acm.org/doi/10.1145/2908812.2908898 
* '''General Note:''' I am aware that this paper investigates this lexicase selection method in the context of regression, and we are doing classification. However, I figure that I might gain some insight into whether I think that this lexicase version might be able to be show promise in the context of our problem. 
* '''Abstract:''' Recall that lexicase selection is a parent selection method that considers test cases separately, rather than in aggregate when performing parent selection. This paper develops a new form of lexicase selection for symbolic regression, named epsilon-lexicase selection. This redefines the pass condition for individuals one each test case in a more effective way. Epsilon lexicase selection is shown to be effective for regression, producing better fit models compared to other techniques such as tournament selection and age-fitness pareto optimization.  The papers experiments show that epsilon-lexicase selection with automatic epsilon produces the most accurate models across tested problems with negligible computational overhead.
* '''Algorithm:''' The parent selection event proceeds as follows:
 1. The entire population is added to the selection pool.
 2. The fitness cases are shuffled.
 3. Individuals in the pool with a fitness worse than the best fitness on the case among the pool are removed.
 4. If more than one individual remains in the pool, the first case is removed and 3 is repeated with the next case. If only one individual remains, it is the chosen parent. If no more fitness cases are left, a parent is chosen randomly from the remaining individuals.
* '''Conclusion:''' I realized like midway through this paper that this paper doesn't seem to be very beneficial for our problem and if it is beneficial, I don't quite see it. Additionally, this paper doesn't seem to give me enough info for me to feel comfortable trying to implement it in EMADE. 
* I did find another paper going over lexicase and epsilon lexicase more in depth. This implementation of epsilon lexicase seems to attempt to address some of the short-comings of the normal lexicase in that the normal lexicase is more sensitive to small sets of training cases and smaller population sizes. 
* I am going to read this paper next week and hopefully just gain some better insight into how the current EMADE implementation of lexicase is working and how epsilon lexicase addresses the shortcomings of the currently implemented version. 
{|class = "wikitable"
!Assignment
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|- 
|Meet with Subteam
|Completed
|November 8th, 2021
|November 10th, 2021
|November 15th, 2021
|- 
|Read Epsilon lexicase paper and see if it will be worth implementing
|Completed
|November 8th, 2021
|November 10th, 2021
|November 15th, 2021
|- 
|Play around with EMADE implementation and see if I can understand what Dr.Zutty is getting at more clearly
|Completed
|November 8th, 2021
|November 10th, 2021
|November 15th, 2021
|}
== November 1st, 2021 == 
'''Meeting Notes/Accomplishments:'''
* We were suggested by Dr. Zutty to look through the dataset and see what the distribution looks like for the dataset when we constrain the problem to images with only one label, but keep it a multi-class problem Multi-class and Multi-label are different things).
* Our idea to make it a binary classification problem kind of complicates things with the selection methods since our study into selection methods was basically to improve EMADES many-objective optimization capability. Making this a binary classifier kind of throws that investigation out of the window. 
* We have 4 new members added to our team. Fortunately, we have 4 "sub-subteams" working on different tasks and we might be able to basically split them up into our sub-subteams accordingly. So tasking doesn't seem to be a big issue. 
* Currently, the lexicase implemented inside of EMADE is sort of a "half-baked" version of lexicase. More specifically, the current lexicase method we have inside of EMADE bases selection on a fitness value aggregated over all of the test cases. The more popular and new lexicase that might be useful for solving this problem considers test cases one at a time in random order. 
* I, hopefully along with somebody from the new students, will begin looking through papers and current implementations of lexicase in order to see if we are able to get an implementation in EMADE in a reasonable time so that we can begin running experiments and comparison our current lexicase in EMADE, NSGA-III, and our new lexicase in our image processing task.
* This week, I began to read a paper implementing lexicase which DEAP actually used as a framework in their implementation. The paper can be found here
 http://faculty.hampshire.edu/lspector/pubs/lexicase-IEEE-TEC.pdf. 
'''Paper Notes/Thoughts:'''
* This paper is apparently what the developers of DEAP used in order to implement lexicase inside of DEAP. At a high level, this paper compares this lexicase selection method with other parent selection methods on a subset of problems which they refer to as "uncompromising problems" which they define as a problem characterized by the requirement that solutions must perform optimally on each of many test cases.
* Lexicase does well in the sense that we are able to see individuals that do good on some subset of training cases, rather than only looking at individuals that do decent on the average. 
* Since we have 14 classes, we might be able to see some individuals that predict some subset of the diseases really well, and not some of the others. Rather than settling with some individual that does '''okay''' in aggregate on all 14 cases.  
* The pseudocode for their lexicase goes something like this:
 To select a parent for use in a genetic operation:
 1) Initialize:
 a) Set candidates to be the entire population.
 b) Set cases to be a list of all of the test cases
 in random order.
 2) Loop:
 a) Set candidates to be the subset of the
 current candidates that have exactly the
 best performance of any individual currently
 in candidates for the first case in cases.
 b) If candidates contains just a single individual then return it.
 c) If cases contains just a single test case then
 return a randomly selected individual from
 candidates.
 d) Otherwise remove the first case from cases
 and go to Loop.
* The problems that this paper compares its selection methods on are 
 1. '''finite algebras problem'''
 2. '''digit multiplier problem'''
 3. '''factorial symbolic regression problem'''
 4. '''The wc problem'''
* The results, as shown below, seem to indicate that lexicase can do well on some uncompromising problems. Namely, problems which must perform optimally on each of the many test cases. 
{|class = "wikitable"
!Assignment
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|- 
|Meet with Subteam
|Completed
|November 1st, 2021
|November 8th, 2021
|November 3rd, 2021
|- 
|Read Lexicase Paper corresponding to DEAP implementation
|Completed
|November 1st, 2021
|November 8th, 2021
|November 7th, 2021
|}
== October 25th, 2021 (Midterm Presentation Week)== 
'''Meeting Notes/Accomplishments:'''
* Presentations happening today!
* Image Processing team is presenting 5th. 
'''NLP Sub-team Presentation Notes:'''
* Seems to have made good progress.
* Doing some complicated NLP stuff that I don't understand.
* They documented the specifics of their literature review which we did not which is something that I am starting to think about more. However, it would've been hard to go in depth in ours since our presentation is already very long without going in depth. 
'''Neural Architecture Search Presentation Notes:'''
* Automatically creating neural networks with EMADE (this is the main idea of the team).
* EMADE had trouble outperforming the seeds, and the individuals were not actually neural networks. 
* They experimented with time stopping in order to lower training time. Apparently dataset was too big to check each batch. 
* It seems that with time stopping earlier at 600 seconds yielded better results than stopping at 2500 seconds, this is an interesting concept that more training time does not necessarily make a better model. 
* They have made some interesting changes to EMADE. 
** They made additional classes for EmadeDataPairs.
** Separated primitive sets and terminals between adfs and main primitives. 
** Changed inputs to adfs
* Did a seeded experiment on cifar-10 but the slide went super fast and I need to ask them to reiterate exactly what they did here. 
* They did something interesting with SQL which basically adds a table for accessing specific NNLearning individuals over time.
* Added interesting analysis methods, thinking we might benefit from creating some helper functions for analysis. 
'''Bootcamp team 2 Presentation:'''
* Used 5 different ML models, had decent results.
* Used SPEA2 as their selection method, which is different from a decent bit of teams. It seems like NSGA-II is kind of the default there.
* Used a squishifier function as my bootcamp team did last semester, which is interesting as it seemed like at the time we were the only ones who had done that. 
* Did not do the bounds correctly in the pareto front.
'''Our turn to present:''' 
* I had a meeting at 7:30pm, so I had to leave after our presentation.
'''Subteam Meeting Notes:'''
* We have changed the problem we are trying to solve to a 0-1 binary classification problem. We just now noticed that the paper is essentially doing this in that they have two labels: with and without pneumonia. This will probably simplify things and might be a factor in the weird stuff that has been going on with our individuals as of late. 
* This means that NSGA-II may be able to do a decent job, since we have changed the problem back to a non-many objective optimization problem.
* Since Dhruv and I were working together, we kinda had him in charge of running and testing the selection methods we implemented and I think that I should start doing runs as well to kind of double the productivity that we can accomplish between Dhruv and I. 
* We are going to continue trying to get runs in with our selection methods changed since we were not able to get decent runs in before the presentation.
* I want to get a run in with the current EMADE implementation of lexicase and see how that compares with the NSGA-II and NSGA-III. 
'''Action Items:'''
{|class = "wikitable"
!Assignment
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|- 
|Meet with Subteam
|Completed
|October 25th, 2021
|November 1st, 2021
|October 27th, 2021
|- 
|Do Presentation
|Completed
|October 25th, 2021
|October 25th, 2021
|October 25th, 2021
|- 
|Do PACE Run
|IP
|October 25th, 2021
|November 1st, 2021
|November 1st, 2021
|}
== October 18th, 2021 == 
'''Meeting Notes/Accomplishments:'''
* Mid-term presentations next week. 
* When doing our runs, we need to keep in mind the following:
** 1 master - 1 core is constant
** CPU hours should add up to how much time is left in the class
** Number of hosts means how many worker jobs
** Number of CPU per worker is core count per worker 
* We are going to to our EMADE runs in pairs, so dhruv and I will get a run in with NSGA-III on the ChexNet dataset before the presentation and hopefully be able to interpret the results and compare against the baseline. 
'''Subteam Meeting Notes:'''
* We noticed that nsga-III was hard-coded into EMADE, and this is a bit interesting and we are thinking this might be some sort of indication that we should have hard coded nsga-III into the code or if there was something wrong with simply calling NSGA-III and registering it into the toolbox.
* There are 4 changes to the code that we are hoping to test/compare with the baseline. Namely, 
** Hyper Features
** NSGA-III
** Geometric Crossover
** Semantic Crossover
* Hopefully, we will be able to see which of these changes resulted in an improvement over the baseline, and if so, then we can look at how to combine the two and see if the results improve over the separate changes.
'''Action Items:'''
{|class = "wikitable"
!Assignment
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|- 
|Meet with Subteam
|Completed
|October 11th, 2021
|October 13th, 2021
|October 13th, 2021
|- 
|Do Run with NSGA-III
|Issues Encountered
|October 17th, 2021
|October 25th, 2021
|October 25th, 2021
|- 
|Work on presentations
|Completed
|October 17th, 2021
|October 25th, 2021
|October 25th, 2021
|}
== October 11th, 2021 == 
Fall Break, no meeting.
== October 4th, 2021 ==
'''Meeting Notes/Accomplishments:'''
* Peer evaluations are due this week. 
* Heidi and Monil have began coding a grey level transform that enhances contrast, and they are going to add this to EMADE at some point soon.
* We are trying to minimize area under curve for precision recall, reason being that we have multi-label data, traditional accuracy metrics do not work. Luckily, this is already in EMADE so this is an easy implementation.
* Dhruv and I are waiting for us to have a baseline to be able to run and compare various selection methods on the ChexNet Dataset. Currently are planning to compare the results of NSGA-III, NSGA-II, Lexicase, and Hypervolume Indicators on the dataset, and we should have a baseline to compare with by this Friday, So we will for sure have done some comparisons by the next meeting. 
* For now, I have been looking through DEAP and just poking around and trying to understand the ins and outs of all of the selection methods that it has. Additionally, I have been poking through the EMADE code base and also trying to understand the lexicase selection methods and all of the others that are in EMADE. 
* I focused on lexicase because I read a paper on an extension to lexicase that does specifically well with missing data, which might be able to contribute to our success if we decide to cut down on the dataset. 
* The only issue is that the paper that I read seemed to not be telling the whole story, so it is not clear to me how I may go about trying to implement this method, but I do plan on looking into it during fall break.

'''Action Items:'''
{|class = "wikitable"
!Assignment
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|- 
|Meet with Subteam
|Completed
|October 4th, 2021
|October 6th, 2021
|October 6th, 2021
|- 
|Do Peer Evaluations
|Completed
|October 4th, 2021
|October 8th, 2021
|October 7th, 2021
|- 
|Look more into the extended lexicase method
|Completed
|October 4th, 2021
|October 18th, 2021
|October 19th, 2021
|}
== September 27th, 2021 ==
'''Meeting Notes/Accomplishments:'''
* We want to get up and running on PACE-ICE, so we can have a shared conda environment and more easily sync our work together and also be able to use GPU. So we have all planned to get up and running on PACE-ICE by next week.
* For some reason, we are not able to do a seeded run with NNLearners, so Maxim said that he is going to talk to Jason or Anish in order to try and trouble shoot that. 
* Dhruv and I met and put the NSGA-III selection method into emade, but we have not been able to test it yet or at least on the ChexNet dataset as we wanted. So we may just try it on some other dataset which we know emade handles well to see if at least works. 
* The subteam met on 09/29/2021. Maxim set up a shared conda environment for us on EMADE so we can have that consistent shared environment, though I think we need to update DEAP inside of the environment so we can use NSGA-III. 
* I am going to look into some hyperfeature stuff with Heidi and Monil for the time being, since the we kind of already implemented the NSGA-III selection method and they seem to think another hand on deck might be nice. Us three plan to meet so that I can be updated with the lit search and the work they have been doing with them during the weekend. 
* If all goes well, we will be able to get a seeded run and get some sort of baseline score with the original data as well as with our preprocessed data by 10/4/2021, and then we can start to compare and contrast the merits of selection methods that we have at our disposal. 

{|class = "wikitable"
!Assignment
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|- 
|Meet with Subteam
|Completed
|September 27th, 2021
|September 29th, 2021
|September 26th, 2021
|- 
|Get up to date with Hyperfeature work Heidi and Monil have been doing
|Incomplete
|September 27th, 2021
|October 4th, 2021
|October 4th, 2021
|- 
| Get up and running with PACE-ICE
|Complete
|September 27th, 2021
|October 4th, 2021
|October 5th, 2021
|}
== September 20th, 2021 == 
'''Meeting Notes:'''
* Image processing subteam met on 09/23/2021, and we updated each other on our progress in literature search and preprocessing. 
* Personally, read a nice paper comparing the NSGA-II and NSGA-III methods on some selected tasks, and it isn't that either are superior, it is just that NSGA-III tends to perform well on certain tasks as does NSGA-II. 
* We think that NSGA-III might do well with this problem, so dhruv and I are going to implement it in EMADE hopefully before next week. 
* Lexicases advantage is that it can perform well with missing data, but we don't see that giving us any improvements. 

'''Subteam Meeting Notes:''':
{|class = "wikitable"
!Assignment
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|- 
|Begin implementing NSGA-III in EMADE
|Completed
|September 20th, 2021
|September 27th, 2021
|September 26th, 2021
|- 
|Meet with subteam
|Completed
|September 20th, 2021
|September 23rd, 2021
|September 23rd, 2021
|}
== September 13th, 2021 == 
'''Team Meeting Notes/Scrum Notes:'''
* Self-evaluations are due tonight, 09/13/2021. Here is a link to my self-evaluation: https://1drv.ms/w/s!AoNWtXpreh8JgsVRQN7kHfzxhej26Q?e=ujyJs1 
* EMADE has many primitives which are very similar, so we need to determine which primitives have similar functions and find a way to be able to discriminate.
'''Subteam Meeting Notes (09/15/2021):'''
* We have decided to try our hand at ChexNet for lung disease classification. We don't fully expect to reach state of the art with EMADE, especially since a team has already tried doing some things with it in previous semesters. 
* We are looking into new selection methods which might be beneficial in solving this problem, such as NSGA-III, Lexicase, and hypervolume indicators. I have been doing a lit search and trying to find comparisons with NSGA-II and NSGA-III. I also found a paper that uses an extension to Lexicase selection methods for Learning Classifier networks. Lexicase seems to not do well on certain tasks, but it does well in maintaining results with less than the full dataset on some tasks, which is potentially good for us since the ChexNet dataset is massive and we need to cut down on the size. 
* It would be nice to be be able to compare how ChexNet does with less than the full dataset, and then use that same percentage of the dataset and see how we can compare to those results. 
* If we can get EMADE to be able to at least work with ChexNet a little, we might be able to do a comparison of the selection methods to see which fits best with this problem. 
* We also need to talk to Anish, since he has worked with the ChexNet dataset a decent amount and we could get some valuable pointers from him, like which roads lead to dead ends and which roads seem worth exploring based on his experience. 
'''Action Items:'''
{|class = "wikitable"
!Assignment
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|- 
|Complete self-evaluations
|Completed
|September 13th, 2021
|September 13th, 2021
|September 13th, 2021
|- 
|Meet with subteam
|Completed
|September 13th, 2021
|September 13th, 2021
|September 15th, 2021
|- 
|Literature search
|Completed
|September 13th, 2021
|September 19th, 2021
|September 15th, 2021
|}
== September 6th, 2021 == 
'''Team Meeting Notes:'''
* There was no team meeting this week due to Labor Day. 
'''Subteam Meeting Notes (09/09/2021):'''
* Our Meeting time will be every Wednesday at 5:45pm. 
* Met with image processing subteam and came up with some action items before we meet again. 
* We discussed possible directions for the subteam in terms of what our overarching goal is and what subset of image processing we might like to work in. 
* The overarching goal of this team this semester is going to be to add new primitives to EMADE that will assist us in replicating a paper which uses traditional ML methods to do image processing but using EMADE and (hopefully) outperform it. 
* We think this might be decent route to a paper, especially if we can successfully outperform a paper which uses traditional ML methods. 
'''Action Items:'''
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with subteam
|Completed
|September 6th, 2021
|September 13th, 2021
|September 9th, 2021
|-
|Read paper published by VIP subteam
|Completed
|September 6th, 2021
|September 13th, 2021
|September 11th, 2021
|- 
| Refamiliarize with EMADE
| Completed
| September 6th, 2021
| September 13th, 2021
| September 12th, 2021
|-
|Find potential dataset/paper to work with
|Completed
|September 6th, 2021
|September 13th, 2021
|September 8th, 2021
|}
== August 30th, 2021 == 
''' Team Meeting Notes: '''
* We discussed the ideas that we had brainstormed for the direction of each subteam and which subteams were going to stay and which ones we might sideline for the moment. 
* Dr. Zutty seemed to think there was some potential roadblocks in our idea for pursuing GP for Image Registration, so we discussed some possible ways to solve that problem and also some alternative directions for the subteam worth pursuing. 
* By next week we will have all of the teams sorted and will have probably picked team leaders for all of the new subteams. 
* I created a slack channel for the Image Processing team and set up a LettuceMeet to figure out when our weekly meetings will happen. 
''' Action Items: '''
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create Slack channel for subteam
|Completed
|August 30th, 2021
|September 6th, 2021
|August 31st, 2021
|- 
| Agree on Date for Subteam Meetings
| Completed
| August 30th, 2021
| September 6th, 2021
| September 1st, 2021
|}
== August 23rd, 2021 ==
''' Team Meeting Notes: '''
* We brainstormed some ideas for some new teams which might be able to do some interesting research. I thought interpretability and image processing both seemed like intriguing routes.
* I did some research into what sort of research is currently going on in the field of interpretability and how we might be able to do something interesting, and I created the slack channel to brainstorm what kind of direction the team might go in, but I ended up gaining more of an interest in the image processing team. 
* Maxim, Aryaan, and I met up to discuss possible directions and we came across some interesting papers in applications of genetic programming to image registration. We also kind of realized that this might be a tough route to go down because of the lack of current data available. 
* I believe that it might be best to start of doing some more basic image processing with emade, like an image classification problem, so I posed this idea to the newly formed team.
''' Action Items:'''
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Brainstorm for Interesting Subteams
|Completed
|August 23rd, 2021
|August 29th, 2021
|August 27th, 2021
|- 
| Submit Subteam Rankings
| Completed
| August 23rd, 2021
| August 29th, 2021
| August 29th, 2021
|}
= Spring 2021 = 
== April 26th, 2021 ==
'''Team/Breakout Session Notes:'''
* If you have not done peer evals yet, do them.
* We have final presentations this Friday, and need to finish them up
'''Subteam Meeting Notes/Accomplishments:'''
* We met with Cameron to give him an update on what the error might be from, since our initial idea seemed to be incorrect. 
* There was recently a push to the NN branch which flattened the embedding layers, and we tried to add this to the code and see if it made any difference.
 # Handle Keras applications
  elif isinstance(layer, dict):
  if layer['type'] == PretrainedModel.MOBILENET:
      s = data_pair.get_train_data().get_instances()[0].get_stream().get_data().shape
      new_layer = MobileNet(input_shape=s, include_top = False, weights = "imagenet")
      input_layers.append(new_layer.inputs)
      new_layer = new_layer.output
      curr_layer = new_layer
      if layer['type'] == PretrainedModel.INCEPTION:
          s = data_pair.get_train_data().get_instances()[0].get_stream().get_data().shape
          new_layer = InceptionV3(input_shape=s, include_top = False, weights = "imagenet")
          input_layers.append(new_layer.inputs)
          new_layer = new_layer.output
          curr_layer = new_layer
      if layer['type'] == PretrainedModel.VGG:
          s = data_pair.get_train_data().get_instances()[0].get_stream().get_data().shape
          new_layer = VGG16(input_shape=s, include_top = False, weights = "imagenet")
          input_layers.append(new_layer.inputs)
          new_layer = new_layer.output
          curr_layer = Flatten()(new_layer)  '''# Here is where the layer is flattened and we simply added this line to the code to see if this fixed our problem.'''
* Unfortunately, the same error resulted, and we have decided to turn our focus more into assisting with the presentation and data compilation. 
* There are a couple of things that I am in charge of speaking about in the presentation; namely, the SQL errors and the best individual and the differences between the other previous models.
* I have encountered a good deal of the SQL errors and have committed the solutions to many of them to memory. I think we are going to work on having a gallery of solutions to all of the PACE/SQL errors in the future which I think will be a very worth while effort. 
* During the summer, I intend to familiarize myself a lot more with the project as a whole and really gain some insight into the bigger picture of this team and VIP all together. 
* I hope to come back in the fall with a much better understanding of it all and, if all goes well, I might even do some interesting things to contribute to the team during the summer even though VIP is not officially on. I am really intrigued by the work and intend to work on it as much as possible even in off semesters. 
'''Action Items:'''
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Peer Evaluation
|Completed
|April 19th, 2021
|April 27th, 2021
|April 27th, 2021
|-
|Debug the Pretrained Embedding Layer Method
|Incomplete
|April 19th, 2021
|April 29th, 2021
|
|- 
|Focus on presentation and prepare my part of it
|Completed
|April 26th, 20221
|April 30th, 2021
|April 29th, 2021
|}
== April 19th, 2021 ==
'''Team/Breakout Session Notes:'''
* Need to complete peer evaluations
* The presentation will be Friday, the 30th, from 5:00pm-8:15pm. 
'''Subteam Meeting Notes/Accomplishments:'''
* Cameron made a big push to the branch that allows for more worker processes, which in turn allows for us to run more generations in a shorter period of time. 
* I cloned the new branch and I made the necessary changes so that I can have the benefits when debugging and trying to do EMADE runs. 
* Prahlad and I were attempting to find potential solutions to the json.loads error.
* The line that is giving us an error is supposedly giving us an error because it is supposed to deserialize a string of JSON data but the jsonified keras model is not a pure string.
* My simple solution to a seemingly simple solution was to just use json.stringify to convert it to a string then deserialize it after that.
 config = model.to_json()
 print("config: ",type(config))
 data = json.stringify(config)
 data = json.loads(config)
 for layer in data['config']['layers']:
    if layer['class_name']=='Embedding':
       if data_pair.get_datatype() == 'recdata':
          layer['config']['input_dim'] = numforembedding[i]
          i+=1
  elif data_pair.get_datatype() == 'textdata':
  layer['config']['input_dim'] = vocab_size
* However, Prahlad and I noticed that we seemed to get the same error no matter the changes we made. Thus, we decided to try and put some debugging statements to see if we were even looking in the right place for the error. 
* Prahlad made the following changes to the code to see what was going on:
 def PretrainedEmbeddingLayer(data_pair, initializer, layerlist): 
   """Creates Embedding layer  
   Args:   
       empty_model: empty_model as terminal    
       data_pair: given dataset    
       out_dim: ouput dimension    
               layerlist: layerlist to append to 
   Returns:    
       Keras Embedding Layer   
   """ 
 ->print("Reached primitive")
   maxlen = MAXLEN 
   numwords=NUMWORDS   
   out_dim = abs(out_dim)  
   data_pair, vocab_size, tok  = tokenizer(data_pair, maxlen, numwords)
   out = {PretrainedEmbedding.GLOVE:100, PretrainedEmbedding.GLOVEFASTTEXT:501, PretrainedEmbedding.FASTTEXT:300, PretrainedEmbedding.GLOVETWITTER:200 }
   out_dim = out[initializer]   
   initializer = Constant(get_embedding_matrix(initializer, vocab_size, tok))
   layerlist.mylist.append(Embedding(vocab_size,out_dim , input_length=maxlen, embeddings_initializer=initializer))
   return layerlist
* It turns out, our code is not actually even giving us an error where we thought it was and unfortunately, this error might be more complex than we thought. 
* For reference, Prahlad has the stack trace of the error on his notebook. I was having issues running the python files today so we were debugging and doing the runs with his computer. 
'''Action Items:'''
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Peer Evaluation
|Completed
|April 19th, 2021
|April 27th, 2021
|April 27th, 2021
|-
|Debug the Pretrained Embedding Layer Method
|Incomplete
|April 19th, 2021
|April 29th, 2021
|
|}

== April 12th, 2021 ==
'''Team/Breakout Session Notes:'''
* In the breakout session, Cameron compiled a list of things that might be good to work on:
  1) Fix json error with pretrained Embedding Layer primitives.
  2) Try to make NNlearner a subtree in individuals.
  3) Merge PACE and SQL errors into Cache-V2 branch.
'''Subteam Meeting Notes/Accomplishments:'''
* Prahlad and I are tasked to look into Json error with pretrained Embedding layers
* The pretrained embedding layer has an attribute, input_dim, and it is vocab size when learning word embeddings. 
* Tried to set input_dim to vocab size, but it is not being properly set, probably because the jsonified keras object is not a pure string, but it needs to be for it to work. 
* I spent some time trying to debug by editing the Neural Network Methods file and seeing where the problem might be and how to possibly fix it. 
* Here is where Cameron thinks the error most likely lies:
         if data_pair.get_datatype() == 'recdata' or data_pair.get_datatype() == 'textdata':
            n_users = len(user_enc.classes_)
            n_items = len(item_enc.classes_)
            numforembedding = [n_users, n_items]
            i = 0
            config = model.to_json()
            print("config: ",type(config))
            data = json.loads(config)   # '''where the error occurs. basically, the JSONified keras model is not a pure string, so using json.loads doesn't work'''
            for layer in data['config']['layers']:
                if layer['class_name']=='Embedding':
                    if data_pair.get_datatype() == 'recdata':
                        layer['config']['input_dim'] = numforembedding[i]
                        i+=1
                    elif data_pair.get_datatype() == 'textdata':
                        layer['config']['input_dim'] = vocab_size
                json_object = json.dumps(data)
                print(json_object)
                model = model_from_json(json_object)
            if data_pair.get_regression():
                model.compile(loss= 'mean_squared_error', optimizer = optimizer, metrics= ['accuracy'])
            elif len(data_pair.get_train_data().get_target().shape) != 1:
                model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
            else:
                model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
        print(model.summary())
        model = KerasPickleWrapper(model)
'''Action Items:'''
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get an EMADE run in at pace
|Completed
|April 12th, 2021
|April 17th, 2021
|April 14th, 2021
|-
|Choose a task to focus on
|Completed
|April 12th, 2021
|April 17th, 2021
|April 12th, 2021
|}

== April 5th, 2021 ==
'''Team Meeting Notes:'''
* Dr. Zutty gave a presentation on Hypothesis Testing. 
* At a high level, hypothesis testing is somewhat self-explanatory, but the way it is implemented may vary depending on the nature of what you are trying to prove. 
* In hypothesis testing, you essentially have two hypotheses: the null hypothesis, and an alternative hypothesis. 
* The sample data must provide sufficient evidence to reject the null hypothesis and conclude that the effect exists in the population. Ideally, a hypothesis test fails to reject the null hypothesis when the effect is not present in the population, and it rejects the null hypothesis when the effect exists
'''Subteam Meeting Notes/Accomplishments:'''
* This past week, I have mainly been working on figuring out the small errors that I have been encountering in getting set up in PACE. I had some issues with my anaconda environment, but figured that one out kind of quickly. Most annoying of all, I got stuck for a while on an error that essentially stemmed from me not having a file in the correct place. PACE had an interesting way of telling me that so it took me a while to figure out that it was such a small thing causing my trouble.
* We had a meeting on Friday at 6:00 p.m. and it was going over ML and NLP concepts. 
'''Some Notes On Topics Covered:'''
* Supervised vs Unsupervised Learning
 (The main difference between the two types is that supervised learning is done using a ground truth, or in other words, we have prior knowledge of what the output values for our samples should be.)
* Neural Network Basics
 (The basic layout is something like '''Input --> Layer 1 --> Layer 2 ... --> Layer N --> Output'''. I kind of like to think of it like a sequence of functions, even though this is kind of a reductionist way of explaining it. It does work in a simple way, for essentially what is going on is that something is done to the data at each layer, and acts as an input to the next layer on and on until the Nth layer is reached and is passed to the output.)
* Inside of the Layer
 (Operation: The actual computation/mapping on the data | Activation: The introduction on non-linearity of the model.)
* Training a Neural Network
 (Feed forward: evaluate the model | Back propagation: Using gradients to update the weights | Learning Rate: How much are the layers being altered at each step) 
* Be wary of underfitting and overfitting. 
'''Action Items:'''
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get an EMADE run in at pace
|Completed
|April 5th, 2021
|April 7th, 2021
|April 10th, 2021
|}

== March 29th, 2021 ==
'''Team meeting notes:'''
* Was notified that I am going to be working with the NLP team for the remainder of the semester and future semesters
* It might be worth to note that wiki was down this week, so we were not able to access the GitHub repo and look at the NLP teams branch
* As a result, I was not able to do as much catching up and familiarizing myself as I wanted to do
'''Subteam Meetings'''
* In the first meeting, I was not able to make it but I was briefed on what was covered.
* The meeting was essentially becoming familiar with the specifics of what the team is doing and we were tasked with trying to run PACE on our machine. 
* Additionally, Cameron said that he will even release a video of him going through the process of setting it up to kind of walk us through it.
'''Action Items'''
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|- 
|Set up PACE
|Completed
|March 29th, 2021
|April 3rd, 2021
|April 7th, 2021
|}
== March 22nd, 2021 ==
'''Team meeting notes:'''
* Presented and assessed the results to of the three different methods for solving the titanic problem. 
* Listened to the presentations from the existing sub teams, trying to see which one might be most interesting to join now that we are done with bootcamp. 
'''Action Items'''
{| class="wikitable"
!Assignment
!Current Status
!Date Assigned 
!Suspense Date
!Date Resolved
|-
|Fill out canvas with the ranking of preference for subteams
|Completed
|March 22nd, 2021
|March 28th, 2021
|March 23rd, 2021
|}
== March 17th, 2021 ==
'''Team Meeting Notes:'''
* Setting up EMADE
* Note some of the common errors when trying to setup EMADE include wrong deap version, and not being on the GT VPN, as well as not having an anaconda environment that will use the correct python version. 
* Personally, I have been trying to mess around with the code just to be able to visualize a pareto frontier. I have been adding primitives and doing research on EMADE and poking around the source code for ideas. 
'''Subteam 2 Meeting Notes:'''
* We met Friday on the 19th of March and our main objective was to transfer the preprocessing we used into the EMADE version of the code. 
* Doing that, we were able to get a run and 25 generations. We are meeting again on the 21st to attempt to finalize some small errors that we are getting. 
* In the meeting on the 21st of March we kind of fine tuned things and worked on visualizing and improving a pareto frontier as well as worked on our presentation. 
'''Action Items'''
{| class="wikitable"
!Assignment
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get EMADE working with subteam
|Completed
|March 17th, 2021
|March 22nd, 2021
|March 20th, 2021
|-
|Tune parameters in input titanic file in order to be able to visualize a pareto frontier
|Completed
|March 17th, 2021
|March 22nd, 2021
|March 18th, 2021
|-
|Work on Presentation
|Completed
|March 17th, 2021
|March 22nd, 2021
|March 21st, 2021
|-
|Update titanic_splitter.py with our preprocessing for training data and the titanic dataset
|Completed
|March 17th, 2021
|March 22nd, 2021
|March 19th, 2021
|}

== March 10th, 2021 ==
'''Team Meeting Notes:'''
* Made sure everyone was  on the same page as to what needed to be done. 
* Had a "help session", in that we all tried to connect and run EMADE and consulted Dr. Zutty with any issues if they came up.
'''Subteam 2 Meeting Notes:'''
* We are possibly meeting on the 16th of March, to see if we can run EMADE through the remote database, that seems to be the only issue at this point. 
* We can connect remotely and work on the database, but not able to run EMADE remotely yet. 
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run EMADE remotely
|Completed
|March 10th, 2021
|March 17th, 2021
|March 16th, 2021
|-
|Meet with Subteam
|Completed
|March 10th, 2021
|March 17th, 2021
|March 16th, 2021
|-
|Work on running Titanic Solution in SQL
|Completed
|March 10th, 2021
|March 17th, 2021
|March 16th, 2021
|}

== March 3rd, 2021 ==
'''Team Meeting Notes:'''
* Introduction to EMADE
* Finished group presentations on the two titanic solutions
* Assignment: Run EMADE on titanic dataset as a group

'''Subteam 2 Meeting Notes:'''
* Trying to run EMADE and get everyone set up and able to run the SQL database locally and remotely. 
* We were all able to log into and initialize a database locally, but were struggling to get the remote connection. 
{| class="wikitable"
!Assignment
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run EMADE Locally
|Completed
|March 3rd, 2021
|March 10th, 2021
|March 3rd, 2021
|-
|Meet with subteam
|Completed
|March 3rd, 2021
|March 10th, 2021
|March 8th, 2021
|-
|Get connected remotely to SQL Database
|Completed
|March 3rd, 2021
|March 10th, 2021
|March 10th, 2021
|}

== February 24th, 2021 ==
'''Team Meeting Notes:'''
* Subteam Presentations, got through group 3 and are going to finish with the rest of the groups next week.
* Transitioning into using emade, we must install emade before the next meeting. 
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install EMADE
|Completed
|Feb 24th, 2021
|March 3rd, 2021
|March 2nd, 2021
|}

== February 17, 2021 ==
'''Team meeting notes:'''
* Discussion about the techniques each subteam attempted to utilize when attempting to solve the titanic problem with ML. 
* '''New Assignment:''' Use Multi-Objective genetic programming to solve the titanic problem already solved with ML. Essentially, we can get a nice pareto front with a single genetic programming model, compared to the 5 ML models we combined to visualize a pareto front when solving it the week prior. 
'''Subteam 2 meeting notes:'''
* Met up on Saturday, February 20th to discuss whether to preprocess the data in a different way or just proceed with building the genetic algorithm. We decided to keep the current preprocessing pipeline we already developed 1) because we thought we approached it in  a nice way and we didn't see a need to change it and 2) to have a more direct comparison of the results of solving this problem with MOGP and ML. 
* Good progress was made and a working pipeline was created, but we need to meet again to refine the results to get our AUC looking as low as possible and the last population concentrated into a decent looking manner. 
* We met again on Tuesday, February 23rd to clean up the pipeline and finalize the methods of which we would use to solve the problem. We got interesting results and an AUC that is pretty low and skewed towards FPR (our best individual had a FPR = 0). 
* We decided to keep these results and use our teammates notebook because she seemed to have marginally better results. 
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet up with group
|Completed
|Feb 17th, 2021
|Feb 24th, 2021
|Feb 20th, 2021
|-
|Tune DEAP Model
|Completed
|Feb 17th, 2021
|Feb 24th, 2021
|Feb 22nd, 2021
|-
|Research MOGP 
|Completed
|Feb 17th, 2021
|Feb 24th, 2021
|Feb 22nd, 2021
|-
|Work on my xgboost part of presentation
|Completed
|Feb 17th, 2021
|Feb 24th, 2021
|Feb 24th, 2021
|}

== February 10, 2021 ==
* Here is the link to my personal titanic solution: https://colab.research.google.com/drive/1YPow7FtsNSRtFLWO7PrqOlwgBaCozKiM?usp=sharing
* Discussed multiple objectives
* Discussed the team project
* Pareto ranks were used to form the groups and select team leads. I am apart of group 2.
* '''Assignment:''' Each person in each subteam is to use the titanic dataset from the kaggle competition to create their own ML model which must be pareto codominant with every other sub team members. In other words, every subteam member will have an associated FPR and FNR values for their model, and no single member can have lower of both of those values than any other member. 

=== Subteam 2 meeting notes: ===
* Joined the subteam group 2 slack channel and began preprocessing the data.
* Met on Saturday, '''Feb 13, 2021.''' 
* Preprocessed the titanic dataset.
* '''Key takeaways:'''
*# There were a lot of missing values in the "Cabin" and "Age" columns, so we dropped the cabin column all together and filled in the missing values of the age with the median of the column. 
*# We decided that there were a number of features that were not going to be important in predicting the survival of a given passenger. We decided to drop the features, "Name", "Ticket Number", "Cabin". 
*# Additionally, we investigated the possibility of ones "title" being correlated with survival. That is, we built a function that extrapolated any title (miss, master, Dr, etc.) and then calculated the survival rate all of these titles. This was a decent case study, but we did not want to open up the possibility of overfitting to the training set, so we did not use these titles as a predictive feature. 
*# We also used one hot encoding on the gender of the passenger because models love numbers. 
* We also met on '''Feb 14, 2021'''. Here are some of the main objectives of that meeting:
*# Built a more generalized preprocess function and applied it to both the training and the testing sets. 
*# Assigned a model for each member to do and made sure to be clear about the achieved (FPR, FNR) values that everyone achieved with their given model in the hopes that we are codominant.
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Schedule Subteam Meeting
|Completed
|Feb 10th, 2021
|Feb 17th, 2021
|Feb 10th, 2021
|-
|Set Up Google Colab Notebook to Solve Titanic Problem
|Completed
|Feb 10th, 2021
|Feb 17th, 2021
|Feb 13th, 2021
|-
|Meet with Group to Preprocess Data
|Completed
|Feb 10th, 2021
|Feb 17th, 2021
|Feb 14th, 2021
|-
|Tune xgboost Model
|Completed
|Feb 10th, 2021
|Feb 17th, 2021
|Feb 15th, 2021
|}

== February 3, 2021 ==
* Here is the link to my self evaluation rubric. [[files/VIP AAD notebook rubric HarrisBarton.docx]] 
'''Team Meeting Notes:'''
* Submit self-evaluation rubric by next week.
* Complete the last lab by next week.
* We are going to proceed with team projects starting next week.
* Email Dr. Rholing rating and introducing yourself. 
'''Lecture Notes:'''
* Key Terms:
# '''<u>Gene Pool:</u>''' The set of genome to be evaluated from a generation.
# '''<u>Genome:</u>''' Genotypic description of individuals
# '''<u>Search Space:</u>''' The set of all possible genome. I relate this to the universal set, in probability and set theory. 
# '''<u>Phenotype/Objective:</u>''' The reference frame from which each genome is scored against. Note, the words genome and individual are used interchangeably and you might find where someone used genome, they might put (individual) in parenthesis next to it.
# '''<u>Objective Space:</u>''' The set of objectives. Objective scores give an individual a point in objective space. I will go more into detail about objective space and why we use it later below. For now, we can just think of it as the space which we can visualize the objective scores of individuals in reference to each other. Well, that kind of is the definition, honestly. 
# '''<u>True Positive (TP):</u>''' What we predict correctly to be positive.
# '''<u>True Negative (TN):</u>''' What we predict correctly to be negative.
# '''<u>False Positive (FP):</u>''' What we predicted to be positive but it is actually negative. 
# '''<u>False Negative (FN):</u>''' What we predicted to be negative but is actually positive.
# '''<u>Pareto:</u>''' An individual is pareto optimal iff there exists no other individual in the given population that outperforms them on EVERY objective. Note, this does not mean they can't be outperformed in anything. In theory, an individual may be pareto optimal even if they are outperformed on 99/100 of the objectives.
# '''<u>Pareto Front:</u>''' The pareto front is the set of all pareto optimal individuals. Note, it might seem logical to only assign a nonzero probability of selection to individuals in the pareto front; however, we want to maintain diversity as discussed in last lecture, so we have to still assign a nonzero probability to individuals not in the pareto front. Well, we do not have to of course, but it will be beneficial to do so. 
# '''<u>Strength:</u>''' Each individual is given a strength, denoted by '''''S'''''. Strength is how many in the population an individual dominates.
# '''<u>Rank:</u>''' Each individual also receives a rank, '''''R'''''. Rank is the sum of strengths of the individuals which dominate it. Let '''''ind''''' be an individual and let it have rank '''''R''''' = 10. Additionally, let the number of individuals that dominate '''''ind''''' be '''''N''''' = 5. Then, it follows that the rank, '''''R''''' simply means that, between the 5 individuals that dominate '''''ind,''''' the collective strength of all of these 5 individuals is 10.
* We can store the TP, TN, FP, and FN as elements in a matrix known as the confusion matrix. Additionally, these elements that make up this matrix can be used to solve for other important metrics. Below we will define those values:
*# '''<u>Positive Prediction Value (PPV):</u>''' PPV can be defined as the difference between our True Positive value, and the sum of the True Positive Value and the False Positive Value. Using math, PPV = TP / (TP + FP). ------> Bigger is better here
*# '''<u>False Discovery Rate (FDR):</u>''' FDR can be defined as the difference between the False Positive Value and the sum of the True Positive and the False Positive value. Mathematically,                          FDR = FP / (FP + TP) = 1 - PPV. Think of PPV as a complement of FDR? -------> Smaller is better
*# '''<u>Negative Predictive Value (NPV):</u>''' NPV can be defined as the difference between the True Negative value and the sum of the True Negative and the False Negative. Mathematically,       NPV = TN / (TN + FN). ------> Bigger is better.
*# '''<u>Accuracy (ACC):</u>''' Accuracy can be defined as the difference between the sum of the True Positive and the True Negative and the sum of the Positive and Negative. Mathematically,                     ACC = (TP + TN) / (P + N) -----> do I even have to say bigger is better? 

=== Lab 2 - Part 2 (Multi-objective Genetic Programming): ===
* In this part of the lab, we take more than one objective for the evaluation of any given individual, rather than using one objective, like in the last part of the lab. 
* Specifically, our two objectives will be the Mean-squared error (MSE), and the size of the tree.  
* The main focus is to reduce the area under the curve of the pareto front.   
* We used DEAP's mu plus lambda evolutionary algorithm, where mu is the number of individuals to select for the next generation and lambda is the number of children to produce at each generation.   
* I managed to decrease the AUC about 25%, per the goal of the lab. I tried a number of things but I think the main things helped me, adding more functions to the list of primitives, decreasing the probability of mutation, and adding more mutation functions. Naively, I messed around with the mu and lambda values just to observe what might happen with certain values. Upon doing this, I decreased lambda to 1 and saw a huge decrease (about %71) in the AUC of the pareto front. Knowing that this seemed odd, I ran the code a couple of more times and saw that the AUC had a lot of fluctuation. This indicated that this decrease was lucky and that having a lambda that low only adds uncertainty to our results, which I did not want.   
* I could be wrong about the lambda observation, maybe the trade-off between the uncertainty and the low AUC is such that we could optimize the combination of lambda in such a way to optimize this ratio.   
* Overall, I was not happy with where I was able to get. I would have liked to got a lot more of a decrease, but settled for this result for now. I do plan to look into it more after the suspense date to do some more work and hopefully decrease the AUC more.   
* Below is the lowest AUC I was able to achieve.
**[[files/AUC .png|none|thumb|71% decrease in AUC from original 2.45]]
* 
{| class="wikitable"
!Task 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review Lectures from Bootcamp
|Completed
|Feb 3rd, 2021
|Feb10th, 2021
|Feb 10th, 2021
|-
|Complete Lab 2
|Completed
|Feb 3rd, 2021
|Feb10th, 2021
|Feb 9th, 2021
|-
|Complete Self-Evaluation
|Completed
|Feb 3rd, 2021
|Feb10th, 2021
|Feb 9th, 2021
|}

== January 27, 2021 ==
'''Team Meeting Notes:'''
# Learn Genetic Programming through the lecture and begin Lab 2
'''Lecture Notes:'''
# Recall, in genetic algorithms, we take some individual and then create some function evaluator to obtain some scores. Genetic Programming differs from that in that, in this case, the individual is the function itself. 
#* For example, say you have a list like, [1,2,3,4,5]. Take that list and then "apply" the individual (the function) like so''''':'''''
#**  '''''[1,2,3,4,5] --> {individual} --> [1,4,9,16,25] --> evaluator.'''''
#** Note, the individual is the function which returns the square of each element. 
# A program can be represented as a tree structure, 
#* Nodes are referred to as primitives and represent functions. 
#* Leaves are called terminals and represent parameters. 
#** The input can be thought of as some particular type of terminal.
#** The output is produced at the root of the tree. 
#*How is the tree stored?
#**To traverse this tree, first visit the root, and then the left subtree, and then the right subtree. Using this, we can see that the tree for this is '''''[-, 2, +, 0, 1]'''''. [[files/Image22.png|thumb|none]]

* '''Crossover in Genetic Programming:'''
** Crossover in this case is essentially just exchanging two subtrees. All you have to do is pick some point at random in the two trees, and replace everything where the new empty slot is and below with the other point and everything below it. Now that might not make sense, so see the diagram below.[[files/Genetic-programming-Tree-encoding-individual-crossover.png|none|thumb|341x341px]]

* '''Mutation in Genetic Programming:'''
** Mutation can involve:
**# Inserting node or subtree.
**# Deleting a node or Subtree.
**# Changing a node. 
'''Lab 2'''
* In this lab, our individual class inherited from the DEAP's '''''PrimitiveTree''''' class instead of a simple list like last lab. This is because our individuals are represented in a tree structure. In genetic programming, as discussed above in the lecture notes, trees are made up of variables called primitives and functions. Each primitive is a node in the tree where the leaves of the nodes are inputs to the parent node.
# Imported the relevant libraries.
# Added a set of mathematical operators to our primitive set. I added two to the primitive set, np.exp and np.sin, both with arity = 1.  
# Registered the genetic operators and the mutation methods. I added an insert method. 
Results thus far:
* Best individual is negative(cos(subtract(sin(subtract(sin(cos(subtract(subtract(cos(add(x, sin(x))), x), sin(multiply(x, multiply(tan(x), sin(x))))))), x)), x))), (0.11103899805699875, 25.0)
* Graph of results with added primitives and added mutation function. [[files/Lab2Plot.png|none|thumb]] 
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Write Lecture Notes for Lecture 2
|Completed
|Jan 27th, 2021
|Feb 3rd, 2021
|Jan 29th, 2021
|-
|Complete Lab 2 pt.1
|Completed
|Jan 27th, 2021
|Feb 3rd, 2021
|Feb 3rd, 2021
|-
|Review Lecture Slides
|Completed
|Jan 27th, 2021
|Feb 3rd, 2021
|Feb 2nd, 2021
|}

{| class="wikitable" !Task !Current Assigned !Suspense Date !Date Resolved |- |Lecture Notes |February 3, 2021 |February 10, 201 |February 9, 2021 |- |Self-Evaluation Rubric |February 3, 2021 |February 10, 2021 |February 9, 2021 |- |Part 2 of Lab 2 |February 3, 2021 |February 10, 2021 |February 10, 2021 |}


== January 20, 2021 ==
'''Team Meeting Notes:'''
* For the first 10 weeks, first semester students will go through a bootcamp, where there is lab to be completed each week. 
** '''Lecture Notes:'''
*** Keywords:
**** ''individual:'' One specific candidate in the population.
**** ''Population:'' group of individuals whose properties will be altered.
**** ''Objective:'' A value used to characterize individuals that you are trying to maximize or minimize.
**** ''Fitness:'' Relative comparison to other individuals. Sort of like a percentile. 
**** ''Evaluation:'' A function that computes the objective of an individual. 
**** ''Selection:'' Gives preference to the better individuals.
**** ''Fitness proportionate:'' The greater the fitness value, the higher probability of being selected.
**** ''Tournament:'' The several tournaments among individuals (number of individuals in each tournament is dependent on tournament size); winners are selected for mating.
**** Mutate: introduces modifications at random to maintain diversity.

* '''Lab 1''' 
** '''''One Max Problem''''': To solve the One Max Problem is to, from a population of individuals, get an individual of the highest fitness. In this case, the size of the population is is 100 bits, and our goal is to get a bit string full of 1's. It follows that we define the fitness of an individual in this case to be the sum of the bits in the string. 
** Solving the problem is broken into 4 'steps' as described below:
**# Evaluate the fitness score of the initial population. Repeat for 40 generations.
**# Use tournament selection as defined above to select the next generation of individuals.
**# Do a two-point cross over of the selected generation.
**# Perform a mutation.
**# Evaluate the fitness.
** '''''N Queens Problem'''''
*** The objective of the problem is to determine a configuration of ''N'' queens on a ''NxN'' chessboard s.t. no queen can take any other queen. In this edition of the 'N Queens Problem', each queen is assigned to one column, and only one queen can be on each line.
**** '''Procedure'''
****# Created fitness and individual classes, minimizing the weights. The reason we minimized the weights is to hopefully minimize the conflicts between queens. 
****# An individual in this problem would be something like, let's say n = 3. Then, an individual would look like this, [7,0,2]. Let each element be a queen, and individual[i] -1 represent the column number. For example, the first element of this list is a queen at column 8. 
****# Created an evaluation function that counts the number of conflicts along the diagonal. 
****# Defined the crossover function. Specifically, partially matched crossover was used to represent swapping pairs of queens positions between two parent individuals. 
****# Defined the mutation function. Namely, we used the ShuffleIndexes() mutation function.
****'''Results'''
[[files/Lab1Plot.png|thumb|325x325px|'''Figure 1.''' A plot showing the average, maximum, and minimum over 100 generations.|center]]

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Join Slack
|Completed
|January 20, 2021
|January 27, 2021
|January 20, 2021
|-
|Install Anaconda, GT GitHub
|Completed
|January 20, 2021
|January 22, 2021
|January 22, 2021
|-
|Install Jupyter Notebook
|Completed 
|January 20, 2021
|January 22, 2021
|January 24, 2021
|-
|setup Notebook and Write Lecture Notes for Lecture 1
|Completed
|January 20, 2021
|January 22, 2021
|January 26, 2021
|-
|Finish Lab 1
|Completed
|January 20, 2021
|January 27, 2021
|January 27, 2021
|}