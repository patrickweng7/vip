=== Harris Barton ===
* Email: Hbarton7@gatech.edu
* Phone: 850-687-9318
* Interests: Machine Learning, Deep Learning, Digital Signal Processing, BJJ, Chess
= Fall 2021 = 
== September 6th, 2021 == 
* There was no team meeting this week due to Labor Day. 
'''Subteam Meeting Notes:'''
* Met with image processing subteam and came up with some action items before we meet again. 
* We discussed possible directions for the subteam in terms of what our overarching goal is and what subset of image processing we might like to work in. 
* The overarching goal of this team this semester is going to be to add new primitives to EMADE that will assist us in replicating a paper which uses traditional ML methods to do image processing but using EMADE and (hopefully) outperform it. 
* We think this might be decent route to a paper, especially if we can successfully outperform a paper which uses traditional ML methods. 
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read paper published by VIP subteam
|In Progress
|September 6th, 2021
|September 13th, 2021
|N/A (yet)
|- 
| Refamiliarize with EMADE
| In Progress
| September 6th, 2021
| September 13th, 2021
| N/A (yet)
|-
|Find potential dataset/paper to work with
|In progress
|September 6th, 2021
|September 13th, 2021
|n/a (yet)
|}
== August 30th, 2021 == 
''' Team Meeting Notes: '''
* We discussed the ideas that we had brainstormed for the direction of each subteam and which subteams were going to stay and which ones we might sideline for the moment. 
* Dr. Zutty seemed to think there was some potential roadblocks in our idea for pursuing GP for Image Registration, so we discussed some possible ways to solve that problem and also some alternative directions for the subteam worth pursuing. 
* By next week we will have all of the teams sorted and will have probably picked team leaders for all of the new subteams. 
* I created a slack channel for the Image Processing team and set up a LettuceMeet to figure out when our weekly meetings will happen. 
''' Action Items: '''
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create Slack channel for subteam
|Completed
|August 30th, 2021
|September 6th, 2021
|August 31st, 2021
|- 
| Agree on Date for Subteam Meetings
| In Progress
| August 30th, 2021
| September 6th, 2021
| September 1st, 2021
|}
== August 23rd, 2021 ==
''' Team Meeting Notes: '''
* We brainstormed some ideas for some new teams which might be able to do some interesting research. I thought interpretability and image processing both seemed like intriguing routes.
* I did some research into what sort of research is currently going on in the field of interpretability and how we might be able to do something interesting, and I created the slack channel to brainstorm what kind of direction the team might go in, but I ended up gaining more of an interest in the image processing team. 
* Maxim, Aryaan, and I met up to discuss possible directions and we came across some interesting papers in applications of genetic programming to image registration. We also kind of realized that this might be a tough route to go down because of the lack of current data available. 
* I believe that it might be best to start of doing some more basic image processing with emade, like an image classification problem, so I posed this idea to the newly formed team.
''' Action Items:'''
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Brainstorm for Interesting Subteams
|Completed
|August 23rd, 2021
|August 29th, 2021
|August 27th, 2021
|- 
| Submit Subteam Rankings
| Completed
| August 23rd, 2021
| August 29th, 2021
| August 29th, 2021
|}
= Spring 2021 = 
== April 26th, 2021 ==
'''Team/Breakout Session Notes:'''
* If you have not done peer evals yet, do them.
* We have final presentations this Friday, and need to finish them up
'''Subteam Meeting Notes/Accomplishments:'''
* We met with Cameron to give him an update on what the error might be from, since our initial idea seemed to be incorrect. 
* There was recently a push to the NN branch which flattened the embedding layers, and we tried to add this to the code and see if it made any difference.
 # Handle Keras applications
  elif isinstance(layer, dict):
  if layer['type'] == PretrainedModel.MOBILENET:
      s = data_pair.get_train_data().get_instances()[0].get_stream().get_data().shape
      new_layer = MobileNet(input_shape=s, include_top = False, weights = "imagenet")
      input_layers.append(new_layer.inputs)
      new_layer = new_layer.output
      curr_layer = new_layer
      if layer['type'] == PretrainedModel.INCEPTION:
          s = data_pair.get_train_data().get_instances()[0].get_stream().get_data().shape
          new_layer = InceptionV3(input_shape=s, include_top = False, weights = "imagenet")
          input_layers.append(new_layer.inputs)
          new_layer = new_layer.output
          curr_layer = new_layer
      if layer['type'] == PretrainedModel.VGG:
          s = data_pair.get_train_data().get_instances()[0].get_stream().get_data().shape
          new_layer = VGG16(input_shape=s, include_top = False, weights = "imagenet")
          input_layers.append(new_layer.inputs)
          new_layer = new_layer.output
          curr_layer = Flatten()(new_layer)  '''# Here is where the layer is flattened and we simply added this line to the code to see if this fixed our problem.'''
* Unfortunately, the same error resulted, and we have decided to turn our focus more into assisting with the presentation and data compilation. 
* There are a couple of things that I am in charge of speaking about in the presentation; namely, the SQL errors and the best individual and the differences between the other previous models.
* I have encountered a good deal of the SQL errors and have committed the solutions to many of them to memory. I think we are going to work on having a gallery of solutions to all of the PACE/SQL errors in the future which I think will be a very worth while effort. 
* During the summer, I intend to familiarize myself a lot more with the project as a whole and really gain some insight into the bigger picture of this team and VIP all together. 
* I hope to come back in the fall with a much better understanding of it all and, if all goes well, I might even do some interesting things to contribute to the team during the summer even though VIP is not officially on. I am really intrigued by the work and intend to work on it as much as possible even in off semesters. 
'''Action Items:'''
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Peer Evaluation
|Completed
|April 19th, 2021
|April 27th, 2021
|April 27th, 2021
|-
|Debug the Pretrained Embedding Layer Method
|Incomplete
|April 19th, 2021
|April 29th, 2021
|
|- 
|Focus on presentation and prepare my part of it
|Completed
|April 26th, 20221
|April 30th, 2021
|April 29th, 2021
|}
== April 19th, 2021 ==
'''Team/Breakout Session Notes:'''
* Need to complete peer evaluations
* The presentation will be Friday, the 30th, from 5:00pm-8:15pm. 
'''Subteam Meeting Notes/Accomplishments:'''
* Cameron made a big push to the branch that allows for more worker processes, which in turn allows for us to run more generations in a shorter period of time. 
* I cloned the new branch and I made the necessary changes so that I can have the benefits when debugging and trying to do EMADE runs. 
* Prahlad and I were attempting to find potential solutions to the json.loads error.
* The line that is giving us an error is supposedly giving us an error because it is supposed to deserialize a string of JSON data but the jsonified keras model is not a pure string.
* My simple solution to a seemingly simple solution was to just use json.stringify to convert it to a string then deserialize it after that.
 config = model.to_json()
 print("config: ",type(config))
 data = json.stringify(config)
 data = json.loads(config)
 for layer in data['config']['layers']:
    if layer['class_name']=='Embedding':
       if data_pair.get_datatype() == 'recdata':
          layer['config']['input_dim'] = numforembedding[i]
          i+=1
  elif data_pair.get_datatype() == 'textdata':
  layer['config']['input_dim'] = vocab_size
* However, Prahlad and I noticed that we seemed to get the same error no matter the changes we made. Thus, we decided to try and put some debugging statements to see if we were even looking in the right place for the error. 
* Prahlad made the following changes to the code to see what was going on:
 def PretrainedEmbeddingLayer(data_pair, initializer, layerlist): 
   """Creates Embedding layer  
   Args:   
       empty_model: empty_model as terminal    
       data_pair: given dataset    
       out_dim: ouput dimension    
               layerlist: layerlist to append to 
   Returns:    
       Keras Embedding Layer   
   """ 
 ->print("Reached primitive")
   maxlen = MAXLEN 
   numwords=NUMWORDS   
   out_dim = abs(out_dim)  
   data_pair, vocab_size, tok  = tokenizer(data_pair, maxlen, numwords)
   out = {PretrainedEmbedding.GLOVE:100, PretrainedEmbedding.GLOVEFASTTEXT:501, PretrainedEmbedding.FASTTEXT:300, PretrainedEmbedding.GLOVETWITTER:200 }
   out_dim = out[initializer]   
   initializer = Constant(get_embedding_matrix(initializer, vocab_size, tok))
   layerlist.mylist.append(Embedding(vocab_size,out_dim , input_length=maxlen, embeddings_initializer=initializer))
   return layerlist
* It turns out, our code is not actually even giving us an error where we thought it was and unfortunately, this error might be more complex than we thought. 
* For reference, Prahlad has the stack trace of the error on his notebook. I was having issues running the python files today so we were debugging and doing the runs with his computer. 
'''Action Items:'''
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Peer Evaluation
|Completed
|April 19th, 2021
|April 27th, 2021
|April 27th, 2021
|-
|Debug the Pretrained Embedding Layer Method
|Incomplete
|April 19th, 2021
|April 29th, 2021
|
|}

== April 12th, 2021 ==
'''Team/Breakout Session Notes:'''
* In the breakout session, Cameron compiled a list of things that might be good to work on:
  1) Fix json error with pretrained Embedding Layer primitives.
  2) Try to make NNlearner a subtree in individuals.
  3) Merge PACE and SQL errors into Cache-V2 branch.
'''Subteam Meeting Notes/Accomplishments:'''
* Prahlad and I are tasked to look into Json error with pretrained Embedding layers
* The pretrained embedding layer has an attribute, input_dim, and it is vocab size when learning word embeddings. 
* Tried to set input_dim to vocab size, but it is not being properly set, probably because the jsonified keras object is not a pure string, but it needs to be for it to work. 
* I spent some time trying to debug by editing the Neural Network Methods file and seeing where the problem might be and how to possibly fix it. 
* Here is where Cameron thinks the error most likely lies:
         if data_pair.get_datatype() == 'recdata' or data_pair.get_datatype() == 'textdata':
            n_users = len(user_enc.classes_)
            n_items = len(item_enc.classes_)
            numforembedding = [n_users, n_items]
            i = 0
            config = model.to_json()
            print("config: ",type(config))
            data = json.loads(config)   # '''where the error occurs. basically, the JSONified keras model is not a pure string, so using json.loads doesn't work'''
            for layer in data['config']['layers']:
                if layer['class_name']=='Embedding':
                    if data_pair.get_datatype() == 'recdata':
                        layer['config']['input_dim'] = numforembedding[i]
                        i+=1
                    elif data_pair.get_datatype() == 'textdata':
                        layer['config']['input_dim'] = vocab_size
                json_object = json.dumps(data)
                print(json_object)
                model = model_from_json(json_object)
            if data_pair.get_regression():
                model.compile(loss= 'mean_squared_error', optimizer = optimizer, metrics= ['accuracy'])
            elif len(data_pair.get_train_data().get_target().shape) != 1:
                model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
            else:
                model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
        print(model.summary())
        model = KerasPickleWrapper(model)
'''Action Items:'''
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get an EMADE run in at pace
|Completed
|April 12th, 2021
|April 17th, 2021
|April 14th, 2021
|-
|Choose a task to focus on
|Completed
|April 12th, 2021
|April 17th, 2021
|April 12th, 2021
|}

== April 5th, 2021 ==
'''Team Meeting Notes:'''
* Dr. Zutty gave a presentation on Hypothesis Testing. 
* At a high level, hypothesis testing is somewhat self-explanatory, but the way it is implemented may vary depending on the nature of what you are trying to prove. 
* In hypothesis testing, you essentially have two hypotheses: the null hypothesis, and an alternative hypothesis. 
* The sample data must provide sufficient evidence to reject the null hypothesis and conclude that the effect exists in the population. Ideally, a hypothesis test fails to reject the null hypothesis when the effect is not present in the population, and it rejects the null hypothesis when the effect exists
'''Subteam Meeting Notes/Accomplishments:'''
* This past week, I have mainly been working on figuring out the small errors that I have been encountering in getting set up in PACE. I had some issues with my anaconda environment, but figured that one out kind of quickly. Most annoying of all, I got stuck for a while on an error that essentially stemmed from me not having a file in the correct place. PACE had an interesting way of telling me that so it took me a while to figure out that it was such a small thing causing my trouble.
* We had a meeting on Friday at 6:00 p.m. and it was going over ML and NLP concepts. 
'''Some Notes On Topics Covered:'''
* Supervised vs Unsupervised Learning
 (The main difference between the two types is that supervised learning is done using a ground truth, or in other words, we have prior knowledge of what the output values for our samples should be.)
* Neural Network Basics
 (The basic layout is something like '''Input --> Layer 1 --> Layer 2 ... --> Layer N --> Output'''. I kind of like to think of it like a sequence of functions, even though this is kind of a reductionist way of explaining it. It does work in a simple way, for essentially what is going on is that something is done to the data at each layer, and acts as an input to the next layer on and on until the Nth layer is reached and is passed to the output.)
* Inside of the Layer
 (Operation: The actual computation/mapping on the data | Activation: The introduction on non-linearity of the model.)
* Training a Neural Network
 (Feed forward: evaluate the model | Back propagation: Using gradients to update the weights | Learning Rate: How much are the layers being altered at each step) 
* Be wary of underfitting and overfitting. 
'''Action Items:'''
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get an EMADE run in at pace
|Completed
|April 5th, 2021
|April 7th, 2021
|April 10th, 2021
|}

== March 29th, 2021 ==
'''Team meeting notes:'''
* Was notified that I am going to be working with the NLP team for the remainder of the semester and future semesters
* It might be worth to note that wiki was down this week, so we were not able to access the GitHub repo and look at the NLP teams branch
* As a result, I was not able to do as much catching up and familiarizing myself as I wanted to do
'''Subteam Meetings'''
* In the first meeting, I was not able to make it but I was briefed on what was covered.
* The meeting was essentially becoming familiar with the specifics of what the team is doing and we were tasked with trying to run PACE on our machine. 
* Additionally, Cameron said that he will even release a video of him going through the process of setting it up to kind of walk us through it.
'''Action Items'''
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|- 
|Set up PACE
|Completed
|March 29th, 2021
|April 3rd, 2021
|April 7th, 2021
|}
== March 22nd, 2021 ==
'''Team meeting notes:'''
* Presented and assessed the results to of the three different methods for solving the titanic problem. 
* Listened to the presentations from the existing sub teams, trying to see which one might be most interesting to join now that we are done with bootcamp. 
'''Action Items'''
{| class="wikitable"
!Assignment
!Current Status
!Date Assigned 
!Suspense Date
!Date Resolved
|-
|Fill out canvas with the ranking of preference for subteams
|Completed
|March 22nd, 2021
|March 28th, 2021
|March 23rd, 2021
|}
== March 17th, 2021 ==
'''Team Meeting Notes:'''
* Setting up EMADE
* Note some of the common errors when trying to setup EMADE include wrong deap version, and not being on the GT VPN, as well as not having an anaconda environment that will use the correct python version. 
* Personally, I have been trying to mess around with the code just to be able to visualize a pareto frontier. I have been adding primitives and doing research on EMADE and poking around the source code for ideas. 
'''Subteam 2 Meeting Notes:'''
* We met Friday on the 19th of March and our main objective was to transfer the preprocessing we used into the EMADE version of the code. 
* Doing that, we were able to get a run and 25 generations. We are meeting again on the 21st to attempt to finalize some small errors that we are getting. 
* In the meeting on the 21st of March we kind of fine tuned things and worked on visualizing and improving a pareto frontier as well as worked on our presentation. 
'''Action Items'''
{| class="wikitable"
!Assignment
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get EMADE working with subteam
|Completed
|March 17th, 2021
|March 22nd, 2021
|March 20th, 2021
|-
|Tune parameters in input titanic file in order to be able to visualize a pareto frontier
|Completed
|March 17th, 2021
|March 22nd, 2021
|March 18th, 2021
|-
|Work on Presentation
|Completed
|March 17th, 2021
|March 22nd, 2021
|March 21st, 2021
|-
|Update titanic_splitter.py with our preprocessing for training data and the titanic dataset
|Completed
|March 17th, 2021
|March 22nd, 2021
|March 19th, 2021
|}

== March 10th, 2021 ==
'''Team Meeting Notes:'''
* Made sure everyone was  on the same page as to what needed to be done. 
* Had a "help session", in that we all tried to connect and run EMADE and consulted Dr. Zutty with any issues if they came up.
'''Subteam 2 Meeting Notes:'''
* We are possibly meeting on the 16th of March, to see if we can run EMADE through the remote database, that seems to be the only issue at this point. 
* We can connect remotely and work on the database, but not able to run EMADE remotely yet. 
{| class="wikitable"
!Assignment 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run EMADE remotely
|Completed
|March 10th, 2021
|March 17th, 2021
|March 16th, 2021
|-
|Meet with Subteam
|Completed
|March 10th, 2021
|March 17th, 2021
|March 16th, 2021
|-
|Work on running Titanic Solution in SQL
|Completed
|March 10th, 2021
|March 17th, 2021
|March 16th, 2021
|}

== March 3rd, 2021 ==
'''Team Meeting Notes:'''
* Introduction to EMADE
* Finished group presentations on the two titanic solutions
* Assignment: Run EMADE on titanic dataset as a group

'''Subteam 2 Meeting Notes:'''
* Trying to run EMADE and get everyone set up and able to run the SQL database locally and remotely. 
* We were all able to log into and initialize a database locally, but were struggling to get the remote connection. 
{| class="wikitable"
!Assignment
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run EMADE Locally
|Completed
|March 3rd, 2021
|March 10th, 2021
|March 3rd, 2021
|-
|Meet with subteam
|Completed
|March 3rd, 2021
|March 10th, 2021
|March 8th, 2021
|-
|Get connected remotely to SQL Database
|Completed
|March 3rd, 2021
|March 10th, 2021
|March 10th, 2021
|}

== February 24th, 2021 ==
'''Team Meeting Notes:'''
* Subteam Presentations, got through group 3 and are going to finish with the rest of the groups next week.
* Transitioning into using emade, we must install emade before the next meeting. 
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install EMADE
|Completed
|Feb 24th, 2021
|March 3rd, 2021
|March 2nd, 2021
|}

== February 17, 2021 ==
'''Team meeting notes:'''
* Discussion about the techniques each subteam attempted to utilize when attempting to solve the titanic problem with ML. 
* '''New Assignment:''' Use Multi-Objective genetic programming to solve the titanic problem already solved with ML. Essentially, we can get a nice pareto front with a single genetic programming model, compared to the 5 ML models we combined to visualize a pareto front when solving it the week prior. 
'''Subteam 2 meeting notes:'''
* Met up on Saturday, February 20th to discuss whether to preprocess the data in a different way or just proceed with building the genetic algorithm. We decided to keep the current preprocessing pipeline we already developed 1) because we thought we approached it in  a nice way and we didn't see a need to change it and 2) to have a more direct comparison of the results of solving this problem with MOGP and ML. 
* Good progress was made and a working pipeline was created, but we need to meet again to refine the results to get our AUC looking as low as possible and the last population concentrated into a decent looking manner. 
* We met again on Tuesday, February 23rd to clean up the pipeline and finalize the methods of which we would use to solve the problem. We got interesting results and an AUC that is pretty low and skewed towards FPR (our best individual had a FPR = 0). 
* We decided to keep these results and use our teammates notebook because she seemed to have marginally better results. 
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet up with group
|Completed
|Feb 17th, 2021
|Feb 24th, 2021
|Feb 20th, 2021
|-
|Tune DEAP Model
|Completed
|Feb 17th, 2021
|Feb 24th, 2021
|Feb 22nd, 2021
|-
|Research MOGP 
|Completed
|Feb 17th, 2021
|Feb 24th, 2021
|Feb 22nd, 2021
|-
|Work on my xgboost part of presentation
|Completed
|Feb 17th, 2021
|Feb 24th, 2021
|Feb 24th, 2021
|}

== February 10, 2021 ==
* Here is the link to my personal titanic solution: https://colab.research.google.com/drive/1YPow7FtsNSRtFLWO7PrqOlwgBaCozKiM?usp=sharing
* Discussed multiple objectives
* Discussed the team project
* Pareto ranks were used to form the groups and select team leads. I am apart of group 2.
* '''Assignment:''' Each person in each subteam is to use the titanic dataset from the kaggle competition to create their own ML model which must be pareto codominant with every other sub team members. In other words, every subteam member will have an associated FPR and FNR values for their model, and no single member can have lower of both of those values than any other member. 

=== Subteam 2 meeting notes: ===
* Joined the subteam group 2 slack channel and began preprocessing the data.
* Met on Saturday, '''Feb 13, 2021.''' 
* Preprocessed the titanic dataset.
* '''Key takeaways:'''
*# There were a lot of missing values in the "Cabin" and "Age" columns, so we dropped the cabin column all together and filled in the missing values of the age with the median of the column. 
*# We decided that there were a number of features that were not going to be important in predicting the survival of a given passenger. We decided to drop the features, "Name", "Ticket Number", "Cabin". 
*# Additionally, we investigated the possibility of ones "title" being correlated with survival. That is, we built a function that extrapolated any title (miss, master, Dr, etc.) and then calculated the survival rate all of these titles. This was a decent case study, but we did not want to open up the possibility of overfitting to the training set, so we did not use these titles as a predictive feature. 
*# We also used one hot encoding on the gender of the passenger because models love numbers. 
* We also met on '''Feb 14, 2021'''. Here are some of the main objectives of that meeting:
*# Built a more generalized preprocess function and applied it to both the training and the testing sets. 
*# Assigned a model for each member to do and made sure to be clear about the achieved (FPR, FNR) values that everyone achieved with their given model in the hopes that we are codominant.
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Schedule Subteam Meeting
|Completed
|Feb 10th, 2021
|Feb 17th, 2021
|Feb 10th, 2021
|-
|Set Up Google Colab Notebook to Solve Titanic Problem
|Completed
|Feb 10th, 2021
|Feb 17th, 2021
|Feb 13th, 2021
|-
|Meet with Group to Preprocess Data
|Completed
|Feb 10th, 2021
|Feb 17th, 2021
|Feb 14th, 2021
|-
|Tune xgboost Model
|Completed
|Feb 10th, 2021
|Feb 17th, 2021
|Feb 15th, 2021
|}

== February 3, 2021 ==
* Here is the link to my self evaluation rubric. [[files/VIP AAD notebook rubric HarrisBarton.docx]] 
'''Team Meeting Notes:'''
* Submit self-evaluation rubric by next week.
* Complete the last lab by next week.
* We are going to proceed with team projects starting next week.
* Email Dr. Rholing rating and introducing yourself. 
'''Lecture Notes:'''
* Key Terms:
# '''<u>Gene Pool:</u>''' The set of genome to be evaluated from a generation.
# '''<u>Genome:</u>''' Genotypic description of individuals
# '''<u>Search Space:</u>''' The set of all possible genome. I relate this to the universal set, in probability and set theory. 
# '''<u>Phenotype/Objective:</u>''' The reference frame from which each genome is scored against. Note, the words genome and individual are used interchangeably and you might find where someone used genome, they might put (individual) in parenthesis next to it.
# '''<u>Objective Space:</u>''' The set of objectives. Objective scores give an individual a point in objective space. I will go more into detail about objective space and why we use it later below. For now, we can just think of it as the space which we can visualize the objective scores of individuals in reference to each other. Well, that kind of is the definition, honestly. 
# '''<u>True Positive (TP):</u>''' What we predict correctly to be positive.
# '''<u>True Negative (TN):</u>''' What we predict correctly to be negative.
# '''<u>False Positive (FP):</u>''' What we predicted to be positive but it is actually negative. 
# '''<u>False Negative (FN):</u>''' What we predicted to be negative but is actually positive.
# '''<u>Pareto:</u>''' An individual is pareto optimal iff there exists no other individual in the given population that outperforms them on EVERY objective. Note, this does not mean they can't be outperformed in anything. In theory, an individual may be pareto optimal even if they are outperformed on 99/100 of the objectives.
# '''<u>Pareto Front:</u>''' The pareto front is the set of all pareto optimal individuals. Note, it might seem logical to only assign a nonzero probability of selection to individuals in the pareto front; however, we want to maintain diversity as discussed in last lecture, so we have to still assign a nonzero probability to individuals not in the pareto front. Well, we do not have to of course, but it will be beneficial to do so. 
# '''<u>Strength:</u>''' Each individual is given a strength, denoted by '''''S'''''. Strength is how many in the population an individual dominates.
# '''<u>Rank:</u>''' Each individual also receives a rank, '''''R'''''. Rank is the sum of strengths of the individuals which dominate it. Let '''''ind''''' be an individual and let it have rank '''''R''''' = 10. Additionally, let the number of individuals that dominate '''''ind''''' be '''''N''''' = 5. Then, it follows that the rank, '''''R''''' simply means that, between the 5 individuals that dominate '''''ind,''''' the collective strength of all of these 5 individuals is 10.
* We can store the TP, TN, FP, and FN as elements in a matrix known as the confusion matrix. Additionally, these elements that make up this matrix can be used to solve for other important metrics. Below we will define those values:
*# '''<u>Positive Prediction Value (PPV):</u>''' PPV can be defined as the difference between our True Positive value, and the sum of the True Positive Value and the False Positive Value. Using math, PPV = TP / (TP + FP). ------> Bigger is better here
*# '''<u>False Discovery Rate (FDR):</u>''' FDR can be defined as the difference between the False Positive Value and the sum of the True Positive and the False Positive value. Mathematically,                          FDR = FP / (FP + TP) = 1 - PPV. Think of PPV as a complement of FDR? -------> Smaller is better
*# '''<u>Negative Predictive Value (NPV):</u>''' NPV can be defined as the difference between the True Negative value and the sum of the True Negative and the False Negative. Mathematically,       NPV = TN / (TN + FN). ------> Bigger is better.
*# '''<u>Accuracy (ACC):</u>''' Accuracy can be defined as the difference between the sum of the True Positive and the True Negative and the sum of the Positive and Negative. Mathematically,                     ACC = (TP + TN) / (P + N) -----> do I even have to say bigger is better? 

=== Lab 2 - Part 2 (Multi-objective Genetic Programming): ===
* In this part of the lab, we take more than one objective for the evaluation of any given individual, rather than using one objective, like in the last part of the lab. 
* Specifically, our two objectives will be the Mean-squared error (MSE), and the size of the tree.  
* The main focus is to reduce the area under the curve of the pareto front.   
* We used DEAP's mu plus lambda evolutionary algorithm, where mu is the number of individuals to select for the next generation and lambda is the number of children to produce at each generation.   
* I managed to decrease the AUC about 25%, per the goal of the lab. I tried a number of things but I think the main things helped me, adding more functions to the list of primitives, decreasing the probability of mutation, and adding more mutation functions. Naively, I messed around with the mu and lambda values just to observe what might happen with certain values. Upon doing this, I decreased lambda to 1 and saw a huge decrease (about %71) in the AUC of the pareto front. Knowing that this seemed odd, I ran the code a couple of more times and saw that the AUC had a lot of fluctuation. This indicated that this decrease was lucky and that having a lambda that low only adds uncertainty to our results, which I did not want.   
* I could be wrong about the lambda observation, maybe the trade-off between the uncertainty and the low AUC is such that we could optimize the combination of lambda in such a way to optimize this ratio.   
* Overall, I was not happy with where I was able to get. I would have liked to got a lot more of a decrease, but settled for this result for now. I do plan to look into it more after the suspense date to do some more work and hopefully decrease the AUC more.   
* Below is the lowest AUC I was able to achieve.
**[[files/AUC .png|none|thumb|71% decrease in AUC from original 2.45]]
* 
{| class="wikitable"
!Task 
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review Lectures from Bootcamp
|Completed
|Feb 3rd, 2021
|Feb10th, 2021
|Feb 10th, 2021
|-
|Complete Lab 2
|Completed
|Feb 3rd, 2021
|Feb10th, 2021
|Feb 9th, 2021
|-
|Complete Self-Evaluation
|Completed
|Feb 3rd, 2021
|Feb10th, 2021
|Feb 9th, 2021
|}

== January 27, 2021 ==
'''Team Meeting Notes:'''
# Learn Genetic Programming through the lecture and begin Lab 2
'''Lecture Notes:'''
# Recall, in genetic algorithms, we take some individual and then create some function evaluator to obtain some scores. Genetic Programming differs from that in that, in this case, the individual is the function itself. 
#* For example, say you have a list like, [1,2,3,4,5]. Take that list and then "apply" the individual (the function) like so''''':'''''
#**  '''''[1,2,3,4,5] --> {individual} --> [1,4,9,16,25] --> evaluator.'''''
#** Note, the individual is the function which returns the square of each element. 
# A program can be represented as a tree structure, 
#* Nodes are referred to as primitives and represent functions. 
#* Leaves are called terminals and represent parameters. 
#** The input can be thought of as some particular type of terminal.
#** The output is produced at the root of the tree. 
#*How is the tree stored?
#**To traverse this tree, first visit the root, and then the left subtree, and then the right subtree. Using this, we can see that the tree for this is '''''[-, 2, +, 0, 1]'''''. [[files/Image22.png|thumb|none]]

* '''Crossover in Genetic Programming:'''
** Crossover in this case is essentially just exchanging two subtrees. All you have to do is pick some point at random in the two trees, and replace everything where the new empty slot is and below with the other point and everything below it. Now that might not make sense, so see the diagram below.[[files/Genetic-programming-Tree-encoding-individual-crossover.png|none|thumb|341x341px]]

* '''Mutation in Genetic Programming:'''
** Mutation can involve:
**# Inserting node or subtree.
**# Deleting a node or Subtree.
**# Changing a node. 
'''Lab 2'''
* In this lab, our individual class inherited from the DEAP's '''''PrimitiveTree''''' class instead of a simple list like last lab. This is because our individuals are represented in a tree structure. In genetic programming, as discussed above in the lecture notes, trees are made up of variables called primitives and functions. Each primitive is a node in the tree where the leaves of the nodes are inputs to the parent node.
# Imported the relevant libraries.
# Added a set of mathematical operators to our primitive set. I added two to the primitive set, np.exp and np.sin, both with arity = 1.  
# Registered the genetic operators and the mutation methods. I added an insert method. 
Results thus far:
* Best individual is negative(cos(subtract(sin(subtract(sin(cos(subtract(subtract(cos(add(x, sin(x))), x), sin(multiply(x, multiply(tan(x), sin(x))))))), x)), x))), (0.11103899805699875, 25.0)
* Graph of results with added primitives and added mutation function. [[files/Lab2Plot.png|none|thumb]] 
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Write Lecture Notes for Lecture 2
|Completed
|Jan 27th, 2021
|Feb 3rd, 2021
|Jan 29th, 2021
|-
|Complete Lab 2 pt.1
|Completed
|Jan 27th, 2021
|Feb 3rd, 2021
|Feb 3rd, 2021
|-
|Review Lecture Slides
|Completed
|Jan 27th, 2021
|Feb 3rd, 2021
|Feb 2nd, 2021
|}

{| class="wikitable" !Task !Current Assigned !Suspense Date !Date Resolved |- |Lecture Notes |February 3, 2021 |February 10, 201 |February 9, 2021 |- |Self-Evaluation Rubric |February 3, 2021 |February 10, 2021 |February 9, 2021 |- |Part 2 of Lab 2 |February 3, 2021 |February 10, 2021 |February 10, 2021 |}


== January 20, 2021 ==
'''Team Meeting Notes:'''
* For the first 10 weeks, first semester students will go through a bootcamp, where there is lab to be completed each week. 
** '''Lecture Notes:'''
*** Keywords:
**** ''individual:'' One specific candidate in the population.
**** ''Population:'' group of individuals whose properties will be altered.
**** ''Objective:'' A value used to characterize individuals that you are trying to maximize or minimize.
**** ''Fitness:'' Relative comparison to other individuals. Sort of like a percentile. 
**** ''Evaluation:'' A function that computes the objective of an individual. 
**** ''Selection:'' Gives preference to the better individuals.
**** ''Fitness proportionate:'' The greater the fitness value, the higher probability of being selected.
**** ''Tournament:'' The several tournaments among individuals (number of individuals in each tournament is dependent on tournament size); winners are selected for mating.
**** Mutate: introduces modifications at random to maintain diversity.

* '''Lab 1''' 
** '''''One Max Problem''''': To solve the One Max Problem is to, from a population of individuals, get an individual of the highest fitness. In this case, the size of the population is is 100 bits, and our goal is to get a bit string full of 1's. It follows that we define the fitness of an individual in this case to be the sum of the bits in the string. 
** Solving the problem is broken into 4 'steps' as described below:
**# Evaluate the fitness score of the initial population. Repeat for 40 generations.
**# Use tournament selection as defined above to select the next generation of individuals.
**# Do a two-point cross over of the selected generation.
**# Perform a mutation.
**# Evaluate the fitness.
** '''''N Queens Problem'''''
*** The objective of the problem is to determine a configuration of ''N'' queens on a ''NxN'' chessboard s.t. no queen can take any other queen. In this edition of the 'N Queens Problem', each queen is assigned to one column, and only one queen can be on each line.
**** '''Procedure'''
****# Created fitness and individual classes, minimizing the weights. The reason we minimized the weights is to hopefully minimize the conflicts between queens. 
****# An individual in this problem would be something like, let's say n = 3. Then, an individual would look like this, [7,0,2]. Let each element be a queen, and individual[i] -1 represent the column number. For example, the first element of this list is a queen at column 8. 
****# Created an evaluation function that counts the number of conflicts along the diagonal. 
****# Defined the crossover function. Specifically, partially matched crossover was used to represent swapping pairs of queens positions between two parent individuals. 
****# Defined the mutation function. Namely, we used the ShuffleIndexes() mutation function.
****'''Results'''
[[files/Lab1Plot.png|thumb|325x325px|'''Figure 1.''' A plot showing the average, maximum, and minimum over 100 generations.|center]]

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Join Slack
|Completed
|January 20, 2021
|January 27, 2021
|January 20, 2021
|-
|Install Anaconda, GT GitHub
|Completed
|January 20, 2021
|January 22, 2021
|January 22, 2021
|-
|Install Jupyter Notebook
|Completed 
|January 20, 2021
|January 22, 2021
|January 24, 2021
|-
|setup Notebook and Write Lecture Notes for Lecture 1
|Completed
|January 20, 2021
|January 22, 2021
|January 26, 2021
|-
|Finish Lab 1
|Completed
|January 20, 2021
|January 27, 2021
|January 27, 2021
|}