* Team Member
'''Team Member:''' Hoa Vinh Luu

'''Major:''' Computer Science 

'''Email:''' hluu@gatech.edu 

'''Cell Phone:''' (770)-853-1325 

'''Interests:''' Machine Learning, A.I.

= '''Fall 2020''' =

== '''Week 47: December 2, 2020''' ==

=== General Meeting Notes: ===
* Presentation notes:
** Stock Team:
*** Implemented new primitives:
**** What are the performances of those primitives?
*** Problems:
**** The datasets from the paper are not big enough => need larger datasets
**** Found a new approach and test the results, but need more benchmarks to make a better comparison
** NLP Team:
*** Used DistilBERT [input] layer - similar to transfer learning,  but for NLP
**** It needs to connect with a fully-connected layer to get the results from the last layer
*** Created two crossovers primitives:
**** Single Point: Swap the tails of a point's two parents to get new offsprings
**** Two Point: Similar to the single point but chromosomes swapped instead of tails
*** Problem:
**** DistilBERT only use in the first layer => need to make BERT valid at any positions in the tree
**** Lacks of testing datasets
** Modularity Team:
*** Low performance on MNIST - AUC for FNR vs FPR  = 0.088
**** Why?Load dataset is kinda off
*** Restriction with EMADE datapair
*** Bloat issues with some individuals having up to 40+ ARLS

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Notebook
|Done
|December 2, 2020
|December 3, 2020
|December 3, 2020
|}

== '''Week 46: November 23, 2020''' ==

=== General Meeting Notes: ===
* General scrum

=== Sub-team Meeting Notes: ===
* Main object scores: Precision and Recall
** Don't use Accuracy, because:
*** Accuracy checks ypred matches ytrue exactly
*** In our system, ypred would be a vector of floats between 0 nad 1 
*** while ytrue is a vector of ints 0 and 1
*** => bad accuracy
* Find some ways to present the results from the run one PACE-ICE GPU
* Final results from CIFAR-10 runs:
**[[files/Cifar result.png|none|thumb]]

=== Individual Notes: ===
* Created [https://www.notion.so/PACE-ICE-GPU-for-Ez-CGP-8be7a2e57c6649229f36505d093952dd a PACE-ICE GPU user guide for EzCGP page] with Notion
* Created slides to talk about Object Scores and PACE-ICE GPU

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create a user-guide for ezcgp on pace-ice gpu
|Done
|November 23, 2020
|December 2, 2020
|November 28, 2020
|}

== '''Week 45: November 16, 2020''' ==

=== General Meeting Notes: ===
* General scrum

=== Sub-team Meeting Notes: ===
* Research team working on aging evolution and Caffe & Tensorflow crossover
* Problems:
** Our team evolutionary strategy is too gready
** We would go from 20 individuals to 400+ individuals
*** This caused the run on PACE-ICE GPU doesn't finish a full single generation in 8 hours
*** This happened because we allowed each block to produce children through mating + mutating
*** Solution: limit the population

=== Individual Notes: ===
* Attempted to get a GPU node by getting an interactive session from PACE-ICE GPU (followed the instruction from [https://pace.gatech.edu/pace-ice-instructional-cluster-environment-education pace documentation] - search for gpu):
*# Request GPU interactive session:
*##   qsub -l nodes=1:ppn=8:gpus=1 -l walltime=04:00:00 -q pace-ice-gpu -I 
*# load module:
*## Module load tensorflow-gpu/2.2.0
*## Module load pace/2020.01
*## Module load intel/19.0.5
*# Run test example:
*## python3 testgpu.py gpu 1000
*# Example output:
*## Num GPUs Available: 0
* Problem with getting a GPU node:
** Tensorflow doesn't detect any GPU
* Solution:
** Don't install TensorFlow through conda
** Load tensorflow-gpu after activate a conda environment that doesn't contain tensorflow
** Pip install tensorflow would also work
** Reason:
*** Its something to due with the way conda install tensorflow, it creates a conflict with how tensorflow detect gpu on PACE-ICE GPU

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get a GPU node on PACE-ICE
|Done
|November 16, 2020
|November 23, 2020
|November 19, 2020
|}

== '''Week 44: November 9, 2020''' ==

=== General Meeting Notes: ===
* General scrum

=== Sub-team Meeting Notes: ===
* Timeline for Final presentation:
** Thursday 19th, build out a skeleton/outline
** Over the weekend, fill in our respective slides
** Monday 23rd have  a rough draft ready and finish filling in slides
** Monday 30th go through final draft and make sure we are ready
** Wed Dec 2nd present

=== Individual Notes: ===
* Got [https://drive.google.com/file/d/1yq3IAJNa10mb1E9zbgrEcB2Qba32K91s/view?usp=sharing results] from baseline with problem_tensorflow_keras - CIFAR10:
** 5 epochs after 8 hours => too slow
**Epoch 1/10  6000/6000 - 886s - loss: 2.3039 - accuracy: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.3044 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00
**Accuracy is not good
**Need to debug with Rodd to see the problem
*Problem:
**PACE-ICE only gives cpu-node
*The solution to boosting the number of epochs in 8 hours:
**Instead of submitting a job to the PACE-ICE queue, we will try to get PACE-ICE GPU to work
*PACE-ICE allows submitting a job to 2 queues
** PACE-ICE - no gpu node
** PACE-ICE gpu - Provide gpu node

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get baseline on PACE-ICE
|Done
|November 9, 2020
|November 16, 2020
|November 12, 2020
|}

== '''Week 43: November 2, 2020''' ==

=== General Meeting Notes: ===
* General scrum

=== Sub-team Meeting Notes: ===
* The team reviewed the CV primitive [https://github.com/ezCGP/ezCGP/pull/113 pull request]:
* Abled to log in to PACE-ICE
* Problem with setup ezCGP repo and environments on PACE-ICE

=== Individual Notes: ===
* PACE doesn't link with the GT account, however, PACE-ICE does
* Shift research toward PACE-ICE:
** It similar to PACE with some differences like how to login:
*** ssh someuser3@pace-ice.pace.gatech.edu
* Created a [https://docs.google.com/document/d/1fJ0ihQo8l7LGmRLYf76e0w3RjSXIYS6LtabKrxxrZNc/edit?usp=sharing user-guide] for PACE-ICE
* Create PBS script nearly match with problem_tensorflow_keras from mid-term presentation:
* 
** #PBS -N ezCGP_problem_tensorflow_keras              # name of job
** #PBS -l nodes=1:ppn=4                                             # resources allocated, 1 node 4 processors
** #PBS -l walltime=5:00:00                                         # job will run at most 8 hours
** #PBS -l mem=14gb                                                # job requires 14gb over all nodes
** #PBS -q pace-ice                                                    # job is submitted to pace-ice
** #PBS -j oe                                                            # output and error is combined into the same file
** #PBS -o results.out                                               # output file is named gettingStarted.out
** #PBS -m abe                                                      # Will send a status email based on any combination of a,b,e.
** #PBS -M hluu8@gatech.edu
** cd ~/ezCGP
** echo "Started on `/bin/hostname`"               # prints name of compute node job was started on
** # computation starts here
** module load anaconda3
** source activate ezcgp-py
* ***Important note for PBS script:
** Even thought the presentation of PACE-ICE said the max walltime is 12 hours, we only able to set it max with 8 hours
* Setup ezCGP on PACE-ICE:
** Anaconda on PACE-ICE is a little different than usual, since there is a limit on user storage on PACeE-ICE
** There is a data folder to store large folder like anaconda environment
** Need to link the conda folder in data folder to home folder 
** then load module anaconda3
** create the environment from the ezCGP wiki

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Setup PACE environment for baseline
|Done
|October 26, 2020
|November 2, 2020
|November 7, 2020
|-
|Create baseline pbs script
|Done
|November 2, 2020
|November 9, 2020
|November 9, 2020
|}

== '''Week 42: October 26, 2020''' ==

=== General Meeting Notes: ===
* General scrum

=== Sub-team Meeting Notes: ===
* Welcomed and gave a quick walkthrough of ezCGP for new members
* Research Team:
** Review the most updated version of ezCGP and begin implementing new primitives from research papers
* Code Development Team:
** Trying to get baseline run on PACE
* Tasks:
** Seeding Experiments (Daniel)
** CIFAR10 Baseline run (Henry)
** CV Primitive Testing (Hemang)

=== Individual Notes: ===
* After research PACE:
** Login:
*** ssh GTuser3@login-s.pace.gatech.edu
** Load Modules:
*** module load <module name>: loads module into environment.
*** Module purge: gets rid of any modules loaded.
** Submit a job:
*** qsub <job>.pbs
** Check submmited job:
*** qstat <jobid>
*** qstat -u <gtuser3> -n 
** How to create a pbs script:
*** Example:
**** #PBS -N exampleScript                # name of job  
**** #PBS -l nodes=1:ppn=2               # resources allocated, 1 node 2 processors  
**** #PBS -l walltime=15:00               # job will run at most 15 min  
**** #PBS -q force-6                           # job is submitted to force-6 queue  
**** #PBS -j oe                                   # output and error is combined into the same file 
**** #PBS -o gettingStarted.out          # output file is named gettingStarted.out  
**** # computation starts here  cd $PBS_O_WORKDIR                   
**** # changes into directory where script was submitted from  
**** echo "Started on `/bin/hostname`"   # prints name of compute node job was started on
*** Tag means:
**** -q: the queue the job is going to be submitted to. Ex: #PBS -q force-6
**** -N : job name (name that shows up in queue). Ex: #PBS -N mpiScript
**** -o : names output file. Ex: #PBS -o results.out
**** -j oe: combines output and error into one file. Ex: #PBS -j oe
**** -m <a,b,e>: email. Will send a status email based on any combination of a,b,e. b is when job begins, e is when job ends, and a is if job is aborted. Ex: #PBS -m abe send an email for all three scenarios
**** -M: email addresses to send status emails to. Ex: #PBS -M user1@gatech.edu user2@gatech.edu will send emails to user1 and user2
* Problems:
** Unable to log in to PACE, because I need to ask for an account from PACE's support
** GT account doesn't work with PACE
** Need to be on GT VPN

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Setup PACE environment for baseline
|In-Progress
|October 26, 2020
|November 2, 2020
|
|-
|Research how to create pbs script for PACE
|Done
|October 26, 2020
|November 2, 2020
|November 1, 2020
|}

== '''Week 41: October 19, 2020''' ==

=== General Meeting Notes: ===
* Presentation Day
* Midterm Presentation: 

=== Sub-team Meeting Notes: ===
* Assign new tags to members for the last half of the semester

=== Individual Notes: ===
* Shapes of CIFAR100:
** CIFAR100 is a dataset of 50,000 32x32 color training images
** (number_samples, channels, num_row, num_col) if 'channels_first'
** (50000, 3, 32, 32)
* [https://github.gatech.edu/emade/ezCGP/commit/2b8dc4f7fccca57953eec5764019c7f4496363bf Commits for CIFAR100 and MNIST]:
* Load functions for both datasets only return:
** train_datapair 
** validate_datapair - from the testing set of the datasets
* **Note:
** These load datasets might get change later on since the goal is to load data without tensorflow_keras
** Init ratios for both datasets are not useful right now, since I use the training and testing set from tensorflow_keras
** All these load data functions are built from ezDataLoader with a specific ratio for train_split, validate_split, and test_split
** If anyone later needs to create more load data functions then follow this:
*** Create a child class from the ezDataLoader class
*** Set a default ratio for the dataset
*** Modified the load function 

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create MNIST data loader
|Done
|October 5, 2020
|October 19, 2020
|October 19, 2020
|-
|Create CIFAR100 data loader
|Done
|October 5, 2020
|October 26, 2020
|October 19, 2020
|}

== '''Week 40: October 12, 2020''' ==

=== General Meeting Notes: ===
* General scrum

=== Sub-team Meeting Notes: ===
* Assign presentation sections among team members:
** Intro ezCGP current state (Daniel)
** Code development:
*** Implementation of Code (Hemang):
*** Runs, experimental setup (Henry)
*** Expect Weaknesses/Shortcomings
** Research/Hypothesis (Ford and Bojun):
*** What the team want from papers
*** The goal, metrics the team want to collect and what we expect to see
** Future tasks (Daniel)

=== Individual Notes: ===
* Ran problem_tensorflow_keras locally:
** Able to get some valid individuals => ready to run on PACE-ICE
** Object scores are not great (predicted since I just want to know if it's working or not)
* Expectations for the results after running problem_tensorflow_keras on PACE:
** Test accuracy after 10 epochs: 66.32%
*** Without preprocessing on CIFAR-10
** Test accuracy after 10 epochs: 62.72%
*** Using CIFAR-10 without preprocessing
** Originally tested on MNIST
*** MNIST 20 Epoch Accuracy: 99.04%
*** Preprocessed by normalizing the values of input pixels.
* Datasets:
** CIFAR10, CIFAR100, MNIST
** Right now, we're using datasets provided by TensorFlow, but our goal is to less dependent on Tensorflow
* Experimental Setup - Hyperparameters (based line for running on PACE - will change later if needed):
** Dataset: CIFAR10
** Split ratio: 60% trainning, 20% validation, 20% testing
** 10 univierses
** Random number seed starting at 9
** 50 generations per universe
** 20 individuals to a population
** 10 Epochs training per individual
** Objective scores: Accuracy and F1 scores

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Test problem_tensorflow_keras
|Done
|September 28, 2020
|October 12, 2020
|October 12, 2020
|-
|Midterm presentation
|Done
|October 12, 2020
|October 19, 2020
|October 19, 2020
|}

== '''Week 39: October 5, 2020''' ==

=== General Meeting Notes: ===
* General scrum

=== Sub-team Meeting Notes: ===
* Assigned tasks for upcoming weeks:
** Test data script: [https://github.com/ezCGP/ezCGP/issues/105 Issue #105]
** Create MNIST data loader: [https://github.com/ezCGP/ezCGP/issues/106 Issue #106]
** Create CIFAR 100 data loader: [https://github.com/ezCGP/ezCGP/issues/107 Issue #107]

=== Individual Notes: ===
* Need to find CIFAR 100 in the old branch and move to new version of ezCGP

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Test problem_tensorflow_keras
|In Progress
|September 28, 2020
|October 12, 2020
|
|-
|Test data script to make sure load CIFAR 10 is working
|In Progress
|October 5, 2020
|October 12, 2020
|
|-
|Create MNIST data loader
|To-Do
|October 5, 2020
|October 19, 2020
|
|-
|Create CIFAR 100 data loader
|To-Do
|October 5, 2020
|October 26, 2020
|
|}

== '''Week 38: September 28, 2020''' ==

=== General Meeting Notes: ===
* General scrum

=== Sub-team Meeting Notes: ===
* Status reports:
** The research group merged with the developmental group
*** Will be working on setting up Pace for ezCGP to run problem_tensorflow_keras for first benchmark
* The next target is augmentor pipeline to generate more data
* Introducing more Keras primitives
* Add sklearn library for further development

=== Individual Notes: ===
* Pair-programming with Rodd to testing the problem_tensorflow_keras locally to find any quick-appear bug or issues before running it on PACE:
** [https://github.gatech.edu/emade/ezCGP/commit/fc3c8d4758e883dad168a7072d488b933c3ed332 Git commit]
* Megred Issue #88 to [https://github.gatech.edu/emade/ezCGP/compare/feature/%2398-get_running #98]
* We abled to build and got to evaluate(), we need to check to make sure everything is working correctly before move on to the next step
* We added Image as ezCGP dataset and created a few more loaders (main abstract structure and numpy):
** [https://github.gatech.edu/emade/ezCGP/commit/fc3c8d4758e883dad168a7072d488b933c3ed332 Git Commit]

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Test problem_tensorflow_keras
|In Progress
|September 28, 2020
|October 12, 2020
|
|}

== '''Week 37: September 21, 2020''' ==

=== General Meeting Notes: ===
* General scrum

=== Sub-team Meeting Notes: ===
* Almost ready to start testing the Tensorflow primitives
* Did a quick overview of the data loading process with the code in the individual notes

=== Individual Notes: ===
* [https://github.gatech.edu/emade/ezCGP/commit/f40fe666e6ba9ced628ec94911eb07e81ca10e6c Pair-programming with Rodd]: 
* Structure the problem_tensorflow_keras more formally:
** Added in block arguments with some specific values: 
*** DataAgumentation:
**** ArgumentType_LimitedFloat0to1: 0.5
**** ArgumentType_Int1to10: 1
**** ArgumentType_Int0to25: 1
**** ArgumentType_Float0to10: 0.2
**** ArgumentType_Bool: 1
*** Preprocessing:
**** ArgumentType_FilterSize: 0.33
**** ArgumentType_Bool: 1
**** ArgumentType_Int1to10: 1
**** ArgumentType_Float0to100: 0.25
**** ArgumentType_LimitedFloat0to1: 0.15
**** ArgumentType_Int0to25: 1
*** TransferLearning:
**** ArgumentType_Int0to25: 1
*** TFKeras:
**** ArgumentType_Pow2: 1
**** ArgumentType_TFFilterSize: 1
**** ArgumentType_TFActivation: 1
*** Notes: these values might change in the future
** For this problem, we will have 5 blocks:
*** augmentation, preprocessing, transfer learning, TensorFlow, construct of individual
*** **Note: They will similar codes but in different names so that they are independent => scalable for later

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Restructure problem_tensorflow_keras block structure
|In Progress
|September 21, 2020
|September 28, 2020
|September 24, 2020
|}

== '''Week 36: September 14, 2020''' ==

=== General Meeting Notes: ===
* General scrum
* Self-grading according to the rubric:
{| class="wikitable"
![[files/Hoa V Luu VIP Notebook Self-grade.png|alt=Hoa V Luu VIP Notebook Self-grade|thumb|Hoa V Luu VIP Notebook Self-grade]]
|}

=== Sub-team Meeting Notes: ===
* The research team presented their [https://drive.google.com/drive/folders/1c-JgI_pen4TZ4BCr68coy0sK40SQtwsv?usp=sharing final papers] and found materials:
** "Regularized Evolution for Image Classifier Architecture Search"

=== Individual Notes: ===
* Done with [https://github.com/ezCGP/ezCGP/issues/88 issue #88]
* [https://github.gatech.edu/emade/ezCGP/commit/d72db6e3aa8d8dfa431c500cd65d0032bf591f6f Committed] to GitHub for review
* Objective_function for problem_tensorflow_keras:
** Use mainly sklearn.metrics like f1_score, accuracy for the fitness value
* Use CIFAR10 for this problem as the benchmark data set
** Set the train_size_perc = 0.8, and  validation_size_perc= 0.2
** A good ratio to start with
* Adapted new block structure by implemented BlockShapeMeta_Keras:
** Use BlockShapeMeta_Abstract as the main structure
** Add input and output dtypes as tf.keras.layers
** Set the main count = 25 (need to explore further for explanation)

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create a small problem to benchmark ezCGP
|Completed
|August 27, 2020
|September 21, 2020
|September 21, 2020
|}

== '''Week 35: September 7, 2020''' ==

=== General Meeting Notes: ===
* None - Labor Day

=== Sub-team Meeting Notes: ===
* Finalized the new branch naming convention for features:
** feat/issue#_feature_name
* **Data loading convention:
** Other members when need data set only look in [https://github.com/ezCGP/ezCGP/blob/master/data/data_tools/data_loader.py data_loader.py]
** Keep the data folder clear by just access through data_loader => minimize import multiple files
* Personal task:
** https://github.com/ezCGP/ezCGP/issues/88

=== Individual Notes: ===
* Sample load data file (didn't upload to github since this block of code is using fake data and just for demonstration purpose):
** from data.data_tools.data_types import ezDataSet  
** import numpy as np  
** x_train = np.zeros((10000, 3, 128, 128), dtype = np.uint8) 
** y_train = np.ones((10000),dtype = np.uint8)  
** x_test = np.zeros((5000, 3, 128, 128),dtype = np.uint8)  
** y_test = np.ones((5000),dtype = np.uint8)  dataset = ezDataSet(x_train, y_train, x_test, y_test) 
** x_batch, y_batch = dataset.next_batch_train(100)
*Notes:
**Need x_train, y_train, x_test, and  y_test to create an instant of ezDataSet
**Extract information out of ezDataSet by defined methods like next_batch_train

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create a sample load data
|Completed
|August 27, 2020
|September 10, 2020
|September 8, 2020
|-
|Create a small problem to benchmark ezCGP
|In Progress
|August 27, 2020
|September 21, 2020
|
|}

== '''Week 34: August 31, 2020''' ==

=== General Meeting Notes: ===
* General scrum

=== Sub-team Meeting Notes: ===
* Status update
* Discussed incorporating a pull request based workflow to improve code quality and overall team familiarity with the codebase
* New tasks:
** Implementation team:
*** Add new deep learning primitives (Keras 2.0)
*** Migrating old primitives from 1.0 to 2.0
** Personal:
*** Document structure for data loading

=== Individual Notes: ===
* Loading data in ezCGP:
** [https://github.gatech.edu/emade/ezCGP/blob/master/data/data_tools/data_loader.py data_loader.py]:
*** Already contains method to load cifar 10 dataset - it needed train_size_perc and validation_size_perc
**** it split the original dataset into a specific size for training and testing set
** [https://github.gatech.edu/emade/ezCGP/blob/master/data/data_tools/data_types.py data_types.py]:
*** Define ezDataSet class
*** Need x_train, y_train - x_test and y_test is optional for cifar10 but needed for other data set
** [https://github.gatech.edu/emade/ezCGP/blob/master/data/data_tools/data_utils.py data_utils.py]:
*** New help methods for loading data need to be placed in this file to keep the structure consistent 
* When we loading data, we only use the data_loader file since it should provide all the needed data.

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Familiarize with data loading in new ezCGP
|Completed
|August 27, 2020
|September 3, 2020
|September 2, 2020
|-
|Create a sample load data
|In Progress
|August 27, 2020
|September 10, 2020
|
|-
|Create a small problem to benchmark ezCGP
|In Progress
|August 27, 2020
|September 22, 2020
|
|}

== '''Week 33: August 24, 2020''' ==

=== General Meeting Notes: ===
* Finalized team selection and general scrum

=== Sub-team Meeting Notes: ===
* Meeting Time: Thursday 5-6 PM EST

* Check out [https://github.com/ezCGP/ezExperimental/blob/2020S-student-edits/problem_augmentation.py problem_augmentation.py] as the guide to what blocks we used and so what primitives are used in each block
* Transfer old primitives to the new version of ezCGP
* Focus with "activation layer" arguments over primitives without activation layers
* Add new primitives to [https://github.com/ezCGP/ezCGP/tree/2020F-BaseCodeDevelopment/codes/block_definitions/utilities utilities folder]

=== Individual Notes: ===
* When adding new primitive, we will add new arguments as well.
* Add new arguments to [https://github.com/ezCGP/ezExperimental/blob/2020S-student-edits/argument_types.py argument_types.py]

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Familiarize with data loading in new ezCGP
|In Progress
|August 27, 2020
|September 3, 2020
|
|-
|Create a sample load data
|In Progress
|August 27, 2020
|September 10, 2020
|
|-
|Create a small problem to benchmark ezCGP
|In Progress
|August 27, 2020
|September 21, 2020
|
|}

== '''Week 32: August 17, 2020''' ==

=== General Meeting Notes: ===
* Team introduction
* Overview of the teams' work

=== Sub-team Meeting Notes: ===
* [https://docs.google.com/presentation/d/1HMyyie9ILQwdZ1e5jSMmVhDl_N0wOTZ8WpBCSRvk7-I/edit#slide=id.p Re-intro to ezCGP]
* The main goal for this semester:
** Not code development
** Focus on research and create multiple different block and individual definition
** Questions to ask when developing a block definition:
*** How to mutate?
*** How to mate?
*** How to evaluate?
*** What primitives can we use?
*** What argument/hyperparameter data types can we use?
*** How many nodes, what are the input & output data types?
** Questions to ask when developing an individual definition:
*** How to mutate a list of blocks?
*** How to mate a list of blocks?
*** How to evaluate a list of blocks?

=== Individual Notes: ===
* Setup new version of ezCGP:
** [https://github.com/ezCGP/ezCGP/wiki/New-VIP-Student-ToDo's Follow the general guidelines]
** At this time, TensorFlow only works with python 3.7 or lower
** MacOS: 
*** Command for setup ezCGP conda environment:
**** $ conda create -n ezcgp-py --file conda_environment.yaml
**** $ pip install Augmentor
** Window: 
*** Command for setup ezCGP conda environment:
**** $ conda create -n ezcgp-py python=3.7 -c conda-forge --file requirements.txt
****(only if it fails in requirement.txt file) $ pip install mpi4py
****$ pip install Augmentor

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Pick sub-team
|Completed
|August 17, 2020
|August 24, 2020
|August 21, 2020
|-
|Setup new version of ezCGP
|Completed
|August 21, 2020
|August 24, 2020
|August 22, 2020
|}

= '''Spring 2020''' =

== '''Week 31: April 20, 2020''' ==

=== Meeting Notes: ===
* Final presentation day :
* https://docs.google.com/presentation/d/1TV78U_DNYqzz7cwFAqhuXwjJl6Ou8qVV0f8nkKiARjY/edit#slide=id.g7459e25a8e_19_116

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Present our final presentation
|Completed
|April 20, 2020
|April 20, 2020
|April 20, 2020
|}

== '''Week 30: April 13, 2020''' ==

=== Meeting Notes: ===
* Merge tf_primtive with Tan's branch and run some simple test cases - https://github.com/ezCGP/ezExperimental/commit/60067e9afa002d68481684e2e576c5b7674a471c
* Working on add more arguments and set the needed arguments into the 
* Add slides to final presentation: https://docs.google.com/presentation/d/1TV78U_DNYqzz7cwFAqhuXwjJl6Ou8qVV0f8nkKiARjY/edit#slide=id.g7459e25a8e_19_116

=== Individual Notes: ===
* Add individual works into the final presentation

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Merge tf_primitive with Tan's branch
|Completed
|April 13, 2020
|April 20, 2020
|April 16, 2020
|-
|Finish team end of semester presentation
|Completed
|April 13, 2020
|April 20, 2020
|April 20, 2020
|-
|Add arguments in argument_types
|Completed
|April 6, 2020
|April 13, 2020
|April 13, 2020
|}

== '''Week 29: April 6, 2020''' ==

=== Meeting Notes: ===
* Refactor the new primitives to TF 2.0

=== Individual Notes: ===
* Note how to migrate to new TF 2.0:  pool2_flat = layers.Flatten(input_tensor) => tf.keras.layers.Flatten()(input_tensor)
** put the input value at the end of the code

* To check layers in network go to evaluate.py in BlockTensorFlowEvaluate and print model.layers to see if it used the new dense layer

* Problems:
** Take a lot of time to debug and to see if a primitive is working or not.
** Old dense_layer take a pool2_flat as an argument, but new TF 2.0 want it in a slighly different way.
* Solution: 
** build a simple test case for each primitive to check the syntax for TF 2.0 first then run the new framework
** Idea: Create a mock model and input to see if the dense layer is working or not
**  {| class="wikitable" |def test_dense(self): |- | |try: |- | |input_tensor = tf.keras.layers.Input([3, 32, 32]) |- | |dense_out = tensorflow_operator.dense_layer(input_tensor) |- | |model = tf.keras.Model(input_tensor, dense_out, name="dummy") |- | |fake_input = np.zeros((10000, 3, 32, 32), dtype =np.float64) |- | |out = model.predict(fake_input) |- | |assert(1 == 1) |- | |except Exception as e: |- | |assert(1 == 0) |}
* Commands:
** Run the new framework: python main.py -p problem_tensorflow -s 1
** Run the simple test case: python -m unittest unit_tests/test_[http://tensorflow.py/ tensorflow.py]
* https://github.com/ezCGP/ezExperimental/commit/bfe9a38292e6596a9c8e56ac1cd52b42ff60f654

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Refactor the old primitives to TF 2.0
|Completed
|March 9, 2020
|April 13, 2020
|April 13, 2020
|-
|Create a simple test case for dense layer
|Completed
|April 6, 2020
|April 13, 2020
|April 13, 2020
|-
|Add the new arguments into the new framework
|In Progress
|March 9, 2020
|April 13, 2020
|
|}

== '''Week 28: March 30, 2020''' ==

=== Meeting Notes: ===
* Worked with Tan to port the new primtives and arguments into the framework
* Check out arguments, TensorFlow_operators, operators to understand the requirements.

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Understand and set up the needed arguments in the new frame work
|In Progress
|March 9, 2020
|March 22, 2020
|
|-
|Upgrade and refactor primitives to TF 2.0
|In Progress
|March 9, 2020
|March 22, 2020
|
|}

== '''Week 27: March 23, 2020''' ==

=== Meeting Notes: ===
* Met with new semester teammate
* Look at operator to see the flow of the framework for the primtives

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Understand and set up the needed arguments in the new frame work
|In Progress
|March 9, 2020
|March 22, 2020
|
|-
|Upgrade and refactor primitives to TF 2.0
|In Progress
|March 9, 2020
|March 22, 2020
|
|}

== '''Week 26: March 9, 2020''' ==

=== Meeting Notes: ===
* [https://docs.google.com/presentation/d/1DaGSf2-x87oNFT5oukKR1jfI0m2wtXs--56K3mf7q38/edit#slide=id.p Team Presentations]
* The activation functions doesn't improvement the result by that much.
* It slows down the process of evaluate individuals

=== Testing Notes: ===
* Ran the evolution test for 15 and 16 generations before our runs terminated, because of the wall time limit of pace.
* Looked at best individuals:
** Only contains reLu and eLu
* Explanation for the best individuals:
** The reLu and eLu are the best activation functions for the given problem sets.
** The default activation function are set with reLu and eLu => not much improvement
* Future plan:
** Run the experiment agains with multiple problem sets with different dominated activation.

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Understand and set up the needed arguments in the new frame work
|In Progress
|March 9, 2020
|March 22, 2020
|
|-
|Upgrade and refactor primitives to TF 2.0
|In Progress
|March 9, 2020
|March 22, 2020
|
|}

== '''Week 25: March 2, 2020''' ==

=== Meeting Notes: ===
* Assigned [https://github.com/ezcgp/ezcgp/issues/59 issue #59] 
* Add more mutable activations
* Evaluate the output files for 30 epochs.
* Edit runVAL.pbs and evaluator.py

=== Testing Notes: ===
* Due to the limit of time, we only evaluate the output files for 30 epochs
* Edit runEVAL.pbs:
** #PBS -N sam_200_epochs #<- change this to your job name  #PBS -l nodes=1:ppn=8  #PBS -l pmem=14gb  #PBS -l walltime=12:00:00  #PBS -q pace-ice  #PBS -j oe  #PBS -o sam_200_epochs.out #<- change this to your output file name  #PBS -m abe  #PBS -M abc123[[Mailto:szhang460@gatech.edu|@gatech.edu]] #<- change this to your email
* Edit evaluator.py:
** epochs = 30
*Results:
**Best individual: 82.53% accuracy
**Only contains reLu and eLU for the activation functions

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run the new activation function with more evolution test
|In Progress
|March 2, 2020
|March 9, 2020
|March 7, 2020
|-
|Add more activation function into the pool
|Pending
|March 2, 2020
|March 9, 2020
|
|}

== '''Week 24: February 24, 2020''' ==

=== Meeting Notes: ===
* The accuracy result from the 1st run the new activation function with the evolution test is not consistent
** 1st run only ran for 4 generation 
* Run few more evolution tests with new parameters
** Change epochs to 5 instead of 10 in problem.py. An epoch is a pass over the data. Less passes over the data will mean more generations before we run out of time
** Add a batch_size keyword parameter to TrainingBlock in problem.py and change the batch_size from the default 128 to 256
* When the results from evolution test is consistent then add more activation functions into the pool
* Results:
** Accuracy: %71.73 

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run the new activation function with more evolution test
|In Progress
|February 24, 2020
|February 29, 2020
|
|-
|Add more activation function into the pool
|Pending
|February 24, 2020
|February 29, 2020
|
|}

== '''Week 23: February 17, 2020''' ==

=== Meeting Notes: ===
* Test the new activation function with an evolution test first increase the size of the activation pool-> make sure the function works fine with ezCGP
* Each run should use seperate parameters and should only be done with no data augmentation primitives.
* Use the mode of training_block.py to set the evolution test
* Current activation function pool: tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh, None, tf.nn.elu
* *Note: None => linear activation function
* Result:
** Runtime: 8h with 4 generation 
*** GEN_LIMIT = 50, POP_SIZE = 20, N_EPOCHS = 10, SEED = 17,
*** N_UNIVERSE = 1, N_MUTANTS = 2, N_OFFSPRING = 2), MIN_SCORE = 0.00
** Accuracy 72.88%
** Note:
*** Need to run few more times with the evolution tests to find a consistent result 
*** Need to decrease the population size to increase the amount of generations

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Setup PACE and make sure have access to PACE
|Completed
|February 17, 2020
|February 22, 2020
|February 17, 2020
|-
|Test the new activation function with an evolution test
|Completed
|February 17, 2020
|February 22, 2020
|February 22, 2020
|-
|Add more activation function into the pool
|Pending
|February 17, 2020
|February 22, 2020
|
|}

== '''Week 22: February 10, 2020''' ==

=== Meeting Notes: ===
* Debugged the new activation function
* Need to test with tester.py (dense layer and conv_layer)
* Test the new activation function with the tester.py and the evolution test.
* Debugged notes:
** Add the new activation argument in skeleton_block.py
** Found the type checking in calculate_func_args_inputs in blocks.py that only allow int and float, but the problem with tensor flow activation function (tf.nn.relu) doesn't contain any value => cause an user input error
** Added a temporary way to avoid error by checking a pool of valid type.
* Committed and pushed to [https://github.com/ezCGP/ezCGP/commit/0ad8bce6386fe20765dec3be8abc7d619e73edbd GitHub] 
* Result for conv_layer without augment:
** individual has fitness: (0.8864, 0.9580281772304184) 
** *Need to test with evolution test to compare the result 
*Proof of activation function able to mutate:
**function at: 1 is: {'ftn': <function conv_layer at 0x1c3c738268>, 'inputs': [-1], 'args': [1, 2, 14]} and has arguments: [128 3 None]  function at: 42 is: {'ftn': <function conv_layer at 0x1c3c738268>, 'inputs': [1], 'args': [1, 2, 16]} and has arguments: [128 3 <function elu at 0x12d1a8ae8>]  function at: 43 is: {'ftn': <function conv_layer at 0x1c3c738268>, 'inputs': [42], 'args': [1, 2, 15]} and has arguments: [128 3 <function elu at 0x12d1a8ae8>]

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Debugged and test the new activation function with tester.py (dense layer and conv_layer)
|Completed
|February 10, 2020
|February 15, 2020
|February 10, 2020
|-
|Research more tensor flow activation functions
|Completed
|February 10, 2020
|February 15, 2020
|February 14, 2020
|-
|Test the new activation function with the tester.py for multiple layers
|Completed
|February 10, 2020
|February 16, 2020
|February 10, 2020
|}

== '''Week 21: February 3, 2020''' ==

=== Meeting Notes: ===
* Ideas to create the activation function:
** Create a list of tensor flow activation function and use np.random.choice to select an activation function when mutate get call.
** Remove the current hard-coded and add the new activation argument into the header of the function **Key: set a default value
** When replace the hard-coded activation, remember to add the activation argument into the operDict[that function] args
*Michael assigned [https://app.zenhub.com/workspaces/ezcgp-5e1a094332969d8dab0e61b4/issues/ezcgp/ezcgp/43 issue #43] (implement activation function)
* *Remember to append the new activation argument into the main arguments.

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Research tensor flow activation function to create a pool of activation to mutate 
|Completed
|February 3, 2020
|February 9, 2020
|February 7, 2020
|-
|Remove all the activation hard-coded and replace with new mutated activation in operator.py
|Completed
|February 3, 2020
|February 9, 2020
|February 7, 2020
|-
|Create new argument (activation), modified the mutate function, and add to the main arguments in arguments.py
|Completed
|February 3
|February 9, 2020
|February 7, 2020
|}

== '''Week 20: January 27, 2020''' ==

=== Meeting Notes: ===
* Learn few basic functions of Tensor flow toward the neural net and layers 
* Explore the operators.py
** The activation function in dense_layer is hard-corded 
** Primitives work with the arguments by use operDict[primtive_function] = {"args": [value]} to use the value in the main arguments 
*Explore the the arguments.py:
**Each argument contains 4 functions: _init_, mutate, _str_, _repr
***_init_: 
****If the argument doesn't contain any value then we need to mutate to get value
*****num_samples = 10 for all the arguments
***mutate:
****This is an important function for argument and need to define for every argument
****Define how each argument get to mutate
*Explore training block.py
**Can use to target a specific layer or function by let ezCGP run with only that parameter 

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Learn the basic of tensor flow version 1.0
|In Progress
|January 27, 2020
|February 1, 2020
|
|-
|Explore and understand the ezCGP - primitives branch
|Completed
|January 27, 2020
|February 1, 2020
|February 1, 2020
|}

== '''Week 19: January 25, 2020''' ==

=== Meeting Notes: ===
* Research for new primitives 
* [https://arxiv.org/pdf/1801.01563.pdf Article Note]:
**  DENSER - Deep Evolutionary Network Structured Representation 
***  (Novel approach) Use Evolutionary Computation to automatically design Artificial Neural Networks
***  Automatic design use two distinct levels:
****  Outer level encodes the general structure of the network
****  Inner level encodes the parameters associated with each layer
***  To avoid the traditional laborious process of deciding on the network topology, new approach resort the networks that have already been constructed for a specific task that have show a good performance.
***  Crossover:
****  Use bit-mask crossover - create a mask of bits of the size of the number of number of codons
***Mutation:
****GA Level- encodes the macro structure of the networks:
*****Add layer: Generates a new layer randomly 
*****Replicate Layer: Select a module and copies an existing layer to another portion of the module
*****Remove Layer: Remove random layer from a module 
****DSGE Level - encodes the parameter associated to a layer:
*****Grammatical Mutation
*****Integer/Float mutation
***Evaluation:
****DENSER only allow evolution of the learning hyper-parameters
****Train the hyper-parameters on the task and compare value
***  Future work:
****  Need better ways for assessing the performance of the networks
****  Longer trains to reduce bias search
**Result:
***CIFAR-10:
****Tested on MNIST, Fashion-MNIST
****An average test accuracy: 94.13%
***CIFAR-100
****An average test accuracy: 78.75%
=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read DENSER: Deep Evolutionary Network Structured Representation 
|Completed
|January 25, 2020
|February 1, 2020
|February 10, 2020
|}

== '''Week 18: January 13, 2020''' ==

=== Meeting Notes: ===
* Team went over the Cartesian programming and ezCGP structure to make sure everyone on the same page.
* Goal for each team for this semester:
** Primitives: Increase the pool of primitives to increase the accuracy
** GPU: Decrease the image processing and classification time. => Decrease run time
*[https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/ Article 1 note]:
**Convolutional neural networks (aka CNN) - Looking for low level features such as edges and curves, and then building up to more abstract concepts through series of convolutional layers
***Dropping the classification error record from 26% to 15% in 2012
***Represent an image with X x Y x 3 array of numbers (3 refers to RGB values 0 to 255)
***First Layer - Math Part (Convolutional Layer):
****Filter - flashing sliding across all the areas of the input image.
****Receptive field - the region that it is shining over
******The depth of the filter has to be the same as the depth of the input
***Classic CNN architecture:
****Input -> Conv -> ReLU -> Conv -> ReLU -> Pool -> ReLU -> Pool -> Fully Connected
***Fully Connected Layer: 
****Take input volume and outputs an N dimensional vector where N is the number of classes that the program has to choose from.
****Looks at the output of the previous layer and determine which features most correlate to a particular class
*[https://arxiv.org/pdf/1704.00764.pdf Article 2 note]:
**CNNs have seen huge success in image recognition tasks
**Problem with design CNN architectures:
***Many design parameters exist
***The depth of a network
***Type and parameters of each layer
***Connectivity of the layers
**Use evolutionary computation to design neural network architectures - since each task require a different amount of parameters
**With traditional approaches, optimize the number and connectivity of low-level neurons, but modern deep learning contains a lot of parameters => takes a very long time to optimize all the parameters
**New approach, use genetic programing to design CNN architectures
***Pros:
****Flexibility
****Represent variable-length network structures and skip connections
**Hyperparameter optimization
***Tunes predefines numbers of layers, neurons and type of activation functions
***However, hard to design more flexible architectures from scratch  
**Evolutionary Algorithm
***Traditional CGP:
****Required good amount of fitness evaluation, and it's expensive to do that (required the training of CNN)
***New CGP, evaluate some candidate solutions in parallel at each generation 
**Conclusion:
***Searching the adequate architectures efficiently by adopting the highly functional modules such as ConvBlock and ResBlock
***Problem: Requires much computational cost => considering apply regularization techniques to the optimization process
=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read A Beginner's Guide To Understanding Convolutional Neural Networks
|Completed
|January 11, 2020
|January 20, 2020
|January 15, 2020
|-
|Read A Genetic Programming Approach to Designing Convolutional Neural Network Architectures
|Completed
|January 11, 2020
|January  20, 2020
|January 19, 2020
|}

== '''Week 17: January 6, 2020''' ==

=== Meeting Notes: ===
* Split the team into two sub-teams: Primitives/Data Augmentation and GPU/Parallelization
* Decided to use ZenHub to keep track of the progress of each individual and sub-team
* Takeways from previous semester:
** Parallelization slow
** Fix GPU
** Ton of error:
*** Accuracy was low last semester
*** Deep copying error
*** Arguments error
*This Semester Plan:
**Test a lot of functionalities haven't not been tested (primitives)
**Split into GPU and Primitive Enhancement team
**Primitive Team:
***Benchmark new primitives (aim for accuracy)
**GPU Team:
***Performance benchmark (aim for speed)

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up ZenHub to configure with ezCGP team
|Completed
|January 11, 2020
|January 13, 2020
|January 11, 2020
|-
|Read A Genetic Programming Approach to Designing Convolutional Neural Network Architectures
|In Progress
|January 11, 2020
|January 20, 2020
|
|}

= '''Fall 2019''' =

== '''Week 16: December 2, 2019''' ==

=== Bootcamps Notes: ===
* Final presentations from all sub teams

=== Sub-Team Notes: ===
* [https://docs.google.com/presentation/d/1jAWlWmQj94DfXsNsuke80kzpa3EUEJJKMgvQ2TTC_zg/edit#slide=id.pPreseation Preseation]
* [https://docs.google.com/document/d/1X8jGDXHAKkMBgOCYCtgT5v-wSqSLjVhxtZnGqr2hwz4/edit Design Doc]
* Create a pull request after commit the select n best individuals  

=== Actions Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Make PR for the select n best individuals
|Pending 
|December 2, 2019
|December 5, 2019
|Pending
|-
|Update notebook
|Completed
|December 2, 2019
|December 4, 2019
|December 4, 2019
|}

== '''Week 15: November 25, 2019''' ==

=== Bootcamps Notes: ===
* Scrum from all sub teams

=== Sub-Team Notes: ===
* Understand population structure to find best individuals
** Population contains list of genome => use each genome to create individual object
** Obtain lost and f1_score from individual object.fitness.values (*note: obtain these values with indObject[-1] for old ezCGP)
** Use accuracy = 1 - loss to sort the list of individuals 
* Write select n best individuals after each generation method and return a list of best n individuals (*note: use this for seeding later on also)
* Append each individuals from the list of best n individuals of each generation to the visualizer 
* [https://github.gatech.edu/emade/ezCGP/commit/ade78aa28b1a1a52431ee0886b413a1ce262c857 Committed to GitHub] 

=== Actions Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Understand population structure of the new ezCGP (ezHPC branch)
|Completed 
|November 25, 2019
|December 2, 2019
|November 30, 2019
|-
|Write and commit select n best individuals to ezHPC_viz
|Completed
|November 25, 2019
|December 2, 2019
|December 2, 2019
|}

== '''Week 14: November 18, 2019''' ==

=== Bootcamps Notes: ===
* I was not present for the scrum, because I got a bad cold.

=== Sub-Team Notes: ===
* Find population after each generation to get ready for the select n best individuals 
* Save the best n individuals in a different way because the new version of ezGDP is now support parallel 

=== Actions Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Explore universe.py of visualize branch to get population after each generation
|Completed 
|November 18, 2019
|November 25, 2019
|November 21, 2019
|}

== '''Week 13: November 11, 2019''' ==

=== Bootcamps Notes: ===
* Scrum from every team

=== Sub-Team Notes: ===
* Create an anaconda environment for ezGDP
* Install requires package for ezGDP with the packages file.

=== Actions Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install requires package for ezGDP
|Completed 
|November 11, 2019
|November 18, 2019
|November 11, 2019
|-
|Explore code base of the new version of ezGDP
|Completed
|November 11,2019
|November 18, 2019
|November 17, 2019
|}

== '''Week 12: November 4, 2019''' ==

=== Bootcamps Notes: ===
* Scrum from every team

=== Sub-Team Notes: ===
* Explore code base of the old version of ezGDP
** Universe.py
*** Initialize population with list of individuals and evaluate and get their fitness.
*** Use mute/mate among individuals to make best sub-population
** Problem.py
*** Take skeleton_genome two defines overall genome structure and problem description
* Block structure
** Data Augmentation
** Data Preprocessing
** Model Training / Validation
*Save best individuals of each generation of the population so we can start we those values to file
** => no more recalculate 
** => use that file to setup a visualize graph
*[https://github.gatech.edu/emade/ezCGP/commit/b1eaa91f48632bb81f50f0c1e9350ba925bbe9d2 Committed to GitHub]

=== Actions Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Push a test branch to the public and private Github
|Completed 
|November 4, 2019
|November 11, 2019
|November 4, 2019
|-
|Explore code base of the old version of ezGDP
|Completed
|November 4, 2019
|November 11, 2019
|November 9, 2019
|-
|Save best individuals of each generation to an output file
|Completed
|November 4, 2019
|November 11,2019
|November 9, 2019
|}

== '''Week 11: October 28, 2019''' ==

=== Bootcamps Notes: ===
* Scrum from every team

=== Sub-Team Notes: ===
* Choose ezGDP
* Clone ezGDP  
* Create a test branch to make sure that I can push to the public and private GitHub - Can access the private due to permission access => move to the week

=== Actions Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Clone ezGDP Repo
|Completed 
|October 28, 2019
|November 2, 2019
|November 1, 2019
|}

== '''Week 10: October 21, 2019''' ==

=== Bootcamps Notes: ===
* Presentations on boot camp subteams EMADE titanic results
** Ran 40 generations
** Steady trend of decreasing AUC and individuals moving closer to the origin
** Year 1: AUC = 0.36 (limited amount of individuals with low false positive and high false negative)
** Year 40: AUC = 0.18 (AUC decreased as more individuals with lower either lower false positive or false negative or both started appearing)
** Following the trend, AUC will continue to decrease in future generations
* Returning sub-teams presentations:
** ADF
** ezCGP - Cartesian Genetic Programming
** NLP - Natural Language Processing
** Preprocessing
** Bloat
{|
![[files/3dparetofront.png|thumb|3D pareto front with false positive, false negative, and number of elements]]
|}

=== Sub-Team Notes: ===
* [https://docs.google.com/presentation/d/1PIjGJwwqcjAKTKR6EBEMrHfVfPn1Rtaz0Y0uGcPY48w/edit?usp=sharing Presentation]

=== Actions Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Choose subitem
|Completed 
|October 21, 2019
|October 28, 2019
|October 28, 2019
|}

== '''Week 9: October 16, 2019''' ==

=== Bootcamps Notes: ===
* Hackathon for AAD
* Set-up EMADE and MySQL to run Titanic set smoothly

=== Sub-Team Notes: ===
* Draw a tree of a good individual from the Pareto front with DrawIO.
{|
![[files/An good individual of the pareto front example.png|thumb|An good individual of the pareto front example.]]
|}

=== Actions Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Generate a tree of a good individual
|Completed 
|October 16, 2019
|October 21, 2019
|October 21, 2019
|-
|Run Titanic set with EMADE as worker until 10/19
|Completed
|October 16, 2019
|October 20, 2019
|October 19, 2019
|}

== '''Week 8: October 9, 2019''' ==

=== Bootcamps Notes: ===
* Introduction to EMADE
* src/GPFramework/launchGTMOEP.py: module to start a run of EMADE

=== Sub-Team Notes: ===
* Run EMADE on Titanic set over time - as a group

=== Actions Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run Titanic set with EMADE
|Completed 
|October 9, 2019
|October 16, 2019
|October 16, 2019
|-
|Install and set up MySQL
|Completed
|October 9, 2019
|October 16, 2019
|October 16, 2019
|}
== '''Week 7: October 2, 2019''' ==

=== Bootcamps Notes: ===
* Discussed Titanic Genetic programming assignment

=== Sub-Team Notes: ===
* Presented our GP Titanic results
* [https://docs.google.com/presentation/d/1x665dHPg5bXLiMmNABBlZQuQZUkfW9KvNkt3by5uPY0/edit?usp=sharing GP Titanic Presentation]
* Personal Contributions:
** Tried Weakly-Typed GP => Not work well

==== Results of Titanic ML Assignment ====
{|
![[files/Data after Vectorization.png|thumb|Data after Vectorization of 3 individuals ]]
|}
{|
![[files/Objective Space of Group 4.png|thumb|Objective Space of Group 4]]
|}
{|
![[files/Train Pareto Front.png|thumb|Train Pareto Front]]
|}
{|
![[files/Test Pareto Front.png|thumb|Test Pareto Front]]
|}

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with team to find common feature set
|Completed 
|October 2, 2019
|October 9, 2019
|October 5, 2019
|-
|Complete Titanic ML Assignment
|Completed
|October 2, 2019
|October 9, 2019
|October 9, 2019
|-
|Install EMADE
|Completed
|October 2, 2019
|October 9, 2019
|October 3, 2019
|}
== '''Week 6: September 25, 2019''' ==

=== Bootcamps Notes: ===
* Decide which common feature set to keep and a common evolutionary framework
* '''Goal:''' Use genetic programming framework to output a Pareto front that has a lot of co-dominant individuals

=== Sub-Team Notes: ===
* Vectorizing and Normalizing features
* Decide between weakly-typed vs strongly-typed
* Strongly-typed, multi-objective programming

* Primitives:
** And, or, not, if then else, equal, add, subtract, multiply, protected division
* Assigned responsiblities

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with team to solve generic
|Completed 
|September 25, 2019
|October 2, 2019
|September 29, 2019
|-
|Complete GP Project
|Completed
|September 25, 2019
|October 2, 2019
|October 1, 2019
|}
== '''Week 5: September 18, 2019''' ==

=== Bootcamps Notes: ===
* Strategies to clean up data:
** Normalization and vectorization

=== Sub-Team Notes: ===
* Presented our ML Titanic results to the class
* [https://docs.google.com/presentation/d/19_p-7uLl3fpubdpKZBtaKG4gufW_Q3rmRCZ0-TbysEs/edit?usp=sharing ML Titanic Presentation]
* Personal Contributions:
** Pick co-dominate ML algorithm
** Submitted CSV with personal model

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed 
|September 18, 2019
|September 25, 2019
|September 25, 2019
|}
== '''Week 4: September 11, 2019''' ==

=== Bootcamp Notes: ===
* Introduction for Titanic Disaster Problem
* Select 5 co-dominant algorithms for Titanic on Kaggle

=== Sub-Team Notes: ===
* Team: Tejas, Anuraag, Daniel, Hoa
* Since women and children are priority while the rescue, so they are more likely to survive
* Pick sex and age to be the main parameters for the algorithms.
* Decided to use Stochastic Gradient Descent (SGD) for personal model

=== Titanic ML Assignment: ===
* Algorithms:
** neural_network.MLPCLassifier()
** svm.SVC(kernel='linear)
** RandomForestClassifier(n_estimators = 200, random_state = 100)
** GaussianNB()
** Stochastic Gradient Descent (SGD)
{|
![[files/The 5 codominant algorithms.png|thumb|
The 5 co-dominant algorithms for Titanic Problem
]]
|}
{|
|[[files/The consistence of the 5 selected algorithms..png|thumb|The consistence of the 5 selected algorithms.]]
|}
* Highest score was: 0.83 (Rare)

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with team to find common feature set
|Completed 
|September 11, 2019
|September 18, 2019
|September 17, 2019
|-
|Complete Titanic ML Assignment
|Completed
|September 11, 2019
|September 18, 2019
|September 18, 2019
|}
== '''<small>Week 3: September 4, 2019</small>''' ==

=== '''Bootcamp Notes:''' ===
* Gene pool: The set of genome to be evaluated during the current generation
** Genome:
*** Genotypic description of an individuals
*** DNA
*** GA = Set of values
*** GP = Tree structure, string
** Search Space:
*** Set of all possible genome
* Objective Space:
** Objectives: The set of measurements each genome (or individual) is scored against

* '''Confusion Matrix:'''
{| class="wikitable"
!
!Predicted Positive
!Predicted Negative
|-
|Actual Positive (P)
|True Positive (TP)
|False Negative (FN)
|-
|Actual Negative (N)
|False Positive (FP)
|True Negative (TN)
|}
* Maximization Measures:
** Sensitivity or True Positive Rate (TPR) = TP/P = TP/(TP+FN)
** Specificity (SPC) or True Negative Rate( TPR) = TN/N = TN/(TN+FP)
* Minimization Measures:
** False Negative Rate (FNR) = FN/P = FN/(TP+FN) = 1 - TPR
** Fallout or False Positive Rate (FPR) = FP/N = TN/(FP+TN) = 1 - TNR = 1 - SPC
* Accuracy (ACC) = (TP+TN)/(P+N) = (TP+TN)/(TP + FP + FN + TN) ** Bigger => better
* Pareto Optimality:
** An individual is Pareto if there is no other individual in the population that outperforms the individuals on all objectives
** The set of all Pareto individuals is known as the <u>Pareto frontier</u>

=== Sub-Team Notes: ===
* N/A: No team assigned

=== Lab 2: Genetic Programming and Multi-Objective Optimization ===

==== '''Part 2: Multi-Objective Genetic Programming''' ====
{| class="wikitable"
!Changes
!Best Individual
!
!AUC
!
|-
|No Change
|negative(cos(multiply(add(cos(sin(cos(multiply(add(cos(cos(x)), cos(add(multiply(x, x), sin(x)))), tan(x))))), cos(x)), tan(x))))
|[[files/No Change GP Multi.png|thumb|No Change GP Multi]]
|2.46379
|[[files/No Change Pareto Front Objective Space.png|thumb|No Change Pareto Front Objective Space]]
|-
|Changed
|negative(cos(x)) with fitness: (0.49416153493347187, 3.0)
|[[files/Run -1 of GP Multi.png|thumb|Changed GP Multi]]
|1.37582
|[[files/Run -2.png|thumb|Changed Pareto Front Objective Space]]
|}

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finish Lab 2
|Completed
|September 4, 2019
|September 11, 2019
|September 11, 2019
|}
== '''<small>Week 2: August 28, 2019</small>''' ==

=== Bootcamp Notes: ===
* Introduction to genetic programming with multiple different crossovers and mutation  Tree Representation:
** Represent a program as a tree structure
** Nodes are called primitives and represent function
** Leaves are called terminals and represent parameters
** Produced output at the root of the tree
* How is the Tree stored?
** The tree is converted to a <u>lisp preordered parse tree</u>.
* Crossover in tree-based GP is simply exchange subtrees
* Mutation in GP can involve by:
** Inserting a node or subtree
** Deleting a node or subtree
** Changing a node
* Talked about symbolic regression
* Evaluating a tree:
** Feed a number of input points into the function to get outputs
*** X = [0..2π]
** Run f(x)
** Measure error between outputs and truth

=== Sub-Team Notes: ===
* N/A: No team assigned

=== Lab 2: Genetic Programming and Multi-Objective Optimization ===
'''Part 1: Symbolic Regression'''
{| class="wikitable"
|[[files/Screen Shot 2019-09-04 at 4.02.01 PM.png|thumb|My results with my primitives.]]
|}
{| class="wikitable"
|[[files/Screen Shot 2019-09-04 at 4.10.07 PM.png|thumb|My primitives - Sin and Cos]]
|}

=== Action Items: ===
* Read through DEAP documentation
* Complete Part 1 of the Lab 2
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Part 1 of Lab 2
|Completed
|August 28, 2019
|September 4, 2019
|September 4, 2019
|-
|Take notes on Lab 2
|Completed
|August 28, 2019
|September 4, 2019
|September 4, 2019
|-
|Join Slack
|Completed
|August 21,2019
|September 4, 2019
|September 4, 2019
|}
== '''<small>Week 1: August 21, 2019</small>''' ==

=== '''Bootcamp Notes:''' ===
* Introduction to VIP Notebook
* Introduction for generic programming
* Methods to generate new generation:
** Selection: Represents 'survival of the fittest', gives preference to better individuals => allowing to pass on their genes
** Mate/Crossover: Represents mating between individuals
** Mutate: Introduces random modifications, purpose is to maintain diversity
* Went through One Max Problem

=== '''Sub-team Action Items:''' ===
* N/A: No team assigned

=== '''Action Items:''' ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Setup Python 3.7 and install DEAP
|Completed
|Aug 21, 2019
|Aug 28, 2019
|Aug 22, 2019
|-
|Complete Lab 1
|Completed
|Aug 21, 2019
|Aug 28, 2019
|Aug 28, 2019
|}