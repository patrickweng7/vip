== Team Member ==
Team Member: Jaswanth Sai Pyneni

Email: jpyneni@gatech.edu
Cell Phone: 7326476849

Interests:  Machine Learning, Rugby, Data Science, AI, Python

== January 7th, 2019 ==
'''Team Meeting Notes:'''
* Returning students picked sub-teams to join
* I joined Medical Applications/EEG Sub-Team
'''EEG Sub-Team Notes:'''
* Discussed a plan for the short term: need to figure out meeting time by the end of the week (with least conflicts)
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Figure out common meeting time for subteam
|Waiting until end of Add/Drop
|1/7/19
|1/13/19
|
|-
|Read wiki and old articles to understand what happened in previous semester
|Pending
|1/7/19
|1/13/19
|
|-
|Start Notebook
|Completed
|1/7/19
|1/13/19
|1/8/19
|-
|Critique Scott's article
|Pending
|1/17/19
|1/13/19
|
|}

== January 9th, 2019 ==
'''Personal Notes:'''
* Started notebook and updated team page with names
* Notes about past work based on wiki page:
** Classify whether an individual has lesion in motor complex
** Leverage GP and EMADE to preprocess EEG data for classification 
** Developing an algorithm to generalize to new anatomies for lesion diagnosis
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Figure out common meeting time for subteam
|Waiting until end of Add/Drop
|1/7/19
|1/13/19
|
|-
|Read wiki and old articles to understand what happened in previous semester
|In Progress
|1/7/19
|1/13/19
|
|-
|Start Notebook
|Completed
|1/7/19
|1/13/19
|1/9/19
|-
|Critique Scott's article
|Pending
|1/7/19
|1/13/19
|
|}

== January 14th, 2019 ==
'''Team Meeting Notes:'''
* Gave everyone an update on our team
* Goals for the week: -Get the new members (me!) caught up on what is going on

'''EEG Sub-Team Notes:'''
* Ali broke down the previous semester work into rudimentary language to give me a really good explanation
* Studying EEG (think ECG but for the brain)
*Data: each row has a bunch of channels (14 - 32 electrodes), each recording the potential 
*[ID, channel 1, channel 2, channel 3.....] 
*Analyze that data to gain insight to brain activity
*Ali's Eye Data Set shows that this is not a trivial problem and shows that this is not a useless approach
**Dataset contains 55% open eyes, and 45% closed eyes  (each is the classification 1 or 2) so we want a classifier with greater than 55% accuracy
**By averaging  all channels for closed and all channels for open, Ali realized that there are big gaps between the averages in certain specific channels and focusing only on that, he was able to achieve 75% accuracy
**This shows that we can dig for search data patterns in our EEG readings to successfully classify for lesions or no lesions
*Explained what Fourier Transformation is - decomposes complex graphs to help derive insights
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Figure out common meeting time for subteam
|Pending
|1/7/19
|1/13/19
|
|-
|Read wiki and old articles to understand what happened in previous semester
|Pending
|1/7/19
|1/18/19
|
|-
|Critique Scott's article
|Pending
|1/17/19
|1/18/19
|
|}


== January 18th, 2019 ==
'''EEG Sub-Team Meeting Notes:'''
* We established common meeting times: : Monday 11-12 and Friday 12-1. At least one is compulsory. 
* Figured out assignments/focus for individuals in sub-team for semester:
** Average many samples to make a novel seed (Ali + Me)
** Fix the bug with returning large python objects from multiprocess (Austin)
** Implement Zrenner's Forecasting as a primitive (Joel + James)
** Crop Frequency Domain and Put it into A Neural Net (Joel + James)
** See Emory Lab (James, Joel)
'''Action Items:'''
{| class="wikitable"
!'''Task'''
!'''Current Status'''
!'''Date Assigned'''
!'''Suspense Date'''
!'''Date Resolved'''
|-
|'''Figure out common meeting time for subteam'''
|'''Completed'''
|'''1/7/19'''
|'''1/13/19'''
|'''1/18/19'''
|-
|'''Read wiki and old articles to understand what happened in previous semester'''
|'''Pending'''
|'''1/7/19'''
|'''1/18/19'''
|
|-
|'''Critique Scott's article'''
|'''Pending'''
|'''1/17/19'''
|'''1/18/19'''
|
|-
|'''Connect with Ali to figure out plan of attach'''
|'''Pending'''
|'''1/18/19'''
|'''1/23/19'''
|
|}

== January 21st, 2019 ==
MLK Day - No school - No meeting
* Used time to read old articles
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read wiki and old articles to understand what happened in previous semester
|Completed
|1/7/19
|1/18/19
|1/21/19
|-
|Connect with Ali to figure out plan of attack
|Pending
|1/18/19
|1/23/19
|
|}
== January 25th, 2019 ==
'''EEG Sub-Team Meeting Notes:'''
* Ali's 6 step process for EEG data
*# Deploy ML models of EEG Raw Data
*# Prove cleaner data sets yield better learners by doing pre-analysis 
*# Study the data and extract relevant features
*# Manipulate the data to acquire deterministic components based off of analysis
*# Run ML on manipulated data set
*# Validate with traditional GP

* Ali shared his findings (cleaning data and then averaging frequencies across channels):
** Classification results for raw data: 80%
** Accuracy after cleaning data and taking out outliers: 91%
** Averaging across channels based upon classification: 99%
** Ali Normalized amongst subjects classified as the same: averaged each channel frequency across the test subjects which are classified to the same (open or closed) and then divided each individual channel frequency by the average of that channel for that classification
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Connect with Ali to figure out plan of attach
|Completed
|1/18/19
|1/23/19
|1/25/19
|-
|Watch Ali's Video on Fourier Transformation
|Pending
|1/25/19
|1/28/19
|
|-
|Get Ali's Jupyter notebook and start looking at code
|Pending Ali
|1/25/19
|2/1/19
|
|-
|Help Ali apply his 6 step process to the larger and more complex EEG data
|Pending
|1/25/19
|2/1/19
|
|}

== January 27th, 2019 ==
'''Personal Notes:'''
* Watched video on Fourier Transformation
* https://www.youtube.com/watch?v=spUNpyF58BY

*FT decomposes signal waves into the individual frequencies that make it up
*How can we apply this to EEG data?
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Watch Ali's Video on Fourier Transformation
|Completed
|1/25/19
|1/28/19
|1/27/19
|-
|Get Ali's Jupyter notebook and start looking at code
|Pending Ali
|1/25/19
|2/1/19
|
|-
|Help Ali apply his 6 step process to the larger and more complex EEG data
|Pending
|1/25/19
|2/1/19
|
|-
|Figure out how FT can help EEG data
|Pending
|1/25/19
|1/28/19
|
|}

== January 28th, 2019 ==
'''Team Meeting Notes'''
*Gave everyone an update on our team and the new focus groups we made
* Goals for the week: -
** Get started with our subtasks and focuses
** Run cropped frequency domain dataset with Emade and neural nets
*Asked Ali how we can apply FT to EEG data:
**Transform time vs. channel potential to frequency vs. amplitude of signal
**As such, pull out the fundamental frequencies of the channels, which should be different per class 
**This will allow us to normalize each channel output and feed the data to the classifier(s) 
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get Ali's Jupyter notebook and start looking at code
|Pending Ali
|1/25/19
|2/1/19
|
|-
|Help Ali apply his 6 step process to the larger and more complex EEG data
|Pending
|1/25/19
|2/1/19
|
|-
|Figure out how FT can help EEG data
|Resolved
|1/25/19
|1/28/19
|1/28/19
|}

== January 28th, 2019 ==
'''Team Meeting Notes'''
*Gave everyone an update on our team and the new focus groups we made
* Goals for the week: -
** Get started with our subtasks and focuses
** Run cropped frequency domain dataset with Emade and neural nets
*Asked Ali how we can apply FT to EEG data:
**Transform time vs. channel potential to frequency vs. amplitude of signal
**As such, pull out the fundamental frequencies of the channels, which should be different per class 
**This will allow us to normalize each channel output and feed the data to the classifier(s) 
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get Ali's Jupyter notebook and start looking at code
|Pending Ali
|1/25/19
|2/1/19
|
|-
|Help Ali apply his 6 step process to the larger and more complex EEG data
|Pending
|1/25/19
|2/1/19
|
|-
|Figure out how FT can help EEG data
|Resolved
|1/25/19
|1/28/19
|1/28/19
|}

== February 1st, 2019 ==
'''Team Meeting Notes'''
*We need to establish a control experiment to figure out the objective function
*We need baseline data of how healthy subjects perform when exposed to medium power to establish mean, median, and variances to compare to the paretic subjects. (Ali's realization)
*Started looking at the data with Ali
*New pickle data is read in as a 3D numpy array and not a Pandas DF so we need to fix our preprocessing accordingly
*Numpy 3D array data: array of array of arrays. Each person has an array, with each person having readings per channel over a certain time interval
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get Ali's Jupyter notebook and start looking at code
|Pending Ali
|1/25/19
|2/8/19
|
|-
|Help Ali apply his 6 step process to the larger and more complex EEG data
|Pending
|1/25/19
|2/8/19
|
|-
|Figure out how to convert numpy array into dataframe
|Pending
|2/1/19
|2/4/19
|
|}

== February 4th, 2019 ==
'''Team Meeting Notes'''
*Gave everyone an update on our team and the new focus groups we made
* Goals for the week: 
** Move onto working with new EEG data and apply Ali's learning of averaging and Fourier Transformation to this new EEG Data (Ali + Jas)
** Get healthy data taken at median TMS power (Scott + Ali)
*Issues within group now:
**Training over entire dataset leads to possibly trivial results, need to relabel and train per subject
**New Pickle data is read in as a 3D numpy array, but preprocessing is structured for Pandas DF
**We need metadata about the new EEG data to understand what the 3 dimensions in X_train (1175, 13, 2400)
*Had Jason look at the X_Train data to see if he understands the 3D
*Jason's interpretation: 13 channels, 1175 half second captures
*First row: 13 channels, 2400 time captures --> 1175 of these rows

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get Ali's Jupyter notebook and start looking at code
|Pending Ali
|1/25/19
|2/8/19
|
|-
|Help Ali apply his 6 step process to the larger and more complex EEG data
|Pending
|1/25/19
|2/8/19
|
|-
|Figure out how to convert numpy array into dataframe
|Pending
|2/1/19
|2/4/19
|
|-
|Get the labels for the metadata to understand what the 3 dimensions are (validate Jason's interpretation)
|Pending
|2/4/19
|2/9/19
|
|-
|Help Ali install EMADE onto his Linux 
|Pending
|2/4/19
|2/9/19
|
|}

== February 8th, 2019 ==
'''Team Meeting Notes'''

Goals for the subteam:
* Predict phase
* Make spectrogram (collab/ with Dr. Zutty) and get get new dataset for sanity checks
* More data from Scott's lab in March
* Averaging (me and Ali) --> not much progress this week. Have not been able to meet up; will have more progress next week
* Bug fix group is talking to Dr. Zutty to understand issue

* X_train (1175, 13, 2400) ---> trials x channels x samples
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get Ali's Jupyter notebook and start looking at code
|Pending Ali
|1/25/19
|2/15/19
|
|-
|Help Ali apply his 6 step process to the larger and more complex EEG data
|Pending
|1/25/19
|2/15/19
|
|-
|Figure out how to convert numpy array into dataframe
|Pending
|2/1/19
|2/15/19
|
|-
|Get the labels for the metadata to understand what the 3 dimensions are (validate Jason's interpretation)
|Completed
|2/4/19
|2/9/19
|2/8/19
|-
|Help Ali install EMADE onto his Linux 
|Pending
|2/4/19
|2/12/19
|
|}

== February 10th, 2019 ==
'''Focus Group Meeting Notes (Meeting with Ali)'''

* Possible issue: we need data to figure out day to day variations off EEG measurements (same person, same individual-healthy or unhealthy) ==> need varianaces
* Variances needed to generalize to different anatomies
* We need the original unhealthy dataset
* Is there a way to take the EMADE data and convert back to the original version? GPFramework.data.GTMOEPData back to original
* GPFramework.data.GTMOEPData  is hard to index and work with learners outside of EMADE
* Establish healthy anatomy  baseline and use it to verify our predictions from unhealthy data
* One approach: use the average of each class and treat the problem with a classification model 
* Other approach: Use the samples averages and treat the problem with a regression model
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get Ali's Jupyter notebook and start looking at code
|Completed
|1/25/19
|2/15/19
|2/10/19
|-
|Help Ali apply his 6 step process to the larger and more complex EEG data
|Pending
|1/25/19
|2/15/19
|
|-
|Figure out how to convert numpy array into dataframe
|Dissolved (unnecessary)
|2/1/19
|2/15/19
|2/10/19
|-
|Help Ali install EMADE onto his Linux
|Pending
|2/4/19
|2/22/19
|
|-
|See if I can convert the GPFramework.data.GTMOEPData back to original format
|Pending
|2/1019
|2/11/19
|
|}

== February 11th, 2019 ==
'''VIP Meeting Notes'''

* Gave everyone an update on our team and the new focus groups we made
* I have been unable to convert the GPFramework.data.GTMOEPData back to the original format. I tried various things last night but was unable to index or parse the object as I am unsure of the original data input. Plan to ask Dr. Zutty today for possible approaches.
* Ali dug through his files and found the script that converted the original data to GPFramework.data.GTMOEPData. I'm going to attempt to use that and work backwards
* We talked to James Rick and he showed up how to use the EMADE framework to index and parse the GTMOEPData object directly so we don't need to worry about converting 
* I realized I never git cloned into the EEG repo. My EMADE files were from the days of the Technical Analysis team so I made a new local directory and tried to pull the eeg EMADE repo that Scott forked for us 
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Help Ali apply his 6 step process to the larger and more complex EEG data
|Pending
|1/25/19
|2/15/19
|
|-
|Help Ali install EMADE onto his Linux
|Pending
|2/4/19
|2/22/19
|
|-
|See if I can convert the GPFramework.data.GTMOEPData back to original format
|Unnecessary
|2/1019
|2/22/19
|2/11/19
|-
|Ask Dr. Zutty for suggestions to convert GPFramework.data.GTMOEPData 
|Unnecessary
|2/11/19
|2/11/19
|2/11/19
|-
|Clone the eeg EMADE repo
|Pending
|2/11/19
|2/15/19
|
|}

== February 15th, 2019 ==
'''Sub Team Meeting Notes'''
[[files/Time v Power.jpg|thumb|233x233px|Normal: Time vs Power]]
* Scott was able to get eye open and close data and run FFT in real-time to convert from time-domain into frequency-domain and do a bandpass filter to crop between 8-12 Hz
* Normal: time vs Power ==> need to know when the eyes are open closes
* Post FFT + bandpass (refer to picture)
* Meeting Sunday 9-1
*[[files/Frequency vs power.jpg|thumb|179x179px|Post FFT: Frequency vs Power]]

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Help Ali apply his 6 step process to the larger and more complex EEG data
|Pending
|1/25/19
|2/15/19
|
|-
|Help Ali install EMADE onto his Linux
|Pending
|2/4/19
|2/22/19
|
|-
|Clone the eeg EMADE repo
|Pending
|2/11/19
|2/15/19
|
|}

== February 17th, 2019 ==
'''Sub Team Meeting Notes (Hackathon)'''
* Have to read  [http://cognet.mit.edu/book/analyzing-neural-time-series-data Analyzing Neural Time Series Data]: chapter 12 and chapter 13 ==> Morlet Wavelets
*Locally cloned the eegmade git repo
*Tried to help Scott to clone git repo onto the cluster 
*Had issues with askpass and tried 'unset ASKPASS' to no luck
*Don't have the necessary permissions to set -X forwarding with SSH because not admins on PACE Cluster
*Workaround: Used the $50 Cloud Computing coupon to scp (secure copy) the repo from local machine onto the cluster
*Started writing a method to split EEG pickle data for cross validation:
<code>combined = [(y_i,x_i) for (x_i,y_i) in zip(x,y)] #put together x and y before splitting</code>

<code>random.shuffle(combined)</code>

<code>k = 5 #change this to define number of folds for cross validation</code>

<code>fl = int(len(combined)/k) #fold length- amount of data in each fold</code>

<code>for i in range(0,k):</code>

<code>arr = combined[i*fl:fl*(i+1)] #separating out the fold</code>

<code>with open('x{}.pickle'.format(i+1), 'wb') as f:</code>

<code>pickle.dump([datum[0] for datum in arr], f)</code>

<code>with open('y{}.pickle'.format(i+1), 'wb') as f:</code>

<code>pickle.dump([datum[1] for datum in arr], f)</code>
* Need to request access from Scott for vipPreprocessing git repo to push this code --> Update: Scott granted me access to this repo
* I ran my script to create 5 fold of data and pushed everything to the repo
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Help Ali apply his 6 step process to the larger and more complex EEG data
|No Longer Required
|1/25/19
|2/15/19
|2/17/19
|-
|Help Ali install EMADE onto his Linux
|No Longer Required
|2/4/19
|2/22/19
|2/17/19
|-
|Clone the eeg EMADE repo
|Completed
|2/11/19
|2/15/19
|2/171/9
|-
|Write and push split method, as well as resulting data, for cross validation
|Completed
|2/17/19
|2/17/19
|2/17/19
|}
*

== February 18th, 2019 ==
'''VIP Team Meeting Notes'''
* Gave everyone an update from yesterday's hackathon
* Need to read and understand chapter 12 and 13 in [http://cognet.mit.edu/book/analyzing-neural-time-series-data Analyzing Neural Time Series Data] textbook to understand Morelet Wavelets
* Ali's seed notes:
** New and latest data dimensions: 1459, 12, 5000 (trials, channels, samples)
** Idea to get more data: pull out the samples and trials to get 7,295,000 (1459 x 5000)samples, each with 12 channels. <code>temp = np.ndarray(shape=((1459*5000),12)</code>  <code>for i in range(0,1459):</code>     <code>curr = X[i].transpose</code>
<code>temp.append(curr)</code>

<code>temp = np.concatenate(temp, axis=0)</code>

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read chapter 12 and 13 to understand Morlet Waves
|Pending
|2/18/19
|2/25/19
|
|-
|Help Ali with new seed idea
|Pending
|2/18/19
|2/25/19
|
|}

== February 22nd, 2019 ==
'''Subteam Meeting Notes'''
* Peer reviewed Ali's seed: he claimed to have gotten 99%
* We realized there is dependent points between the test and train data so it was not split well and we can't gauge the accuracy of the learners and this seed
* Fix: split the data before transforming to 7.3 million points (basically keep all data points of each instance together, either in test or train to keep the data disjoint)
* ^zip x and y where x is a (12,5000) matrix and y is the classifcation and then shuffle
* Helped Ali by outlining ML pipeline to him
* Team updates:
* James- Finding papers that demonstrate correlation between EEG features and excitability as measured by TMS
* Joel- feature abstraction
* Ali and JasÂ - out of EMADE classifier 
* Rahul - PACE cluster
* Nick - plugging seeds into EMADE (accessing the FFT for a *space-efficient* series of convolutions)
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read chapter 12 and 13 to understand Morlet Waves
|Pending
|2/18/19
|2/25/19
|
|-
|Help Ali with new seed idea - and splitting test and train data
|Pending
|2/18/19
|2/25/19
|
|}

== February 25th, 2019 ==
'''VIP Team Meeting Notes'''
* Gave an update to everyone
* Ali is still having issues with high classification--> dependent data points between test and train
* I helped him by explaining how we need to split the instances, not the data points, so that all the samples of each instance, stay together, either in test or train
* If a subset of the 5000 points from any instance go into test and another subset goes into train, then there is dependent data in the test and train sets, which is our current issue
* Took the data and flattened it horizontally to preserve time series analysis
* Morelet waves:
** https://www.youtube.com/watch?v=wgRgodvU_Ms
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read chapter 12 and 13 to understand Morlet Waves
|Completed
|2/18/19
|2/25/19
|2/25/19
|-
|Help Ali with new seed idea - and splitting test and train data
|Pending
|2/18/19
|2/25/19
|
|}


== March 4th, 2019 ==
'''VIP Team Meeting Notes'''
* Missed last Friday's meeting because I was out of town
* Team updates:
** We've adapted EMADE for the PBS job scheduler on the PACE cluster
** Visited Emory to collect new data
** Began running EMADE on dataset
** Ali used FFT to decompose a time series EEG recording into its brain waves composition (Delta band, Theta band, Alpha band, Beta band, and Gamma band)
** The flattened data from last week is trivial
*Ali's findings applied to our data shows weird findings: there are barely any differences between the wave compositions of each channel so how can use this 
*Dr. Zutty fixed the issue by normalizing our data! 

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Catch up on what happened
|Completed
|3/1/19
|3/4/19
|3/4/19
|-
|Help Ali with new seed idea - and splitting test and train data
|Completed
|2/18/19
|2/25/19
|2/25/19
|-
|Help Ali put the decomposition into Emade
|Pending
|3/4/19
|3/15
|
|}

== March 6th, 2019 ==
'''Focus Group Meeting Notes'''
* Met with Ali to discuss current status
* Next steps: we are going abstract features to feed into EMADE
* Features: FFT and EEG Power Decomposition
* Idea: get features for all the samples of an instance but also break down a given instance into smaller random subgroups and get features from that as well

'''Action Items:'''
{| class="wikitable"
!'''Task'''
!'''Current Status'''
!'''Date Assigned'''
!'''Suspense Date'''
!'''Date Resolved'''
|-
|Help Ali put the decomposition into Emade
|Pending
|3/4/19
|3/15/19
|
|}

== March 8th, 2019 ==
'''Subteam  Meeting Notes'''
* Started recruiting slides
* Talked about our idea of chunking samples into random sizes to draw insight for various points, not just the instance as whole

'''Action Items:'''
{| class="wikitable"
!'''Task'''
!'''Current Status'''
!'''Date Assigned'''
!'''Suspense Date'''
!'''Date Resolved'''
|-
|Help Ali put the decomposition into Emade
|Pending
|3/4/19
|3/15/19
|
|-
|Finish Recruiting Slides
|Pending
|3/8/19
|3/11/19
|
|}

== March 11th, 2019 ==
'''VIP Meeting Notes'''
* <u>Viz</u>
** Goal: visual interface to make training easier - View how individuals are performing during training
** Visualize: pareto front,  evaluation times, tree structure, AUC graph, number of Pareto Individuals over time, evaluation times
** Current goals: add more visualizations (hereditary), improve usability (UI/UX), refactor code to be efficient
** User Study feedback: document progress, UI needs to handle failure better
*** Segregate generational visualizations from aggregate visualizations
*<u>Group Four (1st Semester)</u>
**ML
***Dropped Name, Ticket and Cabin
***Replaced NaN with averages
***Logistic regression + DT ==> highest Kaggle score (.784)
**GP
***results : .79
***Better than ML
**EMADE
***39 generations over two weeks
***seeded with random forest individual
***maybe better if ran for longer but no time
***AUC: .9138
**Learned:
***EMADE makes building models so much easier
***ML - user needs to make many choices (data prep, future extraction)
***GP - requires more work
**Troubles:
***Installing Emade
***SQL Issues with EMADE
*<u>Group 2</u>
**ML
***Dropped non-numeric/ non-categorical cols
***Used GridSearch to find the best parameters
***Classifiers: DT, Random Forest, AdaBoost, C-SVM
***Best: Random Forest - 83.7%, Decision Tree - 84%
**GP
***Attempt 1: Best Individual fitness: (0, .983)
**EMADE
***Three objectives --> False positives, False negatives, Number of elements
***4 runs, 48 generations
***AUC: 1882.32 -- hypervolume
*<u>Group 5</u>
**Preprocessing
***Title Based NaN replacement
***Separate age groups for Mr. and Mrs.
***Banded age groups into 5 generations
***Embarked was one hotted, cabin was converted to # based on depth, fares was banded into groups
**ML
***Age: averaged based on subgroups split on other values
***DT, Gradient Boosting, KNN
***Hyperparameter selection using using trial and error
***Accuracy: 76%
**GP
***Mean square error for fitness
***Squash output between 0 and 1 with sigmoid
***Accuracy: 79%
**EMADE
***ASK AUSTIN
***Notes from Dr. Rohling: use proper graphs with units/ axis; otherwise hard to visually communicate what's going on
*<u>Caching</u>
** Shit ton of speed efficiency Instead of train subtrees over and over, cache previous generation instead of re-evaluating
*** Just store result of previous Trees
** Built in parameter for EMADE
*** Use_Cache a boolean variable now that can be flagged to make use of this
** Progress since last Semester
*** Documentation web app for caching improvements/ API calls
*** Updated all XML templates within caching to be compatible with branches
*** Working on getting it to work with image data
**** Currently works with stream/ strings
***** Can make running EMADE 500% faster
** Next Steps
*** Get it to work on image data, currently there are errors

* <u>Bootcamp Group 3</u>
** Lab 1
*** N Queens problem
** ML
*** Best score: .78
*** Score after parameter tuning: .77
*** Three models had the same exact score (?)
** GP
*** Added primitives, add subtract power max 2
*** Used mutUniform for mutation
*** AUC: .2310
** EMADE
*** Ran for 30 generations
*** AUC: .199
** Learned
*** ML - least accurate, tended to overfit
*** GP - more accurate because mutations
*** EMADE - Best

* <u>Stock Team</u>
** Goal: use EMADE to predict stock price on next day based on previous few days
** Previously: simple binary classification problem: buy or sell
** Based on technical indicators
** Current focus: time series data
** Predict actual price value as regression problem
** Use signal processing on time series data for forecasting
*** Auto regression
*** Moving averages
*** ARMA/ARIMA
*** SARIMA
*** VAR/VARMA
*** SES
*** HWES
** Future
** EMADE on Google Cloud
** Implement non-classical time series forecasting methods
* <u>Group One</u>
** Data Pre-processing
** ML
*** MLP Classifier - Kaggle Highest Score .775
*** Random Forest Classifier
*** Gradient Booster Classifier
*** Decision Tree Classifier
** GP
*** Strongly typed operators
*** Tournament selection did not work well
*** ^Used NGASII selection instead
*** AUC = 30700
** EMADE
*** Started with sql connection errors
*** Ran for 14 generations
*** Had 2 optimal individuals
*** AUC: 647
*** Used VIZ team data to produce EMADE Pareto front
** Conclusion
*** AUC for EMADE far better than AUC for GP
*** EMADE has best individual that is not overfitted with 4.6 fp and 35 fn

* <u>Group Six</u>
** Feature Engineering
** EMADE
*** Best run - 21 generations
*** EMADE > GP/Ml
** Used EMADE VIZ to generate visualizations
** Hypervolume calculation errors caused random jumps in graphs
** ML
** GP
*** used selSPEA2 selection method

* <u>EEG</u>
** Our Presentation
** Feedback:
*** Run EMADE
*** Put things into EMADE
*** Use EMADE
*** Stop not using EMADE

* <u>Deep Learning</u>
** GP vs. CGP
** CGPs are represented by DAGs
** Not exactly a tree, its a directed acyclic graph
** Blocked primitives?
** Dataset: MNIST
** 99+% accuracy
** Looking for:
*** GCP experience
*** ML + DL experience
*** People with GPUs

'''Action Items:'''
{| class="wikitable"
!'''Task'''
!'''Current Status'''
!'''Date Assigned'''
!'''Suspense Date'''
!'''Date Resolved'''
|-
|Help Ali put the decomposition into Emade
|Pending
|3/4/19
|3/15/19
|
|-
|Finish Recruiting Slides
|Completed
|3/8/19
|3/11/19
|3/11/19
|}

== March 15th, 2019 ==
'''EEG Team Meeting Notes:'''
* Overarching goal by the end of the semester (actually ASAP): Get something into EMADE
* Update of decomposition into EMADE task: Ali is playing around with using decomposition in GP so hasn't moved to EMADE just yet. 
* ^That will be the focus of the next few days and something I will be helping him with as my last few commitments have been handling the recruiting part
* Currently: Real-time FFT is possible within 3 milliseconds
* ^Use that as a foundation for why we should be able to do band decomposition real time
* Made a new branch in eegmade: ali_jas for our band decomposition activity and took the script that does decomposition from pickle file
* Need to write a signal method script to do that^ but for the GTMoep version of the data that Scott put in the datasets
* Link to get pickle data: https://drive.google.com/drive/folders/1zsIZrlgTvGw-9nLCI9WOOPxEidJoF2qq
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Help Ali put the decomposition into Emade
|Pending
|3/4/19
|3/29/19
|
|-
|Write script to decompose data as GTMOEP
|Pending
|3/15/19
|3/29/19
|
|}

== March 21st, 2019 ==
'''Personal Notes:'''
* Slow progress currently as this is Spring Break
* Current team status:
** Nick/Scott: instantaneous phase calculation
** Jas/Ali: frequency band calculation for most common bands (Jupyter notebook ---> EMADE)
** Joel/Rahul: refactor any organizational things  
** James/Austin: need to implement something from a paper that correlates with motor activity
*What is ''butter_bandpass_filter''?
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Help Ali put the decomposition into Emade
|Pending
|3/4/19
|3/29/19
|
|-
|Write script to decompose data as GTMOEP
|Pending
|3/15/19
|3/29/19
|
|}

== March 25th, 2019 ==
'''Team Notes:'''
* Got new people:
** Shlok Natarajan - 1st year CS Premed. Interest: Medical side + reading articles for getting new methods --> James/Austin
** Michael Lee -  2nd year CompE. Interest: DSP --> Scott/Nick
** Kang Shin - 1st year MS CS (Biology Undergrad @ Emory). Interest: ML + Research --> 
** Shruthi Sudhakar - 1st year CS. Interest: ML + general coding --> 
** Some of them have time conflicts for our Friday meetings. Might need to figure out new time
* ''butter_bandpass_filter -'' Python method to cut the extraneous frequencies 
* I will meet with Ali through this week - tentatively Wednesday 
* Talked to Dr. Zutty- <u>Focus</u>: (Note: Joel would be best help with this)
** NOT band decomposition --> GTMOeP.Data 
** Band Decomposition --> primitive:
*** instance by instance to make a flat feature array
**** instance by instance (13 X 5000) --> FFT---> existing primitive 
**** Dictionary of filtered band decomposition from FFT --> new primitive 
**** Sum up bands to make feature array 
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Help Ali put the decomposition into Emade
|Pending
|3/4/19
|3/29/19
|
|-
|Write script to decompose data as GTMOEP
|Pending
|3/15/19
|3/29/19
|
|}

== March 27th, 2019 ==
[[files/Screen Shot 2019-03-27 at 4.23.28 PM.png|thumb|350x350px]]'''Meeting with Ali Notes:'''
* First focus: run EMADE locally on our machines
* Finally got EMADE running on my new machine
* Talk to Joel on Friday about how to use his template to achieve the steps that Dr. Zutty set out for us
* Talk to Scott about how to create .XML files for our purpose and how to connect to SQL because Ali and I are both having SQL connection errors
* Figure the above out by Friday and then we can hopefully accomplish the other goals by Monday, with that information
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Help Ali put the decomposition into Emade
|Pending
|3/4/19
|4/1/19
|
|-
|Write script to decompose data as GTMOEP
|Pending
|3/15/19
|4/1/19
|
|-
|Figure out SQL errors
|Pending
|3/27/19
|3/29/19
|
|-
|Talk to Joel about his template for primitives
|Pending
|3/27/19
|3/29/19
|
|}

== March 29th, 2019 ==
'''Team Meeting Notes:'''
* Updates from Scott: still getting trivial results on EMADE
* Scott made an example primitive called ExamplePrimitive Jupyter Notebook that he is pushing to VipPreprocessing - don't need to wait on Joel
* Rahul helped me fix SQL errors
* Follow Scott's example to adapt existing Band Decomposition into primitive on Jupyter Notebook
* Update:
** Jas/Ali: have 1 primitive in a python notebook (ask Scott for help)
** Austin and James (plan to implement an algorithm in Python from a paper)
** Rahul: Morelet: 
** Michael: read EE book
** Shlok: read neurosci book
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Help Ali put the decomposition into Emade
|Pending
|3/4/19
|4/1/19
|
|-
|Write script to decompose data as GTMOEP
|Pending
|3/15/19
|4/1/19
|
|-
|Figure out SQL errors
|Pending
|3/27/19
|3/29/19
|3/29/19
|-
|Talk to Joel about his template for primitives
|Pending
|3/27/19
|3/29/19
|3/29/19
|}

== April 1st, 2019 ==
'''VIP Meeting Notes:'''
* Gave everyone an update on the current progress: catching up the new members has been the mail goal
* This week has been stressful with other exams and getting the new members up to date so have not made progress on current goals
* Pushing suspense date for those goals by a week
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Help Ali put the decomposition into Emade
|Pending
|3/4/19
|4/8/19
|
|-
|Write script to decompose data as GTMOEP
|Pending
|3/15/19
|4/8/19
|
|}

== April 3rd, 2019 ==
'''Focus Group Meeting Notes:'''
* Shruthi joined Ali and I to get a better understanding
* I helped her by pointing her to the FFT video to catch her up
* Update from Ali about data:
** Apparently the data we have been using thus far is corrupted due to DCOffset and improper procedures during data acquisition 
** This means that the local ML methods we tried in the beginning of the semester could possibly be not trivial and actually worth exploring
** Need to retry later
* Repository notes: (because this is getting a bit confusing) We are currently working out of two repos
** https://github.gatech.edu/sheston3/vipPreprocessing.git 
*** Contains the current preprocessing methods
*** Has Scott's example primitive
** https://github.gatech.edu/sheston3/eegmade.git
*** Contains a branch for me and ali - Ali_Jas
*** Branch has AliLocalScript.ipynb which contains Ali's local code for band decomposition 
*Questions for Scott from current my_fft() method:
**what is contained in test_list?
***first_instance = train_list[0]
***is this a channels by samples array? if so: channels = first_instance.shape[0]
***is this flattened further to just singular data points
***FOR NOW- hardcoding as having 13 channels but will want to derive from data
*Our latest code will be in eegmade under signal methods as my_fft_decomp() method

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Help Ali put the decomposition into Emade
|Completed
|3/4/19
|4/8/19
|4/3/19
|-
|Write script to decompose data as GTMOEP
|Completed
|3/15/19
|4/8/19
|4/3/19
|-
|Verify and Validate Method by showing to Scott
|Pending
|4/3/19
|4/5/19
|
|}

== April 5th, 2019 ==
'''VIP EEG Subgroup Meeting Notes:'''
* Showed our method to Scott and explained what we did
* We need to handle the data pair better for different features and streams
* Scott wrote a python notebook to test primitive methods locally ---> need to run
* He explained structure of data_pair to help us restructure our code: 
** Stream to Features:
*** Takes in time series and spits out pertinent information
*** ex: fft_decomp
** Features to Features:
*** No time series
*** Information about regular data
*** ex: titanic
** Stream to Stream:
*** Transform time series data into more time series data
* New focus:
** I'll continue editing the primitive using Scott's testing to make sure it works
** Ali will work with Michael to find ideas for new primitives

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Verify and Validate Method by showing to Scott
|Completed
|4/3/19
|4/5/19
|4/5/19
|-
|Run Primitive on Scott's Python notebook
|Pending
|4/5/19
|4/8/19
|
|-
|Edit and fix primitive 
|Pending
|4/5/19
|4/12/19
|
|}
== April 6th, 2019 ==
'''Personal Notes:'''
* Used Scott's notebook to start editing the method we wrote
* There's some formatting errors that need to be fixed
* ^ I will continue working on editing and fixing this primitive

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run Primitive on Scott's Python notebook
|Pending
|4/5/19
|4/8/19
|4/6/19
|-
|Edit and fix primitive
|Pending
|4/5/19
|4/12/19
|
|}

== April 8th, 2019 ==
'''VIP Team Meeting Notes:'''
* Gave weekly update
* There's three other primitives in testing stage along with mine: Morelet Wavelet Transforms, State Variance, and Mean State Shift
* Ali and Scott got new base line data which will be used to retry old methods
* Because we realized the data was corrupted, the methods we disregarded in the beginning of the semester were wrongfully disregarded and need to be re-explored
* Ali and I are splitting off:
** Ali and Shruti will re-explore the old ML methods that Ali tried earlier
** I will edit and finalize the primitive we started

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Edit and fix primitive
|Pending
|4/5/19
|4/12/19
|
|}
== April 10th, 2019 ==
'''Focus Team Meeting Notes:'''
* I worked on editing the primitive using Scott's jupyter notebook testing method
* Ali and Shruti cleaned the old corrupted data to re-explore the original local ML methods 
* Realized that we wrote out primitive to handle GTMOEPImagePair and not a GTMOEP data pair, which is fundamentally wrong 
* Edited the method more and will finish it, hopefully, on Friday 

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Edit and fix primitive
|Pending
|4/5/19
|4/12/19
|
|}

== April 12th, 2019 ==
'''Subteam Meeting Notes:'''
* Gave everyone an update:
** Scott - auto encoder
* Continued working on editing my primitive using Scott's tester
* Once it's done, make a seed out of the primitive: follow Scott's example:
** Make a primitive 
** Add to pset
** Make your text of the individual: Seeding_Files folder in the repo
** Then seed it in with the command: 
*** Python src/GPFramework/seeding_from_file.py templates/input_emotion.py name_of_your_text_file
* Have not been able to finish primitive yet due to other work
* Goal is to finish by Monday, have Jason review to detect any early problems, and have evolutionary run for at least a week - leading up to final presentation on the 22nd


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Edit and fix primitive
|Pending - Pushed
|4/5/19
|4/15/19
|
|}

== April 14th, 2019 ==
'''Personal Notes:'''
* Worked on editing the primitive
* In the process of testing something broke
* When I reinstalled and ran Scott's tester, I got these Scoop errors
* Finding no solution, I coped out my primitive and reset my git branch to an old commit in order to retry
* Having spent 3 hours trying to debug this, with an upcoming hell week, I do no think I can finish this primitive fixes before tomorrow's checkpoint :(
{| class="wikitable"
|[[files/Error.png|thumb]]
|-
|The error that I ran into: gaierror: [Errno 8] nodename nor servname provided, or not known
|}

* Fixed the scoop issue by recloning
* Used the my_fft method and the notebook testing script to fix and finish the primitive!
* Next steps:
** Dr. Zutty will verify the primitive tomorrow to point out any visible issues
** Follow Scott's steps to seed and put primitive into the evolutionary run
* My code can be found at: https://github.gatech.edu/sheston3/eegmade/tree/ali_jas
* src/GPFramework/signal_methods.py/my_fft_decomp()
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Edit and fix primitive
|Completed
|4/5/19
|4/15/19
|4/14/19
|-
|Verify and push primitive to run
|Pending
|4/14/19
|4/15/19
|
|}

== April 15th, 2019 ==
'''VIP Meeting Notes:'''
* Gave updates in terms of finishing primitves
* Acknowledging Dr. Zutty's comment for final validation and working with him to finalize:
** There was a lot of discussion with Dr. Zutty, through which I gained valuable insight in terms of how the method was not doing what I thought it was doing
** Scott gave me ECE insight along with that to explain 
** I cannot explain in minute detail because the topic was very ECE heavy and I do not understand enough
** Higher level overview:
*** Run fft on each instance
*** Find the indices that correspond to the input frequencies of each band - in the fft output
*** For each channel: average the values between those indices
*** The second bullet is the important step that I was missing earlier when I was using band frequencies as sampling indices
* Will seed the primitive later this week
* Scott changed the dataset: DEAPdataset
* Our goal is to beat the 78% accuracy that Scott got with a Neural Network - Benchmark

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Verify and push primitive to run
|Completed
|4/14/19
|4/16/19
|4/15/19
|-
|Seed primitive for run
|Pending
|4/15/19
|4/16/19
|
|-
|Peer Evals
|Pending
|4/15/19
|4/23/19
|
|-
|FInal Presentation
|Pending
|4/15/19
|4/22/19
|
|}

== April 16th, 2019 ==
'''Personal Notes:'''
* There was a variable error in my code that Ali pointed out so I fixed and pushed that.
* Finished Peer Evals
* Took me a while but merged my primitive code into master
* Added seed to PSET
* Waiting of Scott for directions to seed further

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Seed primitive for run
|Pending
|4/15/19
|4/19/20
|
|-
|Peer Evals
|Completed
|4/15/19
|4/23/19
|4/16/19
|-
|FInal Presentation
|Pending
|4/15/19
|4/22/19
|
|}

== April 19th, 2019 ==
'''Meeting Notes:'''
* Last team meeting focuses
** Merge branches to master (I did mine)
** Add to PSET (I did mine) 
** Seed
** Final Presentation
** python src/GPFramework/seeding_from_file.py templates/input_valence_all.xml seeding_test_decomp
** I seeded: SingleLearner(myFFTDecomp(ARG0, 5000, 2), ModifyLearnerFloat(learnerType('DepthEstimate', {'sampling_rate': 1.0}), 14.186666666666667))
* Next thing is to edit presentation and add my seed information

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Seed primitive for run
|Pending
|4/15/19
|4/19/20
|4/19
|-
|FInal Presentation
|Pending
|4/15/19
|4/22/19
|
|}

== April 21th, 2019 ==
'''Personal Notes:'''
* Working on presentation
* https://docs.google.com/presentation/d/1UBfFPGBJ5NM3D_9h8j1SWma7n7bXaxWzh4zxxjiS9z8/edit?usp=sharing

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|FInal Presentation
|Completed
|4/15/19
|4/22/19
|4/21/19
|}

== April 22nd, 2019 ==
'''VIP Meeting Notes:'''
* Presentation Day

* First group, Stocks
** Use EMADE to predict stock prices based on historical data
** Regression problem instead of classification (?)
** Implemented technical indicators as feature construction
** Since it's regression instead of classification, implemented completely different primitives (ex: MDP)
** Data used: Text file of GE stock data with some preprocessing
** Time series decomposed into multiple features (trend, seasonality, residuals)
** These preprocessing methods have been Implemented into primitives for emade to work with
** Statistical Models used: SARIMA (Seasonal AutoRegressive Integrated Moving Average), VARMA (Vector AutoRegressive Moving Average), HWES (Holt-Winters Exponential Soothing)
** Deep Learning Models used: MLP, RNN, LSTM
** Accuracy metrics used to evaluate models: sMAPE (symmetric Mean Absolute Percent Error), MASE (Mean Absolute Scaled Error)
** Results: Statistical models have lower error than DL models, likely due to Statistical models fitting directly on testing data
** DL methods also require much tuning
** Results don't produce accurate enough results to use in real life
** Problems
*** Emade timeout errors, workflow of Emade, unfamiliarity with implementation
** Future work: run EMADE, more DL models, Sector market indicators

* Second group, Caching
** Divided into multiple sub-teams
*** support all data types that EMADE supports
*** Create APIs that give users options to use their own cache invalidation method for their own problem space
** Cache Maintenance - add documentation
*** Dockerizing Cache - simplify, standardize process of running on all OS's - makes updates easier, less work for GCP setup
*** Future updates: standardize outputs, build faster, fix conda version problems, test on other OS's
** Cache Validation
*** Want more optimal system to maximize benefit of subtrees stored
*** Solution: use the Dynamic Programming solution to the Knapsack Problem
*** Problems with solution: time ineffective, time cost is large even when the number of subtrees in cache is very small
*** Potential solutions to those problems: Scale weights, benchmarking, using buckets, etc
** Potential optimization of Knapsack
*** Performance vs precision tradeoff
**** Sacrifice precision with approximation algorithm, greedy, scaling & rounding
*** Parallelism
**** Hypercubes vs irregular mesh
** Scripts/ work done
*** Wrote a script to run EMADE automatically

* Third group - EEG(us)
** https://docs.google.com/presentation/d/1UBfFPGBJ5NM3D_9h8j1SWma7n7bXaxWzh4zxxjiS9z8/edit?ts=5cbe4470

* Fourth group, Data Visualization
** Motivations
*** Provide a visual interface to be able to interact with EMADE
** Recap Fall 2018
** Goals for 2019
*** More visualizations
**** Visualizations of pareto front over time
*** Improve usability
*** Refactor Code
*** Make app generalize
*** Visualization of parents: concept
**** Want to have a visualization for where the parent of a dominated from comes from in a front.
** User Study Feedback
*** documentation
**** Clearly state he separation between EMADE and EMADE-visualization
**** more detail on creating an environment
*** UI Feedback
*** Visualization
*** XML Importing
**** Goal:
***** Generalize visualizations for any type of problem and any kind of objective functions
** Pickling First Years
*** Goal Reduce the number of times we make a call to the database in order to increase efficiency
**** Pickling
** Future
*** Make EMADE log meta data in mysql table
*** finish hierarchy visualizations
*** add seed creation GUI
*** GUI for executing sql queries

*Fifth group, DEEP
** Subteam B:
*** Regression Problem Housing Prices
*** Progress since Midterm
**** identified Kaggle Housing Price dataset
**** incorporated changes into ezCGP to support regression problems
**** Added Average Percent Change (APC) and Mean Absolute Error (MAE) as fitness functions for regression problem
**** used StandardScaler to split and normalize training/testing data and preprocessed the housing dataset
*** Housing Price Dateset:
*** Parameter Choices
**** restricted primitives to only dense layers, finding optimal number of dense layers
*** Individual with Best APC:
**** uses about 7 dense layers, 6 hidden
**** predicted price is just 1 value, housing price
**** trained best individual for 35 epochs
*** Results on Housing Dataset
**** compared to other kaggle results
**** regularized NN performed slightly better
** Subteam A:
*** Improving structure of ezCGP
*** Progress since Midterm
**** implemented argument/parameter mutation
**** changed the framework to deal with large datasets
*** Dataset 1: MNIST
**** used because it is easy to evaluate and accessible, population size was 12, epoch set at 1, ran 35 generations
**** Results on MNIST
***** best individual is 95.85% and 98.84%
***** took the individual and trained on full training set
***** got 99.85%
**** Compare to Midterm Results
***** trained model further, about 42 epochs. best accuracy 99.43%
***** assume since its continuously increasing, will keep going up
*** Dataset 2- CIFAR-10
**** Parameters:
***** pop size 9, epochs to 3, 25 generations
**** Results on CIFAR-10:
***** best accuracy of 79.7%, ran for 100 epochs, increased in accuracy by 1.32%
*** Dataset 3- CIFAR -100
**** Parameters:
***** pop size 9, 5 epochs, 50 generations
**** Results:
***** low accuracy but still improved
***** best individual was bad - just a conv block->max pooling->average pooling
***** trained over 200 epochs because accuracy plateaued
***** cifar-100 model under performed when trained on whole dataset. why?
****** lack of genetic diversity
****** smaller models learn faster
****** larger models learn more defining features and therefore generalize better
***** how to fix?
****** increase number of epochs
****** utilize first and second order gradient information to make better judgement whether its done learning
****** kill smaller individuals