== Team Member ==
Name, Program: Jon Greene, OMSCS

Email: jgreene82@gatech.edu

Interests: Physics, CS, AI/ML, EvoComp, SCUBA

Subteam: NLP

== Spring 2021 ==

=== Week of April 26th ===

==== Group Meeting ====
* Dr. Zutty Announcements
** Final presentation Friday!
** Notebooks due Saturday at midnight!
* Stocks team implemented Fibonacci retracement
* ezCGP scraped layers commonly used by pre-trained networks

==== Breakout Meeting ====
* Dr. Zutty joined meeting to answer a few questions
** Are launch queue and min queue working as intended? (by gen 25 there were so many valids individuals that eval step got bogged down)
*** It is working as intended, could be optimized with meta learning and scheduling (for next term)
* Steven found mediocre individuals to seed a 24 hour run to validate evolution, increased accuracy from 60% to 90% (it works!)
* Karthik updated branch with FNR/FPR eval metrics
* Cameron completed new results but noticed slowdown on gen 25 (discussion with Dr. Zutty)
* I will port CV primitives to Wiki, [https://wiki.vip.gatech.edu/mediawiki/index.php/Guide_to_CV_Primitives here]
* Everyone is tasked and their respective parts for the presentation

==== Presentation Preparation (Wednesday) ====
* Ran through copy of midterm presentation
** Each slide was assigned an owner
** New content place slides (empty) were placed appropriately and given owners
** Next term goals and current term accomplishments were discussed
*** Current term
**** Unravelled issues with evolution performance of NNLearner
**** Streamlined PACE environment, setting next term up for success
*** Next term to focus on specific topics in smaller teams

==== Presentation Practice (Thursday) ====
* Discussed outstanding content, new results, and visualizations
* Had practice run of presentation
** Came in at just over 24 minutes
** A slide was skipped due to an absence
* Talked about streamlining the content on a few background slides to help with time

==== Final Presentations ====
* Stocks
** Term work based on paper that uses Piece-wise Linear Representation and Exponential Smoothing
** Added several novel technical indicators and had a thorough analysis
*** They used Cameron's individual visualizer script!
* ezCGP
** Objective to recreate CIFAR-10 results from previous term without transfer learning by improving Neural Architecture Search
** Every convolutional layer was arbitrarily choosing different activation functions
*** ReLU was forced at each layer but curiously no change in performance was observed
** Added fully connected "dense" layers which improved accuracy
** Explored symbolic regression and meta-parameter search on individuals
* Modularity
** Previous ARL implementation selected trees of depth 1 and were based on probability of their occurence in the population
** Revised method allowed for depths of greater than 1 and weighted by ARL size
*** Various methods were explored and combined to handle deeper ARLs
** They also used Cameron's visualizer script!

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Final Presentation Slides and Practice
|complete
|4/26/2021
|4/30/2021
|4/29/2021
|-
|Port CV Primitive Documentation to Wiki
|complete
|4/26/2021
|4/30/2021
|4/29/2021
|}

=== Week of April 19th ===

==== Group Meeting ====
* Stocks team is considering training on individual stocks
* ezCGP new members working on mating and visualization, dense layers added (best ind. accuracy was 55%)
* Modularity team is making headway with database, ARLs are properly made and added to the db

==== Breakout Meeting ====
* Dr. Zutty joined at Cameron's request to discuss NNLearners as subtrees
* Cameron pushed update to PACE files and team branch (adjusted seeding file, naming) on git
* Steven found a couple individuals that don't perform well in standalone tree evaluator to test evolution
* Karthik is playing around with FNR and FPR metrics
* Hua had a few runs (both gpu and cpu only) but they yielded poor results
* Nishant, Prahlad, and Harris have ran EMADE and will review results
* Sumit, Anshul, Heidi, and myself are working through PACE
* Discussed high level goals for end of term
** Streamlining PACE env
** EMADE producing competitive individuals

==== Subgroup Meeting ====
* Cameron W. continuing with many workers implementation
** Many workers successfully spin up but will attach to the same gpu even when specified not to
** Update 4/25: Cameron pushed large updated to git, fully implementing many workers
*** Huge performance increase; typical run completes ~1 generation per hour on Amazon dataset, with many workers 15 generations completed in 2.5 hours (~6 gen per hour)
* Sumit having issues on specific stop words
* Steven working on seeding individuals that performed poorly in the standalone tree evaluator
* Karthik cloning new branch and will get FNR/FPR running
* Hua getting competitive results in his PACE runs
* Nishant, Harris, Prahlad, Cameron B., and myself are troubleshooting miscellaneous PACE issues
** I was receiving a "table history already exists" error
*** Cameron informed me to re-download git setup files as they had been updated
** End of meeting was used for helping each other out

=== Week of April 12th ===

==== Group Meeting ====
* Stocks team is considering applying Welsch test from stats lecture to individual's profit percentage
* ezCGP new members working on mating and visualization
* Modularity team is not having many valid individuals being created on MNIST dataset, looking for other multiclass objective (currently using F1 score)

==== Breakout Meeting ====
* Anshul and Sumit are finishing up ther PACE installation
* Cameron B., Harris, Hua, Karthik, Nishantm Prahlad, and myself have PACE set up
* Karthik created shell [https://github.gatech.edu/cwhaley9/PACE-files/blob/master/pace-login.sh file] to automate SSH connection and launch of EMADE
* Cameron resolved GPU issue
** Completed GPU multi-run 
*** Three 8 hour runs where ouput of run seeded the next
*** 5-6 individuals evaluated (best individual had 0.932 accuracy)
* Steven completed two runs (8 hour GPU, 24 hour CPU only)
** Showed pareto front of his completed run (best individual had 0.9296 accuracy)

==== Subgroup Meeting ====
* Team is tasked per [https://docs.google.com/document/d/1V-etbhOdzUfgjwMLX7qtFNQEVGNnmrx5GfaQxfeosJ4/edit Gdoc]
* Harris, Karthik, and Nishant will explore PACE this weekend and learn how querying the db works
* Temi got PACE set up
* Hua test run got stuck in gen 1, to troubleshoot
* Cameron working on a pull request to have many workers available in one run, re-use turned on to allow 24 hour runs
** Oddly a non-LSTM individual in a run had an accuracy ~0.9
* Steven expanding on analysis he showed in the breakout meeting
** Will seed run with bad individuals to see if evolution works
* I will port NLP primitives Notion doc to [https://wiki.vip.gatech.edu/mediawiki/index.php/Guide_to_NLP_Primitives wiki]

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Port NLP Documentation to Wiki
|complete
|4/12/2021
|4/19/2021
|4/18/2021
|-
|Explore NNLearner Evolvability Issues
|complete
|4/16/2021
|4/23/2021
|4/26/2021
|}

=== Week of April 5th ===

==== Group Meeting ====
* ezCGP completed dense layers, max pooling, and dropout layers
* Modularity working on benchmarking MNIST

==== Breakout Meeting ====
* Steven wrote script that generates kfold splits on a given dataset
* Meeting was used for helping everyone get set up in PACE

==== Subgroup Meeting ====
* Anshul gave Neural Nets 101 [https://docs.google.com/presentation/d/1CB7nFttRU0psaFTDHHWIScy8nFkvT0X5bTc3T_En808/edit#slide=id.gc84dce302c_2_50 presentation] 
* Cameron and Steven helped with PACE
** Three people have been set up in PACE (Karthik, Nishant, and myself)
** Karthik completed a short run, Steven will have long run over the weekend
** Cameron noted issue in PACE which caused gpu runs to fail
*** Update: Cameron found solution to issue, resolved by adding the following line to the "launchEMADE_amazon.pbs" file
**** export LD_LIBRARY_PATH="/usr/local/pace-apps/manual/packages/cuda/11.1/lib64"

PACE Setup Notes:
* Windows 8.1 does not support native ssh (required installation of git bash)
* Followed Cameron's PACE guide [https://www.youtube.com/watch?v=LashYCCJF3E video]
** Process was very straightforward, however, I still made a few mistakes
*** I transferred the setup and launch files to the incorrect folder, accidentally interrupted transfer of EMADE to pace (resulting in an hour delay)
*** After the above issues were resolved I could launch a run of EMADE but nothing would happen
*** I discussed with Cameron; pip installed the requirements file, re-ran setup.py and was able to successfully launch EMADE
**** EMADE launched but the master/workers were missing the transformers modules (pip installed as well as nltk per Steven and relaunched)

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up PACE-ICE
|complete
|3/29/2021
|4/5/2021
|4/11/2021
|}

=== Week of March 29th ===

==== Group Meeting ====
* Stocks discussed a Monte Carlo implementation for hypothesis testing
* ezCGP working on adding dense layers, max pooling, and dropout layers

=== Week of March 29th ===
Breakout Meeting
* New and old members introduced themselves
* Cameron and Anshul offered to give lectures on the basics of EMADE and NNs during Friday's meeting
* Cameron investigating NNLearner not sending individuals to evaluation and not getting print statements (Zutty offered a few places to look)
* Steven revised code, ran EMADE and hit runtime wall, will be designated PACE helper
* All new members and all old members (less Cameron and Steven) are tasked with setting up PACE

Subgroup Meeting
* Most members had issues with accessing wiki (to follow PACE set up guide)
* Cameron gave EMADE 101 "the basics" [https://docs.google.com/presentation/d/1v33k5I9b-_MIR9f3QhO4U81HJaBRwWqt6xzoSecDsoA/edit#slide=id.p presentation]
** Will give EMADE 102 "the NNLearner" presentation in next breakout
** Will create PACE install guide video
*** Update 4/4: PACE install [https://www.youtube.com/watch?v=LashYCCJF3E guide]
* Anshul to give Neural Nets 101 presentation next availability as well
* Cameron figured out why NNLearners were failing
** Amazon dataset is ~20x larger than toxicity dataset, caused PACE to crash (after tokenizing dataset)
** Will try the following revisions:
*** Reducing size of train dataset (simplest)
*** Using scipy sparse matrices
*** Splitting the dataset into folds
*** Varying the MAXLEN parameter used for tokenization & increasing memory in PACE runs
* Steven using standalone tree evaluator, doing a deep-dive in EMADE (researching source code)

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up PACE-ICE
|complete
|3/29/2021
|4/5/2021
|4/11/2021
|}

=== Week of March 22nd ===

==== Group Meeting ====
* Midterm presentations
** Stocks
*** Discussed purpose of group, what technical indicators are, what technical indicators they are using
*** Deep dive on peculiar result of one individual
*** Future work: more technical indicators, look at multiple time scales, penny stocks, crypto, and NNs
** ezCGP 
*** Explained graph based architecture
*** Discussed flow of process and how it doesn't need transfer learning 
** Modularity 
*** Explained abstraction of ARLs
**** How high performing parts of a tree can become a new primitive
*** Using sphinx for documentation
*** Discussed potential for losing genetic information

==== Subgroup Meeting ====
* Team discussed pivoting based on Dr. Zutty's feedback on midterm presentation
** Team will refocus any efforts that will not help investigation of trivial solutions
*** Any members not focused on investigation will get PACE set up and help with troubleshooting/runs
* Team discussed how best to onboard new members
** All new members will get set up in PACE
* Steven and Cameron troubleshooting trivial results
** Cameron brainstormed areas to investigate to isolate issue
* Sumit wrapped up benchmarking and will get set up in PACE

=== Week of March 15th ===

==== Group Meeting ====
* Reminder of midterm presentations
* Stocks completed an EMADE run (new individuals were mediocre)
* ezCGP tested no-transfer-learning problem file, similar results
* Modularity is focusing on database building

==== Breakout Meeting ====
* Team will prioritize midterm [https://docs.google.com/presentation/d/1bpIN_1nL6PB8fMq1yvEDQnuy_ktcSY87HV2nxNsvmas/edit#slide=id.gc84dce302c_2_50 presentation]
* Cameron and Steven
** Cameron will study which primitives result in individuals with infinity fitness
** Steven will debug PACE instance
* Alex, Anshul, and Sumit
** Alex built NNLearner in EMADE using LSTM model
** Anshul and Sumit will focus on presentation
* I will copy previous term's slide deck as a framework for this term's presentation

==== Subgroup Meeting ====
* Primarily discussed presentation, set up additional meeting on Sunday to have a practice run
* Cameron resolved a few issues and started another EMADE run
* Sumit will add FastText functionality to EMADE

==== Sunday Meeting ====
* Discussed work new members will do
* Flow and organization of presentation was iteratively improved through discussion
* Practice run of presentation, ended at 16:20
* Minor revisions made post practice run

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Make midterm presentation
|complete
|3/15/2021
|3/22/2021
|3/21/2021
|-
|Practice run of presentation
|complete
|3/15/2021
|3/22/2021
|3/21/2021
|}

=== Week of March 8th ===

==== Group Meeting ====
* Stocks has had a back and forth about the purpose of a GA in a paper, will start runs next week
* ezCGP had first run, with 47% categorical accuracy
* Modularity team will add more guides to Sphinx docs and is designing architecture for database storage

==== Breakout Meeting ====
* Cameron and Steven 
** Cameron's run had short timer so only 1 generation ran
*** Anish helped install cuda library to speed up
** Steven is running pass, on gen 40, will stop around gen 100
* Alex, Anshul, and Sumit
** Alex is reviewing other Kaggle models, will try building from scratch
** Anshul is recreating old model from scratch to see if error was specific to notebook
** Sumit is exploring other baseline models
*** Most use fasttext embeddings (not yet implemented in EMADE)
* I will compare PyTorch Lightning with PyTorch methods 

==== Subgroup Meeting ====
* Cameron and Steven
** Cameron had seed issue, resolved and run restarted
** Steven's Amazon run went 6 hours without error (seed issue), will seed and restart 
* Alex, Anshul, and Sumit
** Alex will run EMADE model
** Anshul is building LSTM model from scratch outside of EMADE
** Sumit ran baseline fasttext model, 91% accuracy
* I am comparing the existing PyTorch methods file to PyTorch Lightning

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Comparison of PyTorch lightning with existing work
|incomplete
|3/8/2021
|3/15/2021
|canceled
|}

=== Week of March 1st ===

==== Group Meeting ====
* Reminder about keeping notebooks updated weekly
* Stocks added a slew of new technical indicators
* ezCGP ran several benchmarks for comparison
* Modularity team is making steady progress, no updates

==== Breakout Meeting ====
* Cameron and Steven 
** Both instances of PACE set up
*** Cameron will start an Amazon dataset run
*** Steven ran into a tourney selection issue
* Alex, Anshul, and Sumit
** Alex working on Kaggle Colab notebook
*** May start from scratch and update embeddings file
** Anshul and Sumit are working on Amazon dataset in Colab
*** Colab is disconnecting and reconnecting on model fit process and yielding no error
*** Reached out to Stocks team for guidance
* I opened up the discussion about the PyTorch implementation
** What are the difficulties oh a hybrid implementation? What will it require?

==== Subgroup Meeting ====
* Cameron and Steven
** Cameron had outdated Amazon file, updated, and restarted run
** Steven is running Amazon dataset on PACE but hitting max recursion error
*** Anish guided Steven through issue in-meeting
* Alex, Anshul, and Sumit
** Alex working on running Amazon Dataset
** Colab issue persists
*** Sumit resolved unrelated issue then took video of the disconnection/connection issue
*** Anshul will post to stackoverflow and switch gears to a different baseline model
* Will review PyTorch installation/requirements 
** Team discussed starting with population level implementation
*** Once implemented, will discuss with team about finer levels of implementation

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finalize level of PyTorch Implementation
|complete
|3/1/2021
|3/8/2021
|3/5/2021
|-
|Review PyTorch installation Reqs/Avenues
|complete
|3/1/2021
|3/8/2021
|3/7/2021
|}

==== PyTorch Review ====
* Quick [https://www.simplilearn.com/keras-vs-tensorflow-vs-pytorch-article comparison] of Keras, PyTorch, and Tensorflow
* One of the main reasons to implement PyTorch is its superior [https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/ attention] functionality 
** Example attention [https://www.kaggle.com/mlwhiz/attention-pytorch-and-keras notebooks] for both Keras and Pytorch
* Possibly useful [https://github.com/nerox8664/pytorch2keras tool] to port PyTorch models to Keras
* Short [https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/ guide] to using PyTorch for Keras users
* PyTorch [https://towardsdatascience.com/converting-from-keras-to-pytorch-lightning-be40326d7b7d Lightning] may be a good avenue ([https://github.com/PyTorchLightning/pytorch-lightning github])

=== Week of Feb. 22nd ===

==== Group Meeting ====
* Stocks planning Monte Carlo fold by stock
* ezCGP ran with and without transfer learning to compare results
* Modularity is wrapping up documentation, team is focused on depth and storage of ARLs

==== Breakout Meeting ====
* Cameron and Steven 
** Troubleshooting PACE-ICE instances
*** Cameron will build and maintain de facto yaml file to prevent new members from the same pain
* Alex, Anshul, and Sumit
** Alex working on Kaggle and LSTM notebook
*** Working through issues with embeddings
*** Once resolved, will add LSTM primitive
** Anshul is working on Amazon dataset in Colab
*** Colab is disconnecting and reconnecting on model fit process and yielding no error
** Sumit will focus on Anshul's Colab notebook to help troubleshoot
* I will add remaining primitives to the documentation

==== Subgroup Meeting ====
* Cameron and Steven
** Cameron resolved PACE issues and will run test next
** Steven troubleshooting PACE (had working session in meeting with Anish's guidance)
* Alex, Anshul, and Sumit
** Alex will create documentation on EMADE data types and data pairs in Sphinx
** Anshul is working on Kaggle notebook
*** Colab is disconnecting to runtime, reached out to stocks team for help
** Sumit and Anish are also lending help on Colab notebook
* I am finishing [https://www.notion.so/Natural-Language-Processing-6ab51406b2164470ab0fb16675dbdee6 documentation] today
** Will dive into PyTorch, starting with Anish's PyTorch [https://github.gatech.edu/emade/emade/blob/nn/src/GPFramework/pytorch_methods.py methods] file

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add remaining primitives
|complete
|2/22/2021
|3/1/2021
|3/1/2021
|}

=== Week of Feb. 15th ===

==== Group Meeting ====
* Stocks found and discussed various literature
* ezCGP team set up PACE-ICE but ran into memory issues
* Modularity team set up an extensive documentation [[Modularity|page]]

==== Breakout Meeting ====
* Cameron and Steven 
** Setting up PACE-ICE instances
** MySQL and disk quota roadblocks but working through
** Will reach out to Anuurag, Maxim, or Pulak to resolve
* Alex, Anshul, and Sumit
** Alex to reach out to Zutty about potential documentation work
** Anshul is working on Amazon dataset
** Sumit found more literature
* Anish finished Amazon train/test split dataset, will work on literature method for pre-processing dataset next
* I will add half of the remaining primitives to the documentation

==== Subgroup Meeting ====
* Cameron and Steven
** Completed setting up PACE-ICE instances
** Steven tested instance on toxicity dataset, will run a short pass on Amazon dataset
* Alex, Anshul, and Sumit
** Alex working on prepping CIFAR10 dataset 
*** Hitting a few roadbumps using the chest x-ray script 
*** Will need to reshape and onehotencode
*** Will reach out to Zutty about documentation
** Anshul is working on Kaggle notebook
*** Resolved error where embeddings were not stacking properly
*** Colab is disconnecting to runtime
** Sumit working on Amazon dataset
* Anish has Amazon [https://github.gatech.edu/athite3/amznreviews/tree/master dataset] all ready to go (passed off to Steven)
* I am plugging away on [https://www.notion.so/Natural-Language-Processing-6ab51406b2164470ab0fb16675dbdee6 documentation], will request Zutty join next breakout to discuss refactoring to PyTorch

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add half of remaining primitives
|complete
|2/15/2021
|2/22/2021
|2/21/21
|}

=== Week of Feb. 8th ===

==== Group Meeting ====
* Stocks team had review of literature
* ezCGP will focus on updating framework and adding new primitives
* NLP will focus on NN evolvability and primitive documentation
* Modularity had literature review and is discussing how to expand diversity/sophistication of ARLs

==== Breakout Meeting ====
* Cameron and Steven are setting up PACE-ICE
* Anish is finishing up pre-processing and will dabble with chest x-ray dataset
* Alex, Anshul, and Sumit are collecting literature for baseline of Amazon (have one paper from Kaggle so far)
* I will be adding document level primitives to the [https://www.notion.so/Natural-Language-Processing-6ab51406b2164470ab0fb16675dbdee6 Notion page]

==== Subgroup Meeting ====
* Team is focused on getting their machines set up and finalizing the pre-processing of the Amazon dataset
** Sumit found a few papers that used the Amazon dataset
*** [https://ieeexplore.ieee.org/document/8768887 Linguistically independent sentiment analysis]
*** [https://www.sciencedirect.com/science/article/abs/pii/S0167739X20309195 An Attention-based Bidirectional CNN-RNN Deep Model for sentiment analysis]
** Cameron and Steven are getting PACE set up
** Alex is working on learning how to format data (specifically the Amazon dataset) for use in EMADE and wants to create an example walkthrough document
** Anish is half done with dataset pre-processing
** I'm working on document level primitives

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add document level primitives
|complete
|2/8/2021
|2/15/2021
|2/21/21
|-
|Notebook self eval
|complete
|2/8/2021
|2/15/2021
|2/14/21
|}

==== Notebook [[files/Spring notebook rubric.docx|Self Eval]] ====
* I need to be more consistent (in updating my notebook and weekly tasks) and be more detailed about my work

=== Week of Feb. 1st ===

==== Group Meeting ====
* Stocks aren't going to follow paper(s) so strictly (new metrics)
* ezCGP has two members which will focus on old school CV implementation
* NLP goals still fuzzy (Zutty joining breakout session)
* Modularity focusing on diversity and sophistication of ARLs

==== Breakout Session ====
* Discussed term goals
** NLP should be more focused this term (fewer members and previous term was already spread thin)
** Getting to the bottom of the NAS implementation only finding trivial solutions
*** Troubleshoot balance of datasets
*** Use binary classification (Amazon dataset)
*** Refactor code?

==== Subgroup Meeting ====
* Team will focus exclusively on NLP applications (no CV work)
** Evolvability of NNs is pain point currently (NAS only finding trivial solutions)
*** Resolving issue will have highest return on effort
*** Most of team has assembled into subteams to address this issue
**** Alex, Anshul, and Sumit are establishing a control using Keras on [https://www.kaggle.com/bittlingmayer/amazonreviews Amazon dataset]
**** Cameron and Steven are working on the EMADE implementation on the Amazon dataset
**** Anish is helping with dataset pre-processing: cross validation and balancing 
** I will initially focus on documenting NLP primitives

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create Notion Page
|complete
|2/1/2021
|2/8/2021
|2/8/2021
|-
|Start collecting NLP primitive code from Github
|complete
|2/1/2021
|2/8/2021
|2/8/2021
|}
=== Week of Jan. 25th ===

==== Group Meeting ====
* Discussed which teams everyone wanted to be on
** Stocks, Modularity, NLP, ezCGP
* Volunteered to lead NLP team
* Main problem in NLP last term
** Neural Architecture Search implementation in EMADE was only finding trivial solutions (resulting NNs were simple, didn't evolve)

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Establish Weekly Meeting Time (4PM EST on Fridays)
|complete
|1/25/2021
|2/1/2021
|2/1/2021
|-
|Set up Bluejeans Meetings
|complete
|1/25/2021
|2/1/2021
|2/1/2021
|}

== Fall 2020 ==

===November 30th, 2020===
====Group Meeting====
*Updates from subteams, short meeting to allow for focus on term presentations
*Dr. Zutty extended deadline for notebooks to make it easier to focus on presentations
====Breakout Meeting====
*Member updates
*Pulak discussed strategy for presentation, set up practice run meeting for Wednesday
====Presentations====
*Stocks
**Explored novel method to develop trading labels
**Implemented technical indicator primitives
***SImple moving average, exponential moving average, moving average convergence and divergence, stochastic oscillator, relative strength indicator, william's R%, ease of movement, on-balance volume, bollinger bands, Commodity Channel Indicator, and Money Flow Index
**Paper that the group has been competing with appears to have inconsistent calculations resulting in fluke results
*Neural Net
**My slide is number [https://docs.google.com/presentation/d/1Z8PjzaP_LjhyycpGSeXX7rIHdVYkxHzqg8vaKVEYhdU/edit#slide=id.gaecbff40f4_1_25 17] and covered the documentation work found [https://www.notion.so/nnvip/Computer-Vision-Primitives-1d6225122e0c4d6b9abe57ab005e5cf1 here].
*EzCGP
**Ported primitives to be compatible with tensorflow pipeline
**Implemented genome seeding to allow for stopping runs and starting where it left off by pickling individuals from the last generation
**Researched super convergence using cyclical learning rates
*Modularity
**ARLs are selected using differential fitness as a heuristic
***Individual's fitness difference from parent's fitness
***Only ARLs that result in positive differentials will be selected
**Second run had bloat issues with some individuals having over 40 ARLs
**AUROC of both control and ARL trails trend down over generations
====Action Items====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Make slide for term presentation
|complete
|11/30/20
|12/2/20
|12/2/20
|-
|Polish CV Primitive documentation
|complete
|11/30/20
|12/2/20
|12/2/20
|-
|Finish updates to notebook
|complete
|11/30/20
|12/3/20
|12/3/20
|-
|Add documentation on Tushna and Tusheet's new primitives
|complete
|12/2/20
|12/2/20
|12/2/20
|}After adding documentation to the new primitives, the following is my favorite visualization so far (it's a gif so you'll need to click into it), [https://en.wikipedia.org/wiki/Otsu%27s_method#/media/files/Otsu's_Method_Visualization.gif Otsu's Binarization]. After documenting the new primitives, I suggested that the 2D version (which leverages local thresholding) be implemented as the 1D version only sets a global threshold. Tusheet agreed and quickly revised his code.[[files/Otsu's Method Visualization.gif|none|thumb|link=https://vip.gatech.edu/wiki/index.php/files/Otsu's_Method_Visualization.gif]]
===November 23rd, 2020===
====Group Notes====
*Dr. Zutty discussed a few items
**Peer evals are open
**Ensure notebooks are of highest quality to achieve the best grade
**Term presentations on the 2nd
====Subteam Notes====
*[https://docs.google.com/document/d/1mOr7D0yCq0v51za8k7MIaVC_EC-6tPOsOEAZgEw9YgM/edit#heading=h.e6knb9tglupl G doc] for review and approval of new primitives
**Adaptive Mean Thresholding
**Adaptive Gaussian thresholding
**Otsu's Binarization
*X-ray baseline run completed, all run results posted [https://github.gatech.edu/pagarwal80/EMADEResults here]
**665 generations
**Accuracy error was 0.46
*Amazon dataset is ready for testing
*PACE has had several issues, mainly a memory error
**deleted data isn't reflected in PACE so allocated memory only goes up, causing runs to stop
**pip cache purge resolved issue
====Action Items====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Peer Evals
|complete
|11/23/20
|11/30/20
|11/30/20
|-
|Add documentation to morphological functions
|complete
|11/23/20
|11/30/20
|11/30/20
|-
|Add documentation to bitwise logical functions
|complete
|11/23/20
|11/30/20
|11/27/20
|-
|Add documentation to image and scalar math functions
|complete
|11/23/20
|11/30/20
|11/25/20
|}The following are probably my two favorite visualizations so far in the [https://docs.google.com/presentation/d/1Z8PjzaP_LjhyycpGSeXX7rIHdVYkxHzqg8vaKVEYhdU/edit#slide=id.gaecbff40f4_1_25 documentation] (top: [http://datahacker.rs/005-image-arithmetic-and-logical-operations-in-opencv-with-python/ bitwise logic], bottom: [https://docs.opencv.org/master/d9/d61/tutorial_py_morphological_ops.html morphological]).[[files/Bitwise.png|none|thumb|link=https://vip.gatech.edu/wiki/index.php/files/Bitwise.png]][[files/Morph intro.png|none|thumb|link=https://vip.gatech.edu/wiki/index.php/files/Morph_intro.png]]
===November 16th, 2020===
I decided to create documentation on CV primitives for my own edification as well as saving time for new members. Talked to Maxim about the platform he used to create his documentation (Notion).
====Group Notes====
*Dr. Zutty discussed when term presentations will happen, likely the 2nd
*He will post midterm grades to Canvas
====Subteam Notes====
*Tushna and Tusheet will add 3 primitives
**Two adaptive thresholding and Otsu's Binarization
*Discussed what runs we should accomplish by the end of term
**10 runs across the x-ray and toxicity datasets
**Use a combination of PACE and ICEHAMMER
**Baseline runs for both datasets are first in queue
*Showed CV Primitives [https://www.notion.so/Computer-Vision-Primitives-6f160347b15c4f3e8c0ccac10b9bc749 Documentation]
**Maxim suggested we team up and create a team documentation page[[files/CV Prim Doc Header.png|none|thumb|link=https://vip.gatech.edu/wiki/index.php/files/CV_Prim_Doc_Header.png]]
====Action Items====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add code snippets to CV Primitives doc
|complete
|11/16/20
|11/23/20
|11/18/20
|-
|Add [https://download.ni.com/evaluation/pxi/Understanding%20FFTs%20and%20Windowing.pdf background] to window functions
|complete
|11/16/20
|11/23/20
|11/18/20
|-
|Add documentation to window primitives
|complete
|11/16/20
|11/23/20
|11/23/20
|-
|Publish doc to [https://www.notion.so/nnvip/Home-3d063fa7b46148cb985eb97a51fa091e team Notion page]
|complete
|11/16/20
|11/23/20
|11/20/20
|}
===November 9th, 2020===
====Group Meeting====
*Updates from subteams
*Request for Dr. Zutty to join NN breakout to discuss new primitive possibilities
====Breakout Meeting====
*Member updates
*Dr. Zutty joined, answered Tushna's questions
**Edge detection primitives are already implemented
**Another direction is necessary for new primitives
====Subteam Meeting====
*Maxim showed his [https://www.notion.so/Configuring-EMADE-on-PACE-60aedf065abc445096617c3cec875a11 documentation] on setting up PACE
*New Amazon dataset being prepped
*[https://arxiv.org/abs/2010.08512 Paper] was discussed
*Bounded box on YOLO doesn't seem to work well
====Action Items====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review Tushna's primitives materials
|complete
|11/9/20
|11/16/20
|11/11/20
|-
|Review implemented primitives in source code
|complete
|11/9/20
|11/16/20
|11/15/20
|-
|Search for literature on primitives
|complete
|11/9/20
|11/16/20
|11/15/20
|}Review of Tushna's g doc and the github codebase revealed that there are numerous (>50) CV primitives already implemented. My search for literature on CV primitives was essentially for foundational knowledge. I found a few great resources.
*Stanford [https://ccrma.stanford.edu/~jos/sasp/Index_this_Document.html Index] of Signal Processing
*Sandia Lab's [https://prod-ng.sandia.gov/techlib-noauth/access-control.cgi/2017/174042.pdf Catalog] of Taper Window Functions
*OpenCV's [https://docs.opencv.org/master/d2/d96/tutorial_py_table_of_contents_imgproc.html documentation]
===November 2nd, 2020===
====Group Meeting====
*Updates from subteams
====Breakout Meeting====
*Finalized new members tasks
*I summarized a survey on neural architecture search ([https://arxiv.org/pdf/1905.01392.pdf NAS])
*Tasked with connecting with CV subsubteam
====Subteam Meeting====
*Member updates
*Anuraag and I further discussed where I could help the CV subsubteam (primitive research and implementation)
====Action Items====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Connect with key CV subsubteam members
|complete
|11/2/20
|11/9/20
|11/6/20
|}
===October 26th, 2020===
====Monday Meeting====
*Assigned bootcamp students to subteams (I was assigned to NLP/NN)
*Updates from subteams
====Subteam Meeting====
*Updates from existing team members
*Pulak asked new members where they would like to focus
**I asked to have the weekend to read a few papers and decide
====Action Items====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Research and Decide Area of Focus
|complete
|10/30/20
|11/2/20
|11/2/20
|}
===October 19th, 2020===
====Lecture====
Presentations from bootcamp groups and existing subteams
*Stocks
**Implementation based on paper that has CEFLANN architecture
**Uses the following technical indicators price, volume, open interest
***Inspired by actual traders
**Intend to design novel technical indicators
*Modularity
**Attempts to modularize common patterns (encodes common node and leaf combinations into a single function)
**These new constructs are called ARLs (adapted representation through learning)
**Selection algorithm biases towards individuals with more ARLs
*NLP/NN
**Applies EMADE to neuroevolution and abstracts towards automation
**Uses tree based representation to build and search neural architecture (NEAT uses graph)
**Run using a dataset from literature using limited functionality performs almost at state of the art levels
*ezCGP
**Represents GP using cartesian space
**This compact representation allows for arbitrary connections between processing layers (block based)
**Amenable to gaussian and other convolutions
====Action Items====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Submit Subteam Rank List
|complete
|10/19/20
|10/26/20
|10/19/20
|}
===October 14th, 2020===
====Lecture====
*Working session for troubleshooting project and asking Dr. Zutty questions
====Action Items====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Group Work and Presentation
|complete
|10/7/20
|10/19/20
|10/18/20
|}
===October 7th, 2020===
====Lecture====
*Working session for troubleshooting project and asking Dr. Zutty questions
====Action Items====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install EMADE, set up MySQL, and run w/o errors
|complete
|10/7/20
|10/14/20
|10/14/20
|-
|Group Work and Presentation
|complete
|10/7/20
|10/19/20
|10/18/20
|}
===September 30th, 2020===
====Lecture====
*Overview of EMADE (Evolutionary Multi-objective Algorithm Design Engine)
**Installation instructions (also on wiki)
***Install [https://downloads.mysql.com/archives/community/ Mysql 5.5]
***Install [https://git-lfs.github.com/ Git LFS]
***Clone EMADE [https://github.gatech.edu/emade directory]
**Operation instructions
***Ensure template has correct SQL settings
***Adjust hyperparameters
***Launch framework with template xml
====Presentation Feedback====
*Use of tournament selection algorithm was incorrect as it only optimizes one objective
*Use NSGAII to optimize both FNR and FPR (note more uniform distribution)[[files/Back to nsga2 with all prims.png|none|thumb|link=https://vip.gatech.edu/wiki/index.php/files/Back_to_nsga2_with_all_prims.png]]Generational results show that NSGAII had a difficult time balancing the minimization of both parameters simultaneously and tended to favor one or the other[[files/Gen data nsga2 more prims.png|none|thumb|link=https://vip.gatech.edu/wiki/index.php/files/Gen_data_nsga2_more_prims.png]]
====Action Items====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Revisit MOGP work (Presentation Feedback)
|complete
|9/30/20
|10/7/20
|10/7/20
|}
===September 23rd, 2020===
====Lecture====
*Presented and watched other teams present Titanic ML and MOGP comparison
====Action Items====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Clone EMADE Git Repo
|complete
|9/23/20
|9/30/20
|9/29/20
|}
===September 16th, 2020===
====Lecture====
*Presentation guidelines for Titanic MOGP project
**Compare performance of ML results from subteam to MOGP population
**Objectives will be FNR and FPR
====MOGP Project====
*Worked on exploring Titanic dataset via MOGP
**Started with NSGA-II, default parameters from lab 2, and trig, sigmoid, and relu primitives
**Reduced mutation rate to 0.1, achieving the following result[[files/Mut p1 prim all.png|none|thumb|link=https://vip.gatech.edu/wiki/index.php/files/Mut_p1_prim_all.png]]
**Changed selection algorithm to tournament and removed all primitives except add, subtract, multiply, and negative
***This significantly decreased the AUC (note the drastic difference in both averages and minimums)[[files/Gen data best auc.png|none|thumb|link=https://vip.gatech.edu/wiki/index.php/files/Gen_data_best_auc.png]]
***Using tourney was a poor decision as the algo was only optimizing for one objective (hence the lopsided distribution)[[files/Best auc.png|none|thumb|link=https://vip.gatech.edu/wiki/index.php/files/Best_auc.png]]
====Action Items====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Titanic MOGP teamwork and presentation
|complete
|9/16/20
|9/23/20
|9/23/20
|}
===September 9th, 2020===
Lecture
*Class was divided into subteams (decided by algo to balance skills)
*Subteams will use [https://www.kaggle.com/c/titanic Titanic dataset from Kaggle] to develop predictive models
*All members of subteams will use the same pre-processed dataset but different models
*Each member will submit prediction to Canvas
'''Subteam Meeting 9/12'''
*Joined GroupMe set up by team lead
**Members: Aryaan Mehra (lead), Xufei Liu, Bernadette Bal, Hannah Nguyen
*Discussed pre-processing steps, which dataset to use (Aryaan's), and using sklearn
*Encouraged everyone to dabble and meet up later
'''Subteam Meeting 9/14'''
*Revisited which features to keep, model performance, verifying assignment goals
*Second dataset (Xufei's) produced higher scores in notebook than the previous
*Features trained on: Pclass, Sex, Age
**Dataset was split into training (2/3) and testing (1/3)
**Age had missing values which were replaced with the mean of the existing values
*Inline results were promising but Kaggle results were ~.05-.06 lower
====Titanic Results====
*I chose the Multi-Layer Perceptron model for prediction
**The team also used: Tree, SVM, Random Forest, KNN, XGBoost
*I cycled through each argument for the [https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html sklearn MLP] model (manual search)
**I found the following arguments performed better than default for this application
***Node Activation = Tanh (default is relu)
***Solver = Stochastic Gradient Descent (default is Adam, sgd optimizer)
****I found it interesting that the plain sgd performed better than the optimized version (though the latter had more consistent results)
***Max Iterations = 300 (default 100)
*Titanic Confusion (Matrix)
**Inline result 83.3%, Kaggle result 77.7%
[[files/Titanic Confusion Matrix JG.png|none|thumb|link=https://vip.gatech.edu/wiki/index.php/files/Titanic_Confusion_Matrix_JG.png]]
*Plot of member's model results
**Note that there are 2 co-dominant solutions (two are only one error away from being co-dominant and the last is two errors away)
[[files/Titanic Pareto v2 jg.png|none|thumb|link=https://vip.gatech.edu/wiki/index.php/files/Titanic_Pareto_v2_jg.png]]
====Notebook Rubric Self Assessment====
*[https://vip.gatech.edu/wiki/images/e/e4/VIP_AAD_notebook_rubric_jg.pdf Self Eval]
====Action Items====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Lecture Notes
|complete
|9/9/20
|9/16/20
|9/15/20
|-
|Titanic Team Meetings, Discussion
|complete
|9/9/20
|9/16/20
|9/14/20
|-
|Titanic ML Work
|complete
|9/9/20
|9/16/20
|9/13/20
|-
|Notebook Self Assessment
|incomplete
|9/9/20
|9/16/20
|9/15/20
|}
===September 2nd, 2020===
====Lecture====
*Multi-Objective GP Concept
**Definitions
***Gene pool: set of genome to be evaluated in current generation
***Genome: "DNA" of individual (set of values for GA, tree structure for GP)
***Search Space: set of all possible genome
***True Positive: correctly identified true outcome
***False Positive: incorrectly identified true outcome
***Objective Space: set of objectives
***Evaluation: maps genome from location in search space to location in objective space
**Classifier Metrics
***Confusion Matrix: built using Actual Positive, Actual Negative, True Positive, True Negative, False Positive, and False Negative compose a
**Maximization Metrics
***Sensitivity (True Positive Rate, TPR): true positives divided by total positives
***Specificity (True Negative Rate, TNR): true negatives divided by total negatives
**Minimization Metrics
***False Negative Rate, FNR: false negatives divided by total positives
***Fallout(False Positive Rate, FPR): false positives divided by total negatives
**Objective Space
***Individuals are evaluated using above objective functions and mapped in objective space
***An individual is Pareto if no other individual outperforms on all objectives
***Set of Pareto individuals form the Pareto Front
***Pareto Front guides selection
***Nondominated Sorting Genetic Algorithm II (NSGAII) separates population into ranks, then individuals compete in a binary tournaments (lower Pareto rank wins, ties broken by crowding distance)
***Strength Pareto Evolutionary Algorithm 2 (SPEA2) assigns a strength value to each individual (based on number of other individuals it dominates), a rank value (the sum of strengths of individuals that dominate it)
====Lab====
*Multi-Objective GP
**Lab reviews how to optimize a solution with multiple objectives
**To visualize the objective space
***The population is compared individual to individual via the pareto dominance function
***An arbitrary individual is chosen
***The population is sorted by pareto dominance (relative to the chosen individual)
***The results are plotted (blue is the chosen individual, red individuals dominate blue, green are dominated by blue, black are not comparable)[[files/GP (default) objective space.png|none|thumb|link=https://vip.gatech.edu/wiki/index.php/files/GP_(default)_objective_space.png]]
**Below shows the fitness objectives (mean, min) of the best individual by generation
***Note that complexity tends upwards over time (which will hurt our AUC below)[[files/GP best ind.png|none|thumb|link=https://vip.gatech.edu/wiki/index.php/files/GP_best_ind.png]]
***Below shows the pareto front and AUC[[files/GP multi pareto1.png|none|thumb|link=https://vip.gatech.edu/wiki/index.php/files/GP_multi_pareto1.png]]
**By replacing the mutation operator (default of uniform) with [https://github.com/DEAP/deap/blob/master/deap/gp.py shrink], we can combat the increasing complexity
***Best individual performance by generation[[files/GP best ind2.png|none|thumb|link=https://vip.gatech.edu/wiki/index.php/files/GP_best_ind2.png]]
***Pareto front (with an AUC reduction of 53.88%)[[files/GP multi pareto2.png|none|thumb|link=https://vip.gatech.edu/wiki/index.php/files/GP_multi_pareto2.png]]
====Action Items====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Lecture Notes
|complete
|9/2/20
|9/9/20
|9/8/20
|-
|Lab 3
|complete
|9/2/20
|9/9/20
|9/8/20
|-
|Group Self Assessment
|complete
|9/2/20
|9/9/20
|9/2/20
|}
===August 26th, 2020===
====Lecture====
*Genetic Programming Concept
**Evolutionary approach to generating computer programs
***Similar to Genetic Algorithm except that instead using a fitness metric to evaluate an individual, the individual is the evaluation
***Functions are most commonly represented as trees (linear and cartesian are equally valid but not as common)
****Nodes are called primitives and are functions, leaves are called terminals and are parameters
****Crossover occurs by swapping a node and all nodes and/or leaves below with a node (and all nodes/leaves below) from another individual
****Mutation is inserting, deleting, or changing a node but can be difficult
====Lab====
*Symbolic Regression Lab
**added np.sin and np.exp primitives
**compared default mutation operator (uniform) and node replacement operator
**Uniform mutation results
***Fitness by generation[[files/GP ex 1 uniform mut.png|none|thumb|Best individual is sin(negative(x), add(multiply(multiply(x, x), subtract(x, x)), subtract(x, add(x, x)))), (0.0,)|link=https://vip.gatech.edu/wiki/index.php/files/GP_ex_1_uniform_mut.png]]
**Node replacement mutation results
***Fitness by generation[[files/GP ex 1 node rep.png|none|thumb|Best individual is sin(x, sin(x, x)), (0.0,)|link=https://vip.gatech.edu/wiki/index.php/files/GP_ex_1_node_rep.png]]
====Action Items====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Lecture Notes
|complete
|8/26/20
|9/2/20
|9/1/20
|-
|Lab 2
|complete
|8/26/20
|9/2/20
|9/1/20
|}
===August 19th, 2020===
====Lecture====
*Genetic Algorithm Concept
**Initialize population of solutions (individuals), evaluate solutions with fitness metric
**allow most fit solutions to crossover (tourney and roulette most common) and mutate (bit flip for bitstring encoding) to generate offspring (the next generation)
**iterate generations until fitness criteria or generation limit is met
**output most fit solution
*Keywords
**Bitstring Encoding - solution's genome (encoding) represented as string of 1's and 0's (for example 11011100001)
**Crossover - process that reproduces an offspring using two parent solutions
***One Point - to produce offspring: choose arbitrary point in genome of set length, swap subset of Parent A after point with Parent B's subset after point (or vice versa)
***Two Point - to produce offspring: choose two arbitrary points in genome of set length, swap two subsets of Parent A with Parent B's subsets (or vice versa)
**Mutate - process that randomly flips a bit of offspring solution
====Lab====
*OneMax Problem
**Maximizes individual of 100 boolean values using a fitness metric of sum
**Exemplifies how a GA optimizes a population under a simple premise
**Walks through the basics of DEAP
*N Queens Problem
**Minimizes individual of n queens on an nxn board using a fitness metric of (sum of conflicts on the diagonals)
**Reducing the mutation rate from 20% to 5%:
***Increased reliability (7/10 runs found global optima and 9/10 runs found global optima, respectively)
***Decreased average generations to find global optima (38 generations on average and 34 generations on average, respectively)
[[files/NQP plot.png|thumb|none|link=https://vip.gatech.edu/wiki/index.php/files/NQP_plot.png]]
====Action Items====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Join Slack
|complete
|8/19/20
|8/26/20
|8/19/20
|-
|Wiki Notebook Creation
|complete
|8/19/20
|8/26/20
|8/23/20
|-
|Week 1 Lab
|complete
|8/19/20
|8/26/20
|8/25/20
|}