== Team Member ==
[[files/Justin Hinckley Picture.jpg|thumb|79x79px]]
Team Member: Justin Hinckley

Email: jhinckley6@gatech.edu
Cell Phone: 208-515-8082

== Interests: Competitive Programming, Gaming, Running, Swimming ==

= Fall 2021 =

== November 8, 2021 ==

=== Lecture Notes ===
Image Processing:
* Fixed bug in EMADE hyperfeature training
* Going to work on sharpening their hyperfeatures
* Found a good implementation of Lexicase to move forward with
NLP:
* Fixed merge issues
* Getting memory error that needs to be fixed
Modularity:
* Fixing bug inside match_arl algorithm
* Completed fixed extended arl run of EMADE

=== Sub-team Notes ===


=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|November 15, 2021
|November 22, 2021
|In Progress
|-
|Implement solution to NNStatistics tracking
|Completed
|November 15, 2021
|November 22, 2021
|In Progress
|}

== November 8, 2021 ==

=== Lecture Notes ===
Image Processing:
* Attempted to implement a new hyperfeature
* Going to try out lexicase
NLP:
* Presented EMADE and deep learning to new students.
* Working on adding new primitive to output layer.
Modularity:
* Finished run of EMADE with Extended ARLs
* fixed bug in match_arl
* Onboarded new students to google collab

=== Sub-team Notes ===
I ran into problems creating analysis logic in EMADE to track nnlearners across all of its generations.
Problem:
* New hashes are generated for individuals every generation which prevents us from properly keeping track of a single hash.
* We can't guarantee that one child has exactly 1 of 2 parents, since mating and mutation happens potentially multiple times to the same individuals:
```
            print('Mutating offspring (Swap Layer)')
            sys.stdout.flush()
            count = mutate(offspring, _inst.toolbox.mutateSwapLayer, MUTPB, needs_pset=True)
            print('Mutated ' + str(count) + ' individuals')
            sys.stdout.flush()
            
            print('Mutating offspring (Modify Activations)')
            sys.stdout.flush()
            count = mutate(offspring, _inst.toolbox.mutateActivationParam, MUTPB, needs_pset=True)
            print('Mutated ' + str(count) + ' individuals')
            sys.stdout.flush()
            

            print('Mutating offspring (Modify Optimizers)')
            sys.stdout.flush()
            count = mutate(offspring, _inst.toolbox.mutateOptimizerParam, MUTPB, needs_pset=True)
            print('Mutated ' + str(count) + ' individuals')
            sys.stdout.flush()
            
            print('Mutating offspring (Modify Weight Initializers)')
            sys.stdout.flush()
            count = mutate(offspring, _inst.toolbox.mutateWeightInitializerParam, MUTPB, needs_pset=True)
            print('Mutated ' + str(count) + ' individuals')
            sys.stdout.flush()
```
* This means that child X could have been the child of A and B, but B could have been the child of C and D, therefore meaning X's parents are actually all of A, B, C, and D, not just A,B.
* We need to decide how to track these parents in a way that allows us to gather meaningful metrics on complexity.

Fixing problems with primary key duplicate inserts into NNStatisticsTable:
* Fixed the problem by checking if the individual is already in the table, if it is, do not insert and instead update the entry
My Code Changes Below:

                 if (my_str(evaluatedIndividual).__contains__("NNLearner")):
                    cur_nnstatistics = database.selectNN(my_hash_id)
                    if cur_nnstatistics is None:
                        database.insertNN(hash=my_hash_id,
                            age=evaluatedIndividual.age,
                            curr_tree=my_str(evaluatedIndividual),
                            individual=cp.deepcopy(evaluatedIndividual),
                            error_string=data.error_string # TODO: insert error message
                        )
                    else:
                        database.updateNN(
                            row=data,
                            individual=cp.deepcopy(evaluatedIndividual),
                            age=evaluatedIndividual.age,
                            error_string=data.error_string
                        )

Solutions:
* A possible solution to this would be to attach some additional variable to the primitive tree object or instead add an SQL field for parent/parents hash values which allows us to possibly backtrack an individuals growing family shape.

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|November 8, 2021
|November 15, 2021
|November 15, 2021
|-
|Finish NNStatistics tracking
|Completed
|November 8, 2021
|November 15, 2021
|November 15, 2021
|}

== November 1, 2021 ==

=== Lecture Notes ===
First meeting for newly-graduated ex-bootcamp students.

Image Processing:
* Figured out plans for the remainder of the semester
* Plan to look into Lexicase
* Look into hyperparameters
NLP:
* Created Bidirectional Attention layer, modeling layer, and an output layer
* Implemented modeling layer primitive
Modularity:
* Run experiments on extended ARL
* Migrate into Cache V2

=== Sub-team Notes ===
Local runs of EMADE:
* Cameron Bennett created a video tutorial that I used for running EMADE locally on Linux (and Mac).
* Set up local MySQL server
    sudo apt install mysql-server
    sudo mysql -u root
    ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass';
** Had difficulties setting new password for root but found a command that worked
* I tested local EMADE debugging using VSCode and did test run connected to mySQL database
* Viewed run results using MySQL Workbench and set up a profile for local runs.

My Current Task:
* Fix previous errors with primary keys for NNStatistics SQL table
* I designed a way to track previous generations of individuals using hashes
** Each individual will keep track of parent(s) like a linked list in the NNStatistics table.
* Implement Python function that looks at NNStatistics table and given an individual's hash, returns all previous generations

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|November 1, 2021
|November 8, 2021
|November 8, 2021
|-
|Track parents of individuals in EMADE
|Completed
|November 1, 2021
|November 8, 2021
|November 8, 2021
|}

== October 25, 2021 ==

=== Lecture Notes ===
NLP Presentation:
* In progress
Image Processing:
* Focusing on a more specific image processing task: CheXNet, which is chest X-rays
* Baseline results:
** Ran for 30 generations
** Used precision-recall AUC because it was more accurate for their data
** Had individuals with very low AUC, which seems suspicious.
* Looked at 3 selection methods: NSGA-III, Lexicase, and Hypervolume indicators
* Compared results from NSGA-III with NSGA-II which is already in EMADE
* What is different about NSGA-III:
** At the start, defines reference points to try and maintain diversity among its individuals.
* Problems:
** Had Master process die 5-10 minutes in with no error messages.
* Implemented semantic mutation and mating methods such as dp.mutSemantic()
* Had issues with semantic operators in generating valid architectures with non-seeded runs.
* Most generated individuals couldn't generate valid fitness scores
* With Geometric crossover, had similar errors with typing.
* Made "hyper-features" which is the combination of multiple primitives that work well together, they combined grey level with sobel edge detection as one "hyper-feature".
Stocks:
* Goals:
* Demonstrate how EMADE can beat the state of the art using trading algorithms generated.
* Write a paper and publish on findings.
* Use technical indicators to determine whether we should buy/sell a stock at a specific time.
* Updates:
** Running EMADE on PACE
** Constrained compute hours
* Picked certain stocks and try to beat state of the art over the same time periods.
* Objectives:
** Loss percentage objective
** Average loss per transaction
Modularity:
* Uses ARL's to group good primitives together into new primitives
* Allows faster convergence
* Can improve overall fitness
* Data manipulating primitives are the most useful.
* Insights:
** Increased size of ARL's may not necessarily be more useful
** Using the metric as the goal
* Experiment Setup:
** 30 generations
** 5 runs of each comparison

=== Sub-team Notes ===
Presented our midterm presentation (https://docs.google.com/presentation/d/167Jl4jEKVsY1c1Y12fLJIL_RXw7amRiPRUJPDiU1b5c/edit#slide=id.gf74dd2f3e8_0_51) to the other subteams/bootcamp students.
I presented slides 18-20 which displayed the work I accomplished on the team.
Items I presented:
* Creating new SQL Table for NNLearners to track metrics over time
* Talked about new analysis methods that displayed NNLearner Statistics and reduced parentheses.

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|October 25, 2021
|November 1, 2021
|October 25, 2021
|}

== October 18, 2021 ==

=== Lecture Notes ===
Gave status updates from last week and the hackathon.
Discussed how we would break-down our midterm presentation, I was assigned the "New Testing/Analysis Tools" section since my work was mostly in this area.
Slides assigned:
* NNLearner SQL Table
* New Analysis Methods
Link to presentation slides: https://docs.google.com/presentation/d/167Jl4jEKVsY1c1Y12fLJIL_RXw7amRiPRUJPDiU1b5c/edit#slide=id.gf74dd2f3e8_0_64

=== Sub-team Notes ===
My work over the weekend:
* Assigned to work on two tasks this week:
1. Creating function that takes an individual's string representation and removes unnecessary parentheses
* These are generated because we have passthrough layers that do not modify input but created nested parentehses.
* Allows us to more easily view individuals' structure
2. Create function that substitutes ADF's with the actual representation of the ADF.

Completed the function over the weekend and gave to Cameron Bennett to use for the individual representations in our midterm presentation.

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|October 18, 2021
|October 25, 2021
|October 25, 2021
|-
|Create function to remove unneeded parentheses
|Completed
|October 18, 2021
|October 25, 2021
|October 24, 2021
|}

== October 11, 2021 ==

=== Lecture Notes ===
No lecture: Fall Break.

=== Sub-team Notes ===
My progress:
* Finished script to pull amazon dataset automatically
* Main commit here on my personal fork: (https://github.gatech.edu/jhinckley6/emade/commit/bf5dbccdbc65f206e03d90ec74a47c92ae0bf337)
* Problems encountered:
** Getting data from Kaggle: followed Pokemon example by Dr. Zutty
** Kaggle data being bz2'd instead of uncompressed: used module bz2 and method bz2.BZ2File
** Had to transform the data from Kaggle to match out datasets, need the format to be "review",(0|1) - 0 if __label__1 and 1 if __label__2
** test.ft.txt was only 400,000 lines, small enough to put in memory, but train.ft.txt is 3,600,000 lines, too large to easily put into memory.
*** We only need 5% (180,000) of the train.ft.txt, but we need a representative sample, so we must use `sklearn.model_selection.StratifiedShuffleSplit`.
*** To get a representative sample, it seemed like I needed all the data, but what I found is that I only need to provide the labels (0 or 1), which can be stored into memory easily.
*** Using the labels I can get the indices of the sample, so then I can read only the 180,000 lines into memory to process and save into a .gz compressed file.
Things to note:
* Originally we had a train_full.csv which had all 3,600,000 lines, but I did not generate this file. If we did need it, instead of putting it all into memory, we could write the file line by line, then afterwards gzip the plaintext file.
* The new stratified sample is different from the original, since it is being randomly generated (although the script has a set seed).

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|October 11, 2021
|October 18, 2021
|In Progress
|}

== October 4, 2021 ==

=== Lecture Notes ===
First meeting with the NAS subteam.
* Gave progress updates
* Got more help on PACE ICE setup
* I helped Lucas with some PACE ICE issues we were working on together

=== Sub-team Notes ===
* Watched Standalone Tree Evaluator video: (https://bluejeans.com/playback/s/pvervaaE157asZmWHMtm0NNq8AxVnDoNZZ8JzvomrLJgH25qF0hm2lka4OzbefGm)
* Went through overview of the nn-vip branch of EMADE: (https://docs.google.com/presentation/d/1v33k5I9b-_MIR9f3QhO4U81HJaBRwWqt6xzoSecDsoA/edit#slide=id.p)
* Worked through PACE issues:
** Determined that my problems with gzipped files were due to not cloning with Git LFS installed.
** Found problems with my input_amazon.xml file where there was not the correct PACE setup.
Meeting (10/8/2021):
* Went over progress reports:
** Cameron Bennett is working on way to debug EMADE locally using VSCode
** Problems with our EMADE branch, we had a merge that broke our nn-vip branch
** Cameron Whaley worked on a solution by creating a new branch nn-vip-rewind but we can't figure out how to make a pull request to fix nn-vip
*** Discussed moving over to Cameron Whaley's personal EMADE fork
*** During subteam meeting Conner Yurkon figured out how to remind our main nn-vip branch, however, we decided to make issues less difficult in the future we can still use Cameron's personal fork.
* I helped Lucas with PACE ICE errors
* I decided to take on the issue of creating a script to download the Amazon dataset so that we can git clone from inside PACE ICE more easily without using SCP.

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|October 4, 2021
|October 11, 2021
|October 8, 2021
|-
|Write script to pull amazon dataset
|Completed
|October 4, 2021
|October 11, 2021
|October 13, 2021
|-
|Watch EMADE overview and standalone tree evaluator videos
|Completed
|October 4, 2021
|October 11, 2021
|October 8, 2021
|}

== September 27, 2021 ==

=== Lecture Notes ===
Dr. Zutty was not present, Dr. Rohling taught the class.
Since Sriram was not there I presented for our subteam about our latest news for literature search.
* Dr. Rohling thought the idea of convolutional neural networks for stock analysis was interesting to look into.
We planned on talking to Dr. Zutty in person but we could not, so we will schedule a time to meet virtually.

=== Sub-team Notes ===
Update on potentially switching subteams:
* Discussed with Dr. Zutty through Slack that our team situation does not have enough direction
* George, Rishit, and I decided we would prefer to switch subteams rather than stay on the Stocks subteam.
* Dr. Zutty responded saying that we could switch subteams and asked for team preferences:
** Rishit chose NLP and reached out to Steven Leone to ask if they have room.
** George reached out to NLP also.
** I reached out to Conner Yurkon from NAS and asked to join them.

=== NAS Notes ===
* Went to weekly NAS subteam meeting/working session
* Heard progress updates
* Looked at Trello board to see current issues (https://trello.com/b/RbwUW89F/nas-brainstorm)
* Got help from Cameron on PACE ICE setup

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|September 27, 2021
|October 4, 2021
|October 4, 2021
|-
|Fix PACE errors and do a test run of EMADE
|Completed
|September 27, 2021
|October 4, 2021
|October 4, 2021
|-
|Read and watch material given by Cameron
|Completed
|September 27, 2021
|October 4, 2021
|October 4, 2021
|}

== September 20, 2021 ==

=== Lecture Notes ===
* From now on weâ€™re trying to check in scripts that generate datasets instead of the datasets themselves.
* MNIST is an example made by Dr. Zutty of this pattern

=== Sub-team Notes ===
Continued our literature review, keeping in mind Dr. Zutty's advice from last week's meeting:
* Well documented
* Code samples
* Suited well for EMADE

The articles we looked into:
* https://www.sciencedirect.com/science/article/pii/S092523120900040X (Prediction-based portfolio optimization model using neural networks)
** Focused on diversifying portfolio
* Ensuring portfolio has low risk rather than focused on short term time series analysis
* https://ieeexplore.ieee.org/abstract/document/1257413/references#references (Support vector machine with adaptive parameters in financial time series forecasting)
** SVM in financial time series forecasting
** Based on the structural risk minimization (SRM) principle which seeks to minimize an upper bound of the generalization error consisting of the sum of the training error and a confidence interval.
** Five real futures contracts collated from the Chicago Mercantile Market are examined in the experiment. They are the Standard & Poor 500 stock index futures (CME-SP), United Sates 30-year government bond (CBOT-US), Unite States 10-year government bond (CBOT-BO), German 10-year government bond (EUREX-BUND), and French government stock index futures (MATIF-CAC40).
* https://www.researchgate.net/publication/324802031_Algorithmic_Financial_Trading_with_Deep_Convolutional_Neural_Networks_Time_Series_to_Image_Conversion_Approach (Algorithmic Financial Trading with Deep Convolutional Neural Networks: Time Series to Image Conversion Approach)
** Used 15 technical indicators with different parameters
** The models generates two dimensional images of the time series for these indicators over a 15 day period
** These images are categorized as buy, sell, and hold
** Seems to outperform the buy and hold strategy for stocks

Our subteam decided we needed to become more knowledgeable about trading in general so we found resources:
* A good resource to learn more about algorithmic trading: https://github.com/stefan-jansen/machine-learning-for-trading


=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|September 20, 2021
|September 27, 2021
|September 27, 2021
|-
|Meet with Dr. Zutty to discuss Stocks team
|Completed
|September 26, 2021
|September 27, 2021
|September 28, 2021
|}

== September 13, 2021 ==

=== Lecture Notes ===
Sub-teams shared out progress and research problems:
*NLP:
** Problems:
*** Unnamed database amazon
* Stocks:
** Using same methodology as a paper and see if emade gives better results
** Reproduce paper and compare to results from last semester
** Need to pick a paper with purpose that has a good reason or good documentation/code samples, go back to drawing board, each look at paper and decide on one
* Image Processing:
** Detection v.s. Classification
** Problems with EMADE: opencv is tricky to get datatypes coerced.

* MOGP:
** Profit by value
** Profit by percentage
* We produce buy/sell signals for stocks
* Graph of primitive utility we made last semester was good
* Find high performing areas that are underexplored and find ways to encourage their use?

=== Sub-team Notes ===
After talking to Dr. Zutty, we realized that if we are doing a literature search, we need to find a paper that will be able to give us concrete results:
* Things to look for:
** Code samples to reproduce with
** Well documented
** Specific, unique ideas that we can incorporate into EMADE

Weekly Sub-team meetings are now at Thursdays at 6 PM.

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|September 13, 2021
|September 20, 2021
|September 20, 2021
|-
|Find and select a research paper
|Completed
|September 13, 2021
|September 20, 2021
|September 19, 2021
|-
|Setup EMADE on PACE ICE
|Completed
|September 13, 2021
|September 20, 2021
|October 5, 2021
|}

== Self-Evaluation 9/13/2021 ==
* Total out of 100: 98
** (8/10 on updated at least weekly)

== September 6, 2021 ==

=== Lecture Notes ===
No lecture meeting due to Labor Day, only sub-team work.

=== Sub-team Notes ===
* Looked more into the approach from the research paper we found in the previous week (Algorithmic Financial Trading with Deep Convolutional Neural Networks: Time Series to Image Conversion Approach)
** Used 15 technical indicators with different parameters
** The models generates two dimensional images of the time series for these indicators over a 15 day period
** These images are categorized as buy, sell, and hold
** Seems to outperform the buy and hold strategy for stocks
* Updated brainstorming sheet: https://docs.google.com/document/d/1QUhsR4KMv2y8PF5D4O3i9qqwCKDYRBNLA5JrVKLk32M/


Note: The three time conflict stock sub-team members will be working separately (working on a research paper among themselves), while the main group will work together on other goals.

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|September 6, 2021
|September 13, 2021
|September 13, 2021
|-
|Find other research papers with unique ideas
|Completed
|September 6, 2021
|September 13, 2021
|September 10, 2021
|}

== August 30, 2021 ==

=== Lecture Notes ===
Submitted my personal team preference rankings on canvas:
Stock Portfolio Optimization: 1
Interpretability: 2
Modularity: 3
EZCGP: 4

* Everyone submitted sub-team rankings and we looked at results to decide which sub-teams we will move forward with this semester.
* Top picks:
** Stocks!
** Neural Architecture Search
** Modularity
* Dr. Zutty noted that likely only people who picked stocks as #1 have a chance of getting it due to high demand.

=== Sub-team Notes ===
* Created a Google Doc for brainstorming session on 8/26/21: https://docs.google.com/document/d/1QUhsR4KMv2y8PF5D4O3i9qqwCKDYRBNLA5JrVKLk32M/
* Overview of brainstorming session:
** Found a research paper that might be worthwhile to look further into and potentially make into a literature search:
*** https://www.researchgate.net/publication/324802031_Algorithmic_Financial_Trading_with_Deep_Convolutional_Neural_Networks_Time_Series_to_Image_Conversion_Approach
** Problems from the Spring:
*** Getting more technical indicators
*** Finding research paper ideas that are highly applicable to EMADE's strengths
*** Analyze which technical indicators are most useful

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|August 30, 2021
|September 6, 2021
|September 13, 2021
|-
|Brainstorm more ideas for what to work on this semester
|Completed
|August 30, 2021
|September 6, 2021
|September 5, 2021
|}

== August 23, 2021 ==

=== Lecture Notes ===
First Week Notes:
* Discussed ideas for teams this semester
** Some new teams proposed:
*** Interpretability
*** Infrastructure
*** Pipeline automation
*** Image Processing
*** COVID applications
** Some old teams had support to keep:
*** Stock Portfolio Optimization
*** Neural Architecture Search
*** ezCGP
*** Modularity
*** Natural Language Processing

After voting on subteams to gather interest, I was interested in:
* Interpretability
* ezCGP
* Stocks
* NLP

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|August 23, 2021
|August 30, 2021
|September 13, 2021
|-
|Decide on top team choices
|Completed
|August 23, 2021
|August 30, 2021
|August 30, 2021
|}

= Spring 2021 =

== April 26, 2021 ==
'''Subteam Notes:'''
* Reviewed our final presentation slides and noted any changes I should make by our meeting on Thursday.
* I was given feedback to clean up the look of my results table and write speaker notes by Thursday.
'''Individual Notes:'''
* I finished my slides for the final presentation (slides 24-25) https://docs.google.com/presentation/d/1eMU46VktpHKwrQK5wQQ_oSM8ZK6Zzxky1rn5YFm27iw/edit#slide=id.gd062ee5d5a_6_0
** Added speaker notes for slide 24.
** Added speaker notes for slide 25.
** Replaced final fitness image with a Google Slides table because it looks cleaner.
* I practiced speaking over my slides and timing myself.
* I reported my estimated time I will take to talk to the group so we can plan
'''Final Presentation Notes'''
* Stocks:
** Midterm objectives: Implement TA-Lib indicators that they talked about at midterm presentation, test on different datasets/stocks, statistical analysis of individuals, and finding objective functions and evolutionary parameters.
** Overview of paper: talked about Piecewise Linear Representation, which is where a price graph is segemented into linear piecewise segments, I'm assuming this makes it easier to work with.
** Exponential smoothing is where bounds are produced and they decide whether to buy or sell when the price crosses the bounds.
** Implemented technical indicators like VWMA, VWAP, and FIBRET which are not in TA-lib.
** Ran 2 long runs on EMADE with 328 generations and 3 objectives: average profit per transaction, variance profit per transaction, normal CDF on distribution.
** Using a new metric called buy/sell lag which is the difference between a local extrema and the buy/sell point
** I'm wondering if this is a good metric since there could be a very small local extrema? In which case maybe you shouldn't have bought/sold there and waited for a larger one?
* NLP:
** Uses NNLearners which take in a list of layers and fits a model to it, creating a tree based neural net architecture.
** Last semester had problems with using unbalanced datasets
** A big problem last semester was getting trivial solutions which are simple networks, which almost just guessed a label.
** They solved this by using simpler datasets and trying to determine if they can give a seeded network and have EMADE improve it.
** Started using PACE to do runs of EMADE since it's more reliable even if it's hard to get a spot in the queue.
** Worked on foundational things like setting up a .yml conda file that is up to date, fixing errors, etc.
** Lots of MySQL problems that were solved.
** Moved primitive documentation to Notion, over 50 primitives now covered.
** Used the Amazon Product Reviews dataset, which is classiying as positive or negative sentiment, only used a small part for training since it was so big, is this a limit with EMADE or the NLP code specifically?
** Had good results on the Amazon dataset, with 22 generations and seeded individuals, they had a best individual with accuracy of 93% which is better than the benchmark/seeded model.
** Next semester want to look at CV again but without multilabel datasets and get EMADe to work well with unseeded runs which haven't been successful so far.
* Modularity:
** Trying to abstract out sections of individuals and reusuing them as primitives
** Relies on ARLs (adaptive representation through learning).
** ARLs are a function that is dynamically envolved and can be used by other functions.
** Improved dealing with large deptch of ARLs
** Seed runs by selecting 30 individuals from exploratory runs as seeded individuals (isn't this the same as just running EMADE twice?)
** Found "Super Individuals" which seemed to outperform any other model so they didn't change very much.
** In the future want to create new models such as a CNN architecture with decreasing learning rate.
** Want to improve complexity of ARLs using improved selection methods.
** Explore other datasets for ARL training to see which ARLs should be stored in the database.
** Run on more image datasets/multi-class classification datasets.
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|April 26, 2021
|May 1, 2021
|May 1, 2021
|-
|Finish slides 24-25 in final presentation
|Completed
|April 26, 2021
|April 29, 2021
|April 28, 2021
|-
|Write speaker notes for slides 24-25
|Completed
|April 26, 2021
|April 29, 2021
|April 29, 2021
|-
|Clean up slides and remake fitness table
|Completed
|April 26, 2021
|April 29, 2021
|April 28, 2021
|-
|Transfer notes on other subteams' presentations from Google Docs
|Completed
|April 26, 2021
|May 1, 2021
|May 1, 2021
|}

== April 19, 2021 ==
'''Subteam Notes:'''
* Planned out our final presentation
* Wrote out a final presentation outline for members to fill in
* Assigned the Point Mutation function discussed in the research paper to implement
'''Individual Notes:'''
Crossover Research Paper: https://link.springer.com/content/pdf/10.1007%2F978-3-319-77553-1_13.pdf
* My assigned problem for next week: Point Mutation function
* My commit that implements point mutation: https://github.com/ezCGP/ezCGP/commit/e3ecaca1296e4e4600dea4ef5ebee6a079b07a21
* I take a mutation percent which is the percent of the parent's genes that will be mutated to create the child genotype.
<code>
mutation_percent = 0.15

choices = [(node_ind, input_ind) for node_ind in range(block_def.main_count + block_def.output_count) for input_ind in (range(len(mutant_material[node_ind]["inputs"]) + 1) if node_ind < block_def.main_count else [-1])]

num_mutated = np.int(np.ceil(mutation_percent * len(choices)))

selected_gene_inds = sorted(rnd.choice(np.arange(len(choices)), size=num_mutated, replace=False))</code>
* <code>choices</code> contains every possible gene we could pick to mutate, and indices of choices are chosen to mutate in selected_gene_inds.
* I then apply different mutation functions depending on whether the gene is a function or an input (I reused previous mutation functionality for this part)
* Merged my feature/SymbolicRegression-JUSTIN (https://github.com/ezCGP/ezCGP/tree/feature/SymbolicRegression-JUSTIN) branch into feature/Issue171-Mutate_PointMutation to test Point Mutation on the Koza-3 symbolic regression problem
* Found that with a mutation_percent of 0.05, our final generation's individuals would have almost identical fitnesses.
* With a mutation_percent of 0.15 we keep our genetic diversity and individuals are different at the end, however this should be looked into in the future to see if there is a deeper problem.
* Pushed finished branch (https://github.com/ezCGP/ezCGP/tree/feature/Issue171-Mutate_PointMutation) to public and GaTech github
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|April 19, 2021
|April 26, 2021
|April 26, 2021
|-
|Implement Point Mutation Function
|Completed
|April 19, 2021
|April 26, 2021
|April 26, 2021
|-
|Push point mutation function to new branch in ezCGP called feature/Issue171-Mutate_PointMutation
|Completed
|April 19, 2021
|April 26, 2021
|April 26, 2021
|-
|Test different mutation_percent values
|Completed
|April 26, 2021
|April 26, 2021
|April 26, 2021
|-
|Merge Koza-3 problem branch into the point mutation branch
|Completed
|April 19, 2021
|April 26, 2021
|April 26, 2021
|}

== April 12, 2021 ==
'''Subteam Notes:'''
* Finished reading research paper
* Assigned the Koza-3 symbolic regression objective function
'''Individual Notes:'''
Crossover Research Paper: https://link.springer.com/content/pdf/10.1007%2F978-3-319-77553-1_13.pdf
* My assigned problem for next week: Koza-3
* My finished commit: https://github.com/ezCGP/ezCGP/commit/a570e534e13580be44ccc317e6b34ab45615b84c
* My probability distribution: <code>x = np.random.uniform(-1, 1, 20) # U[-1, 1, 20]</code>
* My Objective Function: <code>output = np.power(data, 6) - 2 * np.power(data, 4) + np.power(data, 2) # x^6 - 2x^4 + x^2</code>
* Error Function: <code>error = np.sum(np.abs(np.subtract(actual, predicted)))</code>
* Ran the problem file and found bug in template file that was causing individuals to gradually become nan
* Tried to remove natural log operators to prevent ln(abs(0)) from occurring and creating bad individuals by removing those two functions from operator_dict in <code>/codes/block_definitions/operators/block_operators.py</code>:
<code>del globals()[alias].operator_dict[globals()[alias].ln_f2f]</code>

<code>del globals()[alias].operator_dict[globals()[alias].ln_a2a]</code>
* However, this change did not work and we were still getting nan fitnesses until Mr. Rodd pushed a fix.
* Pushed finished branch (https://github.com/ezCGP/ezCGP/tree/feature/SymbolicRegression-JUSTIN) to public and Gatech github
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|April 12, 2021
|April 19, 2021
|April 15, 2021
|-
|Create new branch in ezCGP called feature/SymbolicRegression-JUSTIN
|Completed
|April 15, 2021
|April 19, 2021
|April 15, 2021
|-
|Implement Koza-3 Objective Function
|Completed
|April 15, 2021
|April 19, 2021
|April 15, 2021
|-
|Debug NAN fitness values
|Completed
|April 15, 2021
|April 19, 2021
|April 15, 2021
|-
|Push new branch to Github
|Completed
|April 15, 2021
|April 19, 2021
|April 15, 2021
|}

== April 5, 2021 ==
'''Subteam Notes (Statistics Presentation):'''
* To know if our changes to EMADE are beneficial or if better results are from randomness:
** We can do A/B testing
** We can run EMADE multiple times and calculate statistics from this sample
** Mean, Standard Deviation, Variance
** We can correct for bias by using a sampled mean when finding variance
* Hypothesis Testing:
** Sampling from a normal distribution
** We want to find the probability of seeing a sample at least as extreme as ours given the assumption of the truth
** We choose a threshold alpha value such as .05
** By performing a t-test we can find the t-statistic which tells us how far off we are from the mean by units of the SD
** From the t-statistic we can use a char to find the p-value
** We use 2-tailed when we want to see if two distributions are not equal, which requires a higher t-statistic
** To lookup the p-value we choose a confidence we want and a degree of freedom which is n-1
** P-value in scipy: <code>scipy.stats.t.sf(np.abs(-0.0959), 9)*2</code>
** If we don't know the truth however, we can use Welch's t-test, which assumes we know very little
** We can take a baseline sample, then make one change, then take a new sample and test whether we can be confident our change improved our results
'''Individual Notes:'''
Crossover Research Paper: https://link.springer.com/content/pdf/10.1007%2F978-3-319-77553-1_13.pdf
* Thursday ezCPG Meeting:
** We will test our crossover functions on symbolic regression problems defined in the research paper
** All problems used the following functions:
*** +, -, sin, cos, ln(abs()), e^n
** Primary criteria was best average fitness over 100 runs.
** Arithmetic crossover performs very well when used for symbolic regression
** Symb. reg. Problems used:
*** Koza-3
*** Nguyen-4
*** Nguyen-7
*** Pagie-1
** My assigned problem for next week: Koza-3
* I read through the research paper on my own and made sure I understood their experiements.
* Wrote the probability distribution of Koza-3 in numpy: <code>x = np.random.uniform(-1, 1, 20) # U[-1, 1, 20]</code>
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|April 5, 2021
|April 12, 2021
|April 6, 2021
|-
|Read research paper in preparation for Thursday meeting
|Completed
|April 5, 2021
|April 8, 2021
|April 8, 2021
|-
|Reread paper and make sure I understand the Koza-3 symbolic regression problem
|Completed
|April 8, 2021
|April 15, 2021
|April 13, 2021
|-
|Create Koza-3 objective function in numpy using the uniform distribution U[-1, 1, 20]
|Completed
|April 8, 2021
|April 15, 2021
|April 14, 2021
|}

== March 29, 2021 ==
'''Subteam Notes:'''
* CGP (Cartesian Genetic Programming)
* Uses a DAG (Directed Acyclic Graph) to represent individual trees
* Features:
** Reusable nodes
** Fixed length
** Inactive nodes (representing unexpressed genes)
* We identify active nodes by starting at the output and working backwards
* Typically we have no mating, using 1+4 evolutionary strategy we have 4 mutant offspring from one parent
'''Individual Notes:'''
* I set up personal ssh keys for both the gatech and normal githubs on my local machine.
* I set up ssh keys for both the gatech and normal githubs for my Pace ICE user.
* I set up local repo and Pace ICE repo for ezCGP
* I successfully ran a sample .pbs script on Pace ICE.
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|March 29, 2021
|April 5, 2021
|April 5, 2021
|-
|Set up local ezCGP repository
|Completed
|March 29, 2021
|April 5, 2021
|April 4, 2021
|-
|Set up local Anaconda environment for ezCGP
|Completed
|March 29, 2021
|April 5, 2021
|April 4, 2021
|-
|ssh into Pace ICE and set up repository
|Completed
|March 29, 2021
|April 5, 2021
|April 5, 2021
|}

== March 22, 2021 (Presentation Day) ==
'''Bootcamp Lecture Notes:'''

Notes on Subteams:
* '''Stocks''':
** Used data source of AlphaVantage after trying alternatives that weren't as successful.
** Use technical indicators as inputs to EMADE such as moving averages, relative strength index, and bias.
*** Are looking into and planning to incorporate pre-made technical indicator libraries for this semester to improve algorithm.
**Use volume data based indicators such as:
***On balance volume
***Chaikin money flow
***Flinger volume oscillator
***Volume price trend
**Problems:
***Most recent non-seeded run of EMADE was very inconsistent, performed great on AUO and AAPL but badly on UMC/VZ.
***I believe their seeded runs are much better but they don't want to have to seed?
**Future Plans:
***Use TI Library
***Look at different assets such as crypto and smaller stocks
***Different time granularities
**My Thoughts:
***The subteam seems to have a really solid direction and knows what they need to do to improve their results.
***I'm not as into stocks as some people are but I am interested in them so this team could be cool.
***Probably will be my first choice as I can see myself being able to contribute by researching current literature and implementing indicators.
*'''ezCPG''':
**Was difficult to understand as someone with less ML knowledge but it seems very interesting.
**My understanding is they use a different approach compared to EMADE (EMADE is DEAP + custom primitives + custom functions).
*'''Modularity:'''
**Explore ways to abstract good parts of individuals into primitives themselves.
**Based on Adaptive Representation through Learning.
**Problems:
***Time/Space complexity, I believe they can only use master process, were discussing ideas like dynamic programming which I'm interested in.
***Didn't seem to have a super clear direction for the rest of the semester.
**My Thoughts:
***Wasn't super interesting to me but it may be an option.
*'''NLP''':
**Progress:
***Fixed environment errors.
***Fixed differences between branches.
**Used Amazon product review dataset to analyze whether a review is positive or negative based on the wording.
**Had a recent non-seeded run which went badly, they think they need to seed NNLearners to fix results.
**Thoughts:
***NLP is really interesting to me, although it seems they have a lot of repetitive work/debugging to do this semester to get valid individuals.
'''Individual Notes:'''
* For presentation I completed the slides relating to EMADE Results and Analysis.
* '''ML/MOGP/EMADE Presentation''': https://docs.google.com/presentation/d/19QUX9vIrU05EoxIxQWgtGbbqUxWiWol98MyDH_-ldfQ/edit#slide=id.gc921320457_1_16
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|March 22, 2021
|March 29, 2021
|March 23, 2021
|-
|Look Over Notes on Subteams
|In Progress
|March 22, 2021
|March 27, 2021
|
|-
|Decide Ranking of Subteams
|In Progress
|March 22, 2021
|March 29, 2021
|
|}

== March 17, 2021 (Bootcamp Week 9) ==
'''Bootcamp Lecture Notes (work day):'''
* Our team asked Dr. Zutty how we are supposed to analyze our results and learned that we should analyze the same way we analyzed our ML and MOGP approaches
* We need to get our data out of our MySQL database using SQL queries, either by exporting to a spreadsheet, or using python.
'''Individual Notes:'''
* My Contributions to EMADE run:
** Edited the input_titanic.xml file to comment out the NumElements objective so that we only optimize on FP and FN.
** Changed titanic_data_splitter.py to use our data preprocessing from ML and MOGP assignments.
** Ran another run of EMADE with updated settings as master for about 3 hours until generation 26.
** After run, our group decided we wanted to use FPR and FNR instead of raw FP and FN, so I wrote Python code to estimate the FNR and FPR:
 train_data = pd.read_csv('train.csv')
 total_survived = train_data['Survived'].sum()
 total_not_survived = len(train_data) - total_survived
 percent_survived = total_survived * 1.0 / len(train_data)
 percent_not_survived = total_not_survived * 1.0 / len(train_data)
 # about 178 rows in Monte Carlo test splits
 mc_survived = 178 * percent_survived
 mc_not = 178 * percent_not_survived
 train_data['FullDataSet False Positives'] = train_data['FullDataSet False Positives'].map(lambda x: x/mc_not)
 train_data['FullDataSet False Negatives'] = train_data['FullDataSet False Negatives'].map(lambda x: x/mc_survived)
[[files/Estimated FPR, FNR Group 3 Spring 2021.png|none|thumb|Plot of estimated FPR VS FNR for EMADE Titanic run.]]
*My Contributions to EMADE group presentation:
**I completed the slides relating to our EMADE results.
**I completed the slides relating to the analysis of our EMADE results:
***AUC, progression of Pareto Frontier, list of pareto individuals, design choices.
**Added snippets of code to slides that reference code our group made ourselves that are of interest.
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|March 3, 2021
|March 10, 2021
|March 10, 2021
|-
|Create EMADE section of Group Presentation for EMADE
|Completed
|March 3, 2021
|March 22, 2021
|March 21, 2021
|}

== March 10, 2021 (Bootcamp Week 8) ==
'''Bootcamp Lecture Notes (work day):'''
* Worked with Dr. Zutty to debug dependency issues:
** Problem was we needed to downgrade deap to a lower version which I did using
 pip install deap=1.2.2
* Made sure everyone in our group could connect to my mySQL server and join EMADE as workers.
'''Individual Notes:'''
* Installed MySQL Workbench.
* I set up our mySQL server by changing my   <code>/etc/mysql/mysql.conf.d/mysqld.cnf</code> to make bind-address 0.0.0.0 instead of localhost to allow all remote connections.
*Gave all permissions to user I created by using a wildcard in the GRANT command.
* Encountered issues with the first tournament selection in EMADE which were resolved in lecture as shown above.
* I was the master process for our first EMADE run which I ran with:
 python src/GPFramework/launchGTMOEP.py templates/input_titanic.xml
* Ran for about 2 hours with no errors, getting above generation 20 before stopping it.
* Decided to wait until March 17 lecture to see how we should analyze our data.
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|March 10, 2021
|March 17, 2021
|March 17, 2021
|-
|Run EMADE on Titanic ML Dataset
|Completed
|March 3, 2021
|March 19, 2021
|March 17, 2021
|-
|Analyze Data from Run
|Completed
|March 10, 2021
|March 22, 2021
|March 19, 2021
|-
|Create Group Presentation for EMADE
|Completed
|March 3, 2021
|March 22, 2021
|March 21, 2021
|}

== March 3, 2021 (Bootcamp Week 7) ==
'''Bootcamp Lecture Notes:'''
* Second half of the groups presented their MOGP/ML findings slideshows.
* Assigned to over the next two weeks generate results for the Titanic ML problem using EMADE with our groups.
'''Individual Notes:'''
* Our group met and I set up a MySQL server and created a new user that has all permissions for other group members to join wth.
* I tried to run EMADE as master, and there were no obvious problems but results were not being generated.
* We will find the solution by March 17th.
* Found solution: I downgraded my deap version using pip.
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|March 3, 2021
|March 10, 2021
|March 10, 2021
|-
|Run EMADE on Titanic ML Dataset
|Completed
|March 3, 2021
|March 19, 2021
|March 17, 2021
|-
|Create Group Presentation for EMADE
|Completed
|March 3, 2021
|March 22, 2021
|March 21, 2021
|}

== February 24, 2021 (Bootcamp Week 6) ==
'''Bootcamp Lecture Notes:'''
* Half of the groups presented their MOGP/ML findings from their slideshows.
* Tasks for next week are to set up EMADE based on the Github instructions.
'''Individual Notes:'''
* Set up EMADE following the instructions on the Github.
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|February 24, 2021
|March 3, 2021
|March 3, 2021
|-
|Set up EMADE
|Completed
|February 24, 2021
|March 3, 2021
|March 3, 2021
|}

== February 17, 2021 (Bootcamp Week 5) ==
'''Bootcamp Lecture Notes:'''
* Using the same Titanic ML problem from last week, our goal is to use Multiple Objective Genetic Programming to solve the problem.
* We cannot use Scikit-learn's premade evolution algorithms, we must implement our own.
* We are optimizing on multiple objectives: false positives and false negatives.
* Will present a slideshow of our results on February 24.
'''Individual Notes:'''
* I met with group to work on our MOGP model implementation.
* I volunteered to share screen to work on the problem since VS Code's pair programming extension had issues.
* I completed slide 6 on MOGP v.s. standard ML, comparing the two and their advantages.
Presentation Slideshow:
https://docs.google.com/presentation/d/12ilB-DrMhybc_PkDkcw4kFwxNHvV6VWTLaEThos2wPQ/edit#slide=id.p
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|February 17, 2021
|February 24, 2021
|February 24, 2021
|-
|Work with group to develop MOGP model
|Completed
|February 17, 2021
|February 24, 2021
|February 24, 2021
|-
|Make Presentation Slideshow
|Completed
|February 17, 2021
|February 24, 2021
|February 24, 2021
|}

== February 10, 2021 (Bootcamp Week 4) ==
'''Bootcamp Lecture Notes:'''
* Kaggle Titanic problem is an introductory ML problem of predicting survivors on the Titanic.
* We will use Scikit-learn's ML models to achieve a Pareto optimal set of solutions based on:
** False negative rate.
** False positive rate.
* Choose which rows of data to not include in our model.
* Divide data set into 80% test and 20% to score.
'''Individual Notes:'''
* Models I tried to minimize false negatives:
** svm.SVC(kernel='linear'): 34/295 False Negatives
** neural_network.MLPClassifier: 29/295 False Negatives
** neighbors.KNeighborsClassifier: 45/295 False Negatives
** tree.DecisionTreeClassifier: 28/295 False Negatives
** GradientBoostingClassifier 26/295 False Negatives
* Team 3:
** Members:
*** Justin
*** Conner
*** Tian
*** Coleman
*** Monil
*** Temi
** Pareto Optimal Set
*** Justin - GradientBoostingClassifier FP=29, FN=26
*** Conner - Random Forest FP=25, FN=27
*** Tian - KNN FP=6, FN=90
*** Coleman - Decision Tree FP=9, FN=45
*** Monil - Decision Tree FP=20, FN=33
*** Temi - AdaBoost Fp=31, FN=25
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook
|Completed
|February 10, 2021
|February 17, 2021
|February 17, 2021
|-
|Work with group on Titanic ML Problem
|Completed
|February 10, 2021
|February 17, 2021
|February 17, 2021
|}

== Self Assessment (as of 2/7/2021): ==
[[files/Justin Hinckley - VIP AAD notebook rubric pdf.pdf|none|thumb|Self Assessment as of 2/7/2021]]

== February 3, 2021 (Bootcamp Week 3) ==
'''Bootcamp Lecture Notes:'''
* '''Multiple Objectives:'''
* The idea of evaluating individuals based on multiple objectives, allowing us to produce a population of solutions that all have their benefits.
* Pareto Optimal: when an individual is not outperformed in every dimension by any other individual.
* Pareto Frontier: set of Pareto individuals.
* We use Pareto dominance to favor Pareto dominant individuals in mating and selection.
* Search space: set of all possible genomes/individuals
* Objective space: set of objectives, called the phenotype
* Classification:
** Confusion Matrix:
{| class="wikitable"
!
!Predicted Positive
!Predicted Negative
|-
|'''Actual Positive'''
|True Positive (TP)
|False Negative (FN) (type II error)
|-
|'''Actual Negative'''
|False Positive (FP) (type I error)
|True negative (TN)
|}
* Objectives:
** False Negative Rate (FNR):
*** FNR = FN / P = FN / (TP + FN) = 1 - TPR
** False Positive Rate (FPR) (Fallout):
*** FPR = FP / N = TN / (FP + TN) = 1 - TNR = 1 - SPC
** Many other metrics...
* Nondominated Sorting Genetic Algorithm II (NSGA II):
** Uses binary tournament selection.
** Population is separated into nondomination/Pareto ranks, where lower frontiers beat higher frontiers.
** Ties are broken by crowding distance, which is the sum of normalized euclidean distances to all points within its front (we want points that aren't near clusters), higher is better.\
* Strength Pareto Evolutionary Algorithm 2 (SPEA2):
** Also uses tournament selection.
** Each individual in population given strength S, which is # of other individuals it dominates.
** Each individual also is given rank R, which is the sum of S's of the individuals that dominate it.
** Pareto individuals would be nondominated so they have R = 0.
** Ties are broken with a kth nearest neighbor, fitness = R + (1 / (nearest + 2))
'''Individual Notes:'''
* '''Lab 2; Part 2:'''
* Goal: We are trying to minimize mean squared error and the size of our parse tree that is approximating f(x) = -x + sin(x^2) + tan(x^3) - cos(x).
* Primitives:
** addition.
** subtraction.
** multiplication.
** negation.
** sin.
** cos.
** tan.
*We are using DEAP's Mu plus Lambda algorithm for evolution, where mu is the number of individuals to select and lambda is the number of children to produce.

* Graph of Pareto Dominance: Blue is the individual of comparison. Green are being dominated by blue, red are dominating blue.
[[files/Objective space alex mcquilkin.png|none|thumb|Objective space minimizing mean squared error and tree size.]]
* Graph of both objectives over time:
[[files/Lab 2 Part II Graph of Results.png|none|thumb|Graph of results of multi objective GP where blue and green are Mean Squared Error and red and orange are tree size.]]
* Graph of original Pareto Front with area under curve AUC of 2.384.
[[files/Lab 2 pareto optimal marc hoeltge.png|none|thumb|Graph of Pareto Front.]]
* Graph of Pareto Front of my new algorithm:
[[files/Lab 2 Part II MOGP Better AUC.png|none|thumb|AUC of 0.980 with Stochastic Universal Sampling, mutEphemeral MU=75 LAMBDA=100 MUTPB=.3]]
* After trying out many different DEAP methods for selection, crossover, and mutation, and trying many different mutation rates and selection sizes, I achieved an AUC of 0.980.
* I used Stochastic Universal Sampling as my selection method and mutEphemeral for my mutation method.
* I used a MU of 75 and LAMBDA of 100 with a mutation chance of 30%.
* Thoughts:
** Before reaching my final result, I had graphs that had extremely low AUC (around 0.2), but there were only 2 red points and no blue points being graphed.
** I'm not sure why this occurred but it seemed like an abnormal Pareto Frontier so I kept trying.
** Perhaps my algorithm had severe clusters which made only a few data points on the graph?
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Go Over Multiple Objectives Powerpoint
|Completed
|February 3, 2021
|February 10, 2021
|February 7, 2021
|-
|Start Lab 2 (Finish Second Half)
|Completed 
|February 3, 2021
|February 10, 2021
|February 8, 2021
|-
|Complete Self Assessment
|Completed
|February 3, 2021
|February 10, 2021
|February 7, 2021
|-
|Update Notebook
|Completed
|February 3, 2021
|February 10, 2021
|February 8, 2021
|}

== January 27, 2021 (Bootcamp Week 2) ==
'''Bootcamp Lecture Notes:'''
* '''Genetic Programming:'''
* Diversity (we want diversity):
** Genotypic Diversity: How different each individual's genes are to the other individuals.
** Phenotypic Diversity: How different the expression of genes are (how different the results are).
*** Does not always go along with genotypic diversity, you can have diverse genes but very similar results.
*Genetic Programming: Instead of taking an individual and evaluating it with a function to get objective scores, the individual is the function itself.
*We need some way to measure error, our goal is to minimize error of the individual.
*Represented as tree with nodes called primitives and are operators, and leaves called terminals that represent parameters.
*We store the individual as a Lisp Preordered Parse Tree, where operator's are followed by inputs in list format (i.e. [+, *, 3, 4, 1] = 3 * 4 + 1).
*Crossover: Swap subtrees with each other (with strongly typed you can only swap subtrees that return the same datatype).
*Mutation: Either inserting, deleting, or changing subtrees.
*This is the basics of what EMADE does.
*We can approximate sin(x) with Taylor Series, which can be represented by our tree.
**We feed many inputs into our individual and compare the output with the expected output to find error (e.g. sum((f(X) - sin(X)) ^ 2)
'''Individual Notes:'''
* '''Lab 2; Part 1:'''
** Imported relevant libraries.
** Added set of operations (primitives) that we can combine as well as added two new primitives: np.sin and np.tan, both with arity=1.
** Registered a new mutation method: gp.mutNodeReplacement.
** Results:
*** Best individual had fitness of 0.0275 and was (((x * x) + (tan(tan(sin(x))))) * (sin(tan(x)))) + (sin(tan(sin(x)))).
*** Graphs of results with 40 iterations: [[files/Aadlab2pic1.png|none|thumb|Default parameters, with added sin() and tan() methods.]][[files/Aadlab2withcustommutation.png|none|thumb|Default parameters except using gp.mutNodeReplacement as mutation function.]][[files/Aadlab2withoutheightlimit.png|none|thumb|Default except without height limit.]][[files/Aadlab2withpoint35crossoverrate.png|none|thumb|Default except .35 crossover rate.]]
**I learned that we can approximate the function x^4 + x^3 + x^2 + x using genetic programming.
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Go Over Genetic Programming Powerpoint
|Completed
|January 27, 2021
|February 3, 2021
|January 29, 2021
|-
|Start Lab 2 (Finish First Half)
|Completed 
|January 27, 2021
|February 3, 2021
|February 3, 2021
|-
|Update Notebook
|Completed
|January 27, 2021
|February 3, 2021
|February 3, 2021
|}

== January 20, 2021 (Bootcamp Week 1) ==
'''Bootcamp Lecture Notes:'''
* Learned how to create a personal notebook and update it throughout the semester. 
* What is a Genetic Algorithm?: where you try to achieve the best individual with maximum fitness by mating and mutating individuals in a generation to create a new generation which is hopefully better. 
* Selection: where you choose individuals to be parents of next generation, can be done where fitter individuals have a higher chance of passing on their genes (e.g. fitness proportionate or tournament). 
* Crossover: Mating two individuals (e.g. single/double point crossover). 
* Mutation: Randomly modifying genes for the purpose of diversity (e.g. bit flipping). 
* Genetic Algorithm Pseudocode: 
# Initialize population with random genes.
# Calculate initial fitness.
# While we haven't found an optimal/good enough individual:
## Select new parents.
## Crossover parents to create children.
## Randomly mutate children.
## Update fitness value.
'''Individual Notes (Lab 1):'''
* Created a new Python virtual environment for AAD.
* Installed Jupyter Notebook into the virtual environment.
* Used Pip to install DEAP into the virtual environment.

* Used command <code>jupyter notebook</code> to access the ''Bootcamp - Lab 1.ipynb'' notebook file.
'''One Max:'''
* Objective: Find a bit string consisting of all 1's (of length 100 in this case) using the DEAP library.
* Read, ran, and understood each code block one by one.
* After running <code>main()</code> 5 times:
** Achieved global maximum fitness (100 1's) in 36 generations.
** Achieved global maximum fitness (100 1's) in 42 generations (Took more than 40 generations).
** Achieved global maximum fitness (100 1's) in 34 generations.
** Achieved global maximum fitness (100 1's) in 51 generations (Took more than 40 generations).
** Achieved global maximum fitness (100 1's) in 28 generations.
*Due to random variation and probability being involved in mutation, crossover, and initialization, we did not always see maximum fitness achieved within 40 generations.
'''N Queens:'''
* Objective: Find a chessboard configuration of n queens on a nxn board so that no queens are attacking each other (one queen assigned to each column, one queen assigned to each row).
* Read, ran, and understood each code block one by one.
* Modified queen evaluation function from counting every conflict to only counting the number of queens under attack:
 # Count the number of queens under attack
 sum_ = sum([1 if (left_diagonal[i+individual[i]] > 1 or right_diagonal[size-1-i+individual[i]] > 1) else 0 for i in range(size)])
* Implemented and tested my custom mutation function that rotates 3 elements:
 def customMutShuffleIndexes(individual, indpb):
 Â Â Â  size = len(individual)
 Â Â Â  for i in range(size):
 Â Â Â Â Â Â Â  if random.random() < indpb:
 Â Â Â Â Â Â Â Â Â Â Â  swap_indx = random.randint(0, size - 2)
 Â Â Â Â Â Â Â Â Â Â Â  swap_indx2 = random.randint(0, size - 2)
 Â Â Â Â Â Â Â Â Â Â Â  if swap_indx >= i:
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  swap_indx += 1
 Â Â Â Â Â Â Â Â Â Â Â  if swap_indx2 >= i:
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  swap_indx2 += 1
 Â Â Â Â Â Â Â Â Â Â Â  individual[i], individual[swap_indx], individual[swap_indx2] = \
 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  individual[swap_indx], individual[swap_indx2], individual[i]
 Â Â Â  return (individual,)

* After running <code>main()</code>:
** With default functions and parameters it took 34 generations to reach the global minimum of 0.
** Using my custom evaluation function it took 39 generations to reach the global minimum of 0 (and often only got to 2.0 fitness within 100 generations).
** Using both my evaluation and mutation functions, it took 67 generations to reach the global minimum of 0.
* Used Pip to install Matplotlib.
* Plots of results:
[[files/AAD nocustom.png|none|thumb|Using default functions and parameters with population of 300.]]
[[files/AAD customevaluation.png|none|thumb|Using my custom evaluation function and population of 300.]]
[[files/AAD bothcustom.png|none|thumb|Using both my custom evaluation and mutation functions with population of 300.]]
* I learned that it is better to count the number of conflicts rather than number of queens under attack like my custom evaluation function did.
* Also, it is better to perform mutations by swapping two elements, not rotating 3 like my custom mutation function did.
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Go Over Genetic Algorithm Powerpoint
|Completed
|January 20, 2021
|January 27, 2021
|January 25, 2021
|-
|Set up Personal Notebook
|Completed 
|January 20, 2021
|January 27, 2021
|January 25, 2021
|-
|Complete Lab 1 (DEAP Jupyter Notebook)
|Completed
|January 20, 2021
|January 27, 2021
|January 25, 2021
|}