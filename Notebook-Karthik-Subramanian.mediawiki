== Team Member ==
[[files/GeorgiaTechBuzz.jpg|thumb|123x123px]]

Team Member: Karthik Subramanian

Email: ksubramanian40@gatech.edu

Cell Phone: 908-240-0310

Interests: Machine Learning, Python, Puzzles, Reading, Cooking
== December 6th, 2021==
'''Main Meeting Notes:'''
* Steven mentioned that runs turned out to be for 24 hrs
(ie: master1.out is 1st 8hrs, master11.out is 2nd 8hrs, master111.out is 3rd 8 hrs)
--> For master11.out and master111.out, set reuse to 1 in order to append to existing database.

* Working with Kevin on "Primitive" section in final presentation

'''Subteam Meeting Notes:'''
* Preparation for final presentation + wrapping up loose ends for the semester
'''Individual Notes:'''
* Doing 24 hr runs PACE (group 8 hr runs together)
* Developed a shell script to automate the process/commands to run SqUaD in PACE
* Link to Shell Script: https://github.gatech.edu/gist/ksubramanian40/103c9748f928abf6642eb1ddbc3cfb2b
* The runs I have done over the past few days were wrong in that I mistakenly assumed that reuse is set from 0 to 1 over the course of the 24 hr runs. This caused my runs to run unseeded, which is not the intent of the experiment
What actually should happen is this:
    1st 8 Hrs: clear the database tables and then run qsub pbsmysql.pbs followed by seeding_from_file.py followed by launchEMADESquad.pbs
    2nd 8 hrs: continue off of the previous run (don't need to reseed) (just do qsub pbsmysql.pbs followed by launchEMADESquad.pbs)
    3rd 8 hrs: continue off of the previous run. Same procedure as "2nd 8 hrs"

* Fortunately, my shell script that automates the process of running SqUaD in PACE should work (though the user will need to clear the database locally for the 1st 8hr run)

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Keep doing Runs as per Steven's guidelines
|In Progress
|December 6th, 2021
|December 10th, 2021
|
|-
|Add information about BiDAF primitives to final presentation
|In Progress
|December 6th, 2021
|December 8th, 2021
|
|-
|Update pace-login.sh script for SqUaD Dataset
|Completed
|December 8th, 2021
|December 8th, 2021
|December 8th, 2021
|}
== November 29th, 2021==
'''Main Meeting Notes:'''
* Code Freeze to get runs in for final presentation

'''Subteam Meeting Notes:'''
* Steven provided a link to the latest version of our repo (ie: the code freezed version). Link is: https://github.gatech.edu/sleone6/emadebackup

'''Individual Notes:'''
* Been debugging PACE in order to get a seeded run throguh
* Objective functions in use are MSE (since we are doing a regresssion problem), and num_params (for evaluating "complexity" of individual)
* I had issues with my database, so I needed to reinstall it and that fixed the issue of "entries/columns" not recognized in the table
* Configured MySQL workbench environment to query the pareto front for the latest generation 
* Over the weekend, I got 2 runs in. Plan on doing more runs for this week to supply more data for the statistics "sub-subteam"
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Debugging PACE issues to get 1st seeded run in for QA
|Completed
|November 29th, 2021
|December 6th, 2021
|December 4th, 2021
|-
|}
== November 22nd, 2021==
'''Main Meeting Notes:'''
* Dr. Zutty discussed possible statistical test (take the area between naive Pareto Front and EMADE pareto front. Want that area to be bigger in order to show that EMADE is performing better
* NNLearner2 bug fixes (tune for regression problems, curretnly works on classification)
* Possibility of changing scope of subteam project (to a classification problem instead of a regression problem)

'''Individual Notes:'''
* In the case where NLP subteam is pivoting to a classification problem, I have repurposed the squad dataset to now have 1s for answerable
questions and 0s for unanswerable questions. Those backup files have been uploaded to EMADE-304
* UPDATE 11/29/21: Looks like Steven and Kevin were able to get the BiDAF layers to work in the "features/nn-learner 2". Prelim runs seem to have been done, but the results are not very meaningful. To make analysis easier, I reprocessed the SqUad data such that the target is the start word index of the answer (originally, the indices of the "answer_start" column in SqUad are relative to the character)
* I'm currently working on trying to get the seeded runs from feature/nnlearner2 branch to work

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Generate Answerable/Unanswerable repurposed SqUad data
|Completed
|November 22nd, 2021
|November 22nd, 2021
|November 22nd, 2021
|-
|Reprocess SqUad data (ensure that word indices are present)
|November 27th, 2021
|November 29th, 2021
|November 28th, 2021
|-
|Complete Peer evaluation
|November 29th, 2021
|December 8th, 2021
|December 1st, 2021
|}
== November 15th, 2021==
'''Main Meeting Notes:'''
* Found out that getting two data pairs into "nn_learner" is a hard problem and Devan is working on trying to debug that. Manipulating the output layer to work only with the current nn_learner would cause me to have to go in and modify other files in the codebase and that's intractable given the limited time.

* Scope has been reduced to predicting start index of the answer (so a one-variable regression problem)
'''Subteam Meeting Notes:'''
* Working on getting my PACE environment to work with a new port number (essentially, my sql job closes in PACE since others are using the same port)
* Integration Team task: get standalone tree evaluator to work and make sure runs happen with the newly preprocessed data
'''Individual Notes:'''
* Batch Size is an important parameter in Keras since this adds flexibility to pass in different size examples into the BiDAF pipeline
* George and I fixed issue with the Attention Layer dimension. The BiDAF attention now is able to accept different batch sizes. We aimed to have output dimensions in a form like: (None, 8, 800). In this particular example, "None" is the flexible batch size, 8 represents the number of tokens in the context, 800 is 8*(vector embedding size)
* Error in concatenation in the megamerge step was fixed (fixed the axis), got rid of "convert_to_tensor" function
* Link to Colab notebook: https://colab.research.google.com/drive/1sZfLfxzt1IF904cKh6FmRwLJQbN1nh-w#scrollTo=J4GFfh6jcsmI
* Below is a screenshot with the changes (made in the call() function for BiDAF Attention Layer)
<img width="628" alt="call_func" src="https://github.gatech.edu/storage/user/35351/files/f81afc2f-03a6-4763-adbb-2921799ecfff">
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Go through Cameron's PACE Setup video and have port change
|Completed
|November 17th, 2021
|November 19th, 2021
|November 17th, 2021
|-
|Fix Bidirectional Attention Layer Dimensions with George in AAD Hack Day
|Completed
|November 21st, 2021
|November 21st, 2021
|November 21st, 2021
|}
== November 8th, 2021==
'''Main Meeting Notes:'''
* Steven assigning responsibilities for subteams within NLP subteam
* My subteam is "Output Layer" team. We focus on getting nn_learner to work properly with the output layer in BiDAF
'''Individual Notes:'''
* Currently working on trying to do a run of standalone_tree_evaluator with BiDAF architecture
* Modifications of Modeling Layer with Shiyi in Google Colab Notebook
* Repreprocessed SqUaD dataset to have (context, answer_start) and (query, answer_start). In other words, the target label is changed to start index of answer instead of the actual answer string (Note: For now, I have dropped rows in the SqUaD dataset where the answer isn't known)
* Next steps: If standalone_tree_evaluator works, then I plan on working with Rishit to modify nn_learner to do 2 predictions (one for start index and one for end index). Put the predictions together to get answer string from context and then pass into f1_string_match
* Fixed database connection issue. Had to change the suffix "-l" to "-r" after looking at the host of the sql job in "qstat"
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Fix Modeling Layer with Shiyi (dimension issue)
|Completed
|November 8th, 2021
|November 10th, 2021
|November 9th, 2021
|-
|Fix SQL Connection Error from standalone_tree_evaluator run on re-processed data
|Resolved
|November 12th, 2021
|November 15th, 2021
|November 15th, 2021
|}
== November 1st, 2021==
'''Main Meeting Notes:'''
* Dr. Zutty advised that we write some basic unit tests to ensure that our new QA primitives work properly

'''Subteam Meeting Notes:'''
* Introduce new members to QA
* Working on ensuring dimensions work properly for each layer

'''Individual Notes:'''
* Me, Kevin, Shiyi worked on a first draft of an implementation of the bidirectional attention layer. We found a great article that helped us understand the paper on BiDAF. Link to the article: https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-3-attention-92352bbdcb07

* Initial Draft of Code can be found at this gist: https://github.gatech.edu/gist/ksubramanian40/35a233744bd46a6c64489e2ddade6873
* UPDATE 11/7/21: Shiyi, George, Karthik fixed the implementation of Bidirectional Attention, Modeling Layer and Output to ensure that the proper dimensions appear in the output. A small unit test was written to ensure that the intended behavior for various input sizes

* Link to Colab notebook describing updated code: https://colab.research.google.com/drive/1sZfLfxzt1IF904cKh6FmRwLJQbN1nh-w#scrollTo=WdWJF5_OFYR8

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Rough draft of Bidirectional Attention Layer implementation
|Completed
|November 1st, 2021
|November 2nd, 2021
|November 1st, 2021
|-
|Fix dimension error in Bidirectional Attention with Shiyi, George
|Completed
|November 3rd, 2021
|November 8th, 2021
|November 7th, 2021
|-
|Polish Modeling Layer and Output Layer with Shiyi
|Completed
|November 3rd, 2021
|November 8th, 2021
|November 7th, 2021
|}
== October 25th, 2021==
'''Main Meeting Notes:'''
* In regards to our presentation, Dr. Zutty might have an nn_learner that takes in multiple Emade Data Pairs (just email him)
* Presentations happened (returning students + bootcamp students)

* Bootcamp Team 1:
    * Dropped irrelevant values, replaced null values (fare and age imputed with mean), one hot encoded "sex" and "embarked" columns
    * ML Algorithms: SVM Classifier, Gradient Descent, Neural Network, Random Forest Classifier, Gaussian Process Classifier
    * Objective functions used were FPR and FNR
    * GP: primitives (add, subtract, multiply, cos), tournament selection, one point leaf biased crossover, node replacement mutation
    * EMADE: doubled headless chicken rate, reduced most of mutations, 
    * Got AUC of 0, but forgot to split train.csv into X_train and X_test (learning moment)
    * 42 generations, EMADE AUC of 0.1379883
    * Learning moments:
        1. Too many infs (primitive not found)
        2. Check if EMADE is running
        3. Adaboost learner 

* Neural Architecture Search:
    * Create neural networks automatically in EMADE
    * Problems
       * EMADE couldn't outperform seeds
       * Most individuals were not neural networks
       * Few restrictions for out dimensions, batch sizes, etc
       * Nested NNLearners taking up lot of time (no ensembling techniques to take advantage of)
    * Goal: have EMADE create more complex architectures and make EMADE competitive with VGG16 and other SOTA architectures
    * Time Stopping
       * Lower training time same as minimizing number of parameters
       * Put hard stop on how much time to train
    * Preprocessing
       * Text tokenization, One Hot Encoding for Multiclass Target Data
       * Modify data pair once, then feed into individual
    * Growing Deeper Complex Individuals
       * CoDEEPNeat (latest and most effective NAS)
       * Construct smaller neural network structures called 'modules' and build up from there
    * NNLearner SQL Table
       * Table to access specific NNLearner individuals over time
       * Singles out only NNLearner-based individuals instead of any individual  
    * Novelty detection
       * Reward individuals for new architectural features/structures
       * Track layers EMADE generates over all individuals
* Bootcamp 2
    * Preprocessing: Dropped Name, PassengerID, Ticket Number, Fare columns
    * Mapping: Mapped alphabetical values of Sex and Embarked columns using column maps
    * MOGP Design: Used SPEA2 for selection, symbolic regression used
    * MOGP Eval: FPR and FNR (minimize two objectives). Convert float to 0 or 1 using activation function. 30 generations
    * EMADE
        * Should have spent more time on processing data and trying out more primitives + individuals
    * In the future, decide who would run master and worker (or use PACE)

* Image Processing
    * Objective: Improve EMADE on image processing tasks (GP Approach)
    * Multilabel Image Classification is the focus for this semester
    * CheXNet paper (pneumonia classification problem on chest x-ray scans). Look for pneumonia or not 
    * Chestnet Dataset
         * Previous VIP team had some preprocessing on (resizing, normalization, horizontal flipping)
         * New augmentations: rotations, shearing, horizontal/vertical translation
    * Baseline
         * Precision/Recall AUC (saw that the labels are a bit unbalanced since we care about one class)
         * Suspicious AUC
    * Selection Methods
         * NSGA-III, lexicase, hypervolume indicators (promise in solving many-objective optimization problems)
    * Mating and Mutation
         * Geometric crossover operators (manually adjust the diversity manually). See at a given time if we can tune hyperparameters to directly 
           improve diversity
    * Hyper-features
         * Combine 2 or more primitives that work well together to produce improved individuals with better fitness scores
         * Pixel visibility improved
         * Grey level transformation (enhance contrast and brightness), Sobel Filter (edge detection)

* Bootcamp 3
    * Preprocessing
         * Dropped PassengerID, Ticket, Cabin
         * Relatives column = sum(subsp, parch)
         * Used one-hot encoding for genders as well as name titles
         * Imputed with mean of feature (to handle remaining missing values)
     * ML Models
         * Gaussian Naive Bayes
         * MLP
         * SVM
     * GP
         * selLexicase
         * Squared FP and Squared FN (handling to see if these values were high)
     * EMADE
         * FPR and FNR evaluation function
         * selTournamentDCD() error
         * Wished they could run more generations in EMADE to get a hopefully better Pareto front
* Stocks
    * Objectives
         * EMADE for regression on time series data? optimize market trading algorithms? EMADE beating SOTA trading algorithms?
    * Use various Technical Indicators, decide if an individual should buy/sell stock on certain day
    * Best primitives to use for predicting buy/sell
    * Median CDF (lesser is better)
    * Objective Functions
        1. Profit percentage (max)
        2. Average Profit per Transaction (Maximization)
        3. CDF  of Profit (minimization). Show model is better than randomly deciding buy/sell 
        4. Variance
    * Pool of objectives to pick out stocks to conduct EMADE runs on. Run EMADE on all possible combos of objectives and compare AUC
    * Compare Pareto Individuals across trials to that in the existing literature
    * Future Work
        * Explore approaches to portfolio optimization outside of stock price prediction
        * Other applications of autoML to time-series analysis
        * Improve EMADE's time-series analysis capabilities
* Bootcamp 4
    * Preprocessing
        * Dropped Name, Cabin columns
        * One-hot encoding for Embarked features
        * Replaced nulls of Age and Fare with Pclass's median
    * ML
        * MLP, Decision Tree, SVM, XGBoost
        * Handle cases of nonlinear feature relationships 
    * Primitives
        * Strongly typed
        * mathematical, logical operations
        * Ensure boolean features outputted
    * MOGP
        * One point crossover, mutUniform mutation, mutNodeReplacement
        * FPR and FNR used (best individual had FPR = 0, FNR = 0.91228)
    * EMADE
        * 20 generations
        * 23 pareto optimal individuals
        * converted FN and FP to rates
* Modularity
    * Abstract parts of individuals and create "building blocks" that can help with genetic process
    * ARLs (Adaptive Representation through Learning)
         * Introduce modularity and reusability in EMADE (once a "good" section of tree is found, make it a new primitive)
    * Eval --> Selection --> Genetic Operations --> ARLs --> Evaluation workflow to implement ARLs
    * Why ARLs?
         * Global pool of "good" bits could help all individuals
         * Should speed up convergence of population
         * frequency, fitness important to identify valuable subtrees
    * Working on trying to increase complexity of ARLs by increasing tree depth
    * Experiments using Titanic dataset (False Positives, False Negatives as objectives)
    * More data needed to gain insight into effects of ARL complexity performance
         * size may not inherently imply usefulness
    * Stock runs with ARLs
         * Reusing technical indicators from Stocks subteam
         * compare ARLs vs. no ARLs and see significance
    * Studying explainable blocks
'''Subteam Meeting Notes:'''
* Focus shifts to implementing the BiDAF based architecture
* Each person responsible for implementing one of the layers
* BiDAF Paper: https://arxiv.org/pdf/1611.01603.pdf

'''Individual Notes:'''
* Me, Steven, Rishit, Devan responsible for building the bidirectional attention layer using Keras
* A question I have from reading the BiDAF Paper Steven linked regards the Attention Layer. I understand conceptually that Attention in our case
aims to figure out how the context fits into the query and query fits into the context (hence bidirectional). However, on a small detail, I see that the paper computes a similarity matrix among tokens in context to tokens to the query (given the contextual embeddings). However, does Keras Attention layer already existing give the similarity matrix for us? If so, can we treat it as a black box?
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read the Paper Steven linked on BiDAF. Get a basic understanding on how bidirectional attention works
|Completed
|October 27th, 2021
|October 31st, 2021
|October 31st, 2021
|-
|}
== October 18th, 2021==
'''Main Meeting Notes:'''
* Start thinking about how you can statistically verify that your code is making a significant impact relative to the problem you want to solve and relative to a baseline

* Runs take time (number of workers, wall time, number of processes, etc). Plan it out in advance!
'''Subteam Meeting Notes:'''
* Working on midpoint presentation coming up on Monday (that's priority right now)
* Link to presentation (NLP Subteam): https://docs.google.com/presentation/d/1GviS4whmKxNpbxn2cMQgcUthRxp1hsmu_NLfDkY00b4/edit?usp=sharing
'''Individual Notes:'''
* Developed a Python script to generate training/testing data for the SqUad dataset. This script does the train/test split and then generates a csv file that contains (question, answer) pairs and another CSV file containing (context, answer) pairs. The script and dataset can be found here: https://drive.google.com/drive/folders/1LEtzzI0s5sEz9jr47oa1jqoX4qXGqlbT?usp=sharing

* Thanks to Steven's help, I was able to get standalone_tree_evaluator working. I just need to input the SQL database credentials

* Having trouble getting run for at least a few generations (at the start, my processes are queued in pace and then they close all of a sudden. Need to look at my configs)
* Preparing for presentation

* UPDATE 10/20/21: I modified the above Python script mentioned to do 5 fold cross-validation as well (Link is: https://drive.google.com/drive/folders/1z6Ro2vYwkOx19wGjTl2rfwg7nx-1YElY?usp=sharing)
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Be able to run "standalone_tree_evaluator.py" in PACE (EMADE 304 branch)
|Works for the most part. Working on running the pbs script
|October 16th, 2021
|October 20th, 2021
|October 22nd, 2021
|-
|Develop script to generate question.csv and context.csv with train/test split
|Completed
|October 18th, 2021
|October 20th, 2021
|October 20th, 2021
|-
|Develop input_squad.xml + add numberinput parameter of 2
|Steven Already Developed it and it's in the fork!
|October 20th, 2021
|October 21st, 2021
|October 20th, 2021
|-
|Pull down the fork that Steven had on EMADE 304. He made some hotfixes
|Completed. Updated the reinstall.sh from nn-vip branch
|October 20th, 2021
|October 20th, 2021
|October 20th, 2021
|-
|Work on slide about squad data preprocessing in preparation for midpoint presentation
|In Progress
|October 20th, 2021
|October 21st, 2021
|October 24th, 2021
|}
== October 11th, 2021==
'''Subteam Meeting Notes:'''
* Need to merge changes from nn-vip into the EMADE 304 branch Dr. Zutty gave
'''Individual Notes:'''
* Dr. Zutty provided a snippet/branch in Cache V2 which supports loading in multiple EMADE Data Pairs. We need to modify the input.xml to contain multiple trials (so a data pair for train_context/test_context and for train_question/test_question)
* Using the example xml file in the branch Dr. Zutty provided, I have developed a Python script to generate EmadeDataPairs for both the question and context as we have discussed in the past few weeks. Plan on talking to Steven about this and seeing if I have the necessary ingredients for the modified "input_squad.xml" file
* Went through diffs between EMADE-304 and nn-vip branch and trying to identify merge conflicts we need to ask Dr. Zutty about
* On October 16th, there was a "workday"/hackathon for AAD. Steven, Kevin and I were working on resolving the merge conflicts between EMADE 304 branch and nn-vip. We have merged the nn-vip branch into EMADE 304 and working on getting "standalone_tree_evaluator.py" to run in PACE
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Reading through CacheV2 Branch and identifying differences between that branch and nn-vip branch
|On Hold due to codebase cleanup
|October 6th, 2021
|October 13th, 2021
|October 13th, 2021
|-
|Discuss with Steven my code for generating the multiple EMADE Data Pairs prior to the input_squad.xml integration
|Completed
|October 13th, 2021
|October 14th, 2021
|October 13th, 2021
|-
|Look through fork Steven made and identify what files we need to ask Dr. Zutty about to resolve those merge conflicts
|Completed
|October 13th, 2021
|October 15th, 2021
|October 14th, 2021
|-
|Resolve merge conflicts between nn-vip and EMADE-304 branch
|Completed
|October 14th, 2021
|October 15th, 2021
|October 15th, 2021
|}
== October 4th, 2021==
'''Main Meeting Notes:'''
* Dr. Zutty told us that he's looking for the functionality in Cache V2 branch to generate multiple EMADE Data Pairs (since we have a context and query)
* We don't want to change the "datasetDict" variable in "data.py" script since that variable is used throughout, so we want to use already existing functionality to minimize our risk and save debugging time
* Steven wrote up a "load_qa_data_from_file()" that returns the question, context columns

'''Subteam Meeting Notes:'''
* Still waiting on Dr. Zutty to get the code supporting the functionality of loading in multiple EMADE Data Pairs at once
* "standalone_tree_evaluator.py" script can be thought of as a unit test to evaluate one individual (skip the evolutionary steps)
* Two main subtasks: 
1. Familiarizing ourselves with CacheV2 branch. Once we know where the functionality of loading in multiple EMADE Data Pairs is, we need to merge that functionality into "nn-vip branch"

2. Coming up with seeded individuals to test according to literature on SqUad dataset

'''Individual Notes:'''
* Glad that we realized earlier from Dr. Zutty that there is a function to load in multiple EMADE Data Pairs. Was a bit concerned that if we changed our load function and type in the "data.py" script, that would cause unnecessary hours of debugging + maintainability nightmare
* Goal is to familiarize myself with Cache V2 so that I know how we need to structure the "nn-vip" branch during the data engineering aspect of our project

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Reading through CacheV2 Branch and identifying differences between that branch and nn-vip branch
|Not yet started
|October 6th, 2021
|October 13th, 2021
|
|-
|}



== September 27th, 2021==
'''Main Meeting Notes:'''
* Dr. Zutty wasn't present, so the team met with Anish to discuss a way to leverage EMADE architecture to create datapairs structured as "((Context, Query), Answer/Label)". We initially went with an approach of inventing our own datatype, but that would cause us to go in and modify each later in the "neural_networks_methods.py" script. Instead of inducing that risk, Anish suggested that we have an overloaded "nn_learner" method to take in "two inputs" and perform embedding separately and then do BiDaf on the context and query. 

* Link to image of architecture Anish proposed: https://drive.google.com/file/d/16sbKZ2JRheoqeHsNChpx3daltsVClF0K/view?usp=sharing

'''Subteam Meeting Notes:'''

'''Individual Notes:'''
* Solidified our approach with Anish's help.

* Want to build an overloaded "NNLearner" function that takes in "Emade Data Pair" for Question and "Emade Data Pair" for Context

* General Steps:
    1. XML File for SqUAd Dataset (EMADE.py file to get dataset in)
        - "loadSquadDataset()" function + identification of return type
        - "buildClassifier()" function
    2. Input Layers ("QueryInputLayer" object + "ContextInputLayer" object) for neural_network_methods.py
* Trello Board Link: https://trello.com/b/6lcDBEj1/distilling-qa-systems-with-emade
* Subteam decided to migrate to a new repo for QA-type work. Need to ask Steven on Monday how I should set up my environment

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Design XML + build_classifier() function pipeline for SqUAd dataset
|Completed by Steven
|September 29th, 2021
|October 6th, 2021
|October 6th, 2021
|-
|Ask Steven on Setup for new Repo for NLP subteam (fork of emade)
|Already completed. I don't need to work on the fork of EMADE
|September 30th, 2021
|October 4th, 2021
|October 4th, 2021
|-
|Complete Peer evaluation
|Completed
|October 4th, 2021
|October 8th, 2021
|October 4th, 2021
|}
== September 20th, 2021==
'''Main Meeting Notes:'''
* Dr. Zutty hinted that "num_params" might not be a good objective function to use with F1 score. This is due to past experience of not getting a smooth Pareto frontier. Might need to find a 3rd objective
* Devan asked about how we can force Word Embeddings, Contextual Embeddings, and Positional Embeddings to occur before the evolution and Dr. Zutty hinted at creating a subclass of "EmadeDataPair" that would be able to take in raw data and spit out the "EmadeDataPair" object that resulted from the word embeddings. In our team meeting after SCRUM, Steven hinted that the Word Embeddings was already included as a part of the "nn-vip" branch.
* I showed Steven my EDA of the SqUAd dataset. He was happy that I was able to translate the json into a Pandas DataFrame. I was wondering if I needed to do any preprocessing (like: common tools for processing natural language data) and for now, it seems like EMADE should handle that. Also, for a QA system, we might not need to do things like get rid of stopwords since the whole context might matter
* Devan's presentation on basics of QA was extremely useful
* For now, I can hold off on the EDA since on Wednesday, we plan on building out the architecture of the QA system in EMADE, which will involve porting the datasets into the repo or having a Python script that pulls in the dataset to instantiate the "EmadeDataPair" object

'''Subteam Meeting Notes:'''
* To start building out the QA architecture, how do we differentiate between our inputs of Question and Context? This is what needs to be investigated

'''Individual Notes:'''
* I finally understand what Attention Layer means conceptually. According to Devan's presentation, Attention Layer basically tells how a word relates to the essence of the whole passage. Stuff like contextual and positional embeddings are local to the sentence/token, but Attention Layer gives an aggregate relationship between the words within the context of the paragraph
* Came to a potential realization that "EmadeDataPair" class is a pair of "EmadeData" instances. Seems like from looking at "GPFramework.data" file, a direct plan of action could be to subclass the "EmadeData" instances and create a "Question" and "Context" subclasses. Main problem is that an input is defined as "(Context, Question)" tuple, so we need to specify EMADE that this tuple goes in along with the label from the dataset as an "EmadeData" instance. (ie: "EmadeData = ((Context, Question), Label)")
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Work on EDA for SqUAd dataset and get a sense of some properties of the data
|Completed
|September 15th, 2021
|September 20th, 2021
|September 20th, 2021
|-
|Start building out the primitives in EMADE on Wednesday's subteam meeting
|Delayed
|September 21st, 2021
|September 22nd, 2021
|Not yet resolved due to blocker from the subteam meeting
|-
|Look in the EMADE codebase and understand how the dataset is built into an EMADE Data Pair (`data.py`). Also, examine what the layers in `neural_network_methods.py` do to the dataset
|Completed. Anish helped us out on this
|September 22nd, 2021
|September 29th, 2021
|September 27th, 2021
|-}
== September 13th, 2021==
'''Main Meeting Notes:'''
* Discussing updates for NLP Subteam
* Dr. Zutty advised us to conduct a literature review and go through the "checklist" to make our task of building a QA system "EMADE-able"
* Link to my self evaluation rubric: https://docs.google.com/document/d/1xnKHrqcLZhAPb4iIz2bVFx2RMrf3jTQy/edit?usp=sharing&ouid=110383498255970078532&rtpof=true&sd=true

'''Subteam Meeting Notes:'''
* Discussed plan of action for NLP Subteam given most of the team have `nn-vip` branch working
* Important Action items:
    1.lit review on QA systems
    2.look at other models/papers and see what types of layers(or anything else you think will work) can be used as primitives
    3.checkout the dataset and see if we would need to manipulate it all to work for our purposes (null data, bias, data formatting, etc)
* Trello board for NLP subteam created

''Individual Notes:'''
* Started creation of Colab notebook to do EDA on SqUAd dataset 
* I was looking through some of the papers relating to models that performed well on QA problems with the SqUAd dataset. I came across a term thrown around called "Transformer Layer" and spent some time understanding it. This Youtube video was very helpful in clarifying the role of the encoder and decoder layer in doing translation (eg: English to French translation): https://www.youtube.com/watch?v=TQQlZhbC5ps&ab_channel=CodeEmporium
* One of the papers I delved into was a QA architecture called "Retrospective Reader". At a high level, this architecture mimics how a human would answer a reading comprehension question (ie: get a general grasp of the passage and then dive into the detail to answer the question). There are two aspects of the architecture from the paper
   * 1. Determine if a question can be answered (External Front verification)
   * 2. If a question is answerable, produce possible places where the answer exists along with probabilities. (Internal front verification)
Link to the paper: https://arxiv.org/pdf/2001.09694.pdf

* Link to EDA on SqUAd Dataset: https://colab.research.google.com/drive/1dx66YZSFYXimJAcZtU35yTkt_PYpiVL9#scrollTo=359z-WevwWaY
(Note that the access is "view only". You will have to make a copy of the notebook and ensure that the SqUAd dataset is in the proper directory)
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Work on EDA for SqUAd dataset and get a sense of some properties of the data
|In Progress
|September 15th, 2021
|September 20th, 2021
|
|-
|Find an article in Literature relating to QA systems and analyze what models are used and how the layers/architecture are structured
|Completed
|September 15th, 2021
|September 20th, 2021
|September 20th, 2021
|-
|Read up on QA systems and get a feel for how they work (look at diagrams, literature, etc)
|Completed
|September 8th, 2021
|September 13th, 2021
|September 14th, 2021
|}
== September 6th, 2021==
'''Main Meeting Notes:'''
* Discussing ideas for problems to tackle during the Fall 2021 semester for NLP Subteam
* NLP subteam seemed to agree on trying to use EMADE to build a QA system (Question/Answer system). Given some context and a question, the QA system should output a matrix of probabilities of the keyword answering the question
* Dr. Zutty hopped into our Bluejeans call and we told him the idea we wanted to do for the NLP subteam. One important takeaway is that EMADE is much more helpful for supervised learning type problems and based on the dataset we found (Squad dataset is what SOTA QA systems use), Dr. Zutty seemed to like our idea and the problem seems doable in EMADE

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run the Amazon Dataset in EMADE from last semester (Spring 2021) in PACE
|Completed
|September 8th, 2021
|September 13th, 2021
|September 11th, 2021
|-
|Read up on QA systems and get a feel for how they work (look at diagrams, literature, etc)
|In Progress
|September 8th, 2021
|September 13th, 2021
|
|}

'''Individual Notes:'''
* Started to look into how the SqUAd dataset works. This article https://towardsdatascience.com/the-quick-guide-to-squad-cae08047ebee was very helpful
* Starting a brief exploration of the dataset to get a feel for it

== August 30th, 2021 ==
'''Main Meeting Notes:'''
* Discussing ideas about possible subteams and problems to tackle for this semester
== April 30th, 2021 - Final Presentation for Semester == 
'''Presentation Notes:'''
'''Stocks Team:'''

* How can EMADE help to analyze stock market trends?

* Objectives
   * Implement TA-Lib indicators in EMADE
   * Increase "Evolvability of EMADE individuals" (prevent individuals from getting stuck at particular generations)
   * Large dataset to improve performance?
   * Statistical evaluations of individuals

* Background
   * Piecewise Linear representation to represent price in linear segments
   * EMADE individuals predicting trading value derived from PLR
   * Exponential Smoothing gives threshold for when to do buy/sell prediction

* Current work
   * Technical Indicator primitives (transition to TA-lib) to increase reliability and reduce risk of miscalculation
   * Incorporating larger datasets to optimize   

   * TA-Lib indicator primitives include:
       * ADX (time series strength of particular trend)
       * VMMA (Volume weighted moving average) - Price weighted based on volume of trading activity

* Objectives in the EMADE Runs 
   * profit percentage (maximize)
   * average profit per transaction (maximize)
   * variance profit per transaction (minimize)
   * normal CDF on Distribution (minimize)

* 2nd Run
   * Same as first run, but remove objective on profit percentage

* Monte Carlo Simulation as an objective
   * Random buy and sell stocks
   * Create a gaussian with mean and standard deviation of profits
   * CDF better captures performance on trending stocks as opposed to absolute profit percentage
   
* Buy Hold Comparison
   * Compare profit to buy hold strategy when trading a stock

* Future Work
   * Statistical analysis of seeding types
   * Bounded objective functions
   * Use Fundamental analysis (underlying value of company independent of market price) 
   * hourly/daily/weekly trading frequencies (does EMADE give different levels of effectiveness)


'''ezCGP'''
* Node reusability (structure not bounded by restriction on tree)
* Block structure

* Objective
   * Improve Neural Architecture search
   * Recreate CIFAR-10 results without relying on transfer learning
   * Improve ability to visualize genomes
   * Research, develop, test new mating methods for Cartesian Genetic Programming

* Individual Size (Experiment)
   * Validate depth of neural architectures generated (4-5 layers)
   * Initial population matched target distribution
   * Larger individuals performed worse (more room for parameters to break/not perform well)

* Activation Function (Experiment)
   * Algorithm was selecting and mixing algorithms between convolution layer
   * Limit architecture to single activation (only RELU) --> no significant improvement in performance of best individual

* Max Pooling and Dropout Layer (Experiment)
   * Analyze impact of primitives compared to midterm benchmark
   * Tune probability of each type of primitive

* Dense Layers (Experiment)
   * Would evolving dense layer improve performance?

* Visualization
   * Built easy to use CLI (command line interface) to visualize any number of individuals
* Crossover methods in CGP
* Meta Parameter (new way to run ezCGP so that we can easily run different iterations of the same problem while making small changes)

* Future Work
   * Research, develop, test new mating methods
   * Incorporate existing CNN architecture to develop new genome seeds

'''NLP Final Presentation Feedback'''
* When we do Auto ML problems, good to partition the train set for ourselves so that the search is over a portion of the training data (use the standard test set to be validation)
* Consider (False Positive Rate, False Negative Rate, Num Parameters) as objective space


'''Modularity:'''
* Exploring ways to abstract parts of individuals
(create building blocks that can help with the genetic process)

* Primitives can be ML models instead of simple mathematical operations

* ARL: Adaptive Representation through learning (once a good section of tree is found, make it a new primitive)

* Increase complexity of ARL (increase tree depth and allow imbalanced trees)

* Titanic and MNIST datasets used for ARL testing

* Potential Objective functions
   * Precision score, Recall score, F1 score, Accuracy score, Cohen Kappa Score

* Titanic Datasets Objectives: FPR, FNR

* MNIST Run: seeded file comprised of 30 valid individuals obtained in previous MNIST EMADE runs
(got more than 316 valid individuals by gen 50)

* Future Work
   * Deep ensemble models
   * Modify evolutionary selection method and help diversify ARLs
   * Diversity measure
   * Integrate ARLs with EMADE's original architecture

'''Individual Reflection:'''
One major thread I seemed to notice with the presentations was discussion on the objective function used. From the presentations shown, for next semester, I hope to critically look into the objective functions used and see how that affects the type of individuals generated and the performance attained.

== April 26th, 2021==
'''Team Meeting Notes:'''
* Final Presentations April 30th

'''Subteam Meeting Notes:'''
* Prepare slides for final presentation
* Dr. Zutty commented on latency aspect along with evaluation time of 
individuals in auto ML
(some individuals might be time consuming to evaluate whereas other
individuals could be so bad that it probably won't make it to the next generation)

* Queue size in EMADE designed such that once the queue size drops to a 
certain level, next generation kicks off (we don't want workers to idle while
the master is prepping the next generation)
* Plan on doing a rehearsal for the presentation 4/29/21 6pm

'''Individual Reflection:'''
* I'm currently doing a seeded EMADE run in PACE with no GPU set up/in use (that might have caused my run/job to be stuck in the queue). The run will evaluate on
2 objective functions (FPR and FNR) 

* Plan is for me to visualize the pareto front

* UPDATE 4/27/21: My approximately 8 hour run has finished today morning (I ran with no GPU usage and set the wall time to 8 hours so that the SQL connection timeout happens later). I visualized the pareto front through uploading the master.out file into a visualizer script created by one of the teammates. Below is the Pareto Front I'm getting 
[[files/Paretopace.PNG]]


* 4/28/21: I did another seeded run for 8 hours of EMADE in PACE with False Positive Rate and False Negative Rate as objective functions. Below is the pareto front I got along with the individuals in the latest generation (gen 21 in this case)
[[files/paretoTwo.png]]

The individuals I got from the Pareto Front in generation 21 is the following (with the help of Nishant in setting up/configuring MySQL Workbench to query the pareto front):

*       NNLearner(ARG0, OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, EmbeddingLayer(99, ARG0, randomUniformWeights, InputLayer())))), 90, AdamOptimizer)
*       NNLearner(ARG0, OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, EmbeddingLayer(90, ARG0, randomUniformWeights, InputLayer())))), 89, AdamOptimizer)
*       NNLearner(ARG0, OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, EmbeddingLayer(96, ARG0, randomUniformWeights, InputLayer())))), 94, AdamOptimizer)
*       NNLearner(ARG0, OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, EmbeddingLayer(98, ARG0, randomUniformWeights, InputLayer())))), 98, AdamOptimizer)
*       NNLearner(ARG0, OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, EmbeddingLayer(92, ARG0, randomUniformWeights, InputLayer())))), 92, AdamOptimizer)
*       NNLearner(ARG0, OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, EmbeddingLayer(93, ARG0, randomUniformWeights, InputLayer())))), 91, AdamOptimizer)
*       NNLearner(ARG0, OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, EmbeddingLayer(94, ARG0, randomUniformWeights, InputLayer())))), 93, AdamOptimizer)
*     NNLearner(ARG0, OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, EmbeddingLayer(100, ARG0, randomUniformWeights, InputLayer())))), 100, AdamOptimizer)
*                                                                                NNLearner(ARG0, OutputLayer(EmbeddingLayer(98, ARG0, randomUniformWeights, InputLayer())), myIntMult(passInt(50), 3), FtrlOptimizer)

Looking at the evaluation gen of the individuals in the pareto front, only one indivdiual was evaluated in generation 12 (the last individual from above) while the other individuals were evaluted in generation 0. It seems like the vast majority of individuals in the pareto front were coming from the seeded individuals to begin with, so I might test an unseeded 
run of EMADE in PACE with FPR and FNR objectives.

In terms of the final presentation, I worked on adding slides about FPR/FNR runs along with some run results. I used Nishant's modified visualizer script to better understand the evolution of my Pareto front from the past runs. [https://colab.research.google.com/drive/1COC0IcnuUyjiBiimgyp7xgTB5r24OusY#scrollTo=iVlkPZs_qf1d Link to the modified visualization script can be found here]

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get in 1 run of FPR/FNR in PACE
|Completed
|April 26th, 2021
|April 28th, 2021
|April 27th, 2021
|-
|Add my out file data in Google Drive folder for presentation Cameron provided
|Completed
|April 26th, 2021
|April 28th, 2021
|April 27th, 2021
|-
|Add slides about FPR/FNR runs in the final presentation with Nishant
|Completed
|April 28th, 2021
|April 29th, 2021
|April 28th, 2021
|}

== April 19th, 2021==
'''Team Meeting Notes:'''
* Peer evals came out
* Final presentation April 30th 6pm - 8pm
* Dr. Zutty suggested us to use FPR/FNR as objective functions instead of accuracy

'''Subteam Meeting Notes:'''

* learner() core function: all target values from training data overwritten by what
model predicted on itself --> scoped in one individual

(put target feature as an input feature)


* Try using FPR, FNR, numParameters as objective functions instead of accuracy (2-4 objectives should be good)
for Amazon Product Dataset Binary Classification


* Is switching to FPR, FNR better than just using accuracy score?

* Probably should reduce size of training set to not eat memory (standardize across runs)

* (clear database before seeding) --> Cameron changed the seeding file to make seeded run easier

* repull the branch from Github and change what's on PACE

* Cameron showed how to seed before running EMADE
(in LAUNCH script, there is a "seedAmazon.pbs" script). Running seeding file from input_amazon.xml

* Do '''seeded runs'''

'''April 23rd Subteam Meeting:'''
* prepping for final presentation
(dial in and talk about final results)

* Most likely going to have code freeze Wednesday ("deadline") at noon

* Goal: get a run of EMADE in PACE with FPR, FNR

* Start planning creation of final presentation

'''Individual Reflection:'''
* Was able to clone the updated branch of nn-vip and update the seeding file provided for the amazon dataset
* I configured input_amazon.xml file to run with 3 objectives (FPR, FNR, Number of elements) with the help of the evalFunctions.py file
* When I ran the seeding file in PACE along with the mysql job, it is showing that my jobs were marked as "Closed" and I'm trying to resolve this issue with the team
* I had a SQL database connection issue that I've been stuck on. I resolved this issue by re-doing the granting of privileges
* I currently am running a seeded EMADE run in PACE. 


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Peer Evaluation
|Completed
|April 19th, 2021
|April 27th, 2021
|April 20th, 2021
|-
|Seeded run with FPR/FNR Objective Functions in PACE 
|Completed
|April 19th, 2021
|April 26th, 2021
|April 26th, 2021
|}


== April 12th, 2021==
'''Team and Subteam Meeting Notes:'''

* Steven completed a run and presented results

* Dr. Rohling gave suggestions on use of false positive rate and false negative rate

* Picking tasks to work on

'''Main problems:'''

* Evolution:
   - NNLearners easily fail, can't get very big
   - slight architecture changes leading to failure
   - Delve into evolution problem and trying to make individuals more complex
   - Tackle problem of "trivial individuals"



* Pretrained Embedding Layers:
   - Editing input size to vocab size 
   - can't change input layer to size we want

* NNLearners as subtrees
   - Learners can take outputs of other learners as a feature (similar to ensemble learning)

'''NLP Subteam Meeting Notes:'''

* Cameron working on a pull request related to PACE (functionality to have many workers on a job at PACE)


* Cameron gave a presentation about the NNLearners class in the codebase

'''NNLearners class: '''
* Neural_network_methods.py (main driver file):

* Neural Architecture Search is main objective of NLP subteam

*NNLearner
  - Takes in arg0 (dataset) --> has testing/training set , batch size, optimizer, output layer (neural network methods layer list)

  - Layer list is a python list that holds a bunch of TF/Keras layers 
     - eg: ["input", other layers, .... "output"]
  
  - out_dim: Keras can help connect of the layer (helps build the network)
  - layerList: list of other primitives (create the layer and add it to the list)



  - Core method for neural nets

  - add_layerlist() function: build the neural network recursively accounting for subtrees


"Evolution" Problem Cameron was describing: more of a "NNLearner" thing perhaps. 

'''My Reflection:'''
* For this week, I've been mainly trying to familiarize myself with the problem of trying to find/evolve non-trivial individuals in the context of the Amazon Dataset. 
I'm starting out by running EMADE in PACE and then trying to use Maria DB to query the individuals to wrap my head around the problem

* I think I have a baseline (I might pick up more as I interact with neural_network_methods.py file) understanding of the "NNLearner" function in neural_network_methods.py. Looking through the code, it seems like NNleanrer function's main responsibility is to take the layers from the running layerlist and build the neural network to populate individuals as a parameter to EMADE.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look through neural_network_methods.py file
|Completed
|April 12th, 2021
|April 18th, 2021
|April 18th, 2021
|-
|Practice querying data after running EMADE in PACE
|In Progress
|April 12th, 2021
|April 18th, 2021
|
|}
== April 5th, 2021==
'''Team Meeting Notes:'''
* Dr. Zutty gave a helpful talk/presentation on hypothesis testing
* The underlying idea with hypothesis testing is that we want to see if what we observe is statistically significant to our expected observation
* Two statistical tests to try to incorporate would be Student's t-test and Welch's t-test. Welch's t-test is useful if we don't know our expected observation
* p-value and significance level help to determine if we should reject or fail to reject our null hypothesis

[[files/t-test.png|t-test visual depiction]]

'''Subteam Meeting Notes:'''
* Main objective for new students is to get PACE setup and follow Cameron's video on how to get PACE working
* I was mainly working on PACE setup for the past week with Cameron's video. I had issues with running EMADE in PACE with `qsub pbsmysql.pbs` and `qsub launchEMADE_amazon.py` and realized that I needed to setup my MySQL database and `qdel` the jobs I was previously running.
* Subteam meeting happened on 4/9/21 and main discussion was on ML and NLP
nlp subteam meeting notes:

'''Presentation Notes from 4/9/21 meeting'''
- Supervised vs. unsupervised learning
- Supervised learning: (you have some input/feature data with a label). Learn function that maps inputs --> label
- Loss function: How far is your trained model from the true function
  (RMSE: root mean square error is a popular loss function)


NN: 

Input --> Layer 1 --> Layer 2 --> Output
(each layer does stuff to data points and that result goes to next layer)


What's in a layer?

input --> operation --> Activation

(operation: actual computation on data)
(Activation: introduce non-linearity into the model for complexity)

    - Popular ones: Sigmoid, ReLU
    - Sigmoid: bad for backprop because weights don't change if gradient is zero (squish input to 0 and 1)
      (expensive to compute b/c exponentials) -> output not "zero-centered"
    - Vanishing gradient: gradient approaches zero when input is very positive or very negative
   
    - ReLU: Super cheap to compute (works much better in practice)
         - Cons: sometimes neurons can "die" (large gradients can cause weights to update so that neuron never fires) --> use learning rate


Train Neural Network:
- Feed Forward: evaluate the model (go from input to output, compare output to expected label and compute loss function)
- Back Propagation: change layers (use gradients to update the weights)
- Learning rate: by how much are you changing the layers in each iteration
- Batch Normalization: When you are doing feed forward, using a single data point vs. using larger batches and sending that large batch at the same time


- Train/test/validation split
(train set: model learns from)
(test set: withheld data --> how to measure how well your model did)
(validation data - tell how your model is doing on withheld data during training process)


Why a model could do poorly?
- Overfitting (model overlearns from training data, but it can't generalize well to new data)
   - Possible fix: regularization (penalize complexity)
   - High bias
- Underfitting (high bias)
   - Model didn't do enough work
   - Rely too heavily on prior conceptions
- Healthy balance between underfitting and overfitting


Dense/Linear/Standard Layer:
- fully connected dense layer

Convolutional Layer:
- Relate data spatially
- "convolve/slide over all spatial locations"
- Sliding filter

Recurrent Layer:
- Have memory of previous input
- Very popular in NLP tasks
- Matrix of weights to train
- LSTM (Long-Short term memory) is a fancy Recurrent layer (remember or forget certain bits of info)


NLP Specific Concepts:
- Processing language (eg: speech recognition, translation, chatbots, etc)
- Our VIP: Sentiment Analysis (analyze Amazon Product Review dataset and classify review as positive or negative)
   - Balanced dataset (50/50 split on positive/negative reviews)

- Word Embeddings: convert text data to numbers
 (encode textual data in a meaningful way)

    - Word2vec, fasttext, GLoVe
    - Model relationship between words on massive datasets



'''Individual Notes:'''
* I mainly practiced running EMADE in PACE to get a feel for the environment I will be using
* For fun, I developed a short shell script to automate the logging in to PACE and running EMADE with the Amazon dataset. I presented it to the experienced members and they
thought that it would be a good addition to the repository. 
* [https://github.gatech.edu/cwhaley9/PACE-files Link to shell script to automate PACE RUN for Amazon dataset] 

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Practice running EMADE in PACE
|Completed
|April 5th, 2021
|April 9th, 2021
|April 7th, 2021
|-
|Develop Shell Script to automate running of EMADE on PACE with Amazon dataset (for fun and to get exposure to Bash)
|Completed
|April 5th, 2021
|April 9th, 2021
|April 7th, 2021
|-
|}
== March 29th, 2021==
'''Team Meeting Notes:'''
* I'm placed on the NLP subteam for this semester. The wiki has been down for this week, so I was not able to access the tutorial provided for setting up PACE on the NLP subteam page

'''Subteam Meeting Notes:'''
* Introduction to NLP subteam goals, challenges, tools used
* Friday April 2nd 6pm will be our new decided subteam meeting time from the whenisgood form in the nlp-nn Slack
* Need to set up the PACE environment and clone the nlp-nn branch from the EMADE repo
* April 2nd Meeting: Cameron gave a brief presentation on PACE and a basic overview of the nlp-nn branch in the EMADE Github. Main goal for me is to set up PACE on my laptop.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up PACE environment
|Completed
|March 29th, 2021
|April 3rd, 2021
|April 5th, 2021
|-
|}
== March 22nd, 2021==
'''Team Meeting Notes:'''
* Presented comparison of EMADE, MOGP, ML on Titanic dataset
* Listened to presentations from the existing subteams
* Dr. Zutty made a note on how it was interesting that my team modified the evalFunctions.py directly to calculate FPR and FNR for running EMADE on the titanic dataset
'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Fill out Canvas Poll to rank my subteam preference
|Completed
|March 22nd, 2021
|March 26th, 2021
|March 28th, 2021
|-
|}

== March 17th, 2021==
'''Team Meeting Notes:'''
* Setting up EMADE
* Common Debugging tips such as setting DEAP to version 1.2.2 and using GT VPN to connect to database
* Main issue I'm having on my end is trying to connect to my teammate's MySQL connection. I tried putting in the correct credentials, but I'm getting an error message in MySQL workbench saying "connection not successful". I'm still trying to resolve this issue on my end.
* As of 3/21/21, I was able to resolve the connection issue. Some existing students in the VIP recommended me to restart my laptop and create a fresh connection to the MySQL workbench server and that worked seamlessly for the most part on my end. 
* I'm currently trying to tune the mutation probabilities to see if there is a better Pareto frontier. As of 3/22/21, my team's database is able to find at least 20 individuals in the Pareto front at generation 25. We are currently updating our slides to reflect this update.

'''Subteam Notes:'''
* Subteam 2 for bootcamp met Friday March 19th and main focus of discussion shifted towards getting the preprocessing script we developed from the previous two weeks into EMADE
* We ran EMADE with our preprocessing script and fortunately, we were able to get to 25 generations (as of 3/20/21 when running EMADE from the 24 hour period of 3/19/21 - 3/20/21)
* As of 3/20/21: I'm able to finally connect to my teammate's MySQL database. I'm trying to understand why when I run EMADE as a worker, I'm getting NULL values in "Full Dataset False Positives" and "Full Dataset False Negatives" 
* 3/22/21: My team and I figured out how to get FPR and FNR. We used the formulas FPR = (FP)/(FP + TN) and FNR = (FN)/(FN + TP) to find FPR and FNR. The main problem we had when implementing our function was a small logical hiccup. Through help from Slack, we discovered that "Full Dataset False Positives" and "Full Dataset False Negatives" columns originally contain the average number of False positives and False negatives over 5 folds. This realization gave inspiration to modifying evalFunctions.py to find FPR and FNR 
* During our subteam 2 meeting on 3/21/21, we mainly worked on plotting Pareto frontier for various generations, let EMADE run for the past 24 hours and then work on our presentation

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update the titanic_splitter.py script with my team's preprocessing script for the training data in the Titanic dataset
|Completed
|March 17th, 2021
|March 22nd, 2021
|March 19th, 2021
|-
|Meet up with subteam to get EMADE working
|Completed
|March 17th, 2021
|March 22nd, 2021
|March 20th, 2021
|-
|Tune parameters in the input_titanic.xml file to construct Pareto Frontier
|Completed
|March 17th, 2021
|March 22nd, 2021
|March 22nd, 2021
|-
|Modify prior presentation, but tailor to existing students in VIP
|Completed
|March 17th, 2021
|March 22nd, 2021
|March 21st, 2021
|}


== March 10th, 2021==
'''Team Meeting Notes:'''
* Workday on getting EMADE to work

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get Remote connection to work on EMADE with MySQL Workbench
|Completed
|March 10th, 2021
|March 17th, 2021
|March 15th, 2021
|}
== March 3rd, 2021 ==
'''Team Meeting Notes:'''
* Introduction to what EMADE is conceptually
* Remaining groups presented ML vs. GP analysis on Titanic dataset
* Assignment: Run EMADE on Titanic Dataset as a group
* 3/10/21: I was able to do a remote connection to a SQL database with SQL Workbench. I was not connected to the VPN, so the IP address didn't work.

'''Subteam 2 Meeting Notes:'''
* Trying to set up EMADE
* My team struggled on trying to setup a remote SQL connection. We were able to run the EMADE command locally with localhost though.

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up Lettucemeet for Subteam 2 meeting
|Completed
|March 3rd, 2021
|March 10th, 2021
|March 3rd, 2021
|-
|Meet up with subteam to get EMADE working
|Completed
|March 3rd, 2021
|March 10th, 2021
|March 8th, 2021
|-
|Get remote connection to SQL database to work in MySQL workbench
|Completed
|March 3rd, 2021
|March 10th, 2021
|March 10th, 2021
|}



== February 24th, 2021 ==
'''Team Meeting Notes:'''
* Discussion about each subteam's presentation
* Transitioning into using EMADE, so main task was to set up EMADE

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install EMADE
|Completed
|February 24th, 2021
|March 3rd, 2021
|February 27th, 2021
|}
== February 17th, 2021 ==
Link to my notebook for Titanic Dataset is [https://colab.research.google.com/drive/1skZeUoJzooTEnosJMEb-XrPmljV_LzVb?usp=sharing here]

Link to Subteam 2 page is [https://vip.gatech.edu/wiki/index.php/Group_2 here]

'''Team Meeting Notes:'''
* Discussion about various techniques the subteams used when doing the assignment from week of February 10th
* '''New Team Assignment: '''Instead of manually creating the Pareto frontier with 5 or so ML models, we can use multi objective genetic programming to create a Pareto frontier in an automated. Using the same preprocessed data for the Titanic Dataset from the week of February 10th, we get to build our own genetic programming algorithm using DEAP (but not stuff from the algorithms module in DEAP). 

'''Subteam 2 Notes:'''
* Met up Saturday Feb 20th to build basic working pipeline of Multi Objective genetic programming algorithm for Titanic Dataset. We now have a working pipeline and the task at hand is to get the Area under the curve as low as possible and play around with various selection methods
* We did not use tournament selection as that method of selection apparently did not give good results, so we are exploring the DEAP documentation for other viable selection methods
* We met up Tuesday Feb 23rd to refine our GP pipeline. A major problem I detected was that whenever I would try to adjust the toolbox and re-run the evolutionary algorithm, it seemed like the FPR and FNR values were changing. The cause of this bug was that the best individual/primitive tree was always one column and that dominated all the other trees. The fix I did was to change the evaluation function to now use thresholding to decide on 0 or 1. I used a hyperbolic tangent function to get the value of the tree (value of the function) in between 0 and 1 and then apply a condition to say when to assign label 0 vs. label 1.
* We finalized our submission and decided to use one of my teammate's notebook because she got a good Area Under Curve. We are putting these visuals along with the best individual and FPR/FNR metrics into our slide deck. 
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up Lettucemeet for Subteam 2 meeting
|Completed
|February 17th, 2021
|February 24th, 2021
|February 17th, 2021
|-
|Meet up with group to build basic working genetic programming algorithm
|Completed
|February 17th, 2021
|February 24th, 2021
|February 21st, 2021
|-
|Tune DEAP model hyperparameters/pipeline
|Completed
|February 17th, 2021
|February 24th, 2021
|February 23rd, 2021
|-
|Research MOGP in context of train/test set
|Completed
|February 17th, 2021
|February 21st, 2021
|February 21st, 2021
|-
|Work on my slide about Logistic Regression
|Completed
|February 20th, 2021
|February 24th, 2021
|February 23rd, 2021
|}
== February 10th, 2021 ==
'''Team Meeting Notes:'''
* Continued our discussion on multiple objectives
* Discussion about team project for bootcamp
* Pareto ranks were used to select team leads and bootcamp participants were placed into subteams. I'm team lead for subteam 2
* '''Team Assignment: '''We are delving into Machine Learning and taking a break from Genetic Programming. We are using the Titanic Dataset to create Pareto codominant Machine Learning models (one model per individual in team). After creating the Pareto codominant Machine Learning models, we will see which subteam dominated

* '''Codominant Algorithms: '''Algorithms where one doesn't dominate another on all objectives. For the purposes of the Titanic Dataset, we are trying to classify survivors on the Titanic and want to minimize the objectives of FPR and FNR. 

'''Subteam 2 Notes:'''
* Already set up Slack subchannel for subteam 2 to set up a meeting to preprocess the dataset.
* Met on Saturday Feb 13th, 2021
* Started development of pre-processing the titanic data
* Key Insights:
    - The Ticket Number, Passenger ID shouldn't be factors that decide a passenger's survival
    - While the passenger's name doesn't directly determine survival, my subteam extracted the title of a person's name (eg: Mr. Miss., Master.) and used 
    that to fill in the missing ages using median (imputation)
    - The port that a passenger Embarked from seemed to have a skewed distribution (in favor of port S) and my teammates decided to drop the "Embarked" 
    column in hopes to minimize overfitting
    - One hot encoded the gender/sex of a passenger so that we have numerical entries for a model

* Sunday Feb 14th, 2021 Meeting Notes: 
    - Developed generalized pre-process function for Titanic Dataset and applied it to both train.csv and test.csv files (encompassed our observations from 
    Saturday Feb 13th, 2021)
    - Corrected a small problem with pre-processing test.csv with the general pre-process function (imputed 1 missing entry for "Age" and 1 missing entry 
    for "Fare")
    - Assigned model for each person in the team to train and consistently update the team about (FPR, FNR) performance
* Our resulting Pareto frontier can be found at [https://vip.gatech.edu/wiki/index.php/Group_2 this link]
* My Google Colaboratory Notebook: [https://colab.research.google.com/drive/1skZeUoJzooTEnosJMEb-XrPmljV_LzVb?usp=sharing link]

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up Lettucemeet for Subteam 2 meeting
|Completed
|February 10th, 2021
|February 17th, 2021
|February 10th, 2021
|-
|Set up Google Colaboratory Notebook for Subteam in order to start analyzing Titanic Dataset
|Completed
|February 10th, 2021
|February 17th, 2021
|February 13th, 2021
|-
|Meet up with group to preprocess dataset
|Completed
|February 10th, 2021
|February 17th, 2021
|February 14th, 2021
|-
|Tune Logistic Regression model hyperparameters
|Completed
|February 13th, 2021
|February 17th, 2021
|February 16th, 2021
|-
|Finalize pre-processing pipeline
|Completed
|February 13th, 2021
|February 14th, 2021
|February 14th, 2021 
|}
== February 3rd, 2021 ==
'''Team Meeting Notes:'''
* Multiple Objectives: Evaluation function maps individual to a set of scores
* Evaluation of Genome:
    - TP: how often are we identifying desired object
    - FP: how often are we identifying something else as desired object
    - More? (maybe memory usage, runtime, etc)
* Confusion Matrix: Matrix containing information about TP, FP, TN, FN
[[files/Confusion_matrix_figure.jpg]]

* Generally in Genetic Programming, we want to minimize the False Positive Rate, False Negative Rate. We can think of this problem
as finding the point (FPR, FNR) that is closest to the origin. Each individual's set of scores would be mapped to such a space

* '''Pareto Optimal: '''An individual is Pareto optimal if there is no other individual that outperforms the individual on all objectives (eg: a person who is smart and has brighter eyes outperforms a person who is not as smart and has duller eyes. The smart, brighter eye person would be considered Pareto Optimal)
   - '''Why Pareto Optimal:''' By identifying the Pareto optimal individuals, we can construct the set of all Pareto optimal individuals as some sort of 
   frontier/barrier
[[files/pareto_optimal_explanation.png|200px|thumb|right|pareto optimal individuals]]
* NSGA II: 
   - population sorted into "nondomination ranks"
   - individuals selected with binary tournament ("randomly pull out two individuals, compare")
   - Lower Pareto ranks beat higher Pareto ranks
     (take pareto optimal, eliminate them, find next pareto optimal, repeat)
   - if you pull out individuals from same Pareto rank, use crowding distance to break ties (try to explore unexplored areas) --> sum of euclidean distance 
   to all points on the same rank 
   - '''Why the above tiebreaker method?''' We want to ensure diversity when constructing the parents who mate to produce the next generation offspring. By using "larger Euclidean distance", NSGA II is able to also select individuals from "unexplored regions"

* SPEA 2:
   - each individual given strength S
       - S is how many others in the population the indivdiual dominates
   - each individual receives rank R
       - R is sum of S's of individuals that dominate it
       - Pareto indivduals have rank of zero (sum of null set is zero)
   - Tiebreaker: distance to kth nearest neighbor
   Rank is R + 1/(kth nearest distance + 2) --> larger the distance --> smaller than fraction

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review Notes from Bootcamp
|Completed
|February 3rd, 2021
|February 10th, 2021
|February 8th, 2021
|-
|Complete Lab 2 from Bootcamp (2nd half: Multi Objective functions)
|Completed
|February 3rd, 2021
|February 10th, 2021
|February 8th, 2021
|- 
|Complete Self Evaluation Rubric for Notebook
|Completed
|February 3rd, 2021
|February 10th, 2021
|February 9th, 2021
|}

'''Multi Objective Lab Notes:'''
* In the Symbolic regression problem from last week, we focused on generating individuals using a primitive tree and had mean square error be the objective function. In this lab, mean square error along with the size of the individual are two objectives to minimize.
* We also define a Pareto dominance function to see if individual 1 dominates individual 2
* We then initialize a population to run the genetic algorithm on and using DEAP's Mu Plus Lambda Evolutionary algorithm, we are able to generate a Pareto
frontier and boil the problem down into minimizing the area under the Pareto frontier. 
* '''Why Minimize the Area Under the curve?''' As the area approaches zero, the Pareto frontier would be a curve that is closer to the axes and there would be a higher probability of finding individuals that minimize on both objectives.
* The challenge now is to modify the parameters and find a way to reduce the Area Under the Curve of the Pareto frontier

* What I did was reduce the probability of mutation to 0.01 and I was able to get approximately a 28% decrease in the AUC (from AUC of 2.38 to 1.72)
* You can take a look at my experiment at [https://colab.research.google.com/drive/18fQi0JT3QRNYYeWP1ZvmZB7GyecqWvJJ?authuser=3#scrollTo=YLSc43HpuYdL this link]
[[files/auc_before.png|Before]]
[[files/auc_after.png|After]]

[https://drive.google.com/file/d/1Z_21Ztty8DrlBON-5Q8rAmkcvUZXxn6x/view?usp=sharing Self Evaluation Rubric]

== January 27th, 2021 ==
'''Team Meeting Notes:'''
* Intro to Genetic Programming (last week was mainly a high level overview on motivation for genetic algorithms)
* Question from Week 1: How do we quantify diversity in a population with genetic algorithms?
    - '''genotypic diversity: '''how similar "genotype" looks ("DNA representation of individual")
    - '''phenotypic diversity: '''how similar in "apperance/looks" (eg: in N-queens, how similar a placement of queens on one chessboard compares to another placement of 
    queens on another chessboard). This could be done through objective scores

* Instead of taking individuals and evaluating function to get objective scores, we treat the individual as the function
    - input data --> individual --> output data --> evaluator --> objective score. Goal is to find the function and we can do that through genetic programming

* Tree structure:
    - Nodes: primitives (represent functions)
    - Leaves: terminals (parameters)
        - input: particular type of terminal
        - output: produced at the root of the tree
Example: 
[[files/Gp_tree_example.png]]

f(X) for above tree is 1 + (x*y)

To write the above function in lisp notation, simply run a preorder traversal and enumerate the entries in a list.
eg: f(X) = 1 + (x*y) can be written as [+,1,*,x,y]


Crossover in GP (single point crossover):
select random point, exchange subtrees

An example of crossover can be seen on the right.
[[files/Crossover_tree.png|right|thumb|522x522px]]

'''Mutation in GP:'''
   - insert node or subtree
   - delete a node or subtree
   - change node
   - Try to add genotypic diversity to population


'''Example: symbolic regression'''
'''Problem: '''evolve a solution to y = sin(x)

Primitives: +, *, -, /

Terminals include integers and a variable "x"

Using Taylor series, we form a polynomial approximation to sin(x)

sin(x) = x - x^3/3! + x^5/5! + ... 

'''Evaluate''' tree (let's say for the cubic approximation): feed a bunch of input points (x = [0,2pi])

'''Run''' f(x) (say f(x) = x - x^3/3!)

'''Measure''' outputs between truth using squared error sum ((output - real)^2). This is the 

'''objective function''' to minimize


* What primitives could make life easier? power, factorial, trig functions, etc

* EMADE: automate the idea of algorithm design. Make evolution as easy as possible

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review Lecture 2 Slides from Bootcamp
|Completed
|January 27th, 2021
|February 3rd, 2021
|February 2nd, 2021
|-
|Complete Lab 2 from Bootcamp (Up to Multi-objective functions)
|Completed
|January 27th, 2021
|February 3rd, 2021
|February 3rd, 2021
|}

'''Symbolic Regression Lab:'''
* Individuals are now derived from a Primitive tree containing a set of basic operations we define. 
* To generate the evaluation of an individual (objective score), we compile the primitive tree into a function and find the error
* Instead of trying to fit a function, we are trying to generate an output given that we know the optimal function. We want to find the '''best primitives''' given objectives we are trying to minimize or maximize
* Currently running the evolutionary algorithm and used graphviz library to visualize the "optimal primitive tree". Below is an example of what I was able to visualize. [https://colab.research.google.com/drive/18fQi0JT3QRNYYeWP1ZvmZB7GyecqWvJJ?authuser=1#scrollTo=8nomUnqTuYdF link to code]
[[files/primTree.jpg]]


* I was able to get the results to improve on the symbolic regression as of February 2nd 2021 by increasing the chance of crossover. Consequently, the minimum value of the squared error was able to approach zero much quicker in fewer generations. Link to my notebook is [https://colab.research.google.com/drive/18fQi0JT3QRNYYeWP1ZvmZB7GyecqWvJJ?authuser=1#scrollTo=8nomUnqTuYdF here]. Below is a graph of the output I'm getting

[[files/SymbolicReg.png]]
== January 20th, 2021 ==
'''Team Meeting Notes:'''
* Introduction to the basics of genetic algorithms. Motivation for applicability of genetic algorithms in developing novel algorithms in an automated fashion
* '''Key Vocab:'''
   * '''Genetic Algorithm: '''Algorithm inspired by "Darwin's Natural Selection" principle to find optimal solutions in an automated way. 
   * '''Individual: '''One specific candidate in population (usually one possible solution out of multiple possibilities)
   * '''Population: '''Group of individuals
   * '''Objective Function/Evaluation Function: '''Mapping of individual to some sort of score to help evaluate fitness
   * '''Fitness: '''Loosely put, how an individual's score compares to the rest of the population (kind of like percentile on a test)
   * '''Selection: '''Process of selecting the "best fit" individuals to "mate" for the next generation

* Possible methods of selection
   * '''Fitness Proportionate: '''Greater the fitness value, the more likely a given individual is to be selected for mating (can be thought of like bigger portion on roulette wheel)
   * '''Tournament: '''Randomly pull individuals from population. Individuals with better scores move on

* '''Crossover: '''Fusing the "genomes" of two individuals to make a new individual (eg: merging certain part of list in individual 1 genes with certain part of list in individual 2 genes)

* Genetic Algorithm Basic Heuristic
   * 1.Initialize Population
   * 2. Determine fitness of population
   * 3. Repeat
         * a) select parents based on objective scores/fitness
         * b) perform crossover to create children
         * c) Induce mutation to preserve diversity (kind of like how genetic mutations help to increase biodiversity)
         * d) determine fitness
Below is a nice example of genetic algorithms in action!
[[files/one-max.jpg|left|thumb|522x522px]]
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install DEAP library
|Completed
|January 20th, 2021
|January 27th, 2021
|January 20th, 2021
|-
|Review Lecture 1 Slides from Bootcamp
|Completed 
|January 20th, 2021
|January 27th, 2021
|January 21st, 2021
|-
|Complete Lab 1 from Bootcamp
|Completed
|January 20th, 2021
|January 27th, 2021
|January 22nd, 2021
|}








'''OneMax Problem'''
* Objective/End Goal: Get a bit string of all 1s with a given length
* Evaluation Function: The number of ones in the string (want to maximize this)
* Individual: A list of 1s and 0s of given length
* Crossover: Two point Crossover
* Mutation: Point mutation (flip a randomly selected bit)
* Selection: Tournament Selection

We were able to achieve the optimal solutions within 40 generations, but if we re-run the code several times, sometimes, we might not acheive the optimal solution within 40 generations. 

'''N-Queens Problem'''
* Description: Given an NxN chessboard, how can we place N queens such that the queens cannot attack each other
* Evaluation Function: Count the number of conflicts/potential attacks (want to minimize this)
* Individual: A configuration/placement of queens on the board
* Crossover: Able to experiment with Partially Matched Crossover and Two Point Crossover
* Mutation: Shuffle Indices. I also experimented with inversion mutation
* Selection: Tournament Selection

I'm currently working on how to acheive minimum of zero in less than 100 iterations, but have a consistent optimal solution. 

As of 1/22/21: I was able to reduce the number of generations needed to converge to the minimum of zero with adjusting the mate probability and mutation probability while retaining the original mutation function of shuffling indices. I'm still trying to see if I can acheive a consistent optimal solution. You can view my progress at the following [https://drive.google.com/file/d/17_5vGhrW-buJDl2YajhrZ56z-DXZiYj_/view?usp=sharing link]. Below is a sample visual depicting my attempt to tune the parameters for faster convergence

[[files/viz.jpg|522x522px]]