== Team Member ==
[[files/GeorgiaTechBuzz.jpg|thumb|123x123px]]
Team Member: Karthik Subramanian

Email: ksubramanian40@gatech.edu

Cell Phone: 908-240-0310

Interests: Machine Learning, Python, Puzzles, Reading, Cooking
== September 6th, 2021==
'''Main Meeting Notes:'''
* Discussing ideas for problems to tackle during the Fall 2021 semester for NLP Subteam
* NLP subteam seemed to agree on trying to use EMADE to build a QA system (Question/Answer system). Given some context and a question, the QA system should output a matrix of probabilities of the keyword answering the question
* Dr. Zutty hopped into our Bluejeans call and we told him the idea we wanted to do for the NLP subteam. One important takeaway is that EMADE is much more helpful for supervised learning type problems and based on the dataset we found (Squad dataset is what SOTA QA systems use), Dr. Zutty seemed to like our idea and the problem seems doable in EMADE

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run the Amazon Dataset in EMADE from last semester (Spring 2021) in PACE
|Completed
|September 8th, 2021
|September 13th, 2021
|September 11th, 2021
|-
|Read up on QA systems and get a feel for how they work (look at diagrams, literature, etc)
|In Progress
|September 8th, 2021
|September 13th, 2021
|
|}

'''Individual Notes:'''
* Started to look into how the SqUAd dataset works. This article https://towardsdatascience.com/the-quick-guide-to-squad-cae08047ebee was very helpful
* Starting a brief exploration of the dataset to get a feel for it

== August 30th, 2021 ==
'''Main Meeting Notes:'''
* Discussing ideas about possible subteams and problems to tackle for this semester
== April 30th, 2021 - Final Presentation for Semester == 
'''Presentation Notes:'''
'''Stocks Team:'''

* How can EMADE help to analyze stock market trends?

* Objectives
   * Implement TA-Lib indicators in EMADE
   * Increase "Evolvability of EMADE individuals" (prevent individuals from getting stuck at particular generations)
   * Large dataset to improve performance?
   * Statistical evaluations of individuals

* Background
   * Piecewise Linear representation to represent price in linear segments
   * EMADE individuals predicting trading value derived from PLR
   * Exponential Smoothing gives threshold for when to do buy/sell prediction

* Current work
   * Technical Indicator primitives (transition to TA-lib) to increase reliability and reduce risk of miscalculation
   * Incorporating larger datasets to optimize   

   * TA-Lib indicator primitives include:
       * ADX (time series strength of particular trend)
       * VMMA (Volume weighted moving average) - Price weighted based on volume of trading activity

* Objectives in the EMADE Runs 
   * profit percentage (maximize)
   * average profit per transaction (maximize)
   * variance profit per transaction (minimize)
   * normal CDF on Distribution (minimize)

* 2nd Run
   * Same as first run, but remove objective on profit percentage

* Monte Carlo Simulation as an objective
   * Random buy and sell stocks
   * Create a gaussian with mean and standard deviation of profits
   * CDF better captures performance on trending stocks as opposed to absolute profit percentage
   
* Buy Hold Comparison
   * Compare profit to buy hold strategy when trading a stock

* Future Work
   * Statistical analysis of seeding types
   * Bounded objective functions
   * Use Fundamental analysis (underlying value of company independent of market price) 
   * hourly/daily/weekly trading frequencies (does EMADE give different levels of effectiveness)


'''ezCGP'''
* Node reusability (structure not bounded by restriction on tree)
* Block structure

* Objective
   * Improve Neural Architecture search
   * Recreate CIFAR-10 results without relying on transfer learning
   * Improve ability to visualize genomes
   * Research, develop, test new mating methods for Cartesian Genetic Programming

* Individual Size (Experiment)
   * Validate depth of neural architectures generated (4-5 layers)
   * Initial population matched target distribution
   * Larger individuals performed worse (more room for parameters to break/not perform well)

* Activation Function (Experiment)
   * Algorithm was selecting and mixing algorithms between convolution layer
   * Limit architecture to single activation (only RELU) --> no significant improvement in performance of best individual

* Max Pooling and Dropout Layer (Experiment)
   * Analyze impact of primitives compared to midterm benchmark
   * Tune probability of each type of primitive

* Dense Layers (Experiment)
   * Would evolving dense layer improve performance?

* Visualization
   * Built easy to use CLI (command line interface) to visualize any number of individuals
* Crossover methods in CGP
* Meta Parameter (new way to run ezCGP so that we can easily run different iterations of the same problem while making small changes)

* Future Work
   * Research, develop, test new mating methods
   * Incorporate existing CNN architecture to develop new genome seeds

'''NLP Final Presentation Feedback'''
* When we do Auto ML problems, good to partition the train set for ourselves so that the search is over a portion of the training data (use the standard test set to be validation)
* Consider (False Positive Rate, False Negative Rate, Num Parameters) as objective space


'''Modularity:'''
* Exploring ways to abstract parts of individuals
(create building blocks that can help with the genetic process)

* Primitives can be ML models instead of simple mathematical operations

* ARL: Adaptive Representation through learning (once a good section of tree is found, make it a new primitive)

* Increase complexity of ARL (increase tree depth and allow imbalanced trees)

* Titanic and MNIST datasets used for ARL testing

* Potential Objective functions
   * Precision score, Recall score, F1 score, Accuracy score, Cohen Kappa Score

* Titanic Datasets Objectives: FPR, FNR

* MNIST Run: seeded file comprised of 30 valid individuals obtained in previous MNIST EMADE runs
(got more than 316 valid individuals by gen 50)

* Future Work
   * Deep ensemble models
   * Modify evolutionary selection method and help diversify ARLs
   * Diversity measure
   * Integrate ARLs with EMADE's original architecture

'''Individual Reflection:'''
One major thread I seemed to notice with the presentations was discussion on the objective function used. From the presentations shown, for next semester, I hope to critically look into the objective functions used and see how that affects the type of individuals generated and the performance attained.

== April 26th, 2021==
'''Team Meeting Notes:'''
* Final Presentations April 30th

'''Subteam Meeting Notes:'''
* Prepare slides for final presentation
* Dr. Zutty commented on latency aspect along with evaluation time of 
individuals in auto ML
(some individuals might be time consuming to evaluate whereas other
individuals could be so bad that it probably won't make it to the next generation)

* Queue size in EMADE designed such that once the queue size drops to a 
certain level, next generation kicks off (we don't want workers to idle while
the master is prepping the next generation)
* Plan on doing a rehearsal for the presentation 4/29/21 6pm

'''Individual Reflection:'''
* I'm currently doing a seeded EMADE run in PACE with no GPU set up/in use (that might have caused my run/job to be stuck in the queue). The run will evaluate on
2 objective functions (FPR and FNR) 

* Plan is for me to visualize the pareto front

* UPDATE 4/27/21: My approximately 8 hour run has finished today morning (I ran with no GPU usage and set the wall time to 8 hours so that the SQL connection timeout happens later). I visualized the pareto front through uploading the master.out file into a visualizer script created by one of the teammates. Below is the Pareto Front I'm getting 
[[files/Paretopace.PNG]]


* 4/28/21: I did another seeded run for 8 hours of EMADE in PACE with False Positive Rate and False Negative Rate as objective functions. Below is the pareto front I got along with the individuals in the latest generation (gen 21 in this case)
[[files/paretoTwo.png]]

The individuals I got from the Pareto Front in generation 21 is the following (with the help of Nishant in setting up/configuring MySQL Workbench to query the pareto front):

*       NNLearner(ARG0, OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, EmbeddingLayer(99, ARG0, randomUniformWeights, InputLayer())))), 90, AdamOptimizer)
*       NNLearner(ARG0, OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, EmbeddingLayer(90, ARG0, randomUniformWeights, InputLayer())))), 89, AdamOptimizer)
*       NNLearner(ARG0, OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, EmbeddingLayer(96, ARG0, randomUniformWeights, InputLayer())))), 94, AdamOptimizer)
*       NNLearner(ARG0, OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, EmbeddingLayer(98, ARG0, randomUniformWeights, InputLayer())))), 98, AdamOptimizer)
*       NNLearner(ARG0, OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, EmbeddingLayer(92, ARG0, randomUniformWeights, InputLayer())))), 92, AdamOptimizer)
*       NNLearner(ARG0, OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, EmbeddingLayer(93, ARG0, randomUniformWeights, InputLayer())))), 91, AdamOptimizer)
*       NNLearner(ARG0, OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, EmbeddingLayer(94, ARG0, randomUniformWeights, InputLayer())))), 93, AdamOptimizer)
*     NNLearner(ARG0, OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool, EmbeddingLayer(100, ARG0, randomUniformWeights, InputLayer())))), 100, AdamOptimizer)
*                                                                                NNLearner(ARG0, OutputLayer(EmbeddingLayer(98, ARG0, randomUniformWeights, InputLayer())), myIntMult(passInt(50), 3), FtrlOptimizer)

Looking at the evaluation gen of the individuals in the pareto front, only one indivdiual was evaluated in generation 12 (the last individual from above) while the other individuals were evaluted in generation 0. It seems like the vast majority of individuals in the pareto front were coming from the seeded individuals to begin with, so I might test an unseeded 
run of EMADE in PACE with FPR and FNR objectives.

In terms of the final presentation, I worked on adding slides about FPR/FNR runs along with some run results. I used Nishant's modified visualizer script to better understand the evolution of my Pareto front from the past runs. [https://colab.research.google.com/drive/1COC0IcnuUyjiBiimgyp7xgTB5r24OusY#scrollTo=iVlkPZs_qf1d Link to the modified visualization script can be found here]

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get in 1 run of FPR/FNR in PACE
|Completed
|April 26th, 2021
|April 28th, 2021
|April 27th, 2021
|-
|Add my out file data in Google Drive folder for presentation Cameron provided
|Completed
|April 26th, 2021
|April 28th, 2021
|April 27th, 2021
|-
|Add slides about FPR/FNR runs in the final presentation with Nishant
|Completed
|April 28th, 2021
|April 29th, 2021
|April 28th, 2021
|}

== April 19th, 2021==
'''Team Meeting Notes:'''
* Peer evals came out
* Final presentation April 30th 6pm - 8pm
* Dr. Zutty suggested us to use FPR/FNR as objective functions instead of accuracy

'''Subteam Meeting Notes:'''

* learner() core function: all target values from training data overwritten by what
model predicted on itself --> scoped in one individual

(put target feature as an input feature)


* Try using FPR, FNR, numParameters as objective functions instead of accuracy (2-4 objectives should be good)
for Amazon Product Dataset Binary Classification


* Is switching to FPR, FNR better than just using accuracy score?

* Probably should reduce size of training set to not eat memory (standardize across runs)

* (clear database before seeding) --> Cameron changed the seeding file to make seeded run easier

* repull the branch from Github and change what's on PACE

* Cameron showed how to seed before running EMADE
(in LAUNCH script, there is a "seedAmazon.pbs" script). Running seeding file from input_amazon.xml

* Do '''seeded runs'''

'''April 23rd Subteam Meeting:'''
* prepping for final presentation
(dial in and talk about final results)

* Most likely going to have code freeze Wednesday ("deadline") at noon

* Goal: get a run of EMADE in PACE with FPR, FNR

* Start planning creation of final presentation

'''Individual Reflection:'''
* Was able to clone the updated branch of nn-vip and update the seeding file provided for the amazon dataset
* I configured input_amazon.xml file to run with 3 objectives (FPR, FNR, Number of elements) with the help of the evalFunctions.py file
* When I ran the seeding file in PACE along with the mysql job, it is showing that my jobs were marked as "Closed" and I'm trying to resolve this issue with the team
* I had a SQL database connection issue that I've been stuck on. I resolved this issue by re-doing the granting of privileges
* I currently am running a seeded EMADE run in PACE. 


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Peer Evaluation
|Completed
|April 19th, 2021
|April 27th, 2021
|April 20th, 2021
|-
|Seeded run with FPR/FNR Objective Functions in PACE 
|Completed
|April 19th, 2021
|April 26th, 2021
|April 26th, 2021
|}


== April 12th, 2021==
'''Team and Subteam Meeting Notes:'''

* Steven completed a run and presented results

* Dr. Rohling gave suggestions on use of false positive rate and false negative rate

* Picking tasks to work on

'''Main problems:'''

* Evolution:
   - NNLearners easily fail, can't get very big
   - slight architecture changes leading to failure
   - Delve into evolution problem and trying to make individuals more complex
   - Tackle problem of "trivial individuals"



* Pretrained Embedding Layers:
   - Editing input size to vocab size 
   - can't change input layer to size we want

* NNLearners as subtrees
   - Learners can take outputs of other learners as a feature (similar to ensemble learning)

'''NLP Subteam Meeting Notes:'''

* Cameron working on a pull request related to PACE (functionality to have many workers on a job at PACE)


* Cameron gave a presentation about the NNLearners class in the codebase

'''NNLearners class: '''
* Neural_network_methods.py (main driver file):

* Neural Architecture Search is main objective of NLP subteam

*NNLearner
  - Takes in arg0 (dataset) --> has testing/training set , batch size, optimizer, output layer (neural network methods layer list)

  - Layer list is a python list that holds a bunch of TF/Keras layers 
     - eg: ["input", other layers, .... "output"]
  
  - out_dim: Keras can help connect of the layer (helps build the network)
  - layerList: list of other primitives (create the layer and add it to the list)



  - Core method for neural nets

  - add_layerlist() function: build the neural network recursively accounting for subtrees


"Evolution" Problem Cameron was describing: more of a "NNLearner" thing perhaps. 

'''My Reflection:'''
* For this week, I've been mainly trying to familiarize myself with the problem of trying to find/evolve non-trivial individuals in the context of the Amazon Dataset. 
I'm starting out by running EMADE in PACE and then trying to use Maria DB to query the individuals to wrap my head around the problem

* I think I have a baseline (I might pick up more as I interact with neural_network_methods.py file) understanding of the "NNLearner" function in neural_network_methods.py. Looking through the code, it seems like NNleanrer function's main responsibility is to take the layers from the running layerlist and build the neural network to populate individuals as a parameter to EMADE.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look through neural_network_methods.py file
|Completed
|April 12th, 2021
|April 18th, 2021
|April 18th, 2021
|-
|Practice querying data after running EMADE in PACE
|In Progress
|April 12th, 2021
|April 18th, 2021
|
|}
== April 5th, 2021==
'''Team Meeting Notes:'''
* Dr. Zutty gave a helpful talk/presentation on hypothesis testing
* The underlying idea with hypothesis testing is that we want to see if what we observe is statistically significant to our expected observation
* Two statistical tests to try to incorporate would be Student's t-test and Welch's t-test. Welch's t-test is useful if we don't know our expected observation
* p-value and significance level help to determine if we should reject or fail to reject our null hypothesis

[[files/t-test.png|t-test visual depiction]]

'''Subteam Meeting Notes:'''
* Main objective for new students is to get PACE setup and follow Cameron's video on how to get PACE working
* I was mainly working on PACE setup for the past week with Cameron's video. I had issues with running EMADE in PACE with `qsub pbsmysql.pbs` and `qsub launchEMADE_amazon.py` and realized that I needed to setup my MySQL database and `qdel` the jobs I was previously running.
* Subteam meeting happened on 4/9/21 and main discussion was on ML and NLP
nlp subteam meeting notes:

'''Presentation Notes from 4/9/21 meeting'''
- Supervised vs. unsupervised learning
- Supervised learning: (you have some input/feature data with a label). Learn function that maps inputs --> label
- Loss function: How far is your trained model from the true function
  (RMSE: root mean square error is a popular loss function)


NN: 

Input --> Layer 1 --> Layer 2 --> Output
(each layer does stuff to data points and that result goes to next layer)


What's in a layer?

input --> operation --> Activation

(operation: actual computation on data)
(Activation: introduce non-linearity into the model for complexity)

    - Popular ones: Sigmoid, ReLU
    - Sigmoid: bad for backprop because weights don't change if gradient is zero (squish input to 0 and 1)
      (expensive to compute b/c exponentials) -> output not "zero-centered"
    - Vanishing gradient: gradient approaches zero when input is very positive or very negative
   
    - ReLU: Super cheap to compute (works much better in practice)
         - Cons: sometimes neurons can "die" (large gradients can cause weights to update so that neuron never fires) --> use learning rate


Train Neural Network:
- Feed Forward: evaluate the model (go from input to output, compare output to expected label and compute loss function)
- Back Propagation: change layers (use gradients to update the weights)
- Learning rate: by how much are you changing the layers in each iteration
- Batch Normalization: When you are doing feed forward, using a single data point vs. using larger batches and sending that large batch at the same time


- Train/test/validation split
(train set: model learns from)
(test set: withheld data --> how to measure how well your model did)
(validation data - tell how your model is doing on withheld data during training process)


Why a model could do poorly?
- Overfitting (model overlearns from training data, but it can't generalize well to new data)
   - Possible fix: regularization (penalize complexity)
   - High bias
- Underfitting (high bias)
   - Model didn't do enough work
   - Rely too heavily on prior conceptions
- Healthy balance between underfitting and overfitting


Dense/Linear/Standard Layer:
- fully connected dense layer

Convolutional Layer:
- Relate data spatially
- "convolve/slide over all spatial locations"
- Sliding filter

Recurrent Layer:
- Have memory of previous input
- Very popular in NLP tasks
- Matrix of weights to train
- LSTM (Long-Short term memory) is a fancy Recurrent layer (remember or forget certain bits of info)


NLP Specific Concepts:
- Processing language (eg: speech recognition, translation, chatbots, etc)
- Our VIP: Sentiment Analysis (analyze Amazon Product Review dataset and classify review as positive or negative)
   - Balanced dataset (50/50 split on positive/negative reviews)

- Word Embeddings: convert text data to numbers
 (encode textual data in a meaningful way)

    - Word2vec, fasttext, GLoVe
    - Model relationship between words on massive datasets



'''Individual Notes:'''
* I mainly practiced running EMADE in PACE to get a feel for the environment I will be using
* For fun, I developed a short shell script to automate the logging in to PACE and running EMADE with the Amazon dataset. I presented it to the experienced members and they
thought that it would be a good addition to the repository. 
* [https://github.gatech.edu/cwhaley9/PACE-files Link to shell script to automate PACE RUN for Amazon dataset] 

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Practice running EMADE in PACE
|Completed
|April 5th, 2021
|April 9th, 2021
|April 7th, 2021
|-
|Develop Shell Script to automate running of EMADE on PACE with Amazon dataset (for fun and to get exposure to Bash)
|Completed
|April 5th, 2021
|April 9th, 2021
|April 7th, 2021
|-
|}
== March 29th, 2021==
'''Team Meeting Notes:'''
* I'm placed on the NLP subteam for this semester. The wiki has been down for this week, so I was not able to access the tutorial provided for setting up PACE on the NLP subteam page

'''Subteam Meeting Notes:'''
* Introduction to NLP subteam goals, challenges, tools used
* Friday April 2nd 6pm will be our new decided subteam meeting time from the whenisgood form in the nlp-nn Slack
* Need to set up the PACE environment and clone the nlp-nn branch from the EMADE repo
* April 2nd Meeting: Cameron gave a brief presentation on PACE and a basic overview of the nlp-nn branch in the EMADE Github. Main goal for me is to set up PACE on my laptop.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up PACE environment
|Completed
|March 29th, 2021
|April 3rd, 2021
|April 5th, 2021
|-
|}
== March 22nd, 2021==
'''Team Meeting Notes:'''
* Presented comparison of EMADE, MOGP, ML on Titanic dataset
* Listened to presentations from the existing subteams
* Dr. Zutty made a note on how it was interesting that my team modified the evalFunctions.py directly to calculate FPR and FNR for running EMADE on the titanic dataset
'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Fill out Canvas Poll to rank my subteam preference
|Completed
|March 22nd, 2021
|March 26th, 2021
|March 28th, 2021
|-
|}

== March 17th, 2021==
'''Team Meeting Notes:'''
* Setting up EMADE
* Common Debugging tips such as setting DEAP to version 1.2.2 and using GT VPN to connect to database
* Main issue I'm having on my end is trying to connect to my teammate's MySQL connection. I tried putting in the correct credentials, but I'm getting an error message in MySQL workbench saying "connection not successful". I'm still trying to resolve this issue on my end.
* As of 3/21/21, I was able to resolve the connection issue. Some existing students in the VIP recommended me to restart my laptop and create a fresh connection to the MySQL workbench server and that worked seamlessly for the most part on my end. 
* I'm currently trying to tune the mutation probabilities to see if there is a better Pareto frontier. As of 3/22/21, my team's database is able to find at least 20 individuals in the Pareto front at generation 25. We are currently updating our slides to reflect this update.

'''Subteam Notes:'''
* Subteam 2 for bootcamp met Friday March 19th and main focus of discussion shifted towards getting the preprocessing script we developed from the previous two weeks into EMADE
* We ran EMADE with our preprocessing script and fortunately, we were able to get to 25 generations (as of 3/20/21 when running EMADE from the 24 hour period of 3/19/21 - 3/20/21)
* As of 3/20/21: I'm able to finally connect to my teammate's MySQL database. I'm trying to understand why when I run EMADE as a worker, I'm getting NULL values in "Full Dataset False Positives" and "Full Dataset False Negatives" 
* 3/22/21: My team and I figured out how to get FPR and FNR. We used the formulas FPR = (FP)/(FP + TN) and FNR = (FN)/(FN + TP) to find FPR and FNR. The main problem we had when implementing our function was a small logical hiccup. Through help from Slack, we discovered that "Full Dataset False Positives" and "Full Dataset False Negatives" columns originally contain the average number of False positives and False negatives over 5 folds. This realization gave inspiration to modifying evalFunctions.py to find FPR and FNR 
* During our subteam 2 meeting on 3/21/21, we mainly worked on plotting Pareto frontier for various generations, let EMADE run for the past 24 hours and then work on our presentation

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update the titanic_splitter.py script with my team's preprocessing script for the training data in the Titanic dataset
|Completed
|March 17th, 2021
|March 22nd, 2021
|March 19th, 2021
|-
|Meet up with subteam to get EMADE working
|Completed
|March 17th, 2021
|March 22nd, 2021
|March 20th, 2021
|-
|Tune parameters in the input_titanic.xml file to construct Pareto Frontier
|Completed
|March 17th, 2021
|March 22nd, 2021
|March 22nd, 2021
|-
|Modify prior presentation, but tailor to existing students in VIP
|Completed
|March 17th, 2021
|March 22nd, 2021
|March 21st, 2021
|}


== March 10th, 2021==
'''Team Meeting Notes:'''
* Workday on getting EMADE to work

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get Remote connection to work on EMADE with MySQL Workbench
|Completed
|March 10th, 2021
|March 17th, 2021
|March 15th, 2021
|}
== March 3rd, 2021 ==
'''Team Meeting Notes:'''
* Introduction to what EMADE is conceptually
* Remaining groups presented ML vs. GP analysis on Titanic dataset
* Assignment: Run EMADE on Titanic Dataset as a group
* 3/10/21: I was able to do a remote connection to a SQL database with SQL Workbench. I was not connected to the VPN, so the IP address didn't work.

'''Subteam 2 Meeting Notes:'''
* Trying to set up EMADE
* My team struggled on trying to setup a remote SQL connection. We were able to run the EMADE command locally with localhost though.

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up Lettucemeet for Subteam 2 meeting
|Completed
|March 3rd, 2021
|March 10th, 2021
|March 3rd, 2021
|-
|Meet up with subteam to get EMADE working
|Completed
|March 3rd, 2021
|March 10th, 2021
|March 8th, 2021
|-
|Get remote connection to SQL database to work in MySQL workbench
|Completed
|March 3rd, 2021
|March 10th, 2021
|March 10th, 2021
|}



== February 24th, 2021 ==
'''Team Meeting Notes:'''
* Discussion about each subteam's presentation
* Transitioning into using EMADE, so main task was to set up EMADE

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install EMADE
|Completed
|February 24th, 2021
|March 3rd, 2021
|February 27th, 2021
|}
== February 17th, 2021 ==
Link to my notebook for Titanic Dataset is [https://colab.research.google.com/drive/1skZeUoJzooTEnosJMEb-XrPmljV_LzVb?usp=sharing here]

Link to Subteam 2 page is [https://vip.gatech.edu/wiki/index.php/Group_2 here]

'''Team Meeting Notes:'''
* Discussion about various techniques the subteams used when doing the assignment from week of February 10th
* '''New Team Assignment: '''Instead of manually creating the Pareto frontier with 5 or so ML models, we can use multi objective genetic programming to create a Pareto frontier in an automated. Using the same preprocessed data for the Titanic Dataset from the week of February 10th, we get to build our own genetic programming algorithm using DEAP (but not stuff from the algorithms module in DEAP). 

'''Subteam 2 Notes:'''
* Met up Saturday Feb 20th to build basic working pipeline of Multi Objective genetic programming algorithm for Titanic Dataset. We now have a working pipeline and the task at hand is to get the Area under the curve as low as possible and play around with various selection methods
* We did not use tournament selection as that method of selection apparently did not give good results, so we are exploring the DEAP documentation for other viable selection methods
* We met up Tuesday Feb 23rd to refine our GP pipeline. A major problem I detected was that whenever I would try to adjust the toolbox and re-run the evolutionary algorithm, it seemed like the FPR and FNR values were changing. The cause of this bug was that the best individual/primitive tree was always one column and that dominated all the other trees. The fix I did was to change the evaluation function to now use thresholding to decide on 0 or 1. I used a hyperbolic tangent function to get the value of the tree (value of the function) in between 0 and 1 and then apply a condition to say when to assign label 0 vs. label 1.
* We finalized our submission and decided to use one of my teammate's notebook because she got a good Area Under Curve. We are putting these visuals along with the best individual and FPR/FNR metrics into our slide deck. 
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up Lettucemeet for Subteam 2 meeting
|Completed
|February 17th, 2021
|February 24th, 2021
|February 17th, 2021
|-
|Meet up with group to build basic working genetic programming algorithm
|Completed
|February 17th, 2021
|February 24th, 2021
|February 21st, 2021
|-
|Tune DEAP model hyperparameters/pipeline
|Completed
|February 17th, 2021
|February 24th, 2021
|February 23rd, 2021
|-
|Research MOGP in context of train/test set
|Completed
|February 17th, 2021
|February 21st, 2021
|February 21st, 2021
|-
|Work on my slide about Logistic Regression
|Completed
|February 20th, 2021
|February 24th, 2021
|February 23rd, 2021
|}
== February 10th, 2021 ==
'''Team Meeting Notes:'''
* Continued our discussion on multiple objectives
* Discussion about team project for bootcamp
* Pareto ranks were used to select team leads and bootcamp participants were placed into subteams. I'm team lead for subteam 2
* '''Team Assignment: '''We are delving into Machine Learning and taking a break from Genetic Programming. We are using the Titanic Dataset to create Pareto codominant Machine Learning models (one model per individual in team). After creating the Pareto codominant Machine Learning models, we will see which subteam dominated

* '''Codominant Algorithms: '''Algorithms where one doesn't dominate another on all objectives. For the purposes of the Titanic Dataset, we are trying to classify survivors on the Titanic and want to minimize the objectives of FPR and FNR. 

'''Subteam 2 Notes:'''
* Already set up Slack subchannel for subteam 2 to set up a meeting to preprocess the dataset.
* Met on Saturday Feb 13th, 2021
* Started development of pre-processing the titanic data
* Key Insights:
    - The Ticket Number, Passenger ID shouldn't be factors that decide a passenger's survival
    - While the passenger's name doesn't directly determine survival, my subteam extracted the title of a person's name (eg: Mr. Miss., Master.) and used 
    that to fill in the missing ages using median (imputation)
    - The port that a passenger Embarked from seemed to have a skewed distribution (in favor of port S) and my teammates decided to drop the "Embarked" 
    column in hopes to minimize overfitting
    - One hot encoded the gender/sex of a passenger so that we have numerical entries for a model

* Sunday Feb 14th, 2021 Meeting Notes: 
    - Developed generalized pre-process function for Titanic Dataset and applied it to both train.csv and test.csv files (encompassed our observations from 
    Saturday Feb 13th, 2021)
    - Corrected a small problem with pre-processing test.csv with the general pre-process function (imputed 1 missing entry for "Age" and 1 missing entry 
    for "Fare")
    - Assigned model for each person in the team to train and consistently update the team about (FPR, FNR) performance
* Our resulting Pareto frontier can be found at [https://vip.gatech.edu/wiki/index.php/Group_2 this link]
* My Google Colaboratory Notebook: [https://colab.research.google.com/drive/1skZeUoJzooTEnosJMEb-XrPmljV_LzVb?usp=sharing link]

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up Lettucemeet for Subteam 2 meeting
|Completed
|February 10th, 2021
|February 17th, 2021
|February 10th, 2021
|-
|Set up Google Colaboratory Notebook for Subteam in order to start analyzing Titanic Dataset
|Completed
|February 10th, 2021
|February 17th, 2021
|February 13th, 2021
|-
|Meet up with group to preprocess dataset
|Completed
|February 10th, 2021
|February 17th, 2021
|February 14th, 2021
|-
|Tune Logistic Regression model hyperparameters
|Completed
|February 13th, 2021
|February 17th, 2021
|February 16th, 2021
|-
|Finalize pre-processing pipeline
|Completed
|February 13th, 2021
|February 14th, 2021
|February 14th, 2021 
|}
== February 3rd, 2021 ==
'''Team Meeting Notes:'''
* Multiple Objectives: Evaluation function maps individual to a set of scores
* Evaluation of Genome:
    - TP: how often are we identifying desired object
    - FP: how often are we identifying something else as desired object
    - More? (maybe memory usage, runtime, etc)
* Confusion Matrix: Matrix containing information about TP, FP, TN, FN
[[files/Confusion_matrix_figure.jpg]]

* Generally in Genetic Programming, we want to minimize the False Positive Rate, False Negative Rate. We can think of this problem
as finding the point (FPR, FNR) that is closest to the origin. Each individual's set of scores would be mapped to such a space

* '''Pareto Optimal: '''An individual is Pareto optimal if there is no other individual that outperforms the individual on all objectives (eg: a person who is smart and has brighter eyes outperforms a person who is not as smart and has duller eyes. The smart, brighter eye person would be considered Pareto Optimal)
   - '''Why Pareto Optimal:''' By identifying the Pareto optimal individuals, we can construct the set of all Pareto optimal individuals as some sort of 
   frontier/barrier
[[files/pareto_optimal_explanation.png|200px|thumb|right|pareto optimal individuals]]
* NSGA II: 
   - population sorted into "nondomination ranks"
   - individuals selected with binary tournament ("randomly pull out two individuals, compare")
   - Lower Pareto ranks beat higher Pareto ranks
     (take pareto optimal, eliminate them, find next pareto optimal, repeat)
   - if you pull out individuals from same Pareto rank, use crowding distance to break ties (try to explore unexplored areas) --> sum of euclidean distance 
   to all points on the same rank 
   - '''Why the above tiebreaker method?''' We want to ensure diversity when constructing the parents who mate to produce the next generation offspring. By using "larger Euclidean distance", NSGA II is able to also select individuals from "unexplored regions"

* SPEA 2:
   - each individual given strength S
       - S is how many others in the population the indivdiual dominates
   - each individual receives rank R
       - R is sum of S's of individuals that dominate it
       - Pareto indivduals have rank of zero (sum of null set is zero)
   - Tiebreaker: distance to kth nearest neighbor
   Rank is R + 1/(kth nearest distance + 2) --> larger the distance --> smaller than fraction

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review Notes from Bootcamp
|Completed
|February 3rd, 2021
|February 10th, 2021
|February 8th, 2021
|-
|Complete Lab 2 from Bootcamp (2nd half: Multi Objective functions)
|Completed
|February 3rd, 2021
|February 10th, 2021
|February 8th, 2021
|- 
|Complete Self Evaluation Rubric for Notebook
|Completed
|February 3rd, 2021
|February 10th, 2021
|February 9th, 2021
|}

'''Multi Objective Lab Notes:'''
* In the Symbolic regression problem from last week, we focused on generating individuals using a primitive tree and had mean square error be the objective function. In this lab, mean square error along with the size of the individual are two objectives to minimize.
* We also define a Pareto dominance function to see if individual 1 dominates individual 2
* We then initialize a population to run the genetic algorithm on and using DEAP's Mu Plus Lambda Evolutionary algorithm, we are able to generate a Pareto
frontier and boil the problem down into minimizing the area under the Pareto frontier. 
* '''Why Minimize the Area Under the curve?''' As the area approaches zero, the Pareto frontier would be a curve that is closer to the axes and there would be a higher probability of finding individuals that minimize on both objectives.
* The challenge now is to modify the parameters and find a way to reduce the Area Under the Curve of the Pareto frontier

* What I did was reduce the probability of mutation to 0.01 and I was able to get approximately a 28% decrease in the AUC (from AUC of 2.38 to 1.72)
* You can take a look at my experiment at [https://colab.research.google.com/drive/18fQi0JT3QRNYYeWP1ZvmZB7GyecqWvJJ?authuser=3#scrollTo=YLSc43HpuYdL this link]
[[files/auc_before.png|Before]]
[[files/auc_after.png|After]]

[https://drive.google.com/file/d/1Z_21Ztty8DrlBON-5Q8rAmkcvUZXxn6x/view?usp=sharing Self Evaluation Rubric]

== January 27th, 2021 ==
'''Team Meeting Notes:'''
* Intro to Genetic Programming (last week was mainly a high level overview on motivation for genetic algorithms)
* Question from Week 1: How do we quantify diversity in a population with genetic algorithms?
    - '''genotypic diversity: '''how similar "genotype" looks ("DNA representation of individual")
    - '''phenotypic diversity: '''how similar in "apperance/looks" (eg: in N-queens, how similar a placement of queens on one chessboard compares to another placement of 
    queens on another chessboard). This could be done through objective scores

* Instead of taking individuals and evaluating function to get objective scores, we treat the individual as the function
    - input data --> individual --> output data --> evaluator --> objective score. Goal is to find the function and we can do that through genetic programming

* Tree structure:
    - Nodes: primitives (represent functions)
    - Leaves: terminals (parameters)
        - input: particular type of terminal
        - output: produced at the root of the tree
Example: 
[[files/Gp_tree_example.png]]

f(X) for above tree is 1 + (x*y)

To write the above function in lisp notation, simply run a preorder traversal and enumerate the entries in a list.
eg: f(X) = 1 + (x*y) can be written as [+,1,*,x,y]


Crossover in GP (single point crossover):
select random point, exchange subtrees

An example of crossover can be seen on the right.
[[files/Crossover_tree.png|right|thumb|522x522px]]

'''Mutation in GP:'''
   - insert node or subtree
   - delete a node or subtree
   - change node
   - Try to add genotypic diversity to population


'''Example: symbolic regression'''
'''Problem: '''evolve a solution to y = sin(x)

Primitives: +, *, -, /

Terminals include integers and a variable "x"

Using Taylor series, we form a polynomial approximation to sin(x)

sin(x) = x - x^3/3! + x^5/5! + ... 

'''Evaluate''' tree (let's say for the cubic approximation): feed a bunch of input points (x = [0,2pi])

'''Run''' f(x) (say f(x) = x - x^3/3!)

'''Measure''' outputs between truth using squared error sum ((output - real)^2). This is the 

'''objective function''' to minimize


* What primitives could make life easier? power, factorial, trig functions, etc

* EMADE: automate the idea of algorithm design. Make evolution as easy as possible

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review Lecture 2 Slides from Bootcamp
|Completed
|January 27th, 2021
|February 3rd, 2021
|February 2nd, 2021
|-
|Complete Lab 2 from Bootcamp (Up to Multi-objective functions)
|Completed
|January 27th, 2021
|February 3rd, 2021
|February 3rd, 2021
|}

'''Symbolic Regression Lab:'''
* Individuals are now derived from a Primitive tree containing a set of basic operations we define. 
* To generate the evaluation of an individual (objective score), we compile the primitive tree into a function and find the error
* Instead of trying to fit a function, we are trying to generate an output given that we know the optimal function. We want to find the '''best primitives''' given objectives we are trying to minimize or maximize
* Currently running the evolutionary algorithm and used graphviz library to visualize the "optimal primitive tree". Below is an example of what I was able to visualize. [https://colab.research.google.com/drive/18fQi0JT3QRNYYeWP1ZvmZB7GyecqWvJJ?authuser=1#scrollTo=8nomUnqTuYdF link to code]
[[files/primTree.jpg]]


* I was able to get the results to improve on the symbolic regression as of February 2nd 2021 by increasing the chance of crossover. Consequently, the minimum value of the squared error was able to approach zero much quicker in fewer generations. Link to my notebook is [https://colab.research.google.com/drive/18fQi0JT3QRNYYeWP1ZvmZB7GyecqWvJJ?authuser=1#scrollTo=8nomUnqTuYdF here]. Below is a graph of the output I'm getting

[[files/SymbolicReg.png]]
== January 20th, 2021 ==
'''Team Meeting Notes:'''
* Introduction to the basics of genetic algorithms. Motivation for applicability of genetic algorithms in developing novel algorithms in an automated fashion
* '''Key Vocab:'''
   * '''Genetic Algorithm: '''Algorithm inspired by "Darwin's Natural Selection" principle to find optimal solutions in an automated way. 
   * '''Individual: '''One specific candidate in population (usually one possible solution out of multiple possibilities)
   * '''Population: '''Group of individuals
   * '''Objective Function/Evaluation Function: '''Mapping of individual to some sort of score to help evaluate fitness
   * '''Fitness: '''Loosely put, how an individual's score compares to the rest of the population (kind of like percentile on a test)
   * '''Selection: '''Process of selecting the "best fit" individuals to "mate" for the next generation

* Possible methods of selection
   * '''Fitness Proportionate: '''Greater the fitness value, the more likely a given individual is to be selected for mating (can be thought of like bigger portion on roulette wheel)
   * '''Tournament: '''Randomly pull individuals from population. Individuals with better scores move on

* '''Crossover: '''Fusing the "genomes" of two individuals to make a new individual (eg: merging certain part of list in individual 1 genes with certain part of list in individual 2 genes)

* Genetic Algorithm Basic Heuristic
   * 1.Initialize Population
   * 2. Determine fitness of population
   * 3. Repeat
         * a) select parents based on objective scores/fitness
         * b) perform crossover to create children
         * c) Induce mutation to preserve diversity (kind of like how genetic mutations help to increase biodiversity)
         * d) determine fitness
Below is a nice example of genetic algorithms in action!
[[files/one-max.jpg|left|thumb|522x522px]]
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install DEAP library
|Completed
|January 20th, 2021
|January 27th, 2021
|January 20th, 2021
|-
|Review Lecture 1 Slides from Bootcamp
|Completed 
|January 20th, 2021
|January 27th, 2021
|January 21st, 2021
|-
|Complete Lab 1 from Bootcamp
|Completed
|January 20th, 2021
|January 27th, 2021
|January 22nd, 2021
|}








'''OneMax Problem'''
* Objective/End Goal: Get a bit string of all 1s with a given length
* Evaluation Function: The number of ones in the string (want to maximize this)
* Individual: A list of 1s and 0s of given length
* Crossover: Two point Crossover
* Mutation: Point mutation (flip a randomly selected bit)
* Selection: Tournament Selection

We were able to achieve the optimal solutions within 40 generations, but if we re-run the code several times, sometimes, we might not acheive the optimal solution within 40 generations. 

'''N-Queens Problem'''
* Description: Given an NxN chessboard, how can we place N queens such that the queens cannot attack each other
* Evaluation Function: Count the number of conflicts/potential attacks (want to minimize this)
* Individual: A configuration/placement of queens on the board
* Crossover: Able to experiment with Partially Matched Crossover and Two Point Crossover
* Mutation: Shuffle Indices. I also experimented with inversion mutation
* Selection: Tournament Selection

I'm currently working on how to acheive minimum of zero in less than 100 iterations, but have a consistent optimal solution. 

As of 1/22/21: I was able to reduce the number of generations needed to converge to the minimum of zero with adjusting the mate probability and mutation probability while retaining the original mutation function of shuffling indices. I'm still trying to see if I can acheive a consistent optimal solution. You can view my progress at the following [https://drive.google.com/file/d/17_5vGhrW-buJDl2YajhrZ56z-DXZiYj_/view?usp=sharing link]. Below is a sample visual depicting my attempt to tune the parameters for faster convergence

[[files/viz.jpg|522x522px]]