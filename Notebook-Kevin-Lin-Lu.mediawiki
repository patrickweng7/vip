=== Personal information ===
Name: Kevin Lu

Email: klu@gatech.edu

Phone: 908-304-8194

Other Subteam members:

Gabriel Wang (gwang340@gatech), Bernadette Gabrielle Santiago Bal (bgsanbal@gatech), Xufei Liu (xufeiliu2000@gatech), Angela Young (ayoung97@gatech), Regina Gomez (rquiroz7@gatech), Vincent Huang (vhuang31@gatech)

== January 8, 2020 ==

=== Lecture Notes ===
Genetic algorithms - algorithms that are inspired from natural selection, goal is to take a population and over generations, create a population with the best fitness.

- Individual: one specific candidate in the population (with properties such as DNA)

-Population: group of individuals whose properties will be altered

-Objective: a value used to characterize individuals that you are trying to maximize or minimize (usually the goal is to increase objective through the evolutionary algorithm)

-Fitness: relative comparison to other individuals; how well does the individual accomplish a task relative to the rest of the population?

-Evaluation: A function that computes the objective of an individual

-Selection: represents 'survival of the fittest'; gives preference to better individuals; therefore allowing them to pass on their genes
* Fitness Proportionate: the greater the fitness value, the higher the probability of being selected for mating
* Tournament: several tournaments among individuals (number of individuals in each tournament is dependent on tournament size); winners are selected for mating
-Mate/Crossover: Represents mating of the population

-Mutate: Introduces randomness

-Algorithms: various evolutionary algorithms to create a solution or best individual

== January 14, 2020 ==

=== Lab 1 Notes ===
-Ran through the lab on Jupyter notebook after installing DEAP
[[files/Lab 1 Alternate Mutation method.png|thumb|Alternate Mutation method]]
-Created the another mutation function where only the adjacent positions can be swapped, but it produced worse final offspring. (Best individual is [8, 10, 4, 15, 19, 16, 3, 14, 6, 1, 9, 11, 13, 17, 5, 18, 12, 0, 2, 7], (2.0,))[[files/Lab 1 Output graph.png|thumb|Final output of the Jupyter Notebook graph with default settings]]-Then ran through the notebook with default settings.
[[files/Lab 1 Final Output Graph.png|thumb|Output graph of adjusted parameters]]
-Next, changed the crossover method to csTwoPoint, Tournament to 5, indpb to 3.0/n and mutation chance to 0.3 to get a much more consistent and almost guarantee to get the best individual for n = 300
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Lab 1
|January 8, 2020
|January 14, 2020
|-
|Join Slack
|January 8, 2020
|January 14, 2020
|}

== January 15, 2020 ==

=== Lecture Notes ===
-Genetic Programming: Instead of taking an individual and having a functional evaluator to obtain objective scores, the individual is the function itself

-We can represent a program as a tree structure
* Nodes are called primitives and represent functions
* Leaves are called terminals and represent parameters
-The tree is converted to a lisp preordered parse tree (Operator followed by inputs)

-Crossover in tree-based GP is simply exchanging subtrees
* Start by randomly picking point in each tree
* These points and everything below create subtrees
* The subtrees are exchanged to produce children
-Mutation can involve
* inserting a node or subtree
* Removing a node or subtree
* Changing a node or subtree
-Evaluating a tree
* We can feed a number of input points into the function to get outputs
* Run f(X)
* We can measure error between outputs and truths

== January 21-22, 2020 ==

=== Lab 2 Notes ===
I added these two lines as primitives:
 def sigmoid(a):

     return np.exp(a) / (np.exp(a) + 1)

 pset.addPrimitive(np.sin, arity=1)
[[files/Output graph 1.png|thumb|Output graph after adding two primitives and two mutation methods without adjusting parameters]]
 pset.addPrimitive(sigmoid, arity=1)
Next, I added the two mutations:
 toolbox.register("mutate", gp.mutEphemeral, mode = "all")

 toolbox.register("mutate", gp.mutInsert, pset=pset)
After running it, we achieved an error of 1.012329688161299e-16 in the best individual and produced the graph to the right.
[[files/MSE.png|thumb|Mean Squared error graph]]
Next, I ran through the lab and got the area under the curve to be 2.3841416372199005, so my goal was to reduce it. I used the same code, but changed LAMBDA to 1 and immediately got that the area under the curve was 0.3118423544060799, so I completed the requirement.
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Lab 2
|January 15, 2020
|January 22, 2020
|}

== January 22, 2020 ==

=== Lecture Notes ===
-Basic stats: Type 1 Error/Type 2 Error means false positive/false negative

-Want to: reduce both errors and increase rate of true positive and true negative

-Sensitivity/True Positive Rate (AKA hit rate/recall): TPR = TP/P = TP/(TP+FN)

-Specificity /True Negative Rate: TNR = TN/N = TN/(TN+FP)

-Objective Space
* Each individual is evaluated suing objective functions

* Objective scores give each individual a point in objective space

* This may be referred to as phenotype of the individual
- Pareto Optimality
* An individual is Pareto if there is no other individual in the population that outperforms the individual on all objectives
* The set of all Pareto individuals is known as the Pareto frontier
* These individuals represent unique contributions
* We want to drive selection by favoring Pareto individual (but still maintain some diversity)
-NSGA II
* Population is separated into non domination ranks
* Individuals are selected using a binary tournament
* Lower Pareto ranks beat higher Pareto ranks
* Ties on same front are broken by crowding distance (Summation of normalized Euclidian distances to all points within the front, Higher crowding distance wins)
-SPEA2
* Each individual is given a strength S (how many others in the population it dominates)
* Each individual receives a rank R (R is the sum of S's of individuals that dominate it)
* (basically crowding distance to break ties) A distance, d, to the nth nearest neighbor (is calculated and a fitness of R + 1/(d+2) is obtained

== January 28, 2020 ==

=== More to Lab 2 ===
Last week, I already managed to get a graph with low AUC by changing Lambda which lowers the number of children produced per generation, but I also tried changing other attributes to get a lower AUC. I reduced NGEN to 1 generation so trees don't get the chance to evolve, thus each offspring performs much better in the tree size metric. Reducing the probability of producing offspring with crossover also greatly reduces AUC, because again, the trees stay very small, so they all perform very well on the tree size metric.

== February 1 - 2, 2020 ==
[[files/CFM.png|thumb|Confusion Matrix for my classifier]]

=== Titanic Classifier ===
Met with the team today and decided on features. We decided to use the features age, sex, class, fare, parch, and sibsp. We also decided to use ICA to feature select before running our algorithms before finally splitting up different classifiers we could each use to create our individual models. I then completed constructing my model, using the Stochastic Gradient Descent classifier on sklearn with parameters hing, l2, and max iteration of 100. This is the confusion matrix I got:

True Negatives 173

False Positives 49

False Negatives 20

True Positives 55

== February 5, 2020 ==

=== Adjusted Titanic Classifier ===
Since we used k-fold cross validation, the confusion matrix should actually be a sum or average of all of the models produced by folds, so I adjusted the output and got this confusion matrix:

True Negatives 487

False Positives 190

False Negatives 62

True Positives 152

Then, I still used the best model produced from the k-folds and used it to predict the test data and submitted it to canvas.
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Titanic Classifier
|January 29, 2020
|February 4, 2020
|}

== February 10, 2020 ==

=== Created Titanic classifiers using DEAP ===
We met as a group and decided to use the primitives addition, subtraction, multiplication, multiplying by -1, greater than (1 is true 0 is false), less than, and square. Then, we decided to use cxOnePoint, select with tournament size 3, limit tree depth to 20. We used the same 6 features as before and in mating, we also used Pareto dominance. After selecting by tournament, we let non-pareto individuals have probability .65 of mating and pareto individuals having probability .75 of mating and the final mating probability is the product of the two individual's probabilities. Lastly, we uDow our final pareto frontier to predict the test data. We also put together the powerur data. We also uploaded all of our previous work on creating titanic classifiers as well as our current work to this github:

https://github.gatech.edu/schoudhury40/TitanicProjectGroup1
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|DEAP Titanic Classifier
|January 29, 2020
|February 4, 2020
|}

== February 24-25, 2020 ==
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Download/Install Emade/Mysqsl
|February 19, 2020
|February 25, 2020
|}

== March 7-8, 2020 ==
I ran the Emade command for titanic for 40 generations and then found the Pareto front for it. The other group members ran it as well, although each for a different number of generations. We created graphs to represent all of the Pareto fronts and also a graph to show the change in AUC over time. Then, we combined our Pareto fronts into a larger Pareto front and then found the area under the curve to see how our various Pareto fronts performed. We also finished creating the presentation with slides in here: https://docs.google.com/presentation/d/1XAhszW_1XZNIei0VixClj5FC9dHq_TpkVtQBTuIp3cI/edit?usp=sharing
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Run Emade through Titanic
|February 19, 2020
|March 8, 2020
|}

== March 9, 2020 ==

=== Notes ===
ADF: Automatically defined functions, pipeline already constructed and most things are ready for testing, try to speed up/increase accuracy of emade

Research Fundamentals: Reduce Bloat produced in emade, working on using new programs

NLPs: Looking to learn about text with machine learning, working on using new programs to run experiments

EZCGP: looking for ways to enhance data, also looking to use new programs to run experiments.
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Take notes on presentations
|March 9, 2020
|March 9, 2020
|}

== March 23, 2020 ==

==== Main meeting notes ====
I was assigned to the team for ADFs. We discussed some possible meeting schedules and talked about more administrative issues as well as interests to decide the direction of the team.

== March 30, 2020 ==

==== Individual work ====
I was not able to attend the subteam meeting this week, but as per instructions, I went through most of the presentation which taught us how to specifically use the version of emade our subteam uses and completed the assignment. I briefly read through Discovery of Subroutines in Genetic Programming with the citation:

Justinian P. Rosca and Dana H. Ballard. 1996. Discovery of subroutines in genetic programming. Advances in genetic programming: volume 2. MIT Press, Cambridge, MA, USA, 177â€“201.

I learned in general what ARLs are and what they're supposed to do. These are also what we are calling ADFs. The paper introduced some formal definitions for ARLs, but did not dig deep into any proofs or experiments that show ADFs are actually useful. However, it gave some interesting ideas such as some example utility methods for ARLs/ADFs such as using entropy, etc. The paper also talked about some example problems where ADFs seemed to be useful.

As for downloading the Github and completing a successful seeded run, I ran into some dependency issues, but I soon resolved them by just creating a fresh environment in Anaconda and reinstalling all of the dependencies.
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Download the ADF Subteam Github and complete a successful seeded run
|March 23, 2020
|March 28, 2020
|-
|Read papers on ADFs
|March 23, 2020
|March 30, 2020
|-
|Completed other administrative tasks for the subteam
|March 23, 2020
|March 28, 2020
|}

== April 3, 2020 ==

==== Subteam notes ====
The ADF subteam previously split into multiple smaller groups to work on various projects that we voted to work on. Today, my group met after bluejeans stopped working and discussed our project in Webex after some technical difficulties, creating a new selection method for ADFs. We tried presenting multiple ideas for the new selection method such as adding the size of the trees with the ADF or the proportion of the tree that the ADF takes up as another factor in the fitness for selecting ADFs. In the end, we decided to split off and research some more if need be before finally deciding on Monday what our selection method will add. We also decided that we will try to finish programming the changes by April 14 at the latest in order to complete some trials and gather data before the final presentation.
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Look into ADF selection methods
|April 3, 2020
|
|}

== April 6, 2020 ==

==== Subteam notes ====
In our subteam meeting, we realized that another group in our subteam was looking for a new ADF selection method, so we instead are looking to create a new general selection method. I had personally found no better way in the papers I glanced through from the presentation, so not much work was lost. The idea presented to us was somehow favoring trees with ADFs in the selection method. We discussed and decided that we would try counting the number of ADFS and each write a piece of code to edit the selection method. Then, we would discuss on Friday with our code to decide on a final selection method.
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Edit selectionmethods.py to add in a new selection method
|April 6, 2020
|
|}

== April 9, 2020 ==

==== Individual work ====
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Edit selectionmethods.py to add in a new selection method
|April 6, 2020
|April 9, 2020
|}

I edited the code to add my iteration of changes for the new selection method. My iteration counts the number times ADF shows up in the string representation of the tree for each individual and adds this to their weight. To count this, it just iterates through and checks if the next three letters are ADF. If they are not I increment the index by the proper amount by the number of letters I have confirmed are not ADF. This weight change causes individuals with more ADFs to have a higher probability of being selected for crossover, so this seems like a good way to increase the number of ADFs.

== April 10, 2020 ==

==== Subteam notes ====
We discussed in the team meeting today and we discovered flaws in my approach. I edited the SelTournamentDCD in the selectionsmethod.py file, but that method is used in sel_nsga2_weighted method, not the sel_nsga method used in input_titanic.xml. That's why when I ran trial runs, I didn't notice any difference in the individuals I was seeing. However, we decided not to use this method, because the weighted method also has multiple parameters that we did not want to use in our trial run, so we decided as a group to use an edit to the tournament selection method that one of our group members came up with. This method also counts the number of ADFs in an individual and uses it as a requirement for domination in the tournament selection process. This way, we still heavily favor individuals with more ADFs, but don't have to use messy parameters for just a simple trial run. We will also have our method uploaded to a ADF selection method branch in the Github at https://github.gatech.edu/gwang340/emade/commit/3702835dbc28c8454c9e5876abbc6595e62e908a
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Look at the Github for our group's new selection method
|April 9, 2020
|April 13, 2020
|}

== April 13, 2020 ==

==== Subteam notes ====
We met as a team and just discussed what we want to do with the trial runs. Our main goal is to ensure that the number of average ADFs have increased, so we are going to work on a method that finds the number of average ADFs per individual over a few generations. I said that I would try using python to look through the sequel dump.

==== Individual work ====
After a few attempts at reading the sql dump with Python, I ended up getting this error when I tried to read the file:

UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 1169297: character maps to <undefined>

Thus, I think using Python may be more work than it's worth and might just try to do everything in mysql.
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Write code to find average number of ADFs per individual over a history of generations
|April 13, 2020
|
|}

== April 17, 2020 ==

==== Subteam notes ====
We ended up just using a previously existing sql script to read the sql dump and uploaded all of the average values to a google drive since Python didn't seem to be able to read the SQL dump. Now, we just need to analyze it and each put it on the presentation. We did some preliminary analysis on the data and discussed. We were not expecting to see any significant increase or decreases in the AUC or number of ADFs in our tests. A glance at the files told us the results did not end up being so different. We hypothesized that this may be because our change might be held back in effect by the weighting done for crowding effect in the tournament selection method. If more individuals use the same ADFs, it would actually lower crowding distance of individuals using those ADFs, because they would have more similar nodes, so even though we were trying to select for individuals with more ADFs, the crowding distance condition took priority and could have forced individuals with more ADFs out of the tournament.
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Analyze the number of ADFs per individual with the new selection in comparison with the benchmark
|April 17, 2020
|
|}

== April 18, 2020 ==

==== Individual work ====
I wrote a python script to read in the average number of adfs in the runs we made and conduct the two sample t-test to find if we produced a statistically significantly greater number of ADFs in the genetic algorithms. There were 5 benchmark runs and 3 with the new selection method we created. I created a graph for the presentation and finding the p-value of the last generation. Here are some of the t-values and degrees of freedom that I found from the two per generation:

Generation: 40

Degrees of freedom: 2.7698454098898186

t value: 0.3447899381466757

Generation: 41

Degrees of freedom: 3.215210474215656

t value: 0.6969410030133366

Generation: 42

Degrees of freedom: 3.277158792950698

t value: 0.6755006625586669

Generation: 43

Degrees of freedom: 8.008555928959087

t value: 1.57364848870475

Generation: 44

Degrees of freedom: 9.996995880156105

t value: 1.8208837459830491

Generation: 45

Degrees of freedom: 4.313462808989219

t value: -0.21151908625967386

Generation: 46

Degrees of freedom: 4.737541742700121

t value: 0.1290500225682664

Generation: 47

Degrees of freedom: 3.9704077631491104

t value: -0.4329929151630028

Generation: 48

Degrees of freedom: 10.663207967687757

t value: -0.3729561132702526

Generation: 49

Degrees of freedom: 9.547655566646775

t value: -0.7676983947440684

Here is the link to the my script to find the ADFs:

https://github.gatech.edu/klu77/AAD_scripts/tree/master/Selection%20t-test

The python script is in readsqldump.py. Other than the p-value I put on the presentation, I also tested a few more and none of them were statistically significant, at around .6 to .7. With even just a glance at the data, it was easy to see that the number of ADFs did not increase much and just stayed in the low single digits.
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Analyze the number of ADFs per individual with the new selection in comparison with the benchmark
|April 17, 2020
|April 18, 2020
|}

== April 19, 2020 ==
[[files/Selection Method Presentation New Graph.png|thumb]]

==== Individual work ====
We did a rundown of the presentation today. I updated the Github with my new script that produces new graphs that fixes the mistakes I made before, changes the lines to be averages of all of the runs, and added previously omitted benchmark runs. With the new graph, I mapped the line and the variance, but since one variance isn't visible if they overlap, I slightly translated one graph to the left, so that they are visible. The new graph is shown to the right. The new script was updated at this github:

https://github.gatech.edu/klu77/AAD_scripts/tree/master/Selection%20t-test

== April 20, 2020 ==

=== Notes on presentations: ===

==== Research Fundamentals ====
-Main goal is to reduce bloat

-They split individuals into different species based on the shared topological structure of the trees. They use both the depth and the number of nodes.

-They do fitness sharing which has 3-goals:
* Punish individuals from highly-populated species
* Project more unique individuals
* Punish species without modifying fitness
-They hope for the outcome:
* Being able to properly weight these individuals based on the 3 goals and then use speciation in crossover to increase the distance between individuals
-Their results produced a lot of variance, but their new method actually produced statistically significantly "closer" individuals

-They also created a new crossover called neat crossover. They also had a lot of variance, but had a statistically significantly worse performance.

-Something else they're working on is using PACE to run their experiments. They ran into many bugs, but seem to have made progress

==== NLP ====
-Their goal is to classify text.

-They created new primitives

-One is the Relulayer, which is just applying the relu function on the input

-ELU, the exponential linear unit activation function (which is linear for all inputs greater than or equal to zero, but is exponential for inputs less than zero)

-SeLU, is a normalized version of eLU so that the new layer preserves the mean/variance of the values from the layer before

-Dropout, which selects nodes to be removed with some probability

-Glove Embeddings (In a neurla network, an embedding layer is a trained set of word-to-vector mappings that encode relationships between words, it uses Euclidean distance, and it's a pretrained set of weights to an embedding layer that encodes the 400,000 most common English language words)

-Linear activation function.(however, cannot use gradient descent on this activation function and all layers of the neural network collapse into 1)

-Attention (Takes a set of words and tries to label them)

-One dataset they're working on is the toxicity dataset. It's a sparse dataset, so they ended up using a different loss function known as weighted_cross_entropy.

-Another data set they're working on is the chest x-ray dataset. It's very large, at over 40 gigabytes. They ended up splitting the data into smaller splits that are tractable in EMADE.

-They had pretty good results in general.

-They ran text classification on IMDB reviews, and they produced some baseline runs as well as some runs with their new primitives that apply to text classification. They managed to generate good results with relatively low AUCs.

-They are also working on getting PACE to work and have now gotten automated PACE multiprocessing to work.

-In the future, they want to continue to add new primitives and try to match results with papers they used to apply to their experiments. They might move to computer vision, as well as some other topics to classify.

==== NLP Time Conflict ====
-Their goal is to summarize text

-They also try to move to PACE to be able to efficiently and reliably test data sets.

-They couldn't' have multiple people use MySQL concurrently, which is something else they tried using.

-Some things they finished were adding documentation to PACE for running EMADE, added classification function for multi-dimensional data, defined how "summarydata" should look and seeded examples, and completed some analysis of summary primitives.

-They wanted to create num named entities primitive, where named entities are important text. This way, the primitive can measure just how much useful information is in a paragraph. They used a spaCy library to find teh name entities in a paragraph.

-TFISF Primitive, Term Frequency-Inverse Sentence Frequency. It's a way of assigning numbers to each sentence in a document, in order to label how important that sentence, in the context of the entire document. They calculate the value per sentence and then normalize to prevent longer sentences to arbitrarily have more weight.
* They were able to create the primitive
* Create unit tests
* Baseline testing
-They ended up having slightly worse performance from numnamedentries, but slightly better performance for TFISF

-NumNamedEntities takes around 10 minutes to run per run, while TFISF takes 2 hours to run, so something they need to work on is making TFISF more efficient.

-They also would like to add more documentation and use external MySQL implementation to reduce database issues.

==== EZCGP ====
-They used the dastaset CIFAR-10, which has 6000 images per class and 10 classes. It's a popular benchmark dataset.

-They were supposed to find contemporary CNN architectures and run new models on PACE, so only uses Tensorflow 1.0.

-They used some additional primitives in ezCGP: Conv_layer, Max_pool_layer, Concat_func, Res_block, Dense_layer, Sum_func, Batch_norm

-They ran EZCGP for 39 generations which took 41 hours. They were able to achieve better test accuracies without the augmentation they developed.

-Their evolutionary parameters were F1 Score and Accuracy

-They used VGG with Dropout and manage to get test accuracy after 200 epochs of 94.31%. Without dropout, the test accuracy falls to 73.50% with 100 epochs. Test accuracy marginally increases with data augmentation and batch normalization.

-ParneetK CNN is another architecture they used and without augmentation 72.18% accuracy, but with augmentation test accuracy went to 84%

-Embedded Systems CNN. Testing accuracy after 10 epochs gave 65.67%, but with data augmentation, test accuracy went to 85.9%

-They worked on data augmentation, which is generating a larger good sample from a small sample. Right now, they are implementing this through a library called Augmentor.

-
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Present and take notes on other presentations
|April 20, 2020
|April 20, 2020
|}

== August 17, 2020 ==

====== General meeting ======
Met again with the team. I chose to remain on the ADF subteam which has now been renamed to Modularity/ARLs. EZCGP and NLP remained, while a new subteam, stocks was created.

====== Subteam meeting ======
We discussed some of our goals and what we might want to continue with this semester. I brought up continuing with mutations/selection methods in increasing ARLs. The last thing we did in the semester was pretty hasty and very simple. We hope to improve upon the idea and possibly create a more sophisticated method to increase ARL population.
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Choose Subteam
|August 17, 2020
|August 17, 2020
|}

== August 24, 2020 ==

====== General meeting ======
General discussion of how each team will schedule their meetings, their goals, etc.

====== Subteam meeting ======
We finally decided on our subteam meeting time while will be Friday at 4:00 pm. We also discussed pecific goals that we want to accomplish in the team. Gabriel went through slides that suggested multiple ideas and we picked the ideas we wanted to work on. Again, I chose to continue with what we worked on last semester with new selection methods and mutations. I feel that our end product last semester was still too primitive since we did not have too much time, so I want to improve upon it this semester.
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Discuss goals with subteam
|August 24, 2020
|August 24, 2020
|}

== August 31, 2020 ==

====== General meeting ======

====== Subteam meeting ======
We wanted to get many base runs and decided to begin reviewing papers for our literature review. Over the weekend, I tried running the directory Gabriel told us to run after we discussed on Friday (Differential Fitness). We want to create runs off of the benchmark that we had always used. My runs ended up with too many bugs and I could not get it working. I looked through the bugs, but they did not seem easily fixed. Here is an error message:

i = sum(individual.fitness.values) #uses the sum of all fitnesses UnboundLocalErrorL local variable 'sum' referenced before assignment

I could not figure out how this was happening since sum is a function from python and should be treated as a variable. I tried updating python, pip, anaconda, and all of the required packages, but it didn't work. I tried reinstalling the repository as well. We discussed our runs and it seems that everyone else in the group also had problems running the program. Thus, we decided to switch to using the SelectionMethod branch and are running our benchmark runs off of that.
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Get base runs from our current repository
|August 28, 2020
|
|-
|Begin looking for papers for a literature review
|August 28, 2020
|
|}

== September 4, 2020 ==

====== Subteam meeting ======
During our literature review, everyone brought up new papers we could look at and many introduced new methods of looking at our genetic algorithm .We wanted to enforce more diversity and possibly different ways of looking at our goals. What I spoke about was the papers and abstracts I read in the gecco conference. Here are the links:

https://dl.acm.org/conference/gecco/proceedings

[https://slack-redir.net/link?url=https%3A%2F%2Fdl.acm.org%2Fdoi%2Fpdf%2F10.1145%2F3377929.3389852 https://dl.acm.org/doi/pdf/10.1145/3377929.3389852]

[https://slack-redir.net/link?url=https%3A%2F%2Fdl.acm.org%2Fdoi%2Fproceedings%2F10.1145%2F3377930 https://dl.acm.org/doi/proceedings/10.1145/3377930]

The one that stood out to me was the one about simplifying machine learning algorithms. This one would focus on changing the algorithm as much as possible to estimate with a quadratic equation, which is a solved problem. I was thinking about applying it to our tree nodes and thinking about optimizing each of the learners into simpler functions. I also thought that maybe we could enforce some simplicity in our other primitives, so that equations will be quadratic when it can. In any case, I thought this could be something to think about when creating a new selection method.

Regina couldn't attend the meeting, but Jacob, Gabriel, Ruarai, and Gabriel all had interesting papers, although some did not seem to match the direction of our team.

Before the literature review, we discussed how getting runs on our local machines don't seem viable, so we have to look to alternatives. I was assigned to handle PACE. I will be working to figure out how to get emade to run on PACE. However, I did manage to get one run over 30-40 hours and uploaded the sql dump to our google drive. I don't believe any other members were able to get any runs, but each of them were also assigned to handle getting a new machine running as well as making changes to the algorithm on emade, so we can test some runs once we get it working.
{| class="wikitable"
!Task
!Date Added
!Date Completed
|-
|Get base runs from our current repository
|August 28, 2020
|September 4, 2020
|-
|Begin looking for papers for a literature review
|August 28, 2020
|September 4, 2020
|-
|Figure out how to get runs on PACE
|September 4, 2020
|
|}

== September 11, 2020 ==

====== Subteam meeting ======
We had a subteam meeting and talked about our progress over the week. Most group members had not done much in the way of gaining more access to the clusters we were assigned. I just spent a couple of days trying to set up PACE's environment in terms of python and anaconda as well as moving the emade directory onto PACE. I was successfully able to do so by creating a copy of the emade directory and then manually deleting the .git files as well as extraneous datasets, because otherwise, PACE did not have enough disk space. It took me many attempts to figure this out, so I updated the guide on the wiki for using PACE to give this advice. Here is the wiki link:

https://vip.gatech.edu/wiki/index.php/Guide_to_Using_PACE-ICE

In the subteam meeting,  we also discussed some more specific goals and timelines. I wanted to finish getting PACE running by September 18 at the latest and then continue working on developing more sophisticated or just different selection methods. One example I gave was choosing individuals with more ADFs with higher probability for tournaments rather than just editing the tournament method. I decided that once PACE was running, I would write this method and get some runs testing these new selection methods.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Figure out how to get runs on PACE
|September 4, 2020
|Completed moving repository onto PACE, set up environment
|
|-
|Run new selection method tests on PACE
|September 11, 2020
|Waiting for completion of PACE task
|
|}

== September 14, 2020 ==

====== General meeting ======
For our general meeting, we summarized what we had discussed on Friday. We talked about how we wanted to get other ways of computing power since our GCP credits were running out. We brought up PACE and Collab. We also talked about our direction in creating new selection methods as well as accessing return types of nodes in trees.

The stocks team was still in its initial stages, looking over literature and they found a lot of good papers to start working.

EZCGP and NLP continued with their work they had begun.

We were also assigned self assessment for our notebooks that are due by September 21.

====== Subteam meeting ======
For the subteam meeting after, we just had a short discussion of our progress we made since last Friday. I hadn't worked on much, but I looked into setting up mysql on PACE. I gave my estimated completion date to be before Friday.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Figure out how to get runs on PACE
|September 4, 2020
|Setting up mysql on PACE
|
|-
|Run new selection method tests on PACE
|September 11, 2020
|Waiting for completion of PACE task
|
|-
|Finish Notebook self assessment
|September 14, 2020
|Assigned
|
|}

== September 18, 2020 ==

====== Subteam meeting ======
During the week, I figured out how to get everything running on PACE. The mysql server is running smoothly and bash reinstall.sh worked, so everything for emade is set up. Now, I am just figuring out how to get a run working with the pbs scripts. I believe all of my pbs scripts are written correctly, but there are some new bugs that are PACE specific and are interfering with the runs. However, I will attempt to resolve them by Monday and get a run in before then as well. I am also working on looking through the code to make changes to the selection method, but that implementation will come after making a successful test run on PACE. I also finished my self assessment and here it is:

[[files/Klu notebook.png|1100x1100px]]
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Figure out how to get runs on PACE
|September 4, 2020
|Everything set up, need to work on PACE specific bugs
|
|-
|Run new selection method tests on PACE
|September 11, 2020
|Waiting for completion of PACE task
|
|-
|Finish Notebook self assessment
|September 14, 2020
|Completed
|September 18, 2020
|}

== September 21, 2020 ==

====== General meeting ======
Stocks have progressed into general work and soon implementation. NLP was able to get PACE working. This should make it valuable for me to work with them in getting PACE running.

====== Subteam meeting ======
For the subteam meeting after, we talked about our progress. It seems that things are going along well for Ruarai in his editing of the code. Jacob and Regina have been dealing with running emade again on their personal computers. Aryender still worked on getting Collab to work with his personal mysql server. I have been working on PACE, but I was still not able to get it running. However, I decided to work on a new selection method to run, so I've finished investigating the selection method code and creating a new method. This method will now cause individuals with more ADFs to have higher probabilities of getting selected for tournaments. Before finalizing the code, I want to make sure PACE runs first.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Figure out how to get runs on PACE
|September 4, 2020
|Still working out PACE bugs
|
|-
|Run new selection method tests on PACE
|September 11, 2020
|Completed code
|
|}

== September 25, 2020 ==

====== Subteam meeting ======
We talked about what we did over the week and I spent a lot of time working on PACE. The others worked on Collab and are trying to get runs to start there. The issue they have is with port forwarding, so instead we are looking to use AWS to avoid that issue. We also discussed how we will now try to add the MNIST database to emade. Other than that, here is the code for my new selection method:
https://github.gatech.edu/gwang340/emade/tree/new_selection

The key files I changed were selection_method.py and input_titanic_adf_selection.xml

Here is the pseudocode for the weights I added into my selection method when choosing tournament:

weights = ones(length(individuals))

i = 0

whiel i < length(individuals):

weights[i] += inidividuals[i].adf_count

i += 1

Now from there, I normalize the weights and then select each individuals based on those probabilities for tournament.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Figure out how to get runs on PACE
|September 4, 2020
|Still working out PACE bugs
|
|-
|Run new selection method tests on PACE
|September 11, 2020
|Fixed some more bugs and pushing to Github
|
|}

== September 28, 2020 ==

===== General meeting =====
Stocks have been able to implement a run. We mentioned our PACE bugs and it looks like many others ran into the same problem. We also spoke about our decision to implement MNIST as well as doing some runs with our new selection method.

====== Subteam meeting ======
For the subteam meeting after, we talked about what we did over the weekend. Ruarai was able to make some headway into investigating his problems. We talked about Collab and how we are moving to using Collab exclusively since PACE looks like it will take a long time to get running. I also requested people to run my code on their Collab since I will likely not get the chance until Thursday.

In more detail, over the weekend, we met up and tried to get PACE and Collab working. Gabriel was able to get Collab working, but PACE would not work for me and there are still many bugs that remain. Luckily in the general meeting, others have been met with the same bug, so they may manage to fix the bug and get things working, so I may have the chance to copy off of them.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Figure out how to get runs on PACE
|September 4, 2020
|Suspended indefinitely
|
|-
|Run new selection method tests on PACE
|September 11, 2020
|Getting others to run it on Collab
|
|}

== October 2, 2020 ==

====== Subteam meeting ======
We met again, and just discussed what we were able to accomplish which was generally getting Collab to work. Ruarai found a major issue with Data pairs that heavily impairs what Gabriel and Ruarai want to add to our ADFs. I was able to get the database on Collab and install everything properly, but I could not get seeding to work. I will probably try to fix it and test normal runs over the weekend, but that's it.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Get Collab working
|September 11, 2020
|Moved database onto Collab
|
|}

== October 5, 2020 ==

====== General meeting ======
Jason began by describing what we could and should do for our presentations. I described my new selection method which is just something simple to try to get more ADFs in the population. We discussed our issue with Data pairs and Jason told us that this would be our first priority.

====== Subteam meeting ======
During the meeting we discussed our progress and we found that issues still exist with DifferentialFitness. We also discussed my issue of seeding with Collab and the local machine. It turns out that the ADF_Selection branch is bugged for seeding, so I was still able to perform runs on Collab, but without seeds. We will be working to fix that. We also decided that Ruarai and I will look through the ADF files during the week to fix the Data pair issues. We decided that Aryender would make the slides and the introduction for our talk. We also planned for a work session over the weekend to see if we can solve any issues that we still have left over so that we can comfortably just finish our runs next week.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Get Colab working
|September 11, 2020
|Colab working
|October 5, 2020
|-
|Fix Datapair issue with ARLs
|October 5, 2020
|
|
|-
|Fix issue with seeding in Selection_method branch
|October 5, 2020
|
|
|}

== October 9, 2020 ==

====== Subteam meeting ======
We just met up and talked a bit about our progress. Most of us hadn't done too much due to a busy week. Jacob was able to make some progress getting the MNIST data set onto Emade by using other scripts to convert images into a usable form for training. I just tried to use Colab, but it still has a time issue. It takes over 12 hours just to get around 30 generations. This does not nearly reach the speed we need. I also looked into fixing seeding, but I couldn't get it to work. I had tried hardcoding the seeding file into seeding_from_file.py, but the same error was returned. I also read about of adfs.py to understand generally what's going on, but it was difficult since the code is not exactly documented in detail. However, we decided to have a work session over the weekend to look into the issue and for Gabriel to explain what the code does.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Fix Datapair issue with ARLs
|October 5, 2020
|Read into py file
|
|-
|Fix issue with seeding in Selection_method branch
|October 5, 2020
|Tried hardcoding file
|
|-
|Fix issue with Colab being too slow
|October 9, 2020
|
|
|}

== October 12, 2020 ==

====== General meeting ======
We talked a bit about presentations in general. It seems the other groups have all reached the analysis stage in their projects and are looking to complete more experiments to try to reach/beat papers' accuracies. I asked about how to speed up Colab and Dr. Zutty offered the solution using the reuse or using -W.

====== Subteam meeting ======
Over the weekend, we had our work session. Gabriel explained to me in general how the adfs work and are structured. The data pair issue is mainly that the arg0 for learners cannot be converted into lambdas because then they don't point back to the same data set. To fix this, the initial ADF team ignored all Learners to put in ADFs. To fix this, Gabriel made it so ADFs take in learners, but arg0 cannot be apart of that ADF. However, some other issues about nested ADFs and the like began to arise. These are likely edge cases though, so we decided that we should still go on with runs to see if this new code base would perform better. I also fixed my issue with seeding. As for Colab being too slow, we decided to pair up for the runs. For each run we do, one person runs the master process while someone else runs the with -w to halve the time it takes to make a run.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Fix Datapair issue with ARLs
|October 5, 2020
|
|October 10, 2020
|-
|Fix issue with seeding in Selection_method branch
|October 5, 2020
|
|October 10, 2020
|-
|Fix issue with Colab being too slow
|October 9, 2020
|
|October 12, 2020
|-
|Run on Colab/local for new selection method
|October 12, 2020
|
|
|}

== October 16, 2020 ==

====== Subteam meeting ======
We just gathered today to discuss our presentation and talk about the runs we made. The others weren't able to make any complete runs, but still ran analysis on the generations they were able to make. I was able to make 3 runs to generation 49, but 1 of the runs had poor evaluation. Thus, we ended up using 2 runs for our analysis. I used Ruarai's script that counted the number of ARLs and then ran my own script to run a t-test on every generation when comparing the number of average ARLs per individual. My script is here:

https://github.gatech.edu/klu77/AAD_scripts/blob/master/Selection%20t-test/Readsqldump.py

My script also generated some graphs for the presentation. Anyway, we saw some interesting results that we will probably analyze more after some more runs, but there was no apparent change in the number of ARLs even with the new selection method. This selection method linearly increases the probability of an individual being selected for tournament, so the lack of change in ARLs is surprising and needs more tests. If this continues to be true, we may have some fundamental flaws in ARLs if they perform this poorly in tournament.

Presentation link:

https://docs.google.com/presentation/d/1ZTS5ij1kNA8cymFyIYWU1fkuJe3PuNVl_H2IyzB22RQ/edit#slide=id.p
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Run on Colab/local for new selection method
|October 12, 2020
|
|October 16, 2020
|}

== October 19, 2020 ==

====== General meeting ======
Notes:

Stocks------------------------------------------

Main objective is to improve/replicate existing research on trading in the stock market. This is their first semester, so they spent a lot of time building their infrastructure and tests.

Their algorithm use technical indicators given by papers such as price, volume, etc. They used the CEFLANN network as their learners to run through emade. This way, there were no hidden layers and they could use ELM that was a much faster way that could be used to train their model. For data, they used SPY. They needed some other learners in EMADE, so they had to add multi-layer perceptrons, gaussian processes, and ridge regression. Using their best individual learners, they were not able to make a profit. In the future, they will look into other time windows and likely add in more learners and other ways to train.

Bootcamp group 3:

Good job on time, graphs. Their data looks accurate as well.

Bootcamp group 1:

They had some errors and unbelievable results, but they did well presenting and made understandable errors.

NLP rebrands to NN--------------------------------

They compare to a paper "Evolutionary Neural AutoML for Deep Learning". This runs on multi-objective evolution and focuses on diversity. They fit keras functions to layer models. They added terminals and terminal mutations to introduce variation. They started looking into adaptive mutation functions. They tested it on a Chest X-ray problem/dataset. In terms of problems, they met many of the same ones that we did in PACE. For the rest of the semester, they want to investigate their concatenate layer addition. They also want to shuffle layer mutations as well as test other datasets.

Bootcamp group 2:

They did well presenting on time. Even though they didn't get perfect results, they seemed to understand their runs and likely what caused their data imperfections.

EzGCP-----------------------------------------

They use DAGs, custom primitives/data types, and custom features to train a model. To do this, they create blocks that are split up by data, primitive, selection method, mutation, and evaluation types. Right now, they have a data augmentation block, data preprocessing block, transfer learning block, and neural network block. They have mostly been working on this pipeline. They worked experiments with the CIFAR10 dataset. They played around with greediness and diversity after reading some papers. Another paper they looked at was terminating early based on reference curve, so they could reduce computation/training time. They looked into super-convergence decreases overall runtime since individuals converge faster.

Bootcamp group 4:

Presented well and on time.

====== Subteam meeting ======
Over the weekend, we finished creating our presentation and had a test run with the professors on Saturday. We took the advice and edited our presentation. I took out a slide on statistics that didn't end up being very significant or important. Then, we had another practice run just before the main meeting on Monday.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Present
|October 19, 2020
|
|October 19, 2020
|}

== October 23, 2020 ==

====== Subteam meeting ======
We met again and just briefly went over a bit of tasking. None of us did much over the week, so we decided to continue with what we were working on. All of us except Jacob would continue with runs, Jacob will continue with adding the MNIST data set. Here is the actual commit for the runs I will be doing:

https://github.gatech.edu/gwang340/emade/commit/f95b977f16fc644d3e78ed6f369a5b2520eaf651
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Make more runs on new selection
|October 23, 2020
|
|
|}

== October 26, 2020 ==

====== General meeting ======
We just talked briefly about our progress over the weekend which wasn't much and then our plans for the new additions to our team.

====== Subteam meeting ======
Over the weekend, we just continued with our tasking. I was able to get 3 more runs done on new selection. Gabriel went over emade and some more introductions about ARLs to the new members.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Make more runs on new selection
|October 23, 2020
|Made 3 more runs
|
|}

== October 30, 2020 ==

====== Subteam meeting ======
Extremely brief. We just talked a bit more about our progress in our tasking which is mostly just getting runs done. I was able to get another 3 runs again. Gabriel mentioned that he didn't have time over the weekend, but later he could run his script analyzing area under the curve on my runs. We decided I would just continue getting more runs.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Make more runs on new selection
|October 23, 2020
|Made 3 more runs
|
|}

== November 2, 2020 ==

====== General meeting ======
Gabe mentioned the results from my new selection runs. It had statistical significant in reducing area under the curve during the middle generations of my run. However, it loses that significance and doesn't end up converging faster. We end up hypothesizing that the new selection method favoring ARLs actually end up reducing search space a lot, so the maximization happens quickly, but since there is not much diversity, it becomes difficult for the algorithm to converge in later generations.

====== Subteam meeting ======
Over the weekend, I tried getting more runs, but Gabe's AWS had issues and throttled my runs. We're not exactly sure what the issue is, but it should only occur at the end of the month. After it became November 1, the issues were gone and I was able to get 1 more run in which was used in Gabriel's analysis in the general meeting. After the general meeting we decided that I should keep looking into the population of the runs. I will try to confirm or deny the hypothesis and see what kind of ARLs are being produced to see how we can continue improving the selection method and the selection of ARLs.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Make more runs on new selection
|October 23, 2020
|Made 1 more run
|November 2, 2020
|-
|Analyze the population of the runs
|November 2, 2020
|
|
|}

== November 6, 2020 ==

===== Subteam meeting =====
It looks like it was a busy week for everyone. I haven't been able to do anything since Monday either. Jacob was able to make more progress in importing the MNIST data set.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Analyze the population of the runs
|November 2, 2020
|
|
|}

== November 9, 2020 ==

===== General meeting =====
I talked about the results I found when analyzing my runs. The graph does look suspicious in how similar the new runs are to the benchmark. From here, I will look into the data more, do some sanity checks and from there, decide what we will do next with the selection method, mutations, and/or arl selection.

====== Subteam meeting ======
We discussed mostly just tasking. The others were not able to get much runs done, but Gabriel set up his google accounts for everyone to use to run Collab and help with runs. We split up the first years to join the two groups running emade Datapair runs and difffitness runs. I will continue with my analysis to decide on my next direction.

Over the weekend, I was able to get more analysis done. Here is the output/script commit:

https://github.gatech.edu/klu77/AAD_scripts/commit/9f42511c5868969dfada84acc188d023af4213a8

In the scripts, I sum the different functions found in ARLs and print them out. I saved those in text files and then looked at how they changed over generations. The most frequent functions sometimes start as learners, but never end as learners, so they still don't perform very will in ARLs if we don't restrict ARLs to be only learners. However, from Gabriel's fix, we definitely see that learners have a presence in ARLs. Here is also the suspicious graph that has very similar outputs when comparing the benchmark and the new selection runs:

[[files/ADF_count_for_New_Selection_method.png]]
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Analyze the population of the runs
|November 2, 2020
|Brief ARL analysis
|
|}

== November 13, 2020 ==

====== Subteam meeting ======
Our subteam meeting was fairly short again. We just discussed a little bit about tasking and what I should do. I said I would help Ruarai with his runs on his branch.

I looked into the code more to see if the distribution was in fact uniform and if the individual runs seemed feasible. After graphing, I saw each individual run looked very different, so it seems that my runs were fine. I also saw that my runs did not produce a uniform distribution of arls in the population. However, I did notice some extreme bloat in certain individuals with over 40 arls in them. Thus, I think my current selection method needs the new data pair change to work properly. Otherwise, I am still just choosing the best individual, but now with a bunch of useless arls tacked on. Thus, I decided my current priority would be to do data pair runs in the branch that Ruarai made to update our adf code. Here is the branch I will be running:

https://github.gatech.edu/gwang340/emade/tree/RODataPairOnly

{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Analyze the population of the runs
|November 2, 2020
|Brief ARL analysis
|November 12, 2020
|-
|DataPair runs
|November 12, 2020
|
|
|}

== November 16, 2020 ==

====== General meeting ======
We just generally talked about a couple issues we had with our runs because they take longer now and are harder to manage.

====== Subteam meeting ======
We talked briefly again on our progress of runs. Gabriel made some more alts to run on Colab. I finished two runs over the weekend and have not been having too many issues. I will continue to run and try to get as much data as possible before Friday. Then, I will look into merging Ruarai's branch with mine and then run experiments.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|DataPair runs
|November 12, 2020
|Added 2 runs
|
|}

== November 20, 2020 ==

====== Subteam meeting ======
There were a few problems with my runs. It is possible that I had not properly downloaded the data pair change branch because I did not bash reinstall. I think I checked out another branch after reinstalling and that may have messed up the library, so my runs were not valid. That is probably why my runs were so fast and were able to complete in just 24 hours with just my machine. However, Gabe said he would finish running the data pair to complete the data, so I could continue by merging the data pair branch with mine and just run my new selection method experiments, so that is what I will do this week.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|DataPair runs
|November 12, 2020
|Added a couple runs, but had issues
|November 20, 2020
|-
|Runs with merged datapair and new selection method
|November 20, 2020
|
|
|}

== November 23, 2020 ==

====== General meeting ======
Not much to talk about here. We reported our general progress which is just getting more experiments. Gabe's runs were doing great because he can use another extremely fast cluster.

====== Subteam meeting ======
We just talked briefly on our progress. I was able to get one run to actually work after merging, so once I confirm that there are only data pair ARLs and that the merge doesn't produce any bugs, I will push my branch and get other people to help with my runs. Gabe says other than working on importing MNIST dataset to emade, he will continue to run experiments on his cluster, so everyone else can help me with new selection method runs.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Runs with merged datapair and new selection method
|November 20, 2020
|almost completed 1 run with merge
|
|}

== November 27, 2020 ==

====== Subteam meeting ======
Another brief meeting where we discuss our progress and our presentation. We will work on the presentation on Saturday, but before I complete the presentation, Gabe and I will try to get a couple of more runs on the new selection method branch. Gabe will also work with Jacob to complete the port of MNIST onto emade. The first semesters and Ivanna helped me over the week with my experiments and I believe Ruarai will be finishing his analysis of the data pair runs.

As for the new selection branch, I was able to push it up on Tuesday and get people to help me run it over the week to get a total of 3 runs, here is the commit:

https://github.gatech.edu/gwang340/emade/commit/637391f74603d4ebe7030bf5c8200b3a71098137

A preliminary look at the results were pretty good. I ran my countadftypes.py script to find the number of each ARL and mostly the arts were learners and all of them were data manipulating primitives, so the data pair change definitely worked and are affecting this new selection method run. Here is the presentation that we began

https://docs.google.com/presentation/d/1KU-tlra_DXV93JOS6NjQ5RB24Urj0-M8F99U3YsQQ70/edit#slide=id.ga35d518bbb_1_0
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Runs with merged datapair and new selection method
|November 20, 2020
|3 total runs with merge
|
|-
|Complete presentation with new data
|November 27, 2020
|
|
|}

== November 30, 2020 ==

====== General meeting ======
We just talked about what we presented in our presentation, which was basically wrapping up all of our runs.

====== Subteam meeting ======
We talked about each of our progress in addition to practicing our presentations. I tried to run more over the weekend, but they all stalled for reasons I couldn't identify. However, I was able to work on the presentation and added some of the data that was already collected. Here is a slide with the graphs I created for this powerpoint. It summarizes what we found in our initial runs of new selection:

[[files/Presentation new selection slide.jpg]]

To generate those graphs, I used my readsqldump.py which is now updated in my GitHub. I added an additional p-value graph functionality to it:

https://github.gatech.edu/klu77/AAD_scripts

Next, I updated more on the new selection runs after having merged with data pairs. Gabe generated graphs for the AUC and saw that it did not perform so well, but there are only 3 runs, so our data could be due to variance. Other than that, I also tested the number of ARLs in comparison between the data pair runs and my new selection runs. Here is the slide I made analyzing it:

[[files/New selection merged slide.jpg]]

We can see here that the number of average ARLs have dropped a lot compared to before. I also ran my countadftypes.py to find the number of each type of ARL and we found all ARLs to be data manipulating. There was also no individual with more than 2 ARLs, so we successfully solved the bloat issue as well. Now, we see that the number of ARLs between the normal data pair run and the selection method data pair run aren't very different. The only reason that there is any significance is because the variance is very low. Now strangely the number of ARLs is actually lower in the new selection method even though we scale the probability linearly with the number of ARLs. However, we think it may be again due to the crowding distance tournament selection method. Now that we have a lot more learners which are bound to be close in distance with each other, we may find a lot of individuals with ARLs to be similar, so they don't actually get selected.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Runs with merged datapair and new selection method
|November 20, 2020
|3 total runs with merge
|November 30, 2020
|-
|Complete presentation with new data
|November 27, 2020
|
|
|}

== December 2, 2020 ==

====== General meeting ======
We just talked about what we presented in our presentation, which was basically wrapping up all of our runs.

Stocks: 

- They worked on genetic labeling to train individuals, and they found solutions converge to maximize the profit gained over a time period, however, they found that some of the results were optimizing only for each time window.

-Some primitives they used were William's R% and moving averages, relative strength indicators, etc. from papers. Ease of movement and on-valance volume were primitives they added with some outside research.

-Their results differed some from the paper although they were able to somewhat replicate a couple of graphs.

-They also did some things differently, allowing the magnitude of buys and sells to vary

-Overall, they want to develop more methods and move away from the base paper which they have already begun to do.

-In the future they will try more time ranges and improve their genetic labeling to help their decisions. They also want run new STREAM_TO_FEATURES on normalized data.

NN:

-They used DistilBERT to connect to Keras, the main functionality of their NN experiments. However, DstillBERT was not run because it was added after their code freeze. 

-They created adaptive mutation functions which would reduce mutations of good individuals and increase mutations of bad individuals.

-They set up PACE Cluster and was able to successfully able to do local style runs on PACE with CUDA enabled GPUs.

-They added new CV primitives. Originally threshold methods were not serving their uses, so they added Otsu's Binarization which finds threshold values that minimize the weighted within-class variance.

-Started Notion document that details different types of CV algorithms

-For image classification, they used the YOLO architecture. In their Chest X-Ray Bounded Box Approach, they had many difficulties because only a small number of their images actually had bounded box data.

-They studied Amazon Product Reviews. Non-UTF-8 characters posed an issue and something they had to clean.

-In the future, they would like to move to more multi task learning to better analyze multi label problems. Making BERT layer valid at any position in the tree, not just an Inputlayer. They want to test more datasets.

-Their Chest X-Ray solution actually had some classifications that were actually trivial.

EZCGP:

-They used PACE-ICE and was met with many bugs, but were able to overcome. They were able to get 2 GPUs and get run-times of 8 hours.

-In their runs, they did not see much diversity, but many individuals that performed great in terms of precision and recall.

-NAS Research code, for aging evolution they implemented new individual definition and methods to prune the population

-One experiment they will want to conduct is to continue creating block structures. They hypothesize though that block structures speed up convergence, but over time standard genome structures may actually find more Pareto optimal individuals

-Something in the future they will look to do is add the MNIST data set.

====== Subteam meeting ======
We did one last run of our presentation and added some more to the presentation. We saw that some of the takeaways were not very well explained, so we added more to the presentation.

Throughout the week, I generated graphs and representations of our results. I found a more accurate graph for our presentation which is now updated and the slide in the previous entry reflects that. 
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Complete presentation with new data
|November 27, 2020
|
|December 2, 2020
|}

== January 25, 2021 ==

==== General meeting ====
With the new semester beginning and Gabe being team leader again for modularity, I decide to rejoin modularity. We get a couple of new members a couple of returning members.

====== Subteam meeting ======
Brief introduction and begin to talk about meeting times. We talk a bit about what we will do this semester, such as making more code base changes to allow for larger ARLs. We also want to continue the experiments we didn't finish last semester. 
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Deciding on a meeting time
|January 25, 2021
|
|
|}

== February 1, 2021 ==

==== General meeting ====
We just talk about our goals  which are to make more fundamental changes to ARLs, continue with selection method testing, and continue with experimenting.

====== Subteam meeting ======
Over the last week, we decided to get Sunday afternoon as our new meeting time. We decide to get a literature review for Sunday, and we just briefly confirm what we talked about in the general meeting. I talk about how I want to change the ARLs' construction before I continue with creating and testing new selection methods.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Deciding on a meeting time
|January 25, 2021
|
|February 1, 2020
|-
|Literature Review
|February 1, 2021
|
|
|}

== February 7, 2021 ==

==== Subteam meeting ====
We went over each of our papers. I talked about this paper:
https://link.springer.com/chapter/10.1007/978-3-540-88906-9_54

This paper discusses Mutation only genetic algorithms. Even though I don't think these genetic algorithms should be implemented in emade, I think a concept it discussed was very interesting and that was varying the probability of mutation based on fitness. The more fit the individual, the less likely it will be mutated and the less fit, the more likely it will. For our project, we could implement this idea in conjunction with a crossover method. We want to keep fit individuals with their good ARLs and then try to mutate the rest of the population to get those ARLs. However, at the same time, we don't want the entire population to get the ARL because we still want to keep diversity so we can increase exploration. Fortunately, the rest of the group had multiple papers on diversity and we talked a bit about to how to implement those. Then, we talked a bit about where we want to go from here and everyone said they would like to work on the code base and fix our current issues with how ARLs are generated.

However, first we decided to keep up documentation with the code base, because the current code is very difficult to read.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Deciding on a meeting time
|January 25, 2021
|
|February 1, 2020
|-
|Literature Review
|February 1, 2021
|
|February 7, 2020
|-
|Add proper documentation to ARL code
|February 7, 2021
|
|
|}

== February 8, 2021 ==

==== General meeting ====
We discussed our progress from our meeting on Sunday. We discussed that we wanted to do documentation and Jason suggested we use sphinx.

==== Subteam meeting ====
We decided that we would meet on Thursday or Friday for a peer coding session to get the documentation done and then we can go into the code. Jason also gave us a small tutorial of Sphinx. Here is also my notebook review:

[[files/Notebook self eval.png|1100x1100px]]
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Deciding on a meeting time
|January 25, 2021
|
|February 1, 2020
|-
|Literature Review
|February 1, 2021
|
|February 7, 2021
|-
|Add proper documentation to ARL code
|February 7, 2021
|
|
|}

== February 14, 2021 ==

==== Subteam meeting ====
Gabe went over the general gist of adfs.py and how the code there functioned. We then discussed how we will take our next steps in altering the ARL structure of our code. The first thing we discussed is increasing tree depth of ARLs. We decided that first, we should just try increasing the depth to 2. However, we talked about some problems such as how increasing the depth increases the number of possible subtrees we may need to observe exponentially with the base being the number of children nodes of the primitives. However, Gabe said that the increase in time used to find ARLs is actually not very high, so we could continue to brute force finding ARLs, which would be to just still observe every possible subtree of depth 2 and take the ones with highest frequency. Then we talked about some documentation issues and said we would continue that work next week. I had read more into the code over the week, so I decided that I would start making changes and create a new branch to finish this ARL rework.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Add proper documentation to ARL code
|February 7, 2021
|
|
|-
|Edit ARL structure
|February 14, 2021
|
|
|}

== February 15, 2021 ==

==== General meeting ====
We talked about what we went over on Sunday. We didn't make a particularly large amount of progress, but we discussed how to move on in changing ARLs. We also talked about our team wiki and continuing to change the ARL structure.

==== Subteam meeting ====
We decided that we would meet on Thursday or Friday for a peer coding session to start actually using Sphinx and I said I would like to move onto code changes which is what I will do over the week and continue during the peer coding session. Then, I spoke with Jason about how to structure our ARL search when we increase the ARL size to depth 2. In the end, we thought that a multi-objective search with tree size and frequency would be the best way to search for new ARLs.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Add proper documentation to ARL code
|February 7, 2021
|
|
|-
|Edit ARL structure
|February 14, 2021
|
|
|}

== February 21, 2021 ==

==== Subteam meeting ====
We did the peer coding session and there, we were able to figure out how to use Sphinx. I spend that time instead looking through the ARL code and figuring out a design for how we would change it. We met up and discussed our progress. Vincent, Bernadette, and Gabe were able to look through a lot of the code and complete the documentation. There were only a couple methods left and they wanted to finish it for Monday. I discussed the design of changing our ARL structure to allow higher depth ARLs. We talked a lot about the things we would need to choose such as how we would select the best candidates (the formula we would use to assign them values before choosing ARLs based on their value distribution) and how we would encapsulate the fitness of an individual in our design. I was able to identify the methods that needed to be changed in order to do so. First, we would need to change the _find_adfs method so that we would be able to create ARL candidates that were more than a depth of 1. Then, we would need to make sure update_representation would operate properly since it calls the _find_adfs method. Then, we need to change _generate_adfs because it currently redudantly cycles through the individuals to find ARLs again. Lastly, we would need to make sure find_best_adfs would continue working even with a different population info format. I decided that I would make tasks by Monday for everyone to work on to change the ARL structure. We split into two teams, one with me, Vincent, Bernadette, and Xufei in working on ARL structure change. The other team would be Gabe, Ivanna, and Angela who would change the way we currently store ARLs.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Add proper documentation to ARL code
|February 7, 2021
|close to complete
|
|-
|Edit ARL structure
|February 14, 2021
|still in the design phase
|
|}

== February 22, 2021 ==

==== General meeting ====
We talked about how our documentation was complete. We also explained that we had design issues and might need help.

==== Subteam meeting ====
We went over our design problems with Jason again and just talked about a couple more specifics. Afterwards, I decided to alter the tasks that I had prepared for everyone and I would send it to everyone in slack. The documentation was pretty much already done, except I said I would add the documentation for find_adf eventually, so we don't have documentatino for that yet.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Add proper documentation to ARL code
|February 7, 2021
|
|February 22, 2021
|-
|Edit ARL structure
|February 14, 2021
|tasking
|
|}

== February 28, 2021 ==

==== Subteam meeting ====
On Tuesday, I created the tasks and sent this message on slack:
"

So here are the tasks that I've created, they've mostly remained the same since I think we will push off a lot of what we talked about today until after we get depth two trees working:
# Write method search_individual(individual, dictionary, length): This method is just traversing through a single individual in the population. As we mentioned, the list is a depth first search and nodes have the arity attribute which tells you how many children it has, so write this method with that in mind. When you get to each node, call method add_all_subtrees(individual, node, dictionary, length).
# Write method add_all_subtrees(individual, node, dictionary, length): This method is for when you get to a node, add all subtrees of depth between 1 and length, inclusive to the dictionary as possible ARLs. Note that for this method, you can tell when a node is a leaf by its arity being 0. Now when adding to the dictionary, the key is just the subtree, so something like [(node1.name, 1, 2), (node2.name, 3), node3.name, node4.name], where the numbers in each tuple is the index of the child for that node. Now if the key doesn't exist, we can initialize the value to be equal to [evaluate(subtree, individual), place where the subtree exists] and if it does, just add evaluate(subtree, individual) to value[0] and append the location of the subtree to the end of the value
# Now, the generate_adf method will need to be changed, because we now want to use the value we calculated with add_all_subtrees to be how we select ARLs. Here, just make a new generate_adf that just uses the probability given to you in the dictionary, so also change the parameters to be something like (self, ARL_candidates), and then pick the ARLs at random weighted by their values given by the dictionary for ARL_candidates. Assume that the structure of the dictionary is like how it is described in 2.
"

These methods will change the most essential methods required to allow depth 2 ARLs and we would be able to work on specific designs later. Xufei, Vincent, and Bernadette each chose a different task and are working on them. I wrote the overall method for find_adfs, looked at update_reperesentation, and looked at find_best_adfs to make sure everything was okay. I am also working on finding a good formula for choosing the best ARL. Gabe and his team explained how the database works and would begin work as well editing the code. I spent our work meeting time to help Xufei complete her portion, which was the search_individual method.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Edit ARL structure
|February 14, 2021
|Completing tasks, editing the code
|
|}

== March 1, 2021 ==

==== General meeting ====
We just introduced how our tasking was going and what we'd be doing.

==== Subteam meeting ====
Gabe continued work with his group while my group talked about or tasking and specifics again. I told everyone that they were to do testing to make sure their methods worked. I then helped Xufei with some git again and then we left off. This meeting was very brief since we already had our tasks. I did tell everyone I hoped they would complete their task by Sunday.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Edit ARL structure
|February 14, 2021
|Completing tasks, editing the code
|
|}

== March 7,2021 ==

==== Subteam meeting ====
Over the week, I wrote an evaluation method. This method just currently takes the reciprocal of the fitness, multiplies it by the size of the subtree and returns that value. I have not pushed it to git, but I will after I see that everyone else's methods worked. During the subteam meeting, I answered some of Vincent and Bernadette's questions on their tasks. I clarified the format of the data and how their method would work. Gabe and the others also continued their work on the database. Gabe finished his part. I'm also done with my part since I've read through most of the code I needed to and wrote the methods that complement the tasks I handed out. From here, I will just be waiting for everyone else to finish their task and then combine all of the code into usable depth-2 tree arl code.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Edit ARL structure
|February 14, 2021
|Completing tasks, editing the code
|March 7, 2020
|}

== March 8, 2021 ==

==== General meeting ====
There was not much that we could present during the general meeting again, because it was just mostly working on tasks. One thing to note, Gabe mispoke and my team's tasks were not actually tested yet. Most of the code was written, but testing still needed to be completed.

==== Subteam meeting ====
There was not much to do here. We had met the previous day and concluded our tasks for the week. I did begin a new task and that was to start creating the presentation and writing about our efforts.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Create powerpoint for midterm presentation
|March 8, 2021
|
|
|}

== March 15, 2021 ==

==== General meeting ====
We shared during the general meeting that we would try to get an experiment done by midterm presentations, so we will aim to complete that. The other teams all seem to have conducted experiments.

==== Subteam meeting ====
We met on Sunday, and everyone except Vincent had completed their tasks. Vincent's task was a lot more difficult and time consuming than I expected since there was actually a lot more design and testing difficulty in this task than I foresaw. Then, on Monday, we talked about what we would accomplish this week. Everyone else will work on the powerpoint presentation while Vincent and I will prioritize getting a successful run of emade with this new architecture in. I didn't make the powerpoint presentation because instead Ivanna had already made it and shared it. She also put a few slides in already. I edited the get_best_adfs method to fit the functionality of the new generate_adfs method. I changed the line to call the new generate method:

adf_info = self._new_generate_arl(populationInfo, 2)

I also began working on changing the adf name string format because our ARLs are now of depth 2, so the formatting doesn't fit anymore.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Create powerpoint for midterm presentation
|March 8, 2021
|
|
|-
|Complete ARL structure change
|March 15, 2021
|
|
|}

== March 21, 2021 ==

==== Subteam meeting ====
We did a runthrough of the presentation today. I think everyone did great, although there were a couple of errors on the slides that we had to fix. Overall, this week, I can finally post all of the code changes, because I merged my new code with the old. I added an evaluate method, which is used to give a value to each ARL candidate. This value is used to create a cdf where we randomly pick a subtree to create an ARL. This evaluate method returns length/fitness where length is the number of nodes in the subtree and fitness of the individual where that subtree occurred. Each time the subtree occurs in the population, I add the return of evaluate to the value for the subtree in the cdf.

I also added a variable in search_individual and add_all_subtrees that tracks which individual the subtree occurred in. This way, when we create an ARL, we can go back and replace the subtree in each individual with the new ARL. We could also possibly use that information later to create genetic duplicates.

Here is the commit:

https://github.gatech.edu/gwang340/emade/commit/8af6226ca2fff3fab4dee06cd54c03e4156b636c

This runs without errors, but no ARLs are created. I traced this error back to our old ARL creation function. This ARL saves our ARLs as lambdas, but it assumes the ARL is a subtree of depth 1. Thus, ARLs are just not created. So, I am currently working to fix that issue and have made some progress. Hopefully I can post a commit next week.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Create powerpoint for midterm presentation
|March 8, 2021
|
|March 21, 2021
|-
|Complete ARL structure change
|March 15, 2021
|
|
|}

== March 22, 2021 ==

==== General meeting ====
Stocks team:
* Objective is still the same, use EMADE to optimize existing trading algorithms.
* They used a lot of literature and technical indicators from those papers to use in EMADE. Some indicators: OBV, CMF, KVO, VPT.
* Their best non-seeded individual had huge variance. There was one stock where they got 96% profit, but many other stocks just gave negative profit.
* This individual also has some major good transactions, but also multiple terrible transactions for specific stocks
* Future work: more indicators, more financial derivatives, different time granularities, nn evolution in EMADE, preprocessing
Bootcamp 1:
* Good looking graphs, highly detailed
* Good conclusion
* Many group members were articulate and actually good presenters
EZCGP
* Still working on graph based individuals instead of tree based individuals
*They looked at Transformers and RNN. However, they didn't find much literature in this respect. They tried to recreating some well-known architecture
*They were able to get good results, but it was very resource intensive
*They trained parameter benchmarking and got fit individuals that had relatively high accuracy, and they found that increasing Adam rate might help.
*They experimented with CIFAR-10 which can automatically create complex archiectures.
*They are still working on further visualization so they can make sure their architecture works.
Bootcamp 2:
* Good graphs
* Actually looked into the code
* Great job further experimenting to notice changes made by functions
Bootcamp 3:
* Good visualization of functionality
NLP
* Worked on moving NLP onto PACE-ICE. 
* Worked on merging different branches and reconciling their differences.
* They looked at the Amazon Product Reviews dataset with a binary model.
* They did some benchmarking with the LSTM Model and couldn't replicate
Bootcamp 4:
* Graphs look a bit scarce
* Good conclusion
Bootcamp 5:
* Well presented, well articulated.
== March 29, 2021 ==
==== General meeting ====
Every group is gearing up for the new semester students. We were informed of a few new first semester students. This is the biggest the team has been since I've been. We reported that we would mostly have the first semesters run tests and we have started using the MNIST data set.
==== Subteam meeting ====
Every reported basically that they didn't do much after the presentation. We discussed a bit about the direction we would go, which might be to look into new things, but we only have about 4 weeks left, so it's best not to start anything crazy. I did tell everyone that I would finish up our architectural change and once that's done, we could test it and possibly merge it with the new selection method I was working on last semester. Then, Gabe did the introduction for all of the first semesters and we left.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Complete ARL structure change
|March 15, 2021
|Working on lambda fix for the ARL change
|
|}
== April 4, 2021 ==
==== Subteam meeting ====
We had the first semesters try to run things on Colab and some were able to while some ran into path problems. I did help Xufei and Angela fix their problem and some first semesters as well, where in Colab, they could not find the xml file for some reason. I had ran into the same problem last semester and I fixed it by putting the full gdrive path. I also worked on the lambda fix and here is the commit:
https://github.gatech.edu/gwang340/emade/commit/5aa1dc67e43218ecd55b3b980397601fe71b59bc
However, there is one problem and that's currently there's no good way to keep track of the actual number of parameters required for an ARL. This is because when we create an ARL candidate, we store arity based on that subtree. However, we would need to add more parameters then, so we need to keep track of the actual arity. I had asked Gabe about whether we can find the number of parameters of a node based on just its name and he found that we can't so, in the meeting we decided Vincent would edit the add_all_subtrees method to add tracking for the actual arity of a node. I'd really like to finish this task by tomorrow or at the latest, Wednesday, so I can finally start running some experiments.

{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Complete ARL structure change
|March 15, 2021
|Working on lambda fix for the ARL change
|
|}
== April 5, 2021 ==
==== General meeting ====
We report on our progress with MNIST. The first semesters are beginning their MNIST runs. We want to make sure our port of the MNIST dataset works and that our branch can actually evolve with this data set. We also have some bugs with learnertypes that we ask about.  It turns out learnertypes are not primitives, but ephemerals. We have to make sure they work properly in our arls and that nothing breaks when we check its arguments.
==== Subteam meeting ====
We sort of already gave tasking on Sunday, so in our group meeting, we didn't talk about too much other than arl architecture stuff. There should be some bugs with the lambda function and we are working those out in a work session on Friday. After that, we also need to start working on contract_arls, where we go into the population and change individuals with the arl we chose by replacing the subtree with the one arl node. At the same time, there is some database functions that we have to alter because again, we change the format of our outputs and functions. The functions that do this are update_representation_with_arl_pset and clean_population. However, contract_arls should be the last big method and the other methods should just be some simple bug fixing and output format fixing.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Complete contract arls function in arl architecture
|April 5, 2021
|
|
|-
|Bug fix for arl architecture
|April 5, 2021
|
|
|}
== April 11, 2021 ==
==== Subteam meeting ====
Gabe, Vincent, and I had our work session on Friday. We did a bunch of testing and bug fixing. I had already done some over the week, so let me start with those commits:

https://github.gatech.edu/gwang340/emade/commit/6df2e5d7ce7777aab795644eb8a23c8ac2529bd8

https://github.gatech.edu/gwang340/emade/commit/7501abbc6dbb6279ebd3517b2c7da66fc3a485e9

https://github.gatech.edu/gwang340/emade/commit/b0147b334799ecb155e9088154b74c7cc2be0c7a

In these three commits, what I do is essentially finish the lambda creation function. I now have get_best_arls actually call it and use it to store the function string and other information in a dictionary. Then, I get other functions to use that information such as update_representation_with_arl_pset. Basically, these edits just now make it so that after I pick my arls, the lambda function is called properly and we actually put it into the primitive set. However, there are still many bugs there. Then, during the work session, we had a lot more bug fixes. I'll leave out most of those commits, but basically, we fixed the lambda creation function's bugs and we actually get everything up until contract_arls running. Here is one commit though just for some reference:

https://github.gatech.edu/gwang340/emade/commit/e29f0da9278af53bde2ce27dacdb7cbe0e8f0767

Next, we need to figure out how to also get inputs/outputs of nodes to properly put them into later methods where we'll need them. Our update_representation function right now returns this information, so we'll need to look into that. Anyway, after the work session, I finally also wrote the contract arls function. The only issue is that I currently cannot test it past a certain point because we have to have update_representation_with_arl_pset working and it's not working right now because we don't have the inputs to the nodes in our arls. We will have to look into that over this week. Back to contract_arls, here is the commit:

https://github.gatech.edu/gwang340/emade/commit/b0f6ce80f92a8f92dabd5f10421efc0b6f1bac95

I quickly did another bug fix commit which I'll just include:

https://github.gatech.edu/gwang340/emade/commit/16cefb1840b6f8a4dab0aeb59dc117c647dab9b9

I have structured contract_arls now so that in each instance I find an arl, I contract it into one node. However, arls might overlap and I do account for that edge case. When that happens, I just choose the arl that I picked first. This is random, but I feel that this shouldn't be a huge issue. To explain a little bit about the method, the method works in that I accumulate a dictionary where the keys are individuals and the values are instances where arls occur and I need them replaced. Then, I loop through the keys of this dictionary and replace all instances of arls for each key. This is the most efficient way I think we can do those, because we don't have to search through the whole population this way, since we already recorded all instances.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Complete contract arls function in arl architecture
|April 5, 2021
|
|April 11, 2021
|-
|Bug fix for arl architecture
|April 5, 2021
|
|April 11, 2021
|-
|Figure out how to get inputs/outputs for arls
|April 11, 2021
|
|
|}
== April 12, 2021 ==
==== General meeting ====
We again report on our progress. Our first semesters + Xufei, Angela have finished their runs with MNIST. It looks like most of their runs have little to no valid individuals. That's not exactly what we're hoping for, but the fact that we didn't get no new valid individuals is good news. Xufei and Angela however did have a decent run and they had quite a decent number of valid individuals despite not running for that many generations.

==== Subteam meeting ====
We talk a bit more about our MNIST progress. It looks like all of the first semesters didn't have good runs while Xufei and Angela got a good run. However, none of them did much data analysis themselves, so that's what they are tasked with. They need to look more into the data from these runs and analyze if the individuals were actually being mated properly, whether their runs were just poor due to random chance or due to a flaw in our architecture/port. As for me, I was able to figure out that getting input types of nodes is just through going to the primitive set and getting the args. This should be the line:
self.pset.mapping[node.name].args
From here, I'll need to edit the lambda helper to get the input types of each node to create our arl. This will be for later in the week and we might have a work session.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Figure out how to get inputs/outputs for arls
|April 11, 2021
|found out how, need to implement
|
|}
== April 18, 2021 ==
==== Subteam meeting ====
This week, I was able to add the input types lines to the code. This was done by adding to the arl_info_to_lambda function. In there, I go through all of the arls and structure them into a lambda through string concatenation and various other procedures. To do that, I loop through each arl, so this was the best place to find out the input values. I completed that and returned the input values, but I wasn't sure how to use it properly because I didn't know when we wanted to add to the primitive set. I also wasn't sure if other methods needed to happen first, so I just added the line of adding the arl primtiives in get_best_arls. Here are my commits over the week:

https://github.gatech.edu/gwang340/emade/commit/6e4d2c8ec5389fe7f5aea4875ae4a420fc0989b7

https://github.gatech.edu/gwang340/emade/commit/5f0dbdd479a55a15e79095f3f7ab7cf8208e5d96

However, the places I added were wrong. Gabe and I had another short work session today where we moved adding the arls to the primitive set to another separate method because Gabe said the workers were calling another function in another file. Then, we did some bug fixing again, but mainly, we were able to get update_representation_with_arl_pset done. Our data is now in a very different format, so we had to match everything with the required parameters for this function and basically just updated the old name from adf to arl. Here are the commits:

https://github.gatech.edu/gwang340/emade/commit/da4c41b32d56ee3c8ed14ff818879a3f8a734e55

https://github.gatech.edu/gwang340/emade/commit/63d1709c45be0b2516441dae5f6927747e8667a0

https://github.gatech.edu/gwang340/emade/commit/aa51b68d04b61f6225a7d5bcdcf3b80a9226e070

https://github.gatech.edu/gwang340/emade/commit/91e206e3c0e5e53aa7349587b8f785e512511556

Gabe was able to fix bugs for everything up to contract_arls and I bug fixed for contract_arls. These were just bugs that I was not able to test for before because arls were not yet being added to the primitive set and that is required for some of the later parts of contract_arls, so these bugs are pretty minor and not worth mentioning. I tested it later and it ran without errors, but saw some strange behaviors. I also don't think contract_arls is working properly because I don't actually see the arl in the population. However, that will be more bug fixing for later.

{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Figure out how to get inputs/outputs for arls
|April 11, 2021
|
|April 18, 2021
|-
|Last of bug fixing for the architecture change
|April 18 , 2021
|
|
|}
== April 19, 2021 ==
==== General meeting ====
We reported that we were done with the architecture change, just some minor bug fixes left. We also reported progress with MNIST runs and some analysis we got. We did have a pretty decent run and the other first semesters were not able to get better runs. It looks like the other teams are also doing a lot of testing, so I am hoping to test the architecture change soon. It also looks like the stocks team did a test to make sure of their goal, making an individual to works for every type of stock. I guess our architecture change is in the same vein, making sure arls can become good.
==== Subteam meeting ====
We got some more tasking for the first semesters. They will look into the data more and start making a new seeding file. This way, we can get better seeds and are more likely to get valid individuals. Gabe and I had another work session where we tried to work out bugs. We did find some and fixed them, but we still ended with some problems. Right now, the individuals changed in contract_arls are not being properly updated in the database. We don't really know what's happening and will probably need to do some more runs and tests to figure that out. We will probably have a work session later this week to finally fix that. Then, we also have an issue where arls are being created and we sometimes skip a number towards the beginning. We figured out that some were due to us setting a set number of arls, but actually having less candidates than that, so duplicate arls were causing issues. However, even after we fixed that bug, some arls still skip numbers, although less of them. I also fixed another bug where the lambda function was erroring out, but it wasn't being properly caught and the entire method would end. Thus, I added try and accept to allow us to create more arls anyway. Here are some of my bug fixing commits:

https://github.gatech.edu/gwang340/emade/commit/5a7d14c5dd0a29b036a3b6c6ec6e01faa9354943

https://github.gatech.edu/gwang340/emade/commit/398ba4353fd7ae60f77455302635148e824de7ab


{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Last of bug fixing for the architecture change
|April 18 , 2021
|
|
|}
== April 25, 2021 ==
==== Subteam meeting ====
This week was a bit slow due to all of our other classes and commitments, but I did test a couple more times over the week and tried to fix bugs in our code. However, I just could not fix the issue with ARLs skipping numbers when being named or the fact that individuals with ARLs are not being added. Gabe and I decided that we would do more bug fixing tomorrow. Our runs for MNIST are starting to come together as well and many of the first semesters have created decent seeding files, so they're seeing more valid individuals. We also created the presentation which we will be working on for the next week until the final presentation.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Last of bug fixing for the architecture change
|April 18 , 2021
|
|
|-
|Created presentation
|April 25 , 2021
|
|
|}
== April 26, 2021 ==
==== General meeting ====
Overall, all of the teams are finding results in their runs and they're pushing to get stuff out before the final presentation. We talked about how our architecture is in the last step and we're trying to get runs out before Friday. Our MNIST runs are also doing pretty well, having the first semesters work together to push them out.
==== Subteam meeting ====
First, we scheduled some practice/work sessions together to create the presentation. Then, Gabe and I also started working on more bug fixing. Gabe was able to figure out the ARL numbering fix which was some issue in emade.py calling arl architecture stuff. However, we still can't fix the individual stuff. We found that some issues are occurring around contract_arls though we can't find the bug directly. We can see that in our output, contract is being called, new individuals are being created, they're being added to the population, but the individual sometimes doesn't change. We scheduled a meeting for Tuesday to continue working so we can get experiments out before the presentation.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Last of bug fixing for the architecture change
|April 18 , 2021
|
|
|-
|Created presentation
|April 25 , 2021
|
|
|}
== April 30, 2021 ==
==== General meeting ====
Presentation Notes:

Stocks:
* Implements TA-Lib indicators in Emade
* Increase evolvability 
* Test on larger data sets/different stocks
* Get new objective functions
* Some indicators: VWMA, VWAP, FIBRET
* They analyzed all of the primitives they used and gave them ratings based on individual performances
* They've also been comparing their results to randomness, which by default actually generates profit, rather than just looking at profit by itself which can be misleading

EzCGP:
* Worked on CIFAR-10 in terms of individual size, block size, etc.
* They tested evolving dense layers and got results with good performance and less overfitting
* Improved visualization techniques and right now they can see the entire directed acyclic graph of the individuals.
* They worked on improving seeding by importing more models from tensorflow
* They implemented a new way to run ezCGP so that they can easily run different iterations of the same problem
* They used one-point crossover mating

NLP:
* They used Layerlists
* One of their first goals was to actually be able to run their branch of EMADE. They set everything up on PACE
* They pretrained embedding layers which are vector representations of words learned by the predefined vocabulary, bug issues arose and couldn't be used
* They used the Amazon product data set, but in the latter half of the semester, they realized it was too big and just ended up using a portion of the data set
* They had good individuals with actually very low FPR and FNR rates.
* In the future, they will increase network complexity by examining structure, use CV, and fix pretrained embedding layers

==== Subteam meeting ====
First, Gabe and I met on Wednesday. During that time, we debugged some more. I fixed some edge cases in contract_arls that should have been preventing arls from being inserted at all due to causing an error. Then, Gabe fixed some issue in emade.py that apparently allowed all of the arls to be added to the population. Here are some of the edits I made:

https://github.gatech.edu/gwang340/emade/commit/db22680eb1f7c6cf38905a877d79f3eb06b6e17f

https://github.gatech.edu/gwang340/emade/commit/6824e5a9b3b11d4e48d7b8c7b3d8a3a632cefb1d

https://github.gatech.edu/gwang340/emade/commit/1dafef6fdba84889d09d8560b86c577591a6969f

https://github.gatech.edu/gwang340/emade/commit/8d90c10794a3ee86304a9946e939305ece41039b

https://github.gatech.edu/gwang340/emade/commit/2305ef440c7668bfec067b2ffc5287b777d949fb

One bug that was causing errors was that one of my loops broke when the index was equal to the length of a list, which would often happen after a check into that list, so we would get a list index out of bounds. Another fix was just checking to make sure when we were using a key on the primitive set, that the primitive exists first so that we don't error out.
Then, we met on Thursday to practice our presentation and work on it. Here is the link:

https://docs.google.com/presentation/d/1SLLHwjsy-ZHV4OqAXDBclBeTzqrSbyCNCJNRZs0a8Kc/edit#slide=id.gd39f2ba96f_0_0

I created the slides on arl architecture changes and the major improvements we made and how we did them. As for runs, Gabe was able to get some in and posted the graphs.
{| class="wikitable"
!Task
!Date Added
!Current Status
!Date Completed
|-
|Last of bug fixing for the architecture change
|April 18 , 2021
|
|April 30, 2021
|-
|Created presentation
|April 25 , 2021
|
|April 30, 2021
|}