== Team Member ==
[[files/GeorgiaTechBuzz.jpg|thumb|123x123px]]
Team Member: Michael Piseno

Email: mpiseno@gatech.edu

Interests: Deep Learning, Cybersecurity, Mathematics, Snowboarding

== January 7, 2019 ==
'''Team Meeting Notes:'''
* Rod explained the concept of ezCGP to the group and the goals we were trying to move towards.
* Everybody shared their Github accounts with Rod so that we could be added as contributors.
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look over ezCGP code
|Completed
|Jan 7, 2019
|
|
|-
|}

== January 14, 2019 ==
'''Team Meeting Notes:'''
* Discussing issues the Deep team had last semester and how to split up the groups.
* Went over more CGP topics
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read IEEE paper that was posted on the slack
|
|Jan 14
|
|
|-
|Look at the Genome file in the Github
|
|Jan 14
|
|
|}

== January 28, 2019 ==
'''Team Meeting Notes:'''
* Since last week we've all read the IEEE paper that rod gave us: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6815728. This paper just talked about different mutation methods and their effectiveness in CGP, which we will try to implement.
* We have all run the ezCGP code and began manipulating hyper-parameters and adding new primitives to see the effect on the convergence rate, but it seems to be too random right now
* We plan on spending the new couple weeks integrating Tensorflow/Tensorboard into the code as well as emade.

Here is an example of a run that compares our target function to the hypothesis function and plots the loss over number of generations:
[[files/Seed10 gen199 Exponentiation.png]]

== February 11, 2019 ==
''Team Meeting Notes:'''
* Since the last meeting we have working on implementing Tensorflow primitives into ezCGP and writing now evaluation methods that work specifically for running Tensorflow graphs.
* This week we plan on testing primitives such as convolutional and residual blocks and implementing more functionality so that we can use the evolutionary process with Tensorflow blocks

Issues:
* We are currently running the full dataset through at one time (i.e. the batch size is equal to the dataset size), which slows down our computation. We will work on a fix to this
* We also would like to not rebuild the Tensorflow graph each time we mutate but instead edit the existing parent ones.
* We plan on meeting on Thursday to implement new feature together after having individually tested new Tensorflow primitives.

== February 14, 2019 ==
''Team Meeting Notes:'''
* I've tested several primitives including basic Tensorflow add, subtract, multiply, and divide functions as well as convolutions. It appears that the convolution wasn't working and was taking forever anyway.
* We implemented functionality to feed in batches to the training so that we aren't training the entire training dataset in one pass.

Issues:
* We want to be able to run the full code with Tensorflow functionality from main.py instead of from our custom Testor.py method. This essentially will allow us to run the entire evolutionary process rather than simply testing if Tensorflow blocks work in our code. 

== February 18, 2019 ==
Plans for this coming week:
* Test a primitive exhaustively
* Implement functionality for evaluating validation accuracy
* Try to get the full evolutionary process working

== February 28, 2019 ==
Today we met to discuss the changes made to the code over the past week. We implemented functionality for running the entire evolutionary process from main.py with the tensorflow blocks active. I also added code to problem.py that will allow us to use the CIFAR-10 dataset to test our architecture instead of just the MNIST dataset.

Plans until Monday is simply getting everyone up to date on the code changes and making sure everyone knows what works. On Monday we plan to discuss researching different papers to implement other architectures into our code. I think we also need to start working toward running on Google Cloud.

== March 4, 2019 ==
Today we talked about our current problems in integrating the CIFAR-10 problem into our current codebase and the usability of our code to outsiders. We also discussed what our long-term goals are; Namely, developing a useable framework for other people to evolve architectures and verifying that our block-centered Auto-ML model actually works well so that we can publish.

Goals for next week:
* Prepare for presentation: have a couple people run MNIST problem on their machines and possible CIFAR-10???
* Research some other architecture primitives to implement
* Look at developing a more usable framework that is robust to different datasets
* Look into saving the best model and metadata for Tensorboard visualizations

== March 7, 2019 ==
Today we met as a group to work on the presentation to the first-semester students that is happening on Monday. We're also trying to fix memory issues in our code.

Current Issues:
* When we run our main evolution, we run out of memory very early.
* We still want to add more primitives and make the framework easier to use

Goals for Monday:
* Have a full presentation ready to go
* Fix memory errors and research some primitives to include

Personal Goal:
* Define a specific research goal for the rest of the semester and start reading a lot more papers

== March 28, 2019 ==
We held our usual Thursday meeting and discussed splitting the team into two smaller sub-teams with more specific goals so that it's easier for everyone to contribute and for the first-semester students to get started. More specifically, we have two teams:
* Team A: Team A is a software engineering team that will focus on taking the ezCGP framework and making it a more user friendly framework that other people can use and contribute to.
* Team B: Team B is a more research/experiment focused sub-team that will add more deep learning specific primitives and run experiments such as adding LSTM cells and getting results on CIFAR-100.

We will begin development on our next Thursday meeting. Team A will start with UML diagrams and basic software design while Team B will being trying regression tasks.

Here are links to some current papers we are reading:

* https://link.springer.com/article/10.1007/s10710-018-9339-y
* https://arxiv.org/abs/1801.01563

== April 4, 2019 ==
Today we met up and made significant progress once splitting into our two teams.

Team A (Rodd Squad):
* We met with Rodd and began working out the mutation of arguments to our primitives, which was not working before. We got this working in a relatively short amount of time.
* We began discussing how the code structure might change when we tackle the challenge of turning ezCGP into a user-friendly framework

Team B (Talebi Tubbies):
* I'm not on Team B, but they were working on getting regression problems to work with ezCGP, specifically with the housing price dataset from Kaggle.
* Team B also us actively reading different papers to find good primitives for ezCGP and is currently working on implementing an LSTM primitive.

== April 8, 2019 ==
Today was our normal Monday meeting and we discussed our goals for the week. I'll be reporting mostly on the progress of Team A (Rodd Squad) for the rest of the semester since I'm not on Team B. We want to implement the ability to take it any dataset into our framework like in Tensorflow or PyTorch, rather than our current setup which is specific for each problem. Personally I think we should focus on software design (UML diagrams and a lot of planning) before we start implementing all this functionality. I'll bring this up at the next team meeting on Thursday.

== April 11, 2019 ==
Today Group A simply added in functionality to take in large datasets by loading in smaller subsets of the main dataset into memory so that the entire dataset isn't in memory at one time. This was a matter of simply assuming the user will divide up the large dataset into smaller ones and place them in some subdirectory, then calling our previous data loading function on demand when a new dataset is needed. In completing this task, we have completed all our goal that we set out to complete for this semester and will now begin planning when we want to accomplish for next semester.

== April 22, 2019 ==
Today we have our final presentation. We presented our results on several experiments including predicting housing prices and image classification on MNIST, CIFAR-10 and CIFAR-100. Interestingly, the best CIFAR-10 individual did better when training on CIFAR-100 then the best individual when the evolution was trained on CIFAR-100. We are currently training our best model from the MNIST evolution separately on the cloud to see how high we can get the accuracy. I'm in charge of the concluding slides and the slides dealing with the goals for next semester. Here is a recap from the slide about our accomplishments this semester:

* Successively developed a workable framework that evolves architectures and achieves good performances in general.
* Nearly state-of-the-art performance on MNIST (99.34% accuracy)
* Good performance on CIFAR-10 (81.13% accuracy with no data augmentation and 90.06% with data augmentation)
* Poor performance on CIFAR-100 (41.01%) but decent result when running the best CIFAR-10 model on it (59.13%)
* The framework can support classification and regression problems with support for running on large datasets incapable of being held entirely in memory. 
* The framework leverages Graphics Processing Unit (GPU) support whenever available and can be written to also run on Google Clouds Tensor Processing Units (TPU) for future experiments. 

And finally here is a recap of what we want to work on next semester:

For Team A:
* Make ezCGP more usable from an outsider perspective with documentation and a github pages site
* Incorporate a more complex primitive set (modifying parameters within cells like gates for the LSTM for example) (This could be a Team A and B dual effort)
* Be able to run on TPUs and be in contact with Google about more credits because thats expensive

For Team B:
* Run on a more complex regression problems like stock prediction
* Imagenet?????


== Grade I Think I Deserve ==
I contributed to the code base throughout the semester by adding functionality to support the CIFAR-10 dataset as well as fixing issues such as large datasets causes memory errors when training. I also helped with the organization of the team by suggesting that we split up into team A and B and coming up with several of the completed and future goals for the team. Specifically, I suggested we develop ezCGP into a more user-friendly framework with documentation. Finally, I along with most of my other team met twice a week (including our weekly Monday meeting) and kept up with my notebook all semester. For these reasons, and because I'm only taking the course for 2 credit hours, I believe I deserve an A in the course.

= Fall 2019 =
== August 26, 2019 ==
'''Individual Updates'''
Goals for this week:
* Do a literature review of the current bloat control methods used in practice and see how we might integrate them into emade
* Create a timeline of goals based on the research we find
* Look in to how we can improve upon the current research

We want to fundamentally understand how to define bloat, how to detect it during the evolutionary process, and how to take preventative measures towards it. Furthermore, we want to understand how high-level primitive like the machine learners inside emade affect bloat.

'''Team Updates'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Due
|-
|Collect and read current literature
|Not Started
|August 26, 2019
|August 31, 2019
|-
|Create semester goals
|Not Started
|August 26, 2019
|August 31, 2019
|}

== August 30, 2019 ==
'''Individual Updates'''
* Finished reading "A Comparison of Bloat Control Methods for Genetic Programming" and almost done reading "neat-GP" paper. 

It seems that there are a couple of common ways to deal with bloat. Many of the current techniques simply limit the depth of an individual, while some others focus on punishing individuals based on size. The neat-GP paper argues that bloat is a natural product of evolution because of the change in distribution of individual sizes as evolution occurs, and tries to explicitly correct for that.

Goals for this week:
* Finish the neat-GP paper
* Work with the others to integrate a simple bloat metric in emade and discuss the feasibility of integrating neat-GP into emade

'''Team Updates'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Due
|-
|Collect and read current literature
|In Progress
|August 26, 2019
|August 31, 2019
|-
|Create semester goals
|Finished
|August 26, 2019
|August 31, 2019
|}

We finished collecting a list of papers that we want to read (or are currently reading) that will help us understand bloat and direct our goals for the semester. This list of papers is available below. So far, we have each read the survey of different bloat control methods in genetic programming and are going through the neat-GP paper. Our goals for the month of September will be as follows:
* Continue with our literature review
* Create a basic bloat detection metric within emade and test it on simple benchmarks to see how much bloat is currently being generated
* Look into individual-level bloat removal techniques to see if they are feasible

[[files/NeatGP-Final.pdf]]

[[files/A-comparison-of-bloat-control-methods-for-genetic-programming.pdf]]


== September 6, 2019 ==
'''Individual Updates'''
* Carissa and Josh joined the group so I got them caught up with the papers we are exploring and plans we currently have
* Planned our the benchmarks we're going to use. I will be compiling and cleaning up the datasets using for symbolic regression and later a more advanced classification task like MNIST, as well as running the actual benchmarks once we finish implementing the simple bloat metric

Blockers:
* We don't yet have GCP credits, so we will have to run locally if we can't get this working in time

'''Team Updates'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Due
|-
|Collect and read current literature
|Finished
|August 26, 2019
|August 31, 2019
|-
|Benchmark symbolic regression with simple bloat metric
|Not started
|September 6, 2019
|September 16, 2019
|-
|Begin developing individual level bloat metrics
|Not started
|September 6, 2019
|September 16, 2019
|}

What we did:
* In our previous meeting on Friday we split in to two subgroups - one to work on individual level bloat quantification (Tan, Jefferey, and Aminesh). This means looking at bloat at an individual level, determining what about the individual is bloat, and then deciding how to remove it. The other subgroup (Erik and Michael) will focus on quantifying and  reducing bloat in the overall evolutionary process.
* One of the initial ideas the individual level group is exploring is using caching to see if subtrees within individuals are simply passing data straight through a primitive (i.e. a useless primitive within a tree).


== September 16, 2019 ==
'''Individual Updates'''
* Compiled symbolic regression datasets to model simply polynomial and sinwave functions
** y = x^4 + x^3 + x^2 + x
** y = sin(x^2) + cos(x) - 1

To do this I simply created a generator function that outputs the y-values of a certain number of points on the interval [-1, 1]. Then saved the data to a csv. I did this for both datasets. 
EDIT: The commit and script can be viewed here https://github.gatech.edu/efrankel6/emade/commit/ee4204d8834988bde359fd549f39067fb10e5ad3#diff-7d7f95418289a162ae1ba7e5f0e6445f

Blockers:
* Eric is almost finished with the basic bloat metric implementation in emade
* Still don't have GCP credits, but I'm going to look into creating an AWS instance

'''Team Updates'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Due
|-
|Benchmark symbolic regression with simple bloat metric
|In Progress
|September 6, 2019
|September 16, 2019
|-
|Design individual level bloat metrics
|In Progress
|September 6, 2019
|September 16, 2019
|-
|Look into neat-GP implementation outside emade
|Not Started
|September 16, 2019
|September 23, 2019
|}

What we did:
* Almost finished with basic bloat metric in emade - just a few bugs to sort out
* Got Carissa and Josh caught up with the reading and starting to help with neat-GP and individual bloat metrics

== September 20, 2019 ==
Quick subteam meeting update

What we finished:
* Finished reading neat-GP paper
* Erik finished implementing the simple bloat metric in emade.
* I haven't looked in to the AWS instance yet, but will do that by Monday

What we plan to do the next couple weeks:
* Test benchmarks (MNIST, symbolic regression) to see how much bloat they have
* Implement neat-GP and compare it to non-neat-GP

== September 23, 2019 ==
'''Individual Updates'''
* Since the basic bloat metric has been completed we can begin running symbolic regression benchmarks.
* Looked into AWS instance and it should be fine to just use a basic EC2 instance.
* After we run the symbolic regression benchmarks, I will begin to compile the MNIST dataset

Here are the actual datasets available for download:

[[files/Polynomial_train.csv.gz]]

[[files/Sinwave_train.csv.gz]]

[[files/Polynomial_test.csv.gz]]

[[files/Sinwave_test.csv.gz ]]

'''Team Updates'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Due
|-
|Benchmark symbolic regression with simple bloat metric
|In Progress
|September 6, 2019
|September 16, 2019
|-
|Design individual level bloat metrics
|In Progress
|September 6, 2019
|September 16, 2019
|-
|Look into neat-GP implementation outside emade
|Not Started
|September 16, 2019
|September 23, 2019
|}

== September 30, 2019 ==
'''Individual Updates'''
* Ran test runs for the symbolic regression locally and it works. No other updates for myself this week.

'''Team Updates'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Due
|-
|Benchmark symbolic regression with simple bloat metric
|In Progress
|September 6, 2019
|September 16, 2019
|-
|Continue design of individual level bloat metrics
|In Progress
|September 6, 2019
|September 16, 2019
|-
|Continue neat-GP implementation
|In Progress
|September 16, 2019
|September 23, 2019
|}

What we finished:
* We finished implementing the simple bloat metric in emade
* We compiled benchmark datasets for testing this bloat metric in emade (MNIST and symbolic regression)
* The individual level bloat quantification subteam  has been working closely with DEAP and can now quickly check the output of functions from subtrees, which will allow us to see bloat within specific subtrees.

What we plan to do:
* Actually test on these benchmarks and get an idea of the current bloat in emade
* Josh will continue work on the neat-GP implementation outside of emade - not sure the status of that.

== October 20, 2019 ==
'''Individual Updates'''
* A lot of progress since the last update, and we have our midterm presentation tomorrow.
* Created the AWS instance we were going to use to run out symbolic regression benchmarks, however I was having issue with the disk space. No matter how big of a run I tried to do, I kept getting "out of disk space" error. I tried to only download the dataset I was using but this did not help. I think connected an S3 bucket for this will work, but it might be more beneficial in the short term to just run locally. We haven't yet looked in to using GCP.
* Ran the symbolic regression benchmarks and noticed interesting results. First the bloat which is defined as:

 (change in tree-size) / (change in fitness)

was actually going down as the generations progressed, even though we were not explicitly optimizing for tree size. This is contradictory to the hypothesis stated in the neat-GP paper, however they were using much simpler primitive, which might speak to the unique properties of using high level primitives to solve GP problems like those in emade. Here are the plots for the two different symbolic regression problems.

[[files/Poly_symb_reg.png ]]

[[files/sin_symb_reg.png ]]


The only evaluation metric we used for both of these runs was Continuous MSE. Note that bloat and tree size seems to be highly correlated. And after about 50 generations we see them reach a minimum and then start to increase again.

'''Group Updates'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Due
|-
|Benchmark symbolic regression with simple bloat metric
|Finished
|September 6, 2019
|September 16, 2019
|-
|Continue design of individual level bloat metrics
|In Progress
|September 6, 2019
|September 16, 2019
|-
|Continue neat-GP implementation
|In Progress
|September 16, 2019
|September 23, 2019
|-
|Look into MNIST benchmarking
|Not started
|October 21, 2019
|November 11, 2019
|}

What we've done:
* Over the past couple week we've ran the symbolic regression benchmarks in emade and got data using our bloat metric.
* We created an AWS instance to run larger experiments but are having issues with disk space on the instance
* The others have continued their work on checking for bloat via subtree hashes.

What we plan to do:
* We will continue trying to improve the current results and understand how higher order primitives affect the bloat
* We will try to implement a basic form of the neat-GP algorithm and analyze the bloat using that algorithm
* We will also collect MNIST datasets and try to run the benchmark the same was we did with the symbolic regression
* Created our midterm presentation. https://docs.google.com/presentation/d/1cJccFW1RC8qtmwskcuaJ-xSwl0Heyv_IwihF0YMfo4k/edit#slide=id.g640aa78e01_0_1

== November 4, 2019 ==
'''Individual Updates'''
* Reran symbolic regression benchmarks locally to confirm the trends we saw in our midterm presentation

After the strange results we saw in the midterm presentation (namely, that bloat was actually decreasing without any specific measures being taken to prevent it) as generation increased, I saw fit to rerun the datasets for ~100 generations to confirm these results. I saw the same trends.
* Initial efforts with GCP setup. Knowing that MNIST is a larger datasets, it became necessary to figure out how GCP works. I got in contact with Jason and James to try to set up a meeting time for figuring out GCP.

'''Group Updates'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Due
|-
|Benchmark MNIST with basic bloat metric
|In Progress
|October 23, 2019
|November 11, 2019
|-
|Continue design of individual level bloat metrics
|In Progress
|September 6, 2019
|September 16, 2019
|}

What we've done:
* We completed our midterm presentation and got interesting results: bloat was actually decreasing as generations increased even though tree size was not explicitly optimized for
* Josh found a neat-GP implementation and began running it on the UCI breast cancer dataset for testing.

What we will do:
* We will run MNIST on GCP to benchmark it for the basic-bloat metric that is currently in emade.
* We will try to implement neat-GP into emade and test it on all our benchmark datasets.
* Figure out why bloat decreased in the midterm presentation

== November 11, 2019 ==
'''Individual Updates'''
* Created the MNIST datasets that we will use to benchmark once GCP is up and running.
* Still looking in to GCP setup.

The MNIST dataset that I cleaned up can be downloaded here

[[files/Train.csv.gz]]

[[files/Test.csv.gz]]

The script that generated the data and put it into emade format can be seen with this commit. https://github.gatech.edu/efrankel6/emade/commit/ee4204d8834988bde359fd549f39067fb10e5ad3#diff-b19f71a7dd277ce1ce22e348ab41eb3a

Blockers:
* Various GCP issues: database issues cause emade to crash after the first generation. This error can be seen here.  [[files/Gcp_db_error.docx]]

I resolved this error by simply casting the bloat and hypervolume parameters of the add_bloat() function to the normal python float type instead of numpy.float64. This function was in the sql_connection_orm_master.py file


'''Group Updates'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Due
|-
|Continue design of individual level bloat metrics
|In Progress
|September 6, 2019
|September 16, 2019
|-
|Continue neat-GP implementation
|In Progress
|September 16, 2019
|September 23, 2019
|-
|MNIST benchmarking
|In Progress
|October 21, 2019
|November 11, 2019
|}

What we did:
* Eric did a good writeup on our thoughts about the mmidterm benchmark results, which can be seen in the Novermber 5th entry in his notebook https://vip.gatech.edu/wiki/index.php/Notebook_Eric_Frankel
* Finished preparing MNIST images data for benchmarking
* Ran titanic with "Cleaning", meaning the pareto individuals were cleaned up with an individual level bloat detection process.
* Assigned the new first semester students specific subteams after catching them up with the research papers we've been reading

What we will do:
* Finish GCP setup and run MNIST benchmark for our final presentation

== November 18, 2019 ==
What we've done:
* The rest of the team began implementing neat-GP into emade for further benchmarking, while another group is working on individual level bloat quantification using caching. 
* I've set up the MNIST image datasets to be run in emade as well as set up a GCP instance to run the basic bloat benchmarks on MNIST. This is still having some issues that will be worked out at the hackathon.

Plans for upcoming weeks:
* We plan to continue work on implementing the neat-GP algorithm into emade as well as working on caching for individual level bloat quanitification.
* There are still a couple issues with GCP and once those are dealt with I will run the MNIST benchmarks. The issues with GCP are trying to copy the compute image to all the nodes in the slurm cluster and connecting to the database on each node.

== December 2, 2019 ==
'''Individual Updates'''
* Over the past couple weeks I worked closely with Jason to run the MNIST benchmarks in time for our final presentation. We had to rerun many times due to small GCP errors and one time because I only had 1 evaluation metric defined, which doesn't work well with emade.
* I defined a new evaluation metric for multi-class classification, like MNIST, in emade called multiclass_f1-score() in eval_metrics.py. To make it conform to the usual minimization problem, the metric actually calculates 1 - f1score.
* I did multiple runs both on GCP and locally but the evolution started to significantly slow down after about 60 generations, making it difficult to run much more than that in time. The results were similar to the midterm results in that the bloat decreased with increasing generations despite not optimizing for tree size. Plots for the MNIST results can be seen below

[[files/mnist_benchmark.png]]

As we can see from the plot above, a similar trend occurred as in the midterm presentation, namely there was a decrease in bloat until around generation 50-60 followed by an increase. My best explanation for this is that the high-level primitive such as the machine learners in emade actually serve to decrease bloat by making the tree more expressive (more fit) with less primitives, but after a certain point in the evolution the expressiveness of the primitives is too little and bloat continues to increase. It would be interesting to see how this trend continues if we run for hundred of generations, however like I mentioned before the evolution started to become impossibly slow after about 60 generations.


'''Group Updates'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Due
|-
|Continue design of individual level bloat metrics
|Finished
|September 6, 2019
|September 16, 2019
|-
|Continue neat-GP implementation
|Finished
|September 16, 2019
|September 23, 2019
|-
|MNIST benchmarking
|Finished
|October 21, 2019
|November 11, 2019
|-
|Work out next-semester goals
|Finished
|-
|-
|}

What we did:
* Ran the MNIST benchmarks on the simple bloat metric
* Completed speciation part of neat-GP integration with emade
* Individual level cleaning (i.e. bloat removal) is complete but not integrated with emade due to parsing errors in the individuals.

'''Plans for next semester'''

This semester we developed several methods for bloat detection. We ran the simple bloat detection method on a few benchmarks in different problem domains (symbolic regression and multi-class image classification). We also implemented an implicit bloat control method in emade that was based on speciation in the neat-GP paper and developed methods to detect bloat at an individual level and remove it. These initial stride into this problem create a strong foundation from which to build next semester. Next semester we will focus on finish integrating the neat-GP paper methods into emade which will implicitly control bloat throughout the evolutionary process, then use the same benchmarks on the simple bloat metric and compare pre and post neat-GP emade to see if the implicit bloat control was successful. Further, we plan to continue work on individual level bloat removal integration with emade. Our end-goal is to be able to control bloat throughout the evolutionary process through a combination of neat-GP and individual "cleaning" every few generations.