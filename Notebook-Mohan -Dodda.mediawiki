== Team Member ==
Team Member: Mohan Dodda

Email: mohandodda@gatech.edu
Cell Phone; 474021048

Interests: Machine Learning (NLP, Recommender Systems), Basketball, Foreign Animation, Youtube 

== January 7, 2019 ==
'''Team Meeting Notes:'''
* Learned basis of Genetic Algorithms, learned keywords, learned how to apply it
* Got introduced the lab
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Completed
|-
|Do the Lab
|Finished
|January 7, 2019
|January 13, 2019
|-
|Set up Notebook
|Completed 
|January 7, 2019
|January 13, 2019
|}
Decided to add more info to lab 1: 

N-Queens:
[[files/First.png|thumb]]
n=30 -> picture to the right

increasing the n value seems to make finding the minimum fitness take more generations

However, going too low is not enough (n=5)

== January 14, 2019 ==
'''Teams introductions'''
* Data visualization Team
* EEG team
* Deep Learning Team
* Caching
'''Notes'''
GP: Genetic Programming

Evaluation
* Fitness Computation
* Selection
* Mating
* Mutation

* Genome (list of genes) -> Eval -> Objective Scores

* DEAP
* * Nodes are Primitives +, -, *, /
* * Leaves- Terminals
* ex. 1 +(3*4)
    +
  /   \
 1    x
     /  \
   3  4
[+ 1 * 3 4]

*ex. (0 + 1) -2
ex. [- + 0 1 2]

[+ 1 + 0 1]

Crossover - you switch part of trees

Symbolic parts of trees
 y = sinx = x-x^3/3!+ x^5/5! - x^7/7! ...
primitive set: +, -, *, /
terminals: x, constans
you only do part of it

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Completed
|-
|Do the Lab #2
|Finished
|January 14, 2019
| January 27, 2019
|-
|Describe Lab 
|Finished 
|January 14, 2019
|January 27, 2019
|}
[[files/Afterthat.png|thumb|187x187px]]
* The first picture is the output of the lab after without adding any extra primitives or mutations
* The second picture is the output of the lab after adding the positive and mod primitives and adding the insert mutation.
** The average does not show because it shows it as Nans.  
[[files/Next.png|thumb|170x170px]]
* Removing the mod primitive fixes this problem; however, this results in a similar graph as before. This means that the primitive and mutations are not beneficial in finding the answer we want. 
* Tried all sorts of combinations including mutation shrink, floor divide and mutation replace average did not show. Disregarding the average, the performance did not change despite changing with the parameters

== January 28, 2019 ==
'''Notes:'''
* What are we looking for a mate: nice, funny, smart, loving
* What is an algorithm looking for a mate: accuracy, validation, having good hardware requirements, size, power

* Data set contains
** Positive Samples
** Negative Samples 
* Classify them based on Data set
** leads to confusion matrix ->
** Females have black hair: algorithm
** TP: females who have black hair 3
** FN: females who don't have black hair 1
** FP: males who have black hair 20
** TN: males who dont have black hair 4
** TPR = 3/4
** TNR = 4/24
** FNR = FP/N = FP/(FP +TN)
** FPR = 1 - TNR = 1 - SPC

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Completed
|-
|Do the multiple objective part of the lab
|Finished
|January 28, 2019
| February 2, 2019
|-
|Describing output of lab
|Finished
|January 28, 2019
| February 2, 2019
|}

== February 2, 2019 ==
'''The Lab:                                                                                                                                                                                                                                             Running The lab with given values:'''
[[files/Objectivespace.png|thumb|197x197px]]
[[files/Fitnesswithmultipleobjective.png|thumb|205x205px]]
Individuals dominating our pareto front are closer to the bottom rightÂ 

The second graph shows the fitness over time. 

The tree size is has a higher variance though the seems to be increasing over time. 

However for the MSE, it decreases ever so slightly. 

tree size: orange and red

MSE: blue and green
 Best individual is: negative(cos(multiply(add(cos(sin(cos(multiply(add(cos(cos(x)), cos(add(multiply(x, x), sin(x)))), tan(x))))), cos(x)), tan(x))))
 with fitness: (0.27530582924947056, 25.0)

[[files/Paretofront.png|thumb|196x196px]]

The Pareto Front area is best minimized. 

Area Under Curve: 2.463792426733847 with default values. Messing with the primitives can change the area under the curve.

'''Running the Lab by modifying primitives:'''
[[files/Objectivespace1.png|thumb|153x153px]]
'''Getting rid of the sine, tangent, and cosine functions''' drastically decreased the Pareto function area to 0.6912744703006891 which is a '''71.94266599% decrease'''. This might be because this the best individual was something much less complicated than before: 
 subtract(x, x)
 with fitness: (0.7223441838209306, 3.0)
[[files/Fitnesswithmultipleobjective1.png|thumb|175x175px]]
Getting rid of the the sin, cos, tan primitives got rid of the outlier making the objective space much more visible. 

The green and points are on the top line only. The blue point is on the top too. The red points are on the left side of the graph. 

Getting rid of the outlier seems to have greatly decreased the pareto space area. The pareto front area also seems to look much simpler due to simplifying the primitives

== February 4, 2019 ==
In class Notes:
* Machine learning Titanic Dataset:
* get rid of null values
* get data ready for machine learning
* Use machine learning models from scikit learn 
* tune the hyperparameters 
* See how other models perform 
* submit your data on kaggle 
* Create a pareto front by minimizing FP and FN 
* Plan on meeting  

[[files/Paretofront1.png|thumb|162x162px]]

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Finished Date
!
|-
|Use python/ jupyter notebook or collab to use machine learning model to 
predict titanic survivors on titanic dataset in kaggle 
|Completed
|February 4, 2019
|February 9, 2019
|
|-
|Tweak hyperparameters to improve model, plotting pareto front
|Completed
|February 4, 2019
|February 10, 2019
|
|}

== February 8, 2019 ==
Talked how we are going do the date mining

We decided what variables to use in our model

We looked at the example one on the 4th

Got all members up to date on machine learning

== February 9, 2019 ==
* We started the parsing of the data using pandas.
* We used part of the data for testing
* We started using a MLP classifier for the analysis
* However, the output was all 0s
* We later discovered that the age and fare variables, which data was much bigger than the ones as the other ones were just as classifiers
* We dropped it and we got an answer. 
** We submitted this on kaggle

== February 10, 2019 ==
* We met up and did three other models. 

* After tweaking the hyperparameters, we discovered that desision tree was the best model
** However, after we submitted it on kaggle, we discovered that this was untrue with the classic case of overfitting
* The MLP classifier ended up being the best one on kaggle
*[[files/Paretofrontmachinelearning1.png|thumb]]We did the pareto front. It showed that there was one clear dominant individual: Decision Tree Classifier

== February 11, 2019 ==
Meeting Notes:
* We talked about results
* We got everyone caught up on the results
* We started using setting up the project using Genetic Programming
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Completed
|-
|Do the titanic dataset using Genetic Programming
|Finished
|January 11, 2019
|February 15 , 2019
|-
|
|
|
|
|}

We met up later in the day
* We started adding primitives
* We had a problem with the evaluation function
* We decided to go the helpdesk to get more help

== February 15, 2019 ==
We met with James at the Help Desk and finished our GP project
* We fixed the problem with the evaluation function
* Then we had the problem of zeroing one of the objective and maximizing the other one and not doing anything else
* We made a pareto front and it was not correct
* This was because we used the tournament selection as our selection method 
** We learned that tournament sleection is not a good method for multiple objectives
* We changed our selection function from Tournament to NSGA2
* We changed our mutation function from node replacement to mutUniforn
* We plotted correct pareto front. Here were our outputs before changing the mutation function
** gen = 400 and n = 400: 41366 AUC
** gen = 400 and n = 600: 34532 AUC
** gen = 400 and n = 800: 35639.0 AUC
* After changing it the AUC was 32448.0
*[[files/GP pareto front.png|thumb]]

== Febrary 18, 2019 ==
Meeting Notes:
* We talked out the results
* We started the installation of emade
* Did all of it until the mysql part
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Completed
|-
|Finish the installation of emade
|Finished
|Febrary 18, 2019
| February 24, 2019
|-
|
|
|
|
|}

== February 25, 2019 ==
Meeting Notes:
* Finished emade installation 
* Had to fix up mysql
** installed mysql workbench to fix this
* We learned about running emade with the Titanic dataset using input_titanic.xml file 
* Planned on meeting on Friday to work on project
*  {| class="wikitable" !Task !Current Status !Date Assigned !Date Completed |- |Meet at Help Desk to work on emade |Finished |February 25, 2019 |March 1, 2019 |- |Connecting emade to mysql |Finished |February 25, 2019 |March 1, 2019 |- | | | | |}

== March 1, 2019 ==
Meeting Notes:
* Made a new schema to run the mysql on
* inputed those schema details in input_titanic.xml
* ran master using script on github
* Tried to set up workers: ran into errors.
* Workers got validation lengths error 
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Completed
|-
|Run master on my computer
|Finished
|March 1, 2019 
|March 1, 2019
|-
|Run workers using my comptuter as local host
|Finished
|March 1, 2019
|March 8, 2019
|}
* Fixed the validation lengths error
** SSL connection error:

== March 4, 2019 ==
* We tried to fix SSL connection error
** Connection by windows worked but connections by linux and macs did not work
** Worked with James and Jason to fix this. No Progress {| class="wikitable" !Task !Current Status !Date Assigned !Date Completed |- |Fix SSL connection |Finished |March 4, 2019  |March 8, 2019 |- |Run Emade - See Results |Finished |March 4, 2019 |March 8, 2019 |- |Finish Presentation |Finished |March 4, 2019 |March 11, 2019 |}

== March 8, 2019 ==
Fixed SSL Connection issue:  edit the launch_GTMOEP files to include pymysql and downloaded pymysql to fix this problem

We got the people who showed up to the meeting set up with emade
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Completed
|-
|Get visualizations for emade
|Finished
|March 8, 2019 
|March 11, 2019
|}

== March 9, 2019 ==
Met with Anthony on the viz team to set up the visualization software.

It did not work for Anish and I. However, it worked for Anika so we had her computer for the emade visualizations as it connects to my mysql server
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Completed
|-
|Run emade more and get visuals
|Finished
|March 8, 2019 
|March 8, 2019
|}

== March 10, 2019 ==
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Completed
|-
|Work on presentation
|Finished
|March 10, 2019 
|March 11, 2019
|}
Met up at 10 am with group to work presentation

== March 11, 2019 ==
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Completed
|-
|Polish up Presentation and Assign Roles 
|Finished
|March 11, 2019 
|March 11, 2019
|}
Met up at 3 before the VIP to polish the presentation and assign roles

Meeting Notes:
* Ate pizza and did the presentations
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Completed
|-
|Decide on Sub-Team to join
|In Progress
|March 11, 2019 
|In Progress
|}

== March 25, 2019 ==
'''Team Meeting Notes:'''
* Got picked to DEAP TEAM
* We got introduced to everyone on the team
* DEAP uses its own framework since it uses ezCGP instead of regular genetic programming. Therefore, to learn this, Mr. Rodd went through his slides on Cartesian Genetic Programming and his particular framework, ezCGP, to get an understanding of it. 
** The slides went over the differences between Cartesian Genetic Programming and regular Genetic Programming
* We also went over the github of all of the files of the ezCGP framework

Action Items:
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Completed
|-
|Go over Mr. Rodd's slides to get a much better understanding of the framework
|Finished
|March 25, 2019
|March 29, 2019
|-
|Clone the ezCGP repository and play around with problem.py in Mr. Rodd's symbolic regression branch to get practical experience with the ezCGP framework  
|Finished
|March 25, 2019 
|April 1, 2019
|}

== April 1, 2019 ==
'''Team Meeting Notes:'''
* We split into two subteams: 
** Subteam A is working on making the ezCGP framework easier to use 
** Subteam B is trying on adding more primitives and allow the framework to solve '''regression''' problems as well.
* Decided to join Subteam B
* The first-semester students contiued to play around with the symbolic regression framework. 
** Used problem.py file in symbolic regression branch modeled 1/x using a taylor expansion with CGP 
** changed the operators in operators.py file
** it uses mutations from mutations.py file as mating is not effective for CGP

* Mr. Rodd went over the the tensorflow framework
** He went over applications of tensorflow, and how to use it in your code.
** We also went over the general framework of tensorflow as graphs and how neural networks are built from tensorflow {| class="wikitable" !Task !Current Status !Date Assigned !Date Resolved |- |Continue learning about ezCGP through SymbRegression branch |Completed |April 1st, 2019 |April 7th, 2019 |- |Select a subteam to join |Completed |April 1st, 2019 |April 1st, 2019 |}

== April 8, 2019 ==
'''Team Meeting Notes:'''
* We caught up to what each subteam is doing. We explained what we have learned to Aniruddha, team lead
* switched to subteam-A in order to contribute more to the framework itself
** worked on allowing ezCGP to be more scalable and allow much bigger datasets
* We were given a task in the tensorflow-nn branch in which we could update the number of epochs within the problems.py file itself. Worked with Animesh to get a solution
** quickly figured out solution and pushed changes to the tensorflow-nn branch on our github
** just added an extra parameter in the problem.py and updated the parameter list in the skeleton block
** blocks.py file updated to reflect this change as well {| class="wikitable" !Current Status !Current Status !Date Assigned !Date Resolved |- |Add a new parameter in the problem.py to allow users to more easily set the number of epochs run |Completed |April 8th, 2019 |April 15th, 2019 |}

== April 15, 2019 ==
Team Meeting Notes:
* First semester students were assigned to test and train the best individuals from ezCGP model for classification on MNIST, CIFAR-10, and CIFAR-100
* got used to Google Colab and its GPU to run python tensorflow models on it and how it can be used for multiple members
** decided to use this to speed up the training as using the computer's CPU is not nearly as fast or efficient
* Given access to Aniruddha's previous code to use as a model and inspiration to understand how to do it
{| class="wikitable"
!Current Status
!Current Status
!Date Assigned 
!Date Resolved
|-
|Create a CNN model from scratch using tensorflow image classification using Google Colab and its GPU
|Completed
|April 15th, 2019
|April 18th, 2019
|}

== April 16 - 17, 2019 ==
Looked up documentation for tensorflow's CNN

Used  pre-processing techniques for image classification for tensorflow

traced through Aniruddha's code for MNIST and tensorflow

looked at the CNN architecture from https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c

looked at the CNN architecture from https://colab.research.google.com/drive/1b9AbZyEu8d7BjgrK2CXRJGaKa_TTNQVc

used 3 layers, with cnn layer, maxpool layer which consists of a relu function and a maxpooling layer. After, I used a fully connected layer and used a dropout and batch normalization for CIFAR-10
[[files/Screen Shot 2019-04-23 at 12.38.03 PM.png|thumb|code for the initial CNN model]]

== April 18, 2019 ==
[[files/Screen Shot 2019-04-23 at 12.37.46 PM.png|thumb|186x186px|The accuracy of train and test datasets for the cnn layers]]
Met up with Team on Thursday

worked with Animesh and found the best individual from the MNIST model from the ezCGP framework. 

used the preprocessing from Ani's model. Used the operators.py file to get the what the layers entailed

This was only two layers: an average pool layer and a res_block layer. We found that this is not the best

figured that the GTRI computers crashed so we sent Michael to fix it 

Afterward, we got a better model with a nonlinear, 10 layered CNN model 

== April 22, 2019 ==
Gave Presentation - 
* I did my part of introducing the concept of deep learning
* We first started with the Team B who dealt with the regression problem
** We added one primitive, but we need to add more
* We presented our benchmarks for classification
** It was 
Listened to the other presentations

I especially liked the stocks' presentation. I liked the fact that they got to implement the latest stocks machine learning models and that could be used to get the best model. The stocks one is very similar to our group, DEAP. 

Said our Goodbyes!
{| class="wikitable"
!Current Status
!Current Status
!Date Assigned
!Date Resolved
|-
|Do our Presentation
|Completed
|April 22nd, 2019
|April 22nd, 2019
|}

== Final Entry ==
Before this semester, I had a background on machine learning and python. Therefore, the only problems I dealt with this semester before spring break was understanding emade and its framework. This was difficult as understanding the source code was difficult. Therefore, I had to come on Fridays to do a lot of homework. So even if it was difficult, I made sure I was understanding what I was doing by meeting with James. Afterward, we broke into groups. For the traditional machine learning part, I did it with the knowledge I had in the beginning. Additionally, I even learned more as well. For the GP part, we learned how to do it ourselves, and James helped us a lot for that. For the EMADE part, we had a lot of problems for just getting connected to the SQL database, but afterward, we got that running as well. From this, we learned a lot from EMADE and its framework and this gave a great framework for the others. For the midsemester, I got all As on my peer evals and my notebook documentation. After spring break, I joined the DEAP team. The DEAP team had their own framework, so we first had to learn this framework. I did a lot of messing around with the framework for this. After, I worked for benchmarking the classification models by finding the best individual from the GP run. Overall I feel like I did a lot of work for the VIP and learned a lot. I am also working in the summer with EMADE and I hope I can contribute within EMADE or ezCGP framework extensively over the summer. Therefore, I for all the work I have done and for all the things I have learned, I feel like I deserve an A.

==August 19th-25th, 2019==
===<u>August 19th, 2019</u>===
====Meeting Notes====
*We all organized into subteams
*I decided that I wanted to continue the work I did in the summer within the NLP branch, so I decided to start an NLP subteam
*I gave a brief introduction to all the team members to the work I have done in the summer and how it is organized within emade. 
*Our goal for this week was to brainstorm what to do this semester.
**We can improve Text Classification capabilities by extending upon the previous work done in the summer
**We can try to add other capabilities within emade, such as text summarization and text generation
**We need to add neural network capabilities within emade within either the learner methods or a completely new neural network class primitive
===<u>August 24th, 2019</u>===
'''Sub-team Meeting Notes'''
*We met up with some of our team members. 
*I made a presentation showing all the work I did over the summer. This was really detailed.
**Based on how much everyone knew within NLP and emade, I tried to get everyone caught up
***However, some members did not make the meeting so I had go over everything with them. 
*We looked at adding neural networks within emade
**One option: adding them within learner primitive 
**Make a separate neural network primitive
***One primitive with several parameters depending on type of neural network (This doesn't allow that much flexibility with parameters of each layer)
***Make each layer a primitive
*We looked at ways to improve the vectorization of the text
**Adding word2vec and improve that
==August 26th-September 08th, 2019==
==='''<u>August 26th, 2019</u>'''===
'''Meeting Notes'''
*I presented what we are planning to do for the week: set up google cloud credits, subdivide tasks, reclone/pull emade, further discuss how to incorporate neural networks in emade
*Jason suggested using ADFs for incorporating neural networks
*We made on a doc with future goals and potential tasks to work on; this includes all the stuff I mentioned before and part of speech tagger
*We realized we need GCP credits still, so we asked Jason for this. He is working on this.
*I had my team members try to look up text classification and bag of words and how it works
===<u>August 27th, 2019</u>===
*Zach said he could not make it to our Saturday meeting so I caught him up to where we are right now. I explained all the works. I explained to him the bag of words primitives and the parameters associated with it. 
*I explained the google cloud stuff and the neural network primitive we needed to work on.
===<u>August 31st, 2019</u>===
*Some time conflict members decided to attend our meeting for now as nothing is set up for now. Therefore, I explained bag of words and the parameters associated with it on the board. 
*We decided that implemeting a neural network within the Learner framework works out
*Jason is still working on getting GCP credits so we could not do that as planned, so we decided to have a working version of emade on our local computers instead.
*Also, I realized that the main branch hasn't been merged with all the changes that have been made in the summer!!!!
**My NLP branch is updated with the changes made in the summer, so there is an even steeper learning curve. 
**I asked Jason, and he said to continue to work on our 
*Alex brought up NEAT as way of representing neural networks in emade, so we added this to the task list. 
*We worked on getting emade running on our comptuers.
*I worked on subdividning tasks
**Alex wanted to work on text summarization so I let him look into that.
**I wanted to look at text generation and its capabilities so I am looking at this.
**I assigned Reagan and Bek to have a working version of emade (nlp branch) on their computer and run the branch real quick and see if there are any problems
***I also had them work on adding lemmatization and stemming into the learner method
**The time conflicts (Anish, Yoonwoo, Jiseok) I did not know what to do with them as they are still not officially part of the nlp team
***I had all 3 of them look at NEAT and start implementing this. Before this, I asked them to implement a neural network within the learner framework.
*Our next meeting is at 4 pm on Friday.
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get nlp branch of emade running
|In progress
|August 31
|September 09
|September 03
|-
|Look up Text Generation and see ways to add this within emade
|In progress
|August 31
|September 09
|September 09
|}

=== <u>September 4th, 2019</u> ===
* Zach could not make it to the Friday meeting again. So we decided to meet on Thursday during help desk hours. 
* We worked on installing emade. However, we got an authentication error
** We asked Dr. Zutty for helped and he suggested using a ssh key to solve this.

=== <u>September 5th, 2019</u> ===
* I met with the rest of my team and we all worked on installing nlp branch of emade on our computers.
** We all had several problems. I couldn't connect to mysql server, Reagan was getting mysql problems, Bek got past the credential cache problem Zach had, Alex worked on cloning all of emades as the datasets did not download.
** We decided to get as far as we can, but focus on getting it on the Google Cloud server so we can all see the outputs together. 
** I also showed them what commands to run including the seeding file problem.
* There was some bugs in the nlp branch code, so I made some commits and fixed those.
* A time conflicts group decided to also work on NLP but specifically on developing a neural network architecture.
** Therefore, to make sure we reduced merge conflicts, we made two more branches off of the NLP branch called nlp-nn and nlp-app, which is their branch and our branch respectively.
*** Once they have working neural network architecture working, they can update the main nlp branch for us to pull and work on. 
* Therefore, we decided to work on general nlp tasks in emade. 
* To test the effectiveness, we decided to our work on jupyter notebooks, and if we find good results, we can then implement that in emade
** I made jupyter notebooks for everyone to work on.
** I will work on my text generation work within my jupyter notebook.

== September 9th -September 15th , 2019 ==

=== <u>September 9th, 2019</u> ===
* I presented progress to everyone
* We decided to continue our respective work. 
* I decided to change what I am working on. I will be the main person who runs emade off of. I decided to abandon text generation as that is not as important. 
* My MySQL wasn't working and I showed it to Dr. Zutty. He couldn't fix it, so I decided to just completely uninstall mysql

=== <u>September 13, 2019</u> ===
* I realized that I still had google cloud credits from last semester, so I started to install emade on google cloud
* I started installing emade on my computer making dependencies. 
** Then I realized that I did not have enough space
** Anish from the other nlp team suggested on looking at caching guide so I am using that right now
* I got everyone else set up on the lemmatization and stemming task.
** I suggested Bek, Reagan, and Zach to run the model with lemmatization and stemming to see if it works better than without stemming and lemmatization
** They decided to also work on a POS tagger to help with lemmatization as well {| class="wikitable" !Task !Current Status !Date Assigned !Suspense Date !Date Resolved |- |Installing Emade on Google Cloud |Finished |September 12 |September 13 |September 03 |}

== September 16th -September 22th , 2019 ==

=== <u>September 16, 2019</u> ===
* Alex presented the subteam group progress
* I decided to help Alex on his summarization work
* He explained his progress and problems of putting it into emade
** By the end of it I had a lot of doubts because
*** need dynamic evaluation
*** need to make a complex evaluation function
*** don't know how it would fit within emade
** plan is to make split documents into sentences and output a list of 1's and 0s to indicate the sentences in the summary
*** Each answer can have multiple correct answers so we don't necessarily have truth data attached with each evaluation
*** will need to rework emade greatly
* I halted work on google cloud to help Alex with this

=== <u>September 20, 2019</u> ===
* Made sure text classification subteam was going well and had progress and then split up
* Met up with Alex to iron out more details
** the truth data that we will pass through will be our evaluation score!
*** we have to make a evaluation function primitive that is outputted at the end
** We need to figure out a way to convert each sentence to a numeric value (either vector or value)
*** need to combine sentences'  word vectors together to represent word vector
**** I suggested the mean but there are better ways apparantly !!
** We made a plan for the rest of the semester: 
***Get Dataset week 1
***write feature scales (for sentence embeddings)
***parse documents and evaluate sentence
***throw into emade[[files/Text summarization plan for semester.jpg|thumb|Text Summarization Plan for Semester]] {| class="wikitable" !Task !Current Status !Date Assigned !Suspense Date !Date Resolved |- |Find Dataset |In Progress |September 20 |September 24 |September - |}

==September 23 - 29==

=== <u>September 23, 2019</u> ===
*Team meeting
*Alex and I are working on finding a dataset, need something supervised which has labeled sentences as summaries
**However, most are highlights of phrases instead 
*Jason made some suggestions with an old summarization app, no dataseet though with this
===September 27th, 2019===
*No luck finding dataset
*Do a mix of selecting phrases and then combine them
*Update with text classification,  ,helped Reagan with how to make lemmatization primitive in emade
==September 30 - October 6==

=== <u>September 30, 2019</u> ===
*Team meeting
*We are thinking of making our own dataset
=====Strategy for dataset creation:=====
*Use dynamic programming approach to find optimal set of sentences
**basically run our evaluation function on the each sentence one by one, use the previous values to get new ones
*We can find idea is that we can create an optimal dataset and hope EMADE finds a better feed-forward system with/or without the features we make

=== <u>October 4, 2019</u> ===
However, this turned out to not be the best approach as we are going to be using the same approach to make a summary with our evaluation function and emade will be using the same evaluation function so this would be counteractive and not give us good results at all

== October 7 - October 18 ==

=== <u>Meeting</u> ===
* Dr. Zutty suggested we use a crowdsourcing technique such as Amazon Turks to have our dataset novelly labeled with the way we want to
** Are using BBC News articles datset
** Out of 2225 articles, we are gonna use 1000 articles to be labeled
** 3 reviewers per article, and .1$ per article per review weill cost $300

=== <u>October 8, 2019 - October 18</u> ===
* We have spent this time researching various ways of encoding different information into emade
** Need to parse documents into sentences first as we are doing sentence level classification!
*** Use NLTK sentence tokenizer
**** ("NLP Pipeline: Sentence Tokenization")Â https://medium.com/@makcedward/nlp-pipeline-sentence-tokenization-part-6-86ed55b185e6
**** ("Tokenize Words and Sentences with NLTK")Â https://www.guru99.com/tokenize-words-sentences-nltk.html
*** I used Keras for tokenizing words in the summer; No sentence level equivlent!
*** Use Spacy sentence tokenizer
**** https://github.com/explosion/spaCy/issues/93 seems pretty simple
** After we separated the document into sentences, we need to convert it to a numeric value
*** GLOVE, Word2Vec for word level
**** We can just add all the words in a sentence
*** Do sentence level embeddings, have emade decide what is best
*** We decided some from this paper might be useful: https://www.sciencedirect.com/science/article/pii/S1877050915006869!
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Method of Dataset creation idation
|Done
|September 30
| - 
|October 7
|-
|Reserach ways for sentence level tokenizing
|Done
|October 7
|October 14
|October 14
|-
|Ideation for good ways of representing sentences as numbers in emade
|In Progress
|October 14
|October 18
|October 18
|}

== October 19 ==
* Alex and I also discussed one hot encoding vs tools for encoding the summaries for text
** We are going to try to write primitives that take in text data, and return numeric values
** We will specify this as a text data, to help emade error our individuals which don't have text processing primitives
** We need a loader function that will tokenize the document into sentences. We probably need a separate one.
* We have been waiting to hear back about MTurk so we talked to Jason about getting funding for MTurk set up. We decided that we could just use his Jason's credit card for this. We got this set up.
** Alex and I worked on getting the parameters for Mturk. We decided to set it as we give each worker a news article, and output sentences that would seem the most relevant
** One method of just providing checkboxes next to each sentence required the use of website design so we just did the aforementioned method. This strategy mostly worked. 
** One main problem is demarcating sentences might be different for every person. For this, we used a sentence tokenizer to tokenize our documents into list of sentences.
*** Now we have documents represented as list of strings (sentences). We then just concatenate our sentences with || in between to represent for the whole document into one string and give that to each Mturk submission to make it easy to demarcate sentences!
** There will still be problems of people giving bad summaries or not selecting sentences correctly.
*** We can just worry about marking sentences wrong and right after (Alex would be in charge of this as he has it on his account)
*** The size of our dataset '''should''' account for possible error
* We also worked on the Presentation for Monday!
** We are doing the classification and summarization stuff all in one presentation. Alex and I did our section of the ppt!
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Work on presentations
|Done
|October 13
|October 22
|October 19
|-
|Get Mturk set up
|Done
|October 7
|October 19
|October 19
|-
|Parse through responses in Mturk
|In Progress
|October 19
|October 26
| - 
|}

== October 21 - October 26 ==
Presentations: We are presented today. 

Here is link: https://docs.google.com/presentation/d/1mCHtyfTYjnn-gxYAl86EDCOGsQ9SAwes4w25r7HusSE/edit
* We got good results and had good feedback for our presentation
* We need to add more primitives for our summaries so we need to go back to the drawing board for possible primitives to be added
** We are going to look at the aforemention paper we linked in the presentation for this (https://www.sciencedirect.com/science/article/pii/S1877050915006869): 
*** This paper did a form of extractive summarization using genetic algorithms (not genetic programming). They had several evaluation techniques they used.
*** This paper is similar to what we are doing however they don't go into specifics so we had to do all of that ourselves.
* MTurk Update:
** Alex is doing this as he is the only one access to account: 
*** be more specific about task next timen
*** same person can contribute multiple times (made it quicker, reduced variation)
*** Some common errors: "Yes" or "No" responses, summaries not involving text from the dataset
*** Lot of one sentence summaries but we should accept them as some might think they are valid
**** Since summaries are subjective we aren't the ones that should be accepting or rejecting what a good summary is (except the obvious ones)
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Presentation
|Done
|
|
|October 21
|-
|Parse through responses in Mturk
|Done
|October 19
|October 26
| October 26 
|-
|combining the data altogether in one place 
|In Progress 
|October 26
| -
| -
|-
|Think/ Plan for 1st semesters and task assignments
|In Progress
|October 21
|October 29
|October 29
|-
|Look at possible primitives in paper: 
|In Progress 
|October 21
|October 29
|October 29
|}

== October 28 - November 3rd ==

=== October 28 ===
* Alex Finished parsing responses of Mturk
** Is parsing the responses to be allowable for emade
** putting it into csv file, making labeled truth data
** Since the truth data is multidimensional, it is not straight forward to extract it from mturk to csv
* Results Summary:
** Summaries are significantly shorter than original articles.
** There are a quite a few that still do not fit, but left alone to keep our sanity ok and to increase variation
* First sememsters:
** A LOT of first semesters decided that they wanted to join NLP team
*** thought it would be like last year and there would be restriction who joined who but that's fine
** We had to worry about dividing up everyone, so we had first sememsters meet up during our Friday time and maybe move some of them to the NLP-NN team
*** We had our team and NLP-NN team plan a small presentation for on Friday for this
*** We decided to just continue with ours
** We gave a brief overview of what NLP team does and how it is organized (summaries, classification, neural networks)

=== November 1 ===
* From the aforementioned paper we found the feature set they used to evaluate their summaries. We decided that these would be good primitives 
* Both our team and the nlp-nn subteam gave presentations for where we are and what we need for the first semesters
** We managed to move about half of the people to the nlp-nn subteam. 
** For the first semesters who joined my team we paired them off and assigned them primitives from this primitive list.
*** TSISF - Sentence Location - Num Named Entities
** We had their different primitives on Jupyter Notebook and use whatever online framework needed for this
* Planned to explain how to integrade primitives within emade Monday meeting!
* I decided to start work on making a data loader for emade
** Summary data has multidimensional target data so I think more data is need for this task 
** However the target data won't be of constant length (original document can have various length)  
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Combining the data altogether in one place
|Done
|October 26
|November 4
|November 4
|-
|Assign primitives to first semesters based off of paper
|Done
| -
| -
| November 1 
|-
|Ideation for integration for emade and data loader 
|In Progress 
|November 1
| -
| -
|}

== November 4 - 10 ==
* First Semester:
** First semester students came back with their methods, they seemed to be mostly working with a few questions
*** We also gave students who weren't there the first week primitives to work on
** Moving forward we want them to get their primitives EMADE-ready: mostly just making sure the dimensions are correct, then they should be able to push to the branch
* Alex worked on padding data as it different articles have different number of sentences
** The article with the most sentences is 255, so we decided to pad every article to 255 sentences. 
*** Alex added blank spaces, all sentences are separated by marker '||', 255 is too long but this is the approach we are taking, If sentence is blank space, we showed that as 0s
** Alex sent me this padded csv so I now have something solid as we have constant size data. 
*** We have a 1000(ish) by 255 matrixf sentences. Each row is an article with padded sentences
*** The target data is same size. Each row in target data corresponds if a sentence should be included or not.
*** Now I have solid dimensions to be able to write the data loader
*** Within the data loader I just need to load the data within this format
*** Working on this commit: https://github.gatech.edu/emade/emade/commit/b63575d6c0338e4873ed3964b8ebb782df97e402
* First Semesters:
** Some were still confused how to do this and had few questions, so I gave some pointers on how to do this
** They had questions
*** For TSISF I showed them TFIDFVectorizer primitive. 
*** Num Named Entities had questions regarding to use spacy or nltk. Additionally, theirs showed pairs of named entitiies and how much it appeared
**** For this we only want numbers so I asked them to change this to only give the total '''number''' of named entities per sentence and not focus on what those named entities are
** I also explained how a primitive is formated within emade
*** How to write one. (with parameters and data pair and turning that into a numpy and turning results back into an emade data pair) and put in gp_framework_helper
*** I gave the a brief overview in how data is represented within EMADE (data.py) EmadeDataPair -> EmadeData -> EmadeDataInstance -> EmadeDataObject -> FeatureData -> TextData (data loaders) (caching)
** We also assigned primitives to others who did not make the first meeting (TextRank)
*** helped with this as well (using Glove, it's like pagerank)
*** had to explain aforementioned stuff again
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|combining the data altogether in one place 
|Done 
|October 26
| November 4
| November 4
|-
|Explain primitives within context of emade
|Done
|<nowiki>-</nowiki>
|<nowiki>-</nowiki>
|November 4
|-
|Data Loader for Summaries
|In Progress
|November 4
|November 11
|November 11
|}

== November 11 - 17 ==
* I finished my data loader :  https://github.gatech.edu/emade/emade/commit/b63575d6c0338e4873ed3964b8ebb782df97e402
* We then found out a huge problem that we needed to deal with: How are going to pass data into EMADE as input/output of this isn't in standard structure
** Jason said that EMADE should handle multi-class ouput (since our target data has 255 dimensions)
** However we need to keep all this values as there can be multiple sentences in a potential output and we do not know which spots those sentences would be
** Standard classification algorithms do not work for this format (we double checked in a very quick Jupyter Notebook)
*** The current system is to pass rows of the sentences as x values but we want the columns to the same dimension as we are basically doing sentence level classification

[[files/Ideation for the problem.jpg|center|thumb|Ideation for the problem]]
* Solution: After some ideation, we found out a solution. The above picture is kind of messy (I claim innovation isn't that pretty) but it shows the steps we thought of and what we actually did to fix this problem. 
* We decided to make a dummy method that basically applied any learner which applies it on every sentence dimension. 
** Basically this modifed learner will treat each column as a separate dataset and apply that learner on every ith sentence for the whole dataset and create model that works to predict the ith sentence. 
** The problem with this is that the learner will have to do this for every column which will increase the time for every learner 255 times (except the sentences at the end should mostly be 0 so this shouldn't take that long). 

* The first semesters should be ready for primitives to be put into emade but asked some question regarding dimension size
* The first semester were not ready for primitives to be done so we had to wait on this.
* Therefore, I decided to start working on adding the modifed learner to emade
** Didn't know if I wanted to make completely new primitive (which file to put it in etc.)
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Integrate Primitives from First Semesters 
|In Progress 
|November 15
| November 23
| November 25
|-
|Data Loader 
|Done
|November 4
|November 11
|November 11
|-
|Modified Learner for our dataset ideation
|Done
|November 11
| -
|November 11
|-
|Integrate modified learner
|In Progress
|November 15
| -
| -
|}

== November 18 - 24th ==
* Took a while but first semester finally got their primitive ready. 
** Alex and I had to look over it and make some changes
* The first semesters did not have push access to the Github so we had to talk to Jason to get that fixed up
* We looked at num named entities primitive (seemed to be really ineffective (which makes sense as this is their first tiem using python, numpy and ml related activities))
** Alex optimized this form like 40 lines of code to 5: 
* TFISF made a mistep with their primitive so we had to have thorough discussion with concept of TSISF and this all had to be reworked
* TextRank ended up finishing but used GLOVE which we thought was very ineffective so we had to work with him to fix this up and Alex suggested utilizing some sort of pickle file for this instead
* I found a but with the data loader and started to get unit tests set up. https://github.gatech.edu/emade/emade/commit/c6aba541a8e0c2fc3a087a95944fc42342b15bf5
* I also started some preliminary work on learnermod, but I was still a bit confused. I decided to work on integration of primitives before. (https://github.gatech.edu/emade/emade/commit/d23815a22e86b74cd56d0d5d8c2d583daac7f85a)

=== November 23 ===
* During the hackathon Alex and I worked on integrating the primitives within emade
* To do this I decided to run the primitives on a Unit Test. 
** I tried running Alex's optimized version of num named entities but this took forever. It did not finish at all. I ran it for 25+ minutes but it still did not finish. 
** We spent a lot of time figuring out why this would be a case and I even went to James for this but could not figure out the problem
** Maybe its because it is within the context of EMADE but it took about 5 minutes to run on Alex's computer so we think that this primitive might not be the best for this task as this primitive is meant to be combined with other primitives and if this takes this long then we don't know the feasibility of this one. (results were not the best huh) prob cuz num named entities isn't good at identifying good 
* I also worked on making the alternative Learner primitive: https://github.gatech.edu/emade/emade/commit/d23815a22e86b74cd56d0d5d8c2d583daac7f85a
* We also pushed the new primitives to the branch as well with the same commit (num named entities and text rank https://github.gatech.edu/emade/emade/commit/d23815a22e86b74cd56d0d5d8c2d583daac7f85a) unit test stuff was also there
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Integrate Primitives from First Semesters 
|Finished for textrank and num named entities 
|November 15
| November 23
| November 25
|-
|Test primitives and debug
|In Progress
|November 23
|November 25
|November 25
|-
|Integrate modified learner
|In Progress
|November 15
| November 23
| November 25
|}

== November 25 - Dec 1 ==
I spend a lot of time during Thanksgiving break working on this as I was on campus for the break!
* The TF-ISF primitive was also pushed to the branch
** I fixed some bugs and pushed working primitive to github. https://github.gatech.edu/emade/emade/commit/39c7af226513cd66fe77180694229b1a2b9f4d35 
** Same problem was happenning as num_named_entities. The primitive did not ever finish running on a unit test (took 25+). My explanation for this one is that they are calling to dense for every sentence. which is (255* 1000) times and converting a sparse matrix to a dense matrix takes way too long so this makse sense I think 
* Therefore I had to add last-minute primitives over the break.  
* I made a column wise tfidf vectorizer (Tfidfvectorizermod)
** This is very similar to the existing tfidf vectorizer and the tTF-ISfvectorizer. Basically it treats each column as its own dataset and each sentence as a document. It basically runs the preexisting TF-IDF vectorizer primitive on each column. https://github.gatech.edu/emade/emade/commit/fd94dd610d00fe9b9af588c6af11655285c895e1 
** However I needed to figure out how to wrap it back into an emade data pair 
** Also learner mod needs to be able to get the output back as well. 
* As of now this primitive outputs a Numpy array of sparse matrixes (255 sparse matrixes) https://github.gatech.edu/emade/emade/commit/fd94dd610d00fe9b9af588c6af11655285c895e1
** It turns it into 3 dimensional  
** This encodes the summaries much better than the prexisting primitives (which were meant to be combined somehow) so this primitive actually should work much better 
** When wrapping back to a datapair, the data needs to be divided into rows and set each instance to the row. (numpy array for each row was created  1 by 255 ) with 255 (1 by n ) sparse matrixes 
* In learnermod (new learner primiitive) we need to iterate through each sparse matrix and iterate through each element in target data 
** https://github.gatech.edu/emade/emade/commit/fd94dd610d00fe9b9af588c6af11655285c895e1 and https://github.gatech.edu/emade/emade/commit/a7afe2596fec900be9267e94b2d738986af3ab99 
** To get the data back into the original form (from tfidfmod) I had to vstack the data all together for each column. 
*
*There was some problems with target data getting read into emade because target data was multidimensional!
*Instead of calling get_target, I was forced to just go through every instance and vstack them (get target just hstacks it which turns the target data to one dimension)
**https://github.gatech.edu/emade/emade/commit/b4485d66dfa7c1caf9bfe036a3b3505617b3b420
*I also dealt with a scikit learn error where if the thrget data for train data had all zeros, then scik-it learn errors out
**had to just have it output a zero prediction for that column https://github.gatech.edu/emade/emade/commit/b4485d66dfa7c1caf9bfe036a3b3505617b3b420
*I also added similar vectorizer mod for CountVectorizer and HashingVectorizer (CountVectorizerMod & HashingVectorizerMod)
**A total of 3 new primitives were made: https://github.gatech.edu/emade/emade/commit/3fef391fd76a6a446985fe6ad1cfff1cad289f4f
*After this I got these primitives to get results on the unit test. Now all that was needed was to run on emade
*I then added my primitives to a seeding file, got my MySQL schema setup and added the primtives to gp_framework_helper 
**https://github.gatech.edu/emade/emade/commit/55083db0cd72ffb06589b083076bb81edb89ebc6
*I then did a test run overnight, the primitives weren't evaluating within the first 1 hour, but I just thought multithreading made it take longer than it should have and let it run
**https://github.gatech.edu/emade/emade/commit/67d6b0c99ff32ba388c6de37736540f5cf34b0c3 (fixed more bugs and ran with mysql)
**In the unittest it took a max of 3 minutes
*
* When I woke up tommorrow, I was surprised to see that one of them timed out and the others were still in progress
* I then had Reagan run my stuff 
** Still got same results and none of it ran

* I wanted to make sure that the evaluation methods were working as well (decided that I might need to make modified evaluation methods but realized that the preexisting ones would be fine itself)
** Needed to make sure target and test data had the write shape before getting passed into evalmethods.py
* Nevertheless, I extracted results of the primitive (through the unit test) and got actually great results
* We finished up the slides as well and added all the results and total progress
[[files/Screen Shot 2019-12-04 at 4.18.53 PM.png|center|thumb|results]]
[[files/Screen Shot 2019-12-04 at 4.19.56 PM.png|center|thumb|Sample Summary prediction vs real]]
*Here is link to final presentaion: https://docs.google.com/presentation/d/1KPcNsmbxPipkDncRDKeScHZCRR71gXoDNyumCGimTVc/edit?usp=sharing
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Integrate Primitives from First Semesters 
|Finished TSISF 
|November 15
| November 23
| November 25
|-
|Run TSISF Primitive 
|Finished (took too long  to run)
|November 23
|November 28
|November 28
|-
|Add in new TFIDFMOD primitive
|Finished
|November 28
| November 29
| November 29
|-
|Modify LearnerMod to be able to get this to run
|Finished
|November 29
|November 30
|November 30
|-
|Set up everything for run on emade/ get results
|Finished
|November 30
|December 1
|December 1
|-
|Finish up presentations/ actually present
|Finished
|November 23
|December 1
|December 1
|}

== Grade to Deserve ==
* I think I deserve an A in this class as evidenced by all the work I have done over the semester. I had to first get my group members familiar with not only what NLP is and NLP concepts twice (in the beginning and with new members).I also had to teach the first semesters and my initial group members how to EMADE works and how to work on the codebase. Additionally, I had to work with assigning tasks to everyone and making sure everyone is on the same page with everything. Additionally, I helped around with other groups as well (Automatic Preprocessing and NLP-NN) for explaining how emade works and how to do NLP tasks as well. I also helped with first semester with various questions as well. Additionally, I feel like I did a lot of work for getting EMADE to run our NLP task. Before that, I worked with Alex a lot on what the exact task is, how to approach it and how to solve it within the context of EMADE. Additionally, I also worked on getting various new primitives integrated within EMADE and helping others do this as well. Additionally, I worked on reworking the emade codebase to allow our task to work as well. Due to all of this, I feel like I deserve an A in this class and hope to do great work next semester!

== January 6-12 ==

=== January 6 ===
* Dr. Zutty did a statistics presentation
** He implored us to focus on smaller tasks and use statistical tests to test these tasks
** This includes hypothesis tests (simple python scripts)
* We broke into our existing groups
** I decided to help for the neural network stuff
* Decieded to use PACE to run our stuff

=== January 10 ===
* Cloning Anish's branch of emade
* Plan to push this to nlp-nn branch
* We want to move Anish's stuff as a branch to make stuff more standardized
* I decided to get on PACE
** had problem with the script to ssh onto server
** used directions from ezcgp team
{| class="wikitable"
|-
!Current Status
!Date Assigned
!Date Assigned
!Date Resolved
|-
|PACE-ICE Login
|In progress
|January 6
| -
|-
|Moving Anish's forked repo onto main repo
|In Progress
|January 6
| -
|}

== January 13-19 ==

=== January 13 ===
* A nlp-nn member (Sanket) showed up, so I decided to join applications team with Bek and Reagan to even teams out
* Realized that my pace script problem was just typo
** Got onto PACE
** Read more PACE documentation
** Started cloning emade on PACE

=== '''January 17''' ===
Career Fair so we did not meet
{| class="wikitable"
|-
!Current Status
!Date Assigned
!Date Assigned
!Date Resolved
|-
|PACE-ICE Login
|Finished
|January 6
| January 13
|-
|Submitted qsub to get understanding of PACE
|Finished
|January 13
|January 13
|-
|Clone Emade onto PACE
|In Progress
|January 13
| -
|-
|Set up dependencies/ environment
|In Progress
|January 13
| -
|}

== January 20 - 26 ==

=== January 20 ===
* MLK Holiday

=== January 24 ===
* Finished initial setup
** making environment
** installed dependencies
*** had Anish make a requirements.txt file for this
** reinstall script works
* I then worked on making qsub script
** it kept on erroring out (to my mail)
** It said it exceeded limit
** Ran this multiple times, same problem
** I couldn't figure this out so I decided to ask on Monday Meeting
{| class="wikitable"
|-
!Current Status
!Date Assigned
!Date Assigned
!Date Resolved
|-
|Clone Emade onto PACE
|Finished
|January 13
|January 24
|-
|Set up dependencies/ environment
|In Progress
|January 13
|January 24
|-
|qsub script for emade
|In Progress
|January 24
| -
|}

== January 27 - Feb 1 ==

=== January 27 ===
* Many teams were good with GCP
** maybe we should do this if PACE doesn't work
* Fixed problem - Anish helped and stated that I just had to change the wall time to 12:00
** Continued the process 
* It said that I don't have multiprocess even though I had it installed before in the out file
** realized that I had to activate my environment within the qsub script 
*** Pulak had this problem on Friday, so he helped me with this
* Next problem - gave me permission error - squeue
** Pulak has same problem
** We decided to talk to Jason about this and he said that it was with how multiprocessing is used within emade and PACE
*** Pulak decided to work with Jason to fix this
** I decided to transition
* I helped a lot with getting Sanket caught up
** He was very unfamiliar with working on servers and using Git - so worked to try to get him caught up at the same time as stuff above

=== January 31 ===
* Worked with Reagan to ideate scripts and todo functions
** Reagan worked on making google doc fo this: https://docs.google.com/document/d/1uAkHNQD8d86XneFoFxjwGYxPfzjSWHcLeYKzprN5sps/edit?usp=sharing

* Tried to run at least the seeding script 
** could not get connected to local mysql database!
* Alex suggested to use PACE's mysql
** it is not a module in pace-ice so had to deal with this
* Looked at pace documentation for multi-node access setup: https://docs.pace.gatech.edu/software/mysql/#mysql-on-pace
** Realized that this was very tedious and not worth - decided to ask Dr. Zutty for gcp access as well
** Thought of using my old gcp credits for this
*** realized that it had space problem and configuring this was not worth it so decided that asking Dr. Zutty for his set up was a better option 

{| class="wikitable" 
|- 
!Current Status 
!Date Assigned 
!Date Resolved 
|- 
|qsub script for emade 
|January 24 
|January 27
|- 
|Google doc for scripts and todo functions 
|January 31 
| - 
|}

== Februrary 2-9 ==

=== Februrary 2 ===
* I asked Dr. Zutty about access to GCP - at the end told to get set up on Friday!
** but said that it would be nice if we have access go this at least!
* Spent time getting Sanket caught up - with installing dependencies and qsub
** asked him to do it on his local as well to see if it at least works on local
* Decided to try it on local as well to see how to make script that turns mysql schema to data and extract data from that
** decided that csv works itself
* Reagan apparently broke mysql on PACE so gcp would be nice

=== Februrary 7 ===
* Alex contacted me about PACE progress and I gave update to him
* Anish and Bek tried to connect to their local mysql - Failed!
** Anish later realized that he was on his hotspot which might have been the reason
* Pulak is still continuing on multiprocessing side of this all 
* I met up with Dr. Zutty to figure out GCP setup 
** I am working on same instance as adf
** Dr. Zutty Made a separate bucket for nlp stuff
** was tasked to put emade onto this bucket
*** install gcloud sdk for laptop
*** find command online for this
** I ended up contacting '''ADF Team''' for more help for this
*** gave github/ docs to follow for this
** needed to get rid of git history and datasets for this, so I needed another copy of emade
*** I installed emade on my computer

{| class="wikitable" 
|-
!Current Status 
!Date Assigned 
!Date Resolved 
|- 
|Get GCP Set up - Got progress  
|February 2 
| - 
|- 
|Google doc for scripts and todo functions 
|January 31 
| - 
|- 
|Coordinate PACE setup knowledge with other nlp group 
|Febrary 7 
|February 7 
|}

== Februrary 10 - 16 ==

=== February 10 ===
*I worked to get set up GCP
*I worked on recloning new emade copy on my computer
**This way too long so I had to try again with git lfs install
*Tried again by only cloning a branch of emade
*I worked as well for scripts as well
*Got info about getting data from mysql from ADF Team

=== February 14 ===
* I got finally got emade cloned on my computer
** I got rid of git history and deleted irrelevent datasets
** sent the copy of the GCP bucket 
** 

{| class="wikitable" 
|- 
!Current Status 
!Date Assigned 
!Date Resolved
|- 
|Get GCP Set up - Got progress  
|February 2 
|Febrary 14 
|- 
|Google doc for scripts and todo functions (Bek focusing on this) 
|January 31 
| - 
|}

== Februrary 17 - 23 ==

=== February 17 ===
* I contacted Jason about getting the copy of emade onto a the bucket
** Later sent him email detailing how to run it as well
* Dr. Zutty later mentioned about wanting for the neural network subteam to work towards submitting a workshop paper for conference
** Reagan, Bek, and I made a decision to help NN subteam with their stuff after this week after we finish with our stats subteam task
* Thought about using xgboost and pca as primitives as for another task xgboost worked very well and bag of words vectorization is in a really high dimension space and pca should theoreticlly help for reducing the feature space
** Reagan mentioned that xgboost was already used and was not the best

=== February 21 ===
* Went home for the weekend so couldn't make it to the meeting
* Did Literature Review to see where research in neuro inspired Genetic Programming is as of now
{| class="wikitable"
|-
!Current Status
!Date Assigned
!Date Resolved
|-
|Sending email for GCP set up
|February 17
|Febrary 18
|-
|Google doc for scripts and todo functions (Bek focusing on this)
|January 31
| -
|-
|Test out XGboost and pca
|February 17
|partially done on 21, need to to official jupypter notebook as well
|-
|Literature Review
|February 17
|February 225
|}

== February 23 - March 2 ==

=== February 24 ===
* I assigned myself making a unit test finding p value
** looked at difference of two population - welch t test 
* Found important paper: https://arxiv.org/abs/1902.06827
** They also do neuroevolution with techniques that are different

=== February 28 ===
* We decided that we all need to read the paper very well
** I decided to give some of my time for this as well
** They have their own datasets - Computer Vision and NLP ones do we do theirs or should we do our own
** 
* Pulak is setting up Icehammer, 
* PACE was down so we could not get too much progress in that

=== February 29 ===
* Read paper
** They use coevolution - stuff Reagan worked on
** Initially I thought it would be better to just use our own dataset for the paper, 
*** but after talking to Dr.Zutty we were pointed to focus on integrating the datasets in the paper for a bench ark
** They use a lot of compute nodes - more than we actually have
* Helped with initial powerpoint for presentation
{| class="wikitable"
|-
!Current Status
!Current Status
!Date Assigned
!Date Resolved
!Suspense Date
|-
|Google doc for scripts and todo functions (Bek focusing on this)
|In progress
|January 31
| -
| -
|-
|Literature Review
|Completed
|February 17
|February 25
|February 29
|-
|Unit test for p value - 2 sample t test
|Completed
|February 24
|February 29
|February 29
|-
|Full review of paper
|In Progress
|February 28
| -
|March 2
|-
|Slides
|In Progress
|February 29
| -
|March 9
|}

== March 2 - March 8 ==

=== March 2 ===
* Paper stuff
** We found chart regarding hyperparameters to tune upon
** We are also discussing how our model can be novel. 
** We made google doc of tasks for this: https://docs.google.com/document/d/1vGWQpaWf8FM4PDnd7EG04vEDL_XD7tkP4nuKf33Q3gY/edit
** I decided to focus on dataset integration
*** Let's do text data first because infrastructure is already there
** I realized that their individuals are huge!
*** They use coevolution:
**** They evolve big individual architecture
**** They also subcomponents which solely include layers[[files/Screen Shot 2020-03-11 at 10.19.33 AM.png|center|thumb|276x276px]]
** PACE was still down ..
*** We worked a bit on the powerpoint

=== March 6 ===
* Update on March 5th:
** Toxicity dataset has multiple categories so same problem summary team last semester comes back again.
*** We can just use last semester's work
*** However, at this point it might just be easier to use/work with our own dataset
** Pulak mentioned that there is a separate loss function to solve this
*** Doubtful but decided to try
* Talked to Alex about his progress on PACE
** He mentioned that he used one single node and got it to work
** Therefore, I tried following his steps:
*** edited launchEMADE.py and sqlconnection.py files
*** edited /.my.cnf file and added correct things
*** made sure to download en_core_web_sm (spacy download stuff)
*** changed the pbs scripts for both
** Got connection error!!
* Helped Anish with Neural Network code debugging
** He keeps on getting 50% on emade but on a Jupyter notebook his same primitive gives a 86%
** The problem might be within the Keras Pickle Wrapper

=== March 8 ===
* Pulak ran some tests on Icehammer
** He missed some tests
* Therefore, I decided to run emade on my computer.
** This, however, was a very bad mistake as my computer is notorious for being slow 
*** it just froze up and did even get to the 1st generation after 3 hours
** Unfortunately I spent most of my time doing this, which was very ineffective
* Retested PACE connection to local sql database - did not work
* Helped Anish with neural network stuff
** Getting Cuda errors - is running out of memory in his gpu
** He eventually got it as vocab size was apparently inconsistent 

{| class="wikitable"
|-
!Current Status
!Current Status
!Date Assigned
!Date Resolved
!Suspense Date
|-
|Google doc for scripts and todo functions (Bek focusing on this)
|Basically done
|January 31
|March 8
| -
|-
|Full review of paper
|Completed 
|February 28
|March 2
|March 2
|-
|Toxicity Dataset Review
|Completed 
|March 2
|March 6
|March 6
|-
|Emade Run on my Own Computer 
|Completed (Not much success)
|March 8
|March 8
|March 9
|-
|Presentation Slides
|In Progress
|February 29
| - (polishing up stuff)
|March 6
|}

== March 9 -15 ==

=== March 9 ===
* Presentations!
* Reagan got results from Pulak
** helped him to convert csv data and do sql commands on pandas
** a join is a merge in pandas
** Then realized that data was only one generation!
*Overall presentations went well
**We just put boxplot showing that we have the capability of giving results once we get the data
**We discussed PACE issues mainly

=== March 13 ===
* covid-19 situation - did not meet I was out of town nevertheless
{| class="wikitable"
|-
!Current Status
!Current Status
!Date Assigned
!Date Resolved
!Suspense Date
|-
|Presentation slides
|PRESENTED
|February 29
|March 9
|March 9
|}

== March 16-22 ==
* SPRING BREAK  - did not meet
* Pulak mentioned he was working on getting remote icehammer connection to work

== March 23-29 ==

=== March 23 ===
* First Online meeting
* I presented for my team for this week
** We mainly discussed plans for the rest of the semester
** Discussed plans with pace - run tests on pace - Pulak
** Moving focus to neural network side
** Will work on integrating chest x-ray and toxicity datasets
** Need GPU access
** Add more primitives
** Dr. Zutty mentioned colab as a resource
* Started subteam meeting with first semesters as well
** discussed what first semesters can do
*** Finalized on them working on Jupyter notebooks for chest x-ray and toxicity dataset 
*** was originally going to do pair programming but realized that people that's too hard within remote format so we decided to assign them datasets
** We let the first semesters go and started discussing future plans
*** We decided that we should get the dataset ready and provided them resources before we give it to them
**** Assigned Bek and Reagan for this - Bek does toxicity and Reagan does image - took longer than one week so first seems had to wait
***** Had them look at emade and datasets for reference for now
**** Had Pulak explain PACE issues to me - I let Pulak work with this
**** Pulak is trying to just submit jobs on icehammer for now!
***** This seems to be the best route of success for now
**** Anish decided to work on using colab with emade as they have gpu access!
**** I decided to work on integrating toxicity dataset
**** Explained nature of datasets - chest x-ray is multiclass while toxicity is multilabel

=== March 27 ===
* I worked on integration of toxicity
* I first had to get the toxicity dataset in the correct format into emade
** some of the test labels were negative - had to ask Bek what this meant
*** apparently this means that it was not used in kaggle as testing
*** So I made a script that loaded the test and got rid of the rows which had negative labels
**** From this I made a test dataset for emade
**** I later found out that Bek just used part of the train data as the test instead
** I had to explain how we did not have multilabel capability
** neural_network.py did not have multilabel capability so this needs change - let Anish do this
** data loaders need to be changed for this
*** initially decided to use Pandas and tried to take on task of completely changing the dataloaders
**** but data is stored as classes with lists as a data attribute which is then wrapped around another class
**** Pandas just has it as a big numpy array
***** I could have just made feature data work with pandas but I would have to change how ALL primitives load a datapair as I was trying to make a datapair just two numpy arrays
****** I could have done wrapped a EmadeData as a 2d numpy array instead of an array of emadedatainstances; however I had to also change eval functions as well!
**** At the end I decided that it wouldn't generalize emade as I would be completely ignoring the rest of emade so decided to not work on this
*** I changed my mind, and decided to just use the dataloader I made from last semester
**** input format to data loader is first column of text data and rest of columns is 1 or 0 for all the categories (a piece of text can be in multiple categories)
*** changed data type of to multilabel_text_data in template file, made sure this routed to the new dataloader in emade.py, added new data loader in data.py
* I met up in the meeting to get progress
** Learned that image dataset had a problem as it is too big - 32GB
** Dr. Zutty advised us to use a smaller subset - that worked
** We decided to release the datasets and resources on Monday
** We asked Pulak to run stat stuff to try to get results for 30 gens
*** realized that would take a while so we tried to do this as early as we can 

{| class="wikitable"
|-
!Current Status
!Current Status
!Date Assigned
!Date Resolved
!Suspense Date
|-
|Initial Tasking First Semesters
|Completed
|March 23
|March 27
|March 24 (extend to March 27 because of dataset isues
|-
|Integration of toxicity dataset (matching format of data into emade)
|Completed 
|March 23
|March 27
|March 30
|-
|Integration of toxicity dataset (new dataloader
|Completed 
|March 23
|March 27
|March 30
|}

== March 30 - April 5 ==

=== March 30 ===
* We had a meeting today and Bek spoke for us today
* Afterwards we broke into subteams
* I spent most of the time explaining how emade works what most of the files are
** I went over the whole process (downloading a daset/ making a template file, data.y and a dataloader/ EMADE.py and how it chooses the dataloader/ the primitive/ where the primitives are/ gp_framework_helper/ Unit Tests/ running emade)
* We then had the first semesters choose a dataset and work on it
* 2/3 semesters continued our work: 
** I continued work on toxicity dataset integration

=== April 3 ===
I utilized unit tests to fix bugs in my integration of the toxicity dataset. 
* I realized that I made a good amount of mistakes on new dataloader! - fixed this
* Realized that need to integrate multilabel capability for neural network stuff to continue!

* I went over progress with first semesters
** The image stuff went well
** the toxicity was fine too but got a really high accuracy score which I was VERY dubious about! 

{| class="wikitable"
|-
!Current Status
!Current Status
!Date Assigned
!Date Resolved
!Suspense Date
|-
|Integration of toxicity dataset (Unit tests/bugs/etc.)
|Completed 
|March 30
|April 3
|April 6
|-
|Tasking first sems/ w/ dataset/ help with questions
|Completed
|March 23
|March 27
|March 24 (extend to March 27 because of dataset isues
|-
|Investigate weird high score for toxicity dataset
|In Progress 
|April 3
| -
| -
|}

== April 6 - 12 ==

=== April 6 ===
* Anish got colab working with emade and is working on documentation
** eventually got documentation done and released code to whole team
* I decided it was important to get set up on colab so I decided to work on this
* We also decided that it would be beneficial to get the first semesters to set up with colab
* We also had first semesters choose one of the todos in the google docs to work on!

=== April 10 ===
* Worked on getting colab set up
** followed Anish's steps
** decided to use remotemysql.com
*** it outputs username, password, database, and domain, which I inputted in the template
** however after running it stopped writing to the database after 3 individuals during genrertion 1
*** debugged/brainstormed possible issues
*** maybe space ran out
**** no database shows 5mb
*** maybe collab is taking that long to run
*** maybe ram ran out in Colab and didn't show
*** maybe too many connections to database as the website said you cannot have too many connections
* also talked and debugged stuff with the first semestsr
** Cameron had problem with deap and we looked at fixes for this
** Anshul tried using gloves with movie reviews and got expected results
*** oheorys that glove is dominating the network was proven incorrect
** helped others getting Colab set up
* I worked on looking toxicity dataset and on why it had such a huge percentage
** realized that it was due to accuracy metric and how kekras does that for multilabel cases 

{| class="wikitable"
|-
!Current Status
!Current Status
!Date Assigned
!Date Resolved
!Suspense Date
|-
|Set up Colab using Anish's instructions
|Completed
|April 6
|April 12
|April 13
|-
|Tasking first sems/ w/ colab set up and todo list
|Completed
|April 6
|April 6
|April 6
|-
|Get Remotemysql set up
|Completed
|April 6
|April 9
|April 13
|-
|Figure out problems/fixes for Colab
|In Progress
|April 10
| -
|April 20
|-
|Investigate weird high score for toxicity dataset
|In Progress
|April 3
| -
| -
|}

== April 13- 19 ==

=== April 13 ===
* Before the meeting, I tested my hypothesis. I found out that I was correct. 
** I realize that the way they did accuracy for multilabel Keras does is incorrect.
*** My hypothesis was proven correct with when testing with the first semester notebooks
** I also realized that our model outputs mostly zeros (sparse multilabel problem(
** Used techniques from these posts solve these problems:
*** https://stackoverflow.com/questions/50686217/keras-how-is-accuracy-calculated-for-multi-label-classification
*** https://stats.stackexchange.com/questions/261128/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu/313922#313922
** I made a custom evaluation function that checks if all of the classes 
*** len(np.argwhere(np.all(np.round(predicted)==y_test, axis =1)==True))/len(y_test)
*** Findigs stated in slack: "Ok have tested model, it predicts values greater than .5 only 6 times out of 191490 times. So it basically gives all 0s" 
**** This means that it gave 6 0s out of 31915.0 by 6 datapoints!
** However, the accuracy part of the model doesn't affect the model so I realized that I needed something else
*** The stats.stackexchange article discusses using a different loss function to actually change the model
**** I tested their loss function: weighted_cross_entropy  - it gives more weight to ones than zeros
**** it's basically a keras wrapper to this tensorflow function: tf.nn.weighted_cross_entropy_with_logit
** The accuracy with new weighted_loss_function with new accuracy metric: 74% MUCH more believable and works much better. Number of ones outputted in 100s
** Commits for Jupiter notebook changes: 
*** https://github.gatech.edu/emade/emade/commit/ab7c842b421ace059618fa37c12bd8900aede7e4
*** [https://github.gatech.edu/emade/emade/commit/9e71156811de637997b76884e1da978a25a0bfdf merge: https://github.gatech.edu/emade/emade/commit/9e71156811de637997b76884e1da978a25a0bfdf]
* I also explained my findings to the rest 
** don't think the rest of my group realized the problems of multilabel data and the situation
** A full discourse of the problem and fixes was done for this!
* Started meeting
** Presented Progress from our team to the rest of the VIP team
** Went into subteams
*** Talked about progress of first seems
**** They mostly worked on getting colab set up
*** I also explained my new findings to the first semesters!
*** We decided to move on and make sure everyone was assigned to a task on the google doc
*** We went over their selections and feasibility and usefulness of each task.
**** ex. reassigned Katherine to Dropout instead of Linear layer
*** Most of the primitives to add were activation functions:
**** right now activation functions are passed in string terminals, we decided that we want to move away from this and found that activations can be represented as layers (decided this option)
*** We also went over how neural_networks_methods.py worked as a lot of first sems were unsure how to get started
*** We asked the first semesters to finish their tasks by Wednesday
*** We decided to start work on presentations on Friday
*** We discussed goals for final presentation:
**** Finish getting results for nlp-app stuff and stats test that
**** Run emade on toxicity dataset to get results
***** Do it without new primitives first and do it again with primitives integrated for comparison!
* Anish and I had an infrastructure question on how terminals are used
** Either just do layers or make our own class to add as terminals (like LearnerType)
** decided to work on this later
{| class="wikitable"
|-
!Current Status
!Current Status
!Date Assigned
!Date Resolved
!Suspense Date
|-
|Investigate weird high score for toxicity dataset
|Completed
|April 3
|April 13
| -
|-
|Integrate new loss function/ accuracy metric within emade
|In Progress
|April 13
| -
|April 14
|-
|Looking over tasking for first sem and overview of integration of their primitives
|Completed
|April 12
|April 13
|April 13
|-
|Work on slides (Start that work on Friday)
|In Progress
|April 13
| -
|April 19
|}

=== April 14 ===
* Vietfu worked on a primitive and sent me it  to see if it is good and then realized that first semesters do not have push permissions
** I told them to make a fork on the branch and do merge requests to our branch
** This especially will help with merge conflicts. We can have Anish or someone work on merging them at once.
* I added the loss function within emade - commit after further explanation of integration and usefulness
** https://github.gatech.edu/emade/emade/commit/3b8cdc206f32d31f722869149d2f9241a0276ceb
* Reagan found bug for the code i pushed
** realized that this code is not in tensorflow 2.0 - tf.log -> tf.math.log
* Pushed updated notebook for reference:
** https://github.gatech.edu/emade/emade/commit/f89a9e23933c1524d607027d20dcbb9149aab2b3
** Merge: https://github.gatech.edu/emade/emade/commit/a447f017c56bcdd5c28d35ee6cff656d831e9bfb 

{| class="wikitable"
|-
!Current Status
!Current Status
!Date Assigned
!Date Resolved
!Suspense Date
|-
|Integrate new loss function/ accuracy metric within emade
|Completed
|April 13
| April 14
|April 14
|-
|Help out for PACE Run
|In Progress
|April 14
| -
|April 20
|-
|Work on slides (Start that work on Friday)
|In Progress
|April 13
| -
|April 19
|}

=== April 15 ===
* Reagan found another bug in the new loss function: 
** Another tf 2.0 bug: target -> labels
* Reagan had a weird error/bug "One of them(predictions or ground truth labels) has the wrong shape, specifically it goes to (0,) when the number of instances gets too big"
** I worked with him to try to figure this out
*** I first got Reagan caught up in how he integrated multilabel capability within emade
*** After some talking, Reagan said there was a memory error when running a unit test within emade
**** Then realized that the same memory error might be happennig whenever running emade!
**** Realized the source of this problem all was due to running out of RAM on colab!
** We decided to try pace run! {| class="wikitable" |- !Current Status !Current Status !Date Assigned !Date Resolved !Suspense Date |- |Help for PACE Run (worked on this today) |In Progress |April 14 | - |April 20 |- |Work on slides (Start that work on Friday) |In Progress |April 13 | - |April 19 |}
{| class="wikitable"
|-
!Current Status
!Current Status
!Date Assigned
!Date Resolved
!Suspense Date
|-
|Help out for PACE Run (worked on this today)
|In Progress
|April 14
| -
|April 20
|-
|Work on slides (Start that work on Friday)
|In Progress
|April 13
| -
|April 19
|}

=== April 16 ===
* Helped Anish with making baselines. 
* Decided to hop on a call to get stuff done - w/ Reagan
** decided that we need to be using a much smaller subset: use reduce dataset function for emade run.
** Decided to do both a pace run. We got an initial setup of that. 
*** We worked on getting pace set up for gpus: 
*** use pace-ice-gpu instead of pace-ice as queue takes TOO long
*** We later found out that we were actually not using gpu! LOL! 
** Decided that if we are using smaller subset we can try colab as well. 
*** At the end same error of erroring happened so we discontinued this (finding out why this would be very helpful)
** We just used Reagan's PACE runs for now 

{| class="wikitable"
|-
!Current Status
!Current Status
!Date Assigned
!Date Resolved
!Suspense Date
|-
|Help out for PACE Run (worked on this today)
|In Progress
|April 14
| -
|April 20
|-
|Try for Colab Run
|In Progress
|April 16
| -
|April 20
|-
|Work on slides (Start that work on Friday)
|In Progress
|April 13
| -
|April 19
|}

=== April 17 ===
* Pulak discussed results from icehammer stuff: 
* We had our Friday meeting with everyone.
** First sems gave update: most actually did merge requests to branch
*** Anish tasked himself to integrate all of the merge requests within emade!
*** Anshul still working on his stuff, Sanket didn't do a merge request
*** Cameron's primitive was tooo complicated and he discussed the research on it and integration is not seeming at all
**** told him that in that case we can wait on integration - can just discuss the research on the slide
*** Discussed what to do for the slides.
*** Decided to have a new primitive section in the slides in which each first sem would work on a slide for their primitive
** Pulak gave us more specifics on icehammer updates
* I continued within the call to work with Reagan for runs
** Right now Reagan had runs within the smaller dataset
*** We discussed different options: a full dataset would be more representative, discussed tiered dataset option, maybe just use a bigger dataset
**** decided to try different dataset sizes
** We spent a LOT of time getting colab set up (also fixed annoying deap error by changing deap code within deap's package conda installation (and getting multiple remote MySQLs set up from Pulak, me and Reagan) (also the website was annoying to make new mysqls))
*** I even found a way to request for more ram, so we found out that running out of ram was not the problem/bottleneck
*** At the end attempts were UNSUCCESSFUL since Colab consistently disconnects (We think)
** At the end we just ended up doing pace-ice-gpu runs 
* I worked on the slides and outlining the main parts of it and filled out toxicity/multilabel stuff
* Decided next step was getting runs with first sems stuff
{| class="wikitable"
|-
!Current Status
!Current Status
!Date Assigned
!Date Resolved
!Suspense Date
|-
|Help for PACE Run (worked directly w/ Reagan on this)
|In Progress
|April 14
| -
|April 20
|-
|Try for Colab Run 
|Completed 
|April 16
| April 17
|April 20
|-
|Work on slides (Initial Outline Done)
|In Progress
|April 13
| -
|April 19
|-
|Get set up for PACE Run w/ first sem primitives
|In Progress
|April 17
| -
|April 19
|}

=== April 18 ===
* Reagan got results from run without first sem primitives
* helped Anish with GLOVE primitive integration - problems with git lfs and merge requests
** We had to do manual integration based off of Anshul's commit history
*** helped Anish with this a bit
** At the end there was issue with glove file not going within emade on github
* A lot of individuals errored out according to Reagan
** To help this, I made sure output dimension is positive - commit: 
*** https://github.gatech.edu/emade/emade/commit/cbec9607a66a234bfcb18a656462ed7625cc3db2
*** However need to make sure that last layer is the same dimension of number of classes
**** For now we have specific layer for this task but making it more generalizable is needed for the future.
* I decided to work on making a new seeding file with the new primitives as Anish was working on finishing up new primitive integration. Since glove didn't work out, I changed that up.
** https://github.gatech.edu/emade/emade/commit/8a3c5de61dfe4b596a383e89c36bfebef4fb5e2a
* There seemed to be errors within seeding
** I though there were errors on my end but not haha 
*** In the seeds, I had it as gpframework not taking in any terminals - this not reflected in gp_framework 

{| class="wikitable"
|-
!Current Status
!Current Status
!Date Assigned
!Date Resolved
!Suspense Date
|-
|Help for PACE Run
|Completed
|April 14
|April 18
|April 20
|-
|Work on slides (waiting for others to fill their parts)
|In Progress
|April 13
| -
|April 19
|-
|Get set up for PACE Run w/ first sem primitives (seeding)
|Completed
|April 17-18
|April 18
|April 19
|-
|Set up PACE run w/ first sem primitives and get results
|In Progress
|April 18
| -
|April 19
|}

=== April 19 ===
* Nudged everyone to finish their slides: 
* Xiadong mentioned that his conv1d and maxpool pull request not being merged
** I mentioned the problems we had earlier and had him redo his pull request to make it not have any merge conflicts
** merge commit: https://github.gatech.edu/emade/emade/commit/62ea43bce17784c4dc082a89b74024acf6e0517c
* We ended up doing a slide presentation run with everyone that was available
* I also helped Jehun get caught up who had special circumstances
** Told him to add in the linear layer (prob shouldn't be used) within the correct format
** gave him Vieftu's PR to follow format 

{| class="wikitable"
|-
!Current Status
!Current Status
!Date Assigned
!Date Resolved
!Suspense Date
|-
|Work on slides (waiting for others to fill their parts)
|Completed
|April 13
|April 19
|April 19
|-
|Get set up for PACE Run w/ first sem primitives and get results
|Completed
|April 18
|April 19
|April 19
|-
|Dry run of Presentation w/ everyone/everything
|Completed
|April 19
|April 19
|April 19
|}

== April 20 ==

=== FINAL PRESENTATIONS ===
Before I start this, I would like to appreciate Reagan on all of his work - He did so much and was passionate to get stuff done and I greatly appreciate that. I also appreciate the rest of my group Anish, Pulak, and Saribek for the work they have done within the semester. Also, I appreciate the work the first semesters have put in as well. 

Notes on other Presentations:
* ADF:
** Cool how they did experiments
** seems like ADF Works - but still work in progress
** cool stats tests and analysis - they did more within this field than anyone else
*** Cool that they are going w/ differential fitness idea
** I am unsure about selection idea - not the best
* Bloat - seems like important project
** Honestly I don't know about approach in my opinion
*** should be looking into emade itself for problems instead of integrating other stuff (my opinion)
*** But their work seems cool and different!
** Using hypervolume apparantly
** PACE stuff is interesting 
*** Chris did a nice job in setting it up VERY quickly
**** He used the same strat as us with the submitting master/worker as different jobs using didLaunch.py
* Our Presentation
** I think it went pretty well!
** I actually lost connection near the end during the questions but my teammates picked up the slack so it was fine.
* NLP-Timeconflict
** They spent a lot of time trying to use PACE for MySQL (would have been smarter to see other options)
** Their dataset is hard
** Also their primitives (num_named_entities and tsisf) seems not to be the best
** Alex's approach of using modified svc is very promising
* EZCGP
** Their project is cool as always
** Transfer learning is interesting
** apparently they couldn't do pace run as it was not using gpus on tensorflow 2.0 so they just did gcp runs
*** Realized something bit here, were were probably having the same issue with pace
**** Using reduced dataset fixed problem but only because the time cpu took was much smaller so it was fine
*** So using PACE for our problem is not the best idea either! 
** Configuring GCP seemed like a HUGE pain which is unfortunate
** They mention using Hadoop/spark as a data streaming strategy which is a very interesting strategy
** They also mention using Kubernetes for GCP stuff
*** I recommend this if GCP is being used a lot!
{| class="wikitable"
|-
!Current Status
!Current Status
!Date Assigned
!Date Resolved
!Suspense Date
|-
|Our Presentation/ notes
|Completed
|April 20
|April 20
|April 20
|-
|Finish Notebook 
|Completed
|April 20
|April 26/27
|April 27
|}