== Team Member ==
[[files/GeorgiaTechBuzz.jpg|thumb|123x123px]]
Team Member: Sharma R. Nishant

Email: nsharma93@gatech.edu

Cell Phone: 404-543-2609 

Interests: Computer Architecture, Software Engineering, Music (Guitar, Violin), Gaming 

== April 26, 2021 ==
'''Note:- The image formatting system is causing errors, there is no way to reduce the size of the image'''
==== Lecture and Subteam meeting ====
* We need to start the presentation, an old copy of the presentation was added and we are expected to start making edits to it before Friday (The presentation day).
* Plant to meet on Wednesday (2 days before the presentation), to discuss more about how the presentation will be structured

==== Individual Notes and Work for the presentation ====
* The first thing to be done is implement Cameron's change on Git. 
* Cameron added a huge change that will essentially remove the dependancy of GPU, and make it so that more of the team members wont be stuck in queue to run emade jobs.
* I pulled all the changes and running a seeded run using FPR and FNR was the my responsibility for the presentation. This would give more data to Cameron to perform statistical tests. 
* Step 1:- A way to Visualize Data and making a connection using MySQL Workbench:
** I have been used to looking data through mysql, for the titanin project, and wanted to set up a MySQL connection, to run more queries and be able to see the data upclose. 
** Most of the team member that have been able get successful runs on EMADE directly use the Master.out file to do this, which could take more time.
** I started my mySQL server in PACE, and used the following configurations to get my database connection: 

[[files/MySQL Config.png|center]]

====== First Seeded FPR and FNR run of EMADE ======
* I moved all the files to pace, and also added the CPUPerHostNode parameter to 8 instead of 4 to get more computational resources dedicated to the run (In hopes that the individuals would work faster)
* I started my Seeded run of EMADE, the detains of which are as follows:-
** Run: Seeded 9 Individuals
** Seeded Indviduals:-
*** NNLearner(ARG0,OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool,EmbeddingLayer(100, ARG0, randomUniformWeights, InputLayer())))), 100, AdamOptimizer)
*** NNLearner(ARG0,OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool,EmbeddingLayer(99, ARG0, randomUniformWeights, InputLayer())))), 90, AdamOptimizer)
*** NNLearner(ARG0,OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool,EmbeddingLayer(98, ARG0, randomUniformWeights, InputLayer())))), 98, AdamOptimizer)
*** NNLearner(ARG0,OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool,EmbeddingLayer(97, ARG0, randomUniformWeights, InputLayer())))), 99, AdamOptimizer)
*** NNLearner(ARG0,OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool,EmbeddingLayer(96, ARG0, randomUniformWeights, InputLayer())))), 94, AdamOptimizer)
*** NNLearner(ARG0,OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool,EmbeddingLayer(94, ARG0, randomUniformWeights, InputLayer())))), 93, AdamOptimizer)
*** NNLearner(ARG0,OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool,EmbeddingLayer(92, ARG0, randomUniformWeights, InputLayer())))), 92, AdamOptimizer)
*** NNLearner(ARG0,OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool,EmbeddingLayer(90, ARG0, randomUniformWeights, InputLayer())))), 89, AdamOptimizer)
*** NNLearner(ARG0,OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool,EmbeddingLayer(93, ARG0, randomUniformWeights, InputLayer())))), 91, AdamOptimizer)
** Objectives: False Positive Rate, False Negative Rate
** Resources: CPU only 
** Generations: 26
** AUC of the ParetoFront: approximately - 0.026
** Following is the image of the Pareto Front graph with the area under the curve

[[files/SeededRunAUC.jpg|center]]

* Query to get the Pareto Optimal Individuals - From the power point used in bootcamp presentation:
** Select * from individuals join paretofront on individuals.hash=paretofront.hash where paretofront.generation=(select max(generation) from paretofront);
* Because of using database to query things we can export the data to CSV files easily. - [https://drive.google.com/file/d/1Q5G0QzmFFxGvO07iBdAGx-DXgZ_WwJBU/view?usp=sharing]
* There is also an excel workbook that I created that highlights the individual with good numbers in green, and shows the seeded individuals in yellow. The workbook also has a graph of the pareto optimality and other use ful numbers like evaluation time, which could be a good metric of how long complex the individual is:- [https://drive.google.com/file/d/1JZz3fUOYpL6HdMbWH3-bxum_uIdFHZFz/view?usp=sharing]
** After looking at this (The csv file has the pareto front) the only individual that does not have almost 0.5 for one and almost 0 for the other objective (Mostly trivial solutions that all predict one way), and was not part of the seeding file is as follows:
*** NNLearner(ARG0, OutputLayer(LSTMLayer(16, defaultActivation, 0, trueBool, passBool(trueBool), EmbeddingLayer(100, ARG0, randomUniformWeights, ConcatenateLayer2(FlattenLayer(InputLayer()), InputLayer())))), 100, NadamOptimizer)
*** The objective value of above ind:- FPR = '''0.0318276'''; FNR = '''0.0424876'''
* The tree image of the individual is as follows:

[[files/IndividualParetoOptimalNotFromSeedingFile.png|center]]

====== Second Seeded FPR, and FNR run ======
* The second seeded run had the following pareto optimal graph

[[files/SecondSeededRunParetoOptimal.png|center]]

* The pareto front has been queried:
** The Excel workbook file has the individuals and shows which individuals are from the seeded file, and which ones have good values: [https://drive.google.com/file/d/1_BlZiJKhfxuAGzjKVhvEsZ_9BimWT1T9/view?usp=sharing]
** The Excel shows a couple of individuals and I chose one individual to talk - It had the highest elapsed time and good FPR and FNR values
** The following individual has FPR = '''0.0319'''; FNR = '''0.0382'''

[[files/SecondRunIndividual.png|center]]

====== Responsibilities for Presentation and Visualization ======
* I was tasked with showing the results of the FPR, and FNR runs that were run. I had two runs, and others did some runs as well, however, I noticed good individuals in my runs and chose to talk about those
* The results that I would be presenting were of 2 runs (both were FPR, and FNR)
* My responsibility was to get good visualization of the data (including excel sheets of Pareto front, Pareto optimality graphs, and gif of Pareto optimal individuals)
* In addition Cameron was performing statistical data, and required the AUC value of the seeded individuals. So in addition to visualization I was also tasked with culling the seeded individuals so they are co-dominant, and graph them and get the AUC (Acc score and FPR, FNR)
* I wrote a script to graph Pareto individuals and made the existing notebook compatible with both multiobjective problems that we did.
* Cameron shared his notebook which included code to parse the master.out file of a run, to convert it to a JSON, and getting data about valid individuals and the Pareto front of each generation
* This cut down my work significantly, and all I had to worry about visualizing the data, getting good FPR, and FNR graphs for all generations and making a gif
* Excel Visualization:-
** Every time I did a run of EMADE, I immediately queried the Pareto front and stored the data in a csv file
** After this I created an Excel workbook and after visually analyzing the individuals, I highlight the individuals that are from the seeded file in yellow, and highlight good individuals (Not (0, 0.5) and (0.5, 0) for FPR, FNR) which are not from the seeded file
** This highlighting gives a good idea of how many individuals from the Pareto front were good models, and which can be ignored as they are directly from the seeded file
* Excel Visualization for Run 1:-
[[files/ExcelFirstSeededRun.PNG|center]]

* Excel Visualization for Run 2:- 
[[files/ExcelSecondSeededRun.PNG|center]]


* Pareto Individuals (Final Generation):-
** I did this similar to the way we did it for Titanic problem, Querying Pareto optimal individuals for the maximum generation, and exporting to a csv
** Using Pandas to get data from the CSV, and getting FPR, and FNR values in numpy arrays
** Sorting the FPR, and FNR values (Using ARG Sort) such that the coordinates (FPR, FNR) are correctly sorted
** Adding trivial data points to bound the graph (1,0) and (0,1) 
** And calculating the AUC value
* Following is the code of graphing FPR, and FNR using .csv file: Link:- [https://colab.research.google.com/drive/1ZROmuA294SDFHXMj2dMGaod-PecRBTxF?usp=sharing]
[[files/CSVToParetoFrontCode.PNG|center]]

* Pareto Front Evolution (Gif):-
** This task was a little more involved, and needed the Pareto individuals of all data. Cameron suggested there are two approaches to get the data
** I was querying data directly, Sumit had written a script to parse data directly from a master.out file by converting it to a JSON
** Camero used Sumit's code and added to it (a organize function) that gets data from the JSON and organizes it in a dict format
** A valids dict which will have pareto_front individuals as a list and we can graph it from there
** My task was to add FPR, FNR as objectives (The group was only working with acc score and num params before) graphing functionality, and compile all images to make a gif 
** I added input statements to get the name of the master.out file and choosing 1 or 2 to specify which pair of objectives were used (This was used for graph labels, and adding trivial individuals)
** For Accuracy Score and num params graph the trivial values were 0,1 and 1000000000,0 as this is the bound EMADE uses when calculating Hypervolume
** This made it a little harder to visualize the data, so I commented it out
** We ended up only using the for creating gifs for FPR, FNR and chose not to do that for accuracy score and num_params because we were good for content

* Seeded individuals AUC:-
** Cameron asked me to get the AUC value for the seeded individuals for both FPR, FNR and accuracy score and num params case to aid with the statistical analysis he planned to perform
** I noticed that the seeded individuals were not Pareto optimal (so I removed the weak individuals, which were dominated by some individual on both objectives)
** I tried to write an algorithm for this, but it was a little buggy and due to the lack of time (and since we only needed two graphs with 8 individuals), I decided to eyeball it and remove the weak individuals
** Created the graph and got the AUC score and sent these graphs to Cameron to perform statistical analysis
** I added a function to get this done, that outputs a graph for each generation, and I saved all these images and sent it to Gif maker application

* The following is the link to my personal Google Collab which I copied and worked on to add this functionality:- [https://colab.research.google.com/drive/1ZV0MaDdbcQ5kKJ2sQk9nS5ZgK2nwtYdk?usp=sharing]
* Cameron let me know that I had made a mistake in the FPR, FNR runs in my XML file I forgot to change the Upper bound from 1000000000 (Num params value) to 1 which should be the valid value for FPR, FNR for one of the objective, This led to EMADE calculating the AUC with an incorrect bound - But the presentation and individual analysis were not impacted as I manually added the 0,1 and 1,0 bounds for FPR and FNR to get AUC values of the Pareto optimal individuals - I made this change to the XML file for future runs.

==== Presentation Notes (April 30th Final Presentation) ====

* Stocks:-
** Objectives: Implement TA-Lib; Increasing evolvability; Test on larger datasets; Stats on evaluations of individuals; Objective functions; and Technical Indicator research
** EMADE Run Results
*** 2 Long runs; 328 gens, 4 objectives - Profit Percentage (max), Average Profit per Transaction (max), Variance Profit per Transactions (min), Normal CDF on Distribution (min)
** Showed a few Notable Individuals
** Monte Carlo Simulations:- Find mean and standard deviation of profits and use it to make comparisons
** Another analysis - Buy hold comparison, buy on day 0 and sell on last, the model underperformed in the buy hold analysis
** Also did an analysis using buy sell lag
** Primitive analysis: Feeding good TI's is important - A graph that plots the median of all individuals containing a technical indicator with a particular learner. A good way to understand which combination performing better
** Questions:
*** Q: Adding some technical indicators to EMADE, made a general purpose wrapper around the TA-libs? Did you consider or did you implement them as new indicator to TA-lib, or kept it simple and did it in EMADE
*** A: The TA-Lib API is not written in Python, and its a wrapper around a C-based library
*** Q: Found some interesting results, but did you do any comparisons if this objective worked better for your evolution than the profit percentage which was used previously
*** A: Suggestion that we could do statistical experiments, but not enough time
*** Q: What is the median computed over?
*** A: What TI was in the tree was how it was filtered, seeing media impacts we might not get combined effects, maybe more data to search the whole space (We might find new and better combinations)


* ezCGP:-
** Intro: A block structure - More unique to ezCGP than cartesian; 
** Experiment: Removed augmentation and preprocessing
*** Accuracy - Training: 93.7 % ; Validation 56.3 % (Overfitting)
** Objective:- Further improve neural architecture search, expanding the primitive set in a controlled manner; Improve visualization framework; Mating strategies for CGP
** Validating Depth (Individual Size) - 4-5 layers (Little diversity), why was this happening, manually analyzed this
*** Results: Individual used in initial population matched target distribution, and larger individuals performed worse so they were selected against
** Experiment (Activation function): Noticed every convolutional layer was using a different activation function - Chose to limit architecture to only one activation function, and no significant impact was noticed
** Pooling primitives and Drop out - Result: after 50 gens, the best individual performed better than midterm best validation accuracy was 68.5 % better than 56.3 %; Pooling layers were good, drop out layers not so much (moved dropouts layer to the dense layer block)
** Dense Layers:- Adding it would improve performance (maybe high-level features would be combined); Added a fully connected layer with dense and dropout layers
*** Experiment for dense layers:- Ran 12 gens - Training 70.4 %; Validation 69.6 %; Conclusion was  less overfitting and good performance 
** Visualization - added vis of inactive nodes, multiple individuals at once, easy to use command-line interface - Shows everything, and what was considered by the model as well, not just what was chosen at the end (The process can be seen better here; what was chosen and what was not)
** Mating - One point Cross over
** Mutation (Point mutation), a percent of parent's genes and mutates those
** Conclusion and future tasks


* NLP (My group presentation


* Modularity:-
** Working on ways to abstract individuals (Creating building blocks to assist the process)
** ARL complexity - Try to increase complexity, by adding an unbalanced tree or increasing depth
** New ARL Implementations did have unbalanced trees and greater depths; More complexity means better results
** Experiment Setup: Titanic and MNIST
*** Titanic feature data such as Age, sex etc. Models - Random Forest; Boosting/Bagging
*** MNIST:- Stream Data; images of handwritted digits
*** Objective functions - F1 Score, and accuracy score initially; Looked at some other objectives (Precision; Recall Score; Cohen Kappa) - Trying to answer if changing objectives have meaningful impacts. The issue was regarding multi-class classification. Chose F1 and Cohen Kappa
** Experiment Seeds - 30 Valid Individuals obtained in previous MNIST EMADE runs (Obtained 316 as opposed to 10 valid individuals using seeding)
** Seeding selection - Individuals with F1 Score < 0.1
** Found some really good individuals with Cv2Sqrt and KNN outperforms every other model
** Need more data for understanding impact of new architecture
** Need to do a better job managing super individuals; Looking into objectives more
** Future work - New models; modifying selection methods; quantifiable way of measuring diversity; ARL Database storage (to prevent losing information); EMADE integration (Integrating new ARL Structure with original EMADE structure)
** Question - have any ARL's that were interesting:- Looked over the slide of the ARL's



==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|pull changes regarding PACE runs (GPU runs to CPU runs) and make sure the changes work fine
|Completed
|April 26, 2021
|April 30, 2021
|April 26, 2021
|-
|Create a MYSQL Workbench connection via ssh and pace - and show configs in Notebook (Not assigned, did it for visualizing queries better)
|Completed
|April 26, 2021
|April 26, 2021
|April 30, 2021
|-
|Do FPR, FNR runs and analyze individuals
|Completed
|April 26, 2021
|April 28, 2021
|April 30, 2021
|-
|Code to get pareto optimality graphs and show visualizations for presentation (Pareot graph with AUC; gif of pareto evolution; Excel; Using Cameron's Vis tool for good individuals)
|Completed
|April 26, 2021
|April 29, 2021
|April 30, 2021
|-
|Work on showing the FPR, FNR run results in the presentation (Adding everything to the presentation); The 2 runs that I did were used for the presentation
|Completed
|April 26, 2021
|April 29, 2021
|April 30, 2021
|-
|Get AUC of seeded individuals for both cases (Accuracy score and num params; and FPR, FNR) and send the data to Cameron for statistical experiment
|Completed
|April 26, 2021
|April 29, 2021
|April 30, 2021
|-
|Correct the mistake in the XML file regarding the upper bound (Made a mistake when running FPR, FNR runs in the upper bound value - but didn't impact the presentation)
|Completed
|April 26, 2021
|April 29, 2021
|April 30, 2021
|-
|}

== April 19, 2021 ==
==== Individual Notes ====
* Progress on trying to run emade and get more data on exactly how and why the NNLearners are failing
* Getting a good run on emade (The process of running emade using PACE)
** The EMADE run was not going past the first generation
*** Surmised from a slack thread of someone going through the same issue that the problem is caused by the deap library version
*** Ran pip install deap==1.2.2 - To get the right version of deap installed inside the conda environment
** Dr Zutty recommended the use of FPR and FNR as our objectives instead of accuracy score
** Cameron made some changes to the seeding files, EMADE.py, and eval_methods.py (The change was regarding the False_positive_rates, and the False_negative_rates
** Running a "seeded" run on EMADE
*** Cameron gave a quick demo of how to run a seeded run and what it means 
*** I pulled all the changes from the git repo
*** I first started my SQL database on PACE ICE
*** Ran the seedAmazon.pbs file which contains the code to run a seed_amazon file which contains a few NNLearner individuals that we put into the database before starting the EMADE process

[[files/Seeding.jpg]]

* After this the emade_amazon_out file which records outputs should show have a block about this (Check this for any errors). There were no errors when I ran this.
* The NNLearners are added to the databases, and then we can call launch_EMADE.pbs which has the line that calls emade (python src/GPFramework/launchEMADE.py .. input.xml)
* After this the out file had the line waiting for database connection, and afterward, the updates were added to master and worker file
* I was able to get a seeded run of emade, but this didn't pass the first generation as my mysql database job that was Running automatically stopped after a time threshold
* I later realized I have to change the wall time to get more time for the MySQL database and longer emade runs. 
* I plan to get a longer emade run next week, after making these changes and get some NNLearners to see what might cause them to fail



==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Pull changes from Git
|Completed
|April 19, 2021
|April 26, 2021
|April 19, 2021
|-
|Understand the changes Cameron added and how seeding works
|Completed
|April 12, 2021
|April 19, 2021
|April 20, 2021
|-
|Update the seeding pbs file with my credentials, to make it work
|Completed
|April 12, 2021
|April 19, 2021
|April 22, 2021
|-
|Run a seeded run of emade
|Completed
|April 12, 2021
|April 19, 2021
|April 22, 2021
|-
|Get the shorter version of the dataset from Git
|Completed
|April 12, 2021
|April 19, 2021
|April 25, 2021
|-
|}

== April 12, 2021 ==

==== Subteam Meeting Notes ====
* Choose a task (Current problems that need working on) - Below are the tasks and descriptions provided by Cameron
** Pretrained embedding layers - (Editing input size to vocab size causes a problem) - JSON error
** Evolution
*** NNLearners are not able to get very complex and big without breaking
*** They are too failure prone, and we need to find out why this is happening
*** Possible ideas:-
**** Datatypes aren't always specific enough (Primitives can specify if an arg is an int but can't get sign)
**** NNLearners can lack integram parts (Lack output layers in some instances)
** NNLearners as Subtree
*** Learners are able to take another learner's output as feature vector (We can try and make NNLearners do this as subtrees)
*** NNLearners are always the root that take in ARG0, Output layer and other parameters
** PACE-ICE
*** Merge PACE functionality into cacheV2
*** Merge PACE functionality & DB fix into nn-vip
* I chose to work with the evolution problem inorder to understand how NNLearners work and how to solve the issue of trivial solutions

==== Individual Notes on Cameron's lecture about nn-learners ====
* During the Friday meeting a lecture about nn-learners-methods.py was given
** This is the most integral part of the subteam and understanding it is crucial to working with NNLearners
** We use tensorflow.Keras to build neural networks
** nnm.py (neural-networks-methods.py) stored functions for:-
*** Layer primitives, loss functions, classes for optimizer/activation terminals etc. 
*** nnm.LayerList class:- Basically a python list wrapped in its own class
** NNLearner:
*** Deap expects a specific datatype for primitives parameters
*** We use nnm.LayerList as that datatype - structure: ["input", other layers, ...., "output"]
*** NNLearner first makes a deep copy of the datapair (if data is text data, it tokenizes the feature data)
*** The neural network is they formed by recursively parsing the Layer list
*** Finally it chooses the appropriate loss function and compiles/fits the model

==== Notes on Running Emade with PACE ====
* I had to make a change in the LaunchEMADE.pbs file which is what we run to start EMADE
* The change was for the error "could not load dynamic library libcudnn.so.8"
* After adding this change however, I was not able to run EMADE, as PACE was not letting EMADE runs through 
* After we run qsub, and do qstat we should be able to see the EMADE job running with the status 'R'
* This was not the case I had the status 'Q' and had to ask in slack why this was
* The EMADE job was stuck in queue because probably because PACE doesn't have the hardware necessary to run EMADE

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Choose a task to do and update in the shared google doc
|Completed
|April 12, 2021
|April 19, 2021
|April 13, 2021
|-
|Attend Friday's team meeting and take notes on nn-learner-methods.py lecture
|Completed
|April 12, 2021
|April 19, 2021
|April 16, 2021
|-
|Update the launchEMADE.pbs file with the change to solve the GPU issue
|Completed
|April 12, 2021
|April 19, 2021
|April 16, 2021
|-
|Ask about EMADE being stuck in queue in Slack
|Completed
|April 12, 2021
|April 19, 2021
|April 16, 2021
|-
|Look at Keras tutorial videos to learn more about it
|Completed
|April 12, 2021
|April 19, 2021
|April 19, 2021
|-
|Look through the neural_networks_methods.py file and try to understand it
|Completed
|April 12, 2021
|April 19, 2021
|April 19, 2021
|-
|}

== April 5, 2021 ==
==== Lecture on Statistics Notes ====
* How do we prove we are getting meaningful results?
** We collect some metric in EMADE:-
*** Objective Scores
*** Processing times
*** Area under the curve
*** Number of pareto individuals 
** But given randomness how do we understand what changes we made and was caused by randomness
* Stats Review
** Mean - (Expected value) E[x] = Sum of values divided by the number of values 
** Variance - sigma^2 - E[x^2] - E[x]^2 
** Correct for bias using sampled mean --> s^2 = (n / (n - 1)) * sigma^2
** Standar deviation: sigma, s
* Hypothesis Testing:
** We want to compute the probabality of observing a sample given our assumption of an underlying truth --> p(sample | hypothesis)
*** Type I error (alpha) --> Probability we reject a null hypothesis when it is true (False Negative)
*** Type II error (beta) --> Probability of accepting a hypothesis when it is not true (False Positive)
** Student's t-Test
*** t-statistic = ((x - mu) / (s / sqrt(n)))
** One tailed or two tailed test?
*** One tailed test --> null hypothesis is "Is one mean greater than another?"; H0 = X(bar) > mu0; X(bar) is the sample mean
*** Two tailed test --> trying to show "distributions are not equal"; H0 = X(bar) = mu0;
*** Use a t-table (v = n - 1) where v is degrees of freedom
*** We can use software packages to get p-value from a t-statistic (Python - scipy)
** Welch's t-test:-
[[files/tstat.jpg|center|thumb|200x200px|Welch t-stats]]

==== Setting up EMADE on PACE ====
* Main course of action:- Clone the NN-VIP branch to get the EMADE directory needed and set up PACE
* A guide video was made by Cameron in the NLP team to get set up with PACE
** Clone the NN-VIP branch which has the NLP team EMADE code
** Delete all the other datasets we don't need and move the directory to PACE (after sshing into it - Using WinSCP)
** Create the CONDA environtment with all the appropriate requirements
** Setup the mysql database, and create a new database called amazon
** Use .PBS files to setup connection to database and run EMADE 

==== Notes on Neural Network from Friday Team meeting (4/9) ====
*  Supervised learning:-
** Feature data and labels - We are trying to match features to labels
** There is always some true function that maps all 'x' to a 'y'; Problem is we don't have all the data
** So we split data to a training set, validation set and testing set
** How do we know how good our prediction is (We have a loss function that does this)
*** Example of loss function - Root mean squared for regression problems 
* What is a Neural Network?
** Its a set of layers
** Layers take an input, and with each layer the model returns lesser outputs
* What is a layer?
** Generally consists of an operation and activation
** Operation - actual computation the layer does on the data it recieves
** Activation:- This part introduces non-linearity
*** Activation functions:- Sinusoid, tanh, ReLu (ReLu is the best one to use)
* Training a NN
** Feed forward - Evaluate the model by moving forward through the layers and getting the value
** Back propagation - Change the layers by working back form the evaluation to the layers (This is where they learn)
* After training it:
** Train/test/validation set
** Train - How the model learns
** Test - Data we withhold (don't train on)
** Validation - Testing during the training data (Helps check for problems during training such as: overfitting)
** If model is perofrming poorly:-
** Probably overfitting:- Can be fixed by regularization (adding a regularization parameter to punish the model for getting to complex)
** Underfitting is also possible (Where our model isn't complex enough)
* Layers:
** Convolutional Layer:
*** Relate data spatially
*** Eg: a picture - pixels near each other can be related in some sort of way
** LSTM 
*** A layer with lot of activations giving it the ability to remember or forget
** Attention is all you need (Check out this paper for more details on transformers)
* What is NLP:
** Natural Language - Stuff we speak
*** Includes tasks like machine translation
*** We are doing sentiment analysis classification into Postive (truth label 2) or Negative (Truth label 1)
** Data we are working with is exactly half which is useful 

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Attend Monday lecture and update notes in Notebook on Dr Zutty's Stats lecture
|Completed
|April 5, 2021
|April 12, 2021
|April 12, 2021
|-
|Clone the nn-vip branch of emade 
|Completed
|April 5, 2021
|April 6, 2021
|April 12, 2021
|-
|Watch Cameron's guide to get PACE set up and follow it to set up PACE
|Completed
|April 5, 2021
|April 7, 2021
|April 12, 2021
|-
|Attend team meeting on Friday and take notes on the Neural Network presentation
|Completed
|April 5, 2021
|April 12, 2021
|April 12, 2021
|-
|}
  
== March 29, 2021 == 
==== Note - Wasn't able to keep up with weekly updates because of VIP being updates ====
==== General ====
* Assigned to NLP team! 
* Meeting times for the NLP team is on Fridays at 4:00, but changed due to conflicts among some members to Friday at 6:00
* Breakout meetings, started by introducing ourselves and learning more about what the NLP team plans to achieve 
* Got recommended a few good resources to brush on ML, Neural Network topics
* Working on setting up EMADE
==== First NLP team Meeting notes ====
* a presentation on EMADE - presentation notes:-
** Abstract Data Types in EMADE
*** EmadeDataPair:
**** Contains the train/test data passed to individuals during the evaluation (ARG0)
**** Contains information about the data (Multilabel or regression)
**** Data is stored as feature data and target data
**** Test data is used to evaluate individuals (Target - predictions made by individuals) ; (Test - Actual labels for testing)
*** Important EmadeDataPair methods that are good to know:-
**** data_pair.get_train_data() - (Return) Training Object - (Return Type) EmadeData
**** training_object.get_numpy() - Feature Data - Numpy Array
**** data_pair.get_datatype() - Type of feature data - String
**** training_object.get_target() - Target Data - Numpy array
**** data_pair.get_truth_data() - Target Data of test data - Numpy array
** Running EMADE
*** Running it:- Starting command - "python launchEMADE.py input.xml"
*** Parses XML file to get information regarding database, datasets, evaluation metrics
*** Master - Instantiates database and initial status - handles mating and mutation - handles queue of individuals evaluated by worker
*** Worker - Connects to database and retrives individual for evaluation
*** Important Files to check out and learn:- data.py; EMADE.py; emade_operators.py; etc.

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Inform Dr. Zutty via Slack that the notebook is inaccessible for notebook notes
|Completed
|March 29, 2021
|April 5, 2021
|April 2, 2021
|-
|Review some concepts regarding ML and Neural Networks using resources
|Completed
|March 29, 2021
|April 5, 2021
|April 2, 2021
|-
|Attend team meeting on Friday and take notes on the presentation
|Completed
|March 29, 2021
|April 5, 2021
|April 2, 2021
|-
|}

== March 22, 2021 ==
* Mid semester presentations with bootcamp students and returning students
* The lecture time was on Monday - End Of Bootcamp
==== End Of Bootcamp - Updating Dates changed to Monday ====
==== Lecture Notes ====
* Notes on the returning teams’ presentations:- 
* Stocks:-
** Objectives: 
*** How to use TI’s to predict specific stock trends and generate buy/sell signals?
*** How can we use EMADE to optimize market existing trading-algorithms
** Methods:
*** Finding papers on the topic
*** Learning about technical indicators
*** Recreate trading algorithm using those papers
*** Using EMADE to optimize trading algorithms
** Research Step:
*** Last semester’s paper was inconsisten and found a new paper (from the citations of the old one)
*** Experimenting with new way to generate trading models and finding a model in EMADE to outperform the paper’s method
** Implementation:
*** Run Emade with regression and stream data
*** Writing Technical Indicator Functions as STREAM_TO_FEATURES primitives for data preprocessing
** Data: 
*** Stocks used by paper:
**** US: AAPL, BA, VZ
**** Taiwan: AUO, UMC, EPISTAR
*** Data Source: 
**** AlphaVantage
*** TI’s used in paper
**** Simple Moving Avg, Bias, Relative strength index, etc.
** Technical indicators and implemented, trends are converted to trading signals representing buy/sell points
** The best non seeded individuals had +94.8% profit for one particular Stock (AUO)
** Work for First Sem students:
*** Creating their own TI’s (Technical indicator) primitives for EMADE
*** Research TI’s from papers 
*** Assist with EMADE runs by joining as workers
*** Data analysis of EMADE results
* EZGCP:
** Ez Genetic Cartesian Programming 
** This is different from the other groups as its not using emade, but aims to provide a different way to do the thing that emade does.
** Emade uses a tree structure, and they plan to use a cartesian method. 
** They are using block structures in the cartesian programming, and each block has a unique primitive (no mixing)
** Ex of Block structure:
*** CIFAR-10 Dataset -> Data Augmentation -> Data Preprocessing -> Transfer Learning -> Neural Network -> Classification
** The objective is to recreate similar results on CIFAR-10 without relying on Transfer learning
** Also trying to explore new primitives (Looked at Transformers and RNN) 
** Conclusion:
*** RNN’s are highly resource intensive (GPU memory)
*** Pretrained and Seeded models likely redundant with Transfer Learning ( What was being tried to avoid)
** Experiments
*** Motivation: Are the current training parameters per individual sufficient and effective?
*** Actions Taken: The group recreated known ‘fit’ architectures within a stripped down ezGCP environment and benchmark training
*** Analyze convergence to decide ideal number of epochs, batch size, and learning rate
** Analysis and Conclusion:
*** The ‘fit’ individuals reached a relatively high accuracy, so the parameter are likely sufficient for good initial architectures
** Experiment (CIFAR-10)
*** Augmentation, Preprocessing, Conv2D Blocks
*** Same hyperparameters are tested
*** After 7 generations and 8 hours of running
**** Accuray:
***** Training: 97.2%
***** Validation: 47.8%
** Experiment (After removing augmentation and Preprocessing with the same hyper parameters)
*** Accuray:
**** Training: 93.7%
**** Validation: 56.3%
*** Conclusion
**** Runs without augmentation and preprocessing ran much faster and had better performance
** New Student Work
*** Skills:
**** Comfortable reading python code and documentation
**** Savvy with git + linux
**** A desire to learn and read research papers
**** Developing neural networks with TensorFlow or Keras
* Modularity:
** Exploring ways to abstract parts of individuals. 
** Creating building blocks to help the GP process and could lead to more code reuse.
** EMADE already uses machine learning models as a type of abstraction.
** ARL:
*** ARL – Adaptive Representation Through Learning
*** A way to introduce modularity and reusability
** Main Advantage of ARL:
*** Improving EMADE’s search and optimization
**** Altruistic Approach
**** Population could converge faster
**** Prevent destructive changes (mutation_
*** Does a better job of explaining my certain solutions perform better
** Previous ARL implementation 
*** It affects the genetic process
*** Search the population for combo of parent and children nodes that return an EmadeDataPair (Full tree and depth = 1)
*** Select some combinations based on frequency
*** Individuals get broken into potential ARL candidates
** Current Work
*** Documentation with Sphinx
*** Adding Complexity to ARL (tree with more depth; allowing for partial trees instead of just full)
*** Database storage:
**** Genetic Duplicates
**** Random individuals
**** Tree based storage
** ARL Complexity: 
*** Method overview:
**** find_afds()
***** Basically the main method that calls all the other functions 
***** Purpose is to accumulate information
**** search_individual()
***** Takes in an ARL indicate and traverses through nodes in list order
**** add_all_subtrees()
***** Find all ARL candidates
***** Generates a dict
****** Key – subtree representation
****** Value – A tuple where the first element is the subtree’s “goodness”
****** Dictonary passed to find_adfs()
** Database Structure:
*** ARL’s are inherently a convergent technique 
*** Goal is to prevent too much genetic material being lost
*** Random Individual:
**** EMADE creates random individuals 
**** Creating a set of new random individuals every generation (Same way the initial population of emade is created)
**** ARL’s can also appear in the new individuals
*** Tree Database:
**** Our current lambda functionality for ARL’s is limited
**** Instead of Lambda’s the group is looking to store the ARL as a tree (original form)
**** Allows for more complex operations between ARL’s
** Want to merge the ARL complexity functions and Database changes
** Questions to answer:
*** Relationship between ARL depth and individual performance (Does ARL depth matter, if so how)
*** Time complexity cost (Is it worth it)
*** Does adding genetic material allow for similar exploration compared to baseline runs?
** Recruiting:
*** Lectures and papers 
*** Running and designing experiments
*** Check out the wiki page
*** Meet time – Sunday 2-3 PM
* NLP
** Focussing on Trivial solutions and specifically NLP
** NNLearner Refresher
*** Works just like EMADE
*** Different layers are primitives within EMADE’s tree structure
*** NNLearner takes in a LayerList and fits a Keras Functions model on it
** PACE-ICE
*** We need a place for standardized runs (Shared environment)
*** Free GPU
*** Easier for the first-time setup
*** Cons – Database issues, and long runs (High learning curve)
** Environment Errors:
*** No up-to-data .yml file (fixed)
*** GPFramework installation (reinstall.sh didn’t work)
*** Plan to create a shared environment on PACE
*** MySQL issues – servers won’t start, malformed credentials, Access Denied and Server can’t connect
*** Discrepancies between Branches
** Errors in Misc Files 
*** Seeding files were deprecated or had minor errors
*** Lingering bungs in new mutation functions 
** NLP primitives Documentation
*** Started Notion document that details different types of NLP algorithms, why they are used, and their implementation code
** New Dataset:
*** Getting a new dataset up and running in EMADE
*** Using an Amazon data (text files)
*** We preprocess the text file into train and test lists
*** Convert the lists to CSV (supported by EMADE)
*** CSV files are then zipped (.gz)
*** Now text data can be used in EMADE
** Amazon Product reviews dataset:
*** Binary Classification 
**** Positive vs Negative 
*** Balanced dataset
**** Similar number of positive and negative examples
*** 3 million primarily English textual examples:
**** Labelled with 1 for negative and 2 for positive
*** Dataset Split
**** 90% train
**** 10% test
** Benchmarking LSTM Model
*** First model used had 93.7% accuracy on Kaggle 
*** But couldn’t replicate it due to Colab errors (Runtime errors)
*** This model can’t be replicated in EMADE because EMADE uses a tree structure
** Benchmarking model – FastText Model
*** Model that utilizes FastText (sub-word) embeddings
*** 91.73% Test Accuracy
** Non-Seeded run
*** 67 Generations
*** Bad run, but
**** Highlight the need to seed NNLearner as non-NNLearners fail by design
*** A run seeded with NNLearners gave the same results (Most NNLearners are failing in the same way)
** Why do individuals fail?
*** Errors with NNLearners are easy to spot but what about chronic problems with other primitives.
*** Primitives that are failing:
**** Want to add a functionality that looks for common error messages 
**** Goal is to use this as a tool to find & debug primitives faster
** Future Work:
*** PyTorch – High compatibility with SOTA, and Deep(er) learning functionality
*** Obstacles – Adding function is viable but will limit evolution
*** Solution: Implement at population level
** Goals
*** Get completed runs with non failing individuals on Amazon Dataset
*** Figure out why evolved individuals are trivial solutions if this problem still exists with the new dataset
*** Improving quality of runs by identifying primitives that almost always fail
*** Have a minimum viable implementation for population-level PyTorch runs
** New Members:
*** Meet at 4:00pm on Fridays
*** Natural Language Process – Neural Networks 
**** Experience with Keras and Tensorflow
**** Deep learning and neural architectures
*** Get setup on PACE and experiment

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update Notebook with lecture notes of the presentations (Delayed because of VIP website was down)
|Completed
|March 22, 2021
|March 29, 2021
|April 2, 2021
|-
|Choose A team and complete the assignment for the team preference
|Completed
|March 22, 2021
|March 29, 2021
|March 28, 2021
|-
|}

== March 17, 2021 ==

==== Lecture Meeting ====
* The lecture similar to last time was about asking questions and sorting out any issues our team was having with EMADE

==== Individual Notes and Subteam Meeting Notes ====
* During Our first subteam meeting (March 17th), 2 people out of our team were still not able to connect MySQL server
* However, before our second subteam meeting which was on March 20th both the team members were able to connect to my MySQL error. The problems were connecting to the wrong GT VPN (GTRI VPN instead of GT VPN) and fixing windows firewall rules
* On our second subteam meeting (March 20th), We looked at the data we got from EMADE, and tried to change the titanic splitter script to use our preprocessing
* We made a few functions to do the preprocessing and added that section in the titanic splitter file. The preprocessing part is attached below: 
[[files/Titanic splitter.png|center|thumb|700x700px|Titanic_splitter with our Preprocessing]]
* We understood that the titanic_splitter.py file had to be run separately to get the new zipped .csv files for the Monte Carlo trials
* We got our new zipped train data and test data to run EMADE with. 
* The next subteam meeting (March 21, 2021) was used to solve the next issue, which was to convert FP and FN to FPR's and FNR's. We did this by understanding how the input xml file works to get the evaluation function. We then found out the evalFunctions.py file which contains the functions that actually compute False Positives and False Negatives. 
* The code snippet below shows code changes made to compute the FPR and FNR values:
[[files/FPR and FNR values.png|center|thumb|699x699px|FPR and FNR values computed in evalFunctions.py ]]
* After finalizing this design, we ran EMADE for 25 generations and graphed the Pareto front for our presentation on March 22nd

* Our final problem that needed to be solved was the Pareto front. The Pareto front table from MySQL workbench was graphed for FPR and FNR values. But there were values there that weren't co-dominant (Pareto optimal). This is probably because the Pareto front took into account all three objectives (FPR, FNR, and NumElements)
* We had to remove the points that were not Pareto optimal as we were just interested in graphing FPR and FNR.
* Graphing the Pareto optimal also had a issue with the draw-style parameter (We had to sort the FPR and FNR values using argsort function). The code for graphing the Pareto front is as follows:
[[files/Pareto Front Code.png|center|thumb|507x507px|Pareto Front Code]]
* The below image shows the graph of the Pareto front, from the previous code snippet:
[[files/FPR and FNR Pareto graph .png|center|thumb|338x338px|FPR and FNR Pareto Graph]]
* We finally worked on our presentation using google presentation and split who will present which slides, and were ready to present on Monday March 22 - link - https://docs.google.com/presentation/d/1m9tqaUuRX6-PLot7Zp0gStI6mI8W8VDDnye8ikWMcFY/edit#slide=id.gc91935a7df_6_4

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Attend the first Subteam meeting (March 17th)
|Completed
|March 17, 2021
|March 22, 2021
|March 17, 2021
|-
|Make sure all group members can connect to the MySQL server
|Completed 
|March 17, 2021
|March 22, 2021
|March 19, 2021
|-
|Attend the second Subteam meeting (March 20th)
|Completed
|March 17, 2021
|March 22, 2021
|March 20, 2021
|-
|Understand the input xml file and change the titanic splitter file to implement our preprocessing
|Completed
|March 17, 2021
|March 22, 2021
|March 20, 2021
|-
|Attend the third Subteam meeting (March 21th)
|Completed 
|March 17, 2021
|March 22, 2021
|March 21, 2021
|-
|Understand how the input xml evaluates the FPR and FNR and change the evalFunctions.py file to get FPR and FNR values
|Completed
|March 17, 2021
|March 22, 2021
|March 21, 2021
|-
|Try graphing the Pareto Front data and compare AUC values
|Completed 
|March 17, 2021
|March 22, 2021
|March 21, 2021
|-
|Compile all the data and add it to the presentation
|Completed
|March 17, 2021
|March 22, 2021
|March 21, 2021
|}

== March 10, 2021 ==

==== Lecture Meeting ====
* The lecture was a place to ask questions about EMADE connections and discuss common errors that groups had. The meeting was mainly structured like a Q n A, and had time to work with the group to get the EMADE connection up and running and work on the Titanic Project 
* I had a personal question regarding a specific error in my Worker.err file when running EMADE. EMADE went through the first iteration, but couldn't get past that point and was stuck at 508 individuals. The worker.err file had an error pertaining to the multiprocessing.py module ("Group arguments must be none for now"). 
* I asked this question during the lecture meeting, and learnt that Python3.8.5 which was the version I got with my Anaconda was not the correct version to run multiprocessing (it needed version 3.7 or older)

==== Individual Notes ====
* Me and my group were having issues on connecting to my MySQL server. I tried installing MYSQL workbench on my personal computer and tried to make a connection to laptop (Both the devices were on different WIFI networks and connected to the Gatech VPN). This worked successfully. 
* I had also reached out to Dr. Zutty on Slack the day before for any advice on the matter. And he was also able to make a successful connection (The major changes I made from when I was trying previously with my team was, adding an inbound rule in my windows firewall to allow connections via port 3306. Later a team member also tried to connect to my MySQL server using EMADE, and I was able to see him in my "client connections" tab
* Reinstalling EMADE, with Python 3.7. I had to look up how to manage multiple pythons and learned about maintaining a conda environment. I created a conda environment with python version 3.7.9. As this seemed to be the version that ran without any errors on my system
* I installed all the libraries within this environment and changed the path in my input.xml file
* I also made sure I was in the environment when I called the python command that ran EMADE
* I was able to see the master.out and worker.out logs now, and could see changes in my table as well. Individuals were being evaluated, and everything looked similar to Dr. Zutty's output when he was running EMADE as a demonstration on the March 10th lecture
* I was able to run emade locally and see good results and ensure that connection to my MySQL server was possible. 
* After this, I took a few individual calls with my group members to make sure my team members could connect to my MYSQL server. 

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Ask about the worker.err file error (group arguments must be none for now) and clarify how to fix the issue
|Completed
|March 10, 2021
|March 17, 2021
|March 10, 2021
|-
|Seek help from Dr. Zutty via slack regarding MySQL connection issues
|Completed 
|March 10, 2021
|March 17, 2021
|March 10, 2021
|-
|Implement the fix suggested for running EMADE correctly (Get a new environment and reinstall EMADE)
|Completed
|March 10, 2021
|March 17, 2021
|March 11, 2021
|-
|Get on individual calls to help out available team members to set up EMADE or get the remote connection running
|Completed
|March 10, 2021
|March 17, 2021
|March 12, 2021
|}

== March 3, 2021 ==

==== Lecture Meeting ====
* Setting up EMADE and starting a run of EMADE
* Running EMADE:- Navigate to the top level directory (where EMADE is cloned) and run the following command:-
** python src/GPFramework/launchGTMOEP.py templates/input_titanic.xml
** The first part of the command has the .py file which takes in a input file to configure a run, the second half (templates and onwards) has the input file in the form of an XML. 
* The input file has various blocks and each contain essential information about the EMADE run 
** Python block
*** The python block gives the path where EMADE can find python. This is especially useful if the machine has various pythons installed and we need to specify a particular environment
** Database configuration
*** This block contains information to get the MySQL connection - (server hostname, SQL username, SQL password, and database name)
*** The hostname can be the localhost if we are trying to connect to our local server
*** The user and password are created in the MySQL workbench and must have all privileges in order for this to work
** Datasets
*** This block contains information about the data set we are planning to use
*** In the titanic case, we have cross folded the data 5 times creating 5 Monte Carlo trials (5 train and test pairs)
** Objectives
*** This block has information pertaining to the objectives 
*** The names of the objectives are used as columns in the database
*** Weight specifies if the objectives is minimized or maximized (Same as genetic programming and genetic algorithms) -1.0 is minimized and 1.0 is maximized
*** evaluation function is the name of the function in the evalFunctions.py file
** Additional Parameters 
*** There are parameters after this that specify the type of mutation and mating to carry and the probability at which to apply these
*** The selection function used selNSGA2
* Finally, the output can be seen by running the SQL commands (select * from individuals) 
** This command shows all the individuals, that are put there by the EMADE process

==== Individual notes ====
* Installing EMADE:- 
** Go to the GIT repository and follow directions from the readme file
** Get Git, and clone the repository (This is a long process, the EMADE file is long and needs Git LFS initialized before cloning)
** Install all the dependencies (Anaconda3) - While doing this some libraries such as tensorflow and keras were causing issues when I tried using conda, using pip install instead solves the problem 
** Add anaconda3 to the path, this will cause EMADE to use anaconda3's python where all the dependencies have been installed 
** Get MySQL server installed and setup, create a connection to localhost and create an empty schema titanic
** Create a user and grant that user all privileges in localhost 
** Update the input file (input_titanic.xml) in dbconfig - hostname = localhost, username - (username created in SQL), password - (password created in SQL)

* Run EMADE using the command above and tables should be created in titanic (4 tables were created - individuals, history, paretofront, and statistics) 
* The master process seems to run correctly, but the worker process has an error (group argument must be None for now) - This is possibly because in the didLaunch.py file we have processes=num_workers. the num_workers value is more than 0 in the input file and it expects more worker processes

==== Subteam meeting ====
* Got everyone up to speed regarding installing EMADE and once all the team members had EMADE installed and running on their respective localhost, we tried connecting to a common MySQL server 
* We all connected to the Gatech VPN, and my team mates tried connecting to my ipaddress 
* I created a user for a wildcard host and set a password for it, and my team members tried connecting to my ip_address using that user. The code used to create such a user is shown below
** CREATE USER 'username'@'%' IDENTIFIED BY 'password';  GRANT ALL PRIVILEGES ON * . * TO 'username'@'%';
** The percent sign here is the wildcard, meaning this user is applicable for any host.
** I tested this by making a connection in MySQL workbench to my own ip address using this user, and I was able to connect.
** But my teammates were not able to connect to my ip address. 
** We are currently looking for online resources, and seeking help from experienced members of the class to see if we are missing anything here 

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up EMADE and run on localhost
|Completed
|March 3, 2021
|March 10, 2021
|March 7, 2021
|-
|Subteam meeting 1 (setting up EMADE)
|Completed 
|March 7, 2021
|March 7, 2021
|March 7, 2021
|-
|Subteam meeting 2 (connecting to a common server)
|Completed
|March 8, 2021
|March 8, 2021
|March 8, 2021
|-
|Try a connection to common MySQL server with group members
|Completed
|March 3, 2021
|March 10, 2021
|March 10, 2021
|}

== February 24, 2021 ==
Presentation Week:- Compiled all our findings into a presentation and presented it

Link to our presentation:- https://docs.google.com/presentation/d/1pdSQMsz4_AnJ1usPGDI_O7rsLevXvKyY0OoqOffircU/edit?usp=sharing

Peer evaluations will be released on March 1'st and when they close, they can't be reopened. Complete them ASAP

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Presentation - Mid term presentation
|Completed
|February 17, 2021
|February 24, 2021
|February 24, 2021
|-
|Complete Midterm Peer evals 
|Completed 
|March 1, 2021
|March 5, 2021
|March 2, 2021
|}

== February 17, 2021 ==

==== Lecture Meeting ====
* Week's assignment:-
** Use genetic Programming to create a model (function) for the titanic problem 
** We did machine learning models for the titanic problem last week, this week we will use genetic programming to get a function with the same group
** Tips:-
*** Can use strongly typed genetic programming (GP) to boolean predict answers
*** Use strongly typed genetic programming to get floats and then convert to boolean
*** Use loosely typed genetic programming to get values and map them to boolean answers
** Presentation next week:-
*** how was data preprocessed and what machine learning models were used
*** How was the evolutionary algorithm designed
*** Compare MOGP and ML results

==== Subteam Meeting (Group 2) ====
* '''Meeting 1 (Start Evolutionary algorithm - evaluation, select, mutation, mating)'''
** We coded the GP section together after deciding to keep the preprocessing unchanged 
** 2 classes were created using creator (FitnessMin, and Individual) - FitnessMin has weights=(-1.0, -1.0) to suggest that there are two objectives and we are trying to minimize both
** We use the same X_train DataFrame (X_train was obtained after splitting the train DataFrame we obtained from train.csv file)
** We create a copy of the X_train and y_train and use X_train_copy (copied as a list) as the input, and y_train as the output list
** Pset:-
*** PrimitiveSet has arity of 7 (7 inputs after all the preprocessing)
*** Primitives used:- multiply, add, subtract, negative, sin, cos, tan
*** Arguments renamed to x1, x2, x3, x4, x5, x6, x7
** '''Evaluation''' function:-
*** We compile the individual (which is in tree form) and get a function 
*** The individual outputs a float right now, we run all input values from X_train_copy
*** Mapping these floats to boolean - Using a hyperbolic tan function to get the values between 0 and 1 (tanh(f(inputs))) 
*** Using a value to split --> 0.6, if value < 0.6 then the result = 0; else the result = 1
*** classify the results into tp, tn, fn, and fp values and compute '''fpr and fnr''' and return then
** Select Function:- We looked through a few and decided to go with selSPEA2 which uses the SPEA2 algorithm, this is better than the selTournament method. selTournament only works for single-objective problems. However, we are working on a multi-objective problem and SPEA2 algorithm works for that.
** Mating and Mutation function:- These were kept the same - Mating was done using cxOnePoint, and Mutation was done using mutUniform
** We created a population of 300 individuals and ran an evolutionary algorithm for 40 generations and a hof hall of fame) set to tools.ParetoFront()

* '''Meeting 2 (Pareto Optimal)'''
** We finalized the select, mate, and mutate function. We originally used selBest, but decided to use selSPEA2 later
** Discussed the possibility of using strongly typed genetic programming
** Evaluation function didn't have a way to map floats to booleans at the end of meeting 1, we added the '''tanh function to get 0's and 1's during meeting 2'''
** We also created a function to get a .csv file using the '''test''' data
** the function ran the test data through all the individuals in the hof (hall of fame) set 
** We also compiled all this information left to work on our presentation slides

* Link to my notebook including GP, and ML algorithms:- https://colab.research.google.com/drive/1ZROmuA294SDFHXMj2dMGaod-PecRBTxF?usp=sharing
[[files/Pareto Optimal.jpg|center|thumb|Pareto Optimal set using Genetic Programming ]]

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Subteam Meeting 1 - get a working evolutionary loop and basic GP structure
|Completed
|February 17, 2021
|February 24, 2021
|February 20, 2021
|-
|Subteam Meeting 2 - compile all the results and get it ready for presentation 
|Completed
|February 17, 2021
|February 24, 2021
|February 23, 2021
|-
|Finish presentation slides
|Completed
|February 17, 2021
|February 24, 2021
|February 23, 2021
|}

== February 10, 2021 ==

==== Lecture Meeting ====
* Assigned Groups for the Titanic Machine Learning challenge
* Problem challenge can be accessed in Kaggle:- https://www.kaggle.com/c/titanic/overview
* The goal of the problem:- Given parameters such as name, age, class, ticket number, fare, etc. predict who survived
* The groups come up with one algorithm for each group member and make sure that they are Pareto optimal based on FNR and FPR

==== Subteam Meeting (Group 2) ====
* '''Meeting 1 (Preprocess the data)'''
** Panda library
*** The panda library is used to interpret and manipulate data easily
*** Start by reading the .csv files into two '''data frames'''
*** Preprocessing:-
**** Name:- The name parameter shouldn't have a direct correlation to someone surviving, but all the names involve a title such as Mr, Mrs, Miss, Master, Dr, Lady, Countess, Rev, etc. These titles can be used as an important indicator as there is a relationship between age and a person's title. The idea of extracting the title is from the following Kaggle notebook:- https://www.kaggle.com/vinothan/titanic-model-with-90-accuracy 
**** Age:- The age variable was one of the parameters that had some null values (Nan values in excel), and we decided to use the median to fill these because the median is '''resistant to outliers.''' After realizing the correlation between title and age we got the median age of the particular title to fill a Nan age. Eg:- if the age of Mr. is missing, we use the median of ages of all the individuals with the Mr. title
**** Sex:- The sex variable could have been mapped to 0 and 1, but we chose to use the get_dummies function to change the sex to two boolean parameters is_male and is_female. We later drop the sex parameter. We use something called '''One-Hot encoding''' here to do this. This is used to encode categorical data to avoid mapping different values to numbers (as this could result in the model thinking value mapped to 2 is greater than the one mapped to 1). So we create two extra columns and make it a boolean value.
**** Embarked:- We tried to look for a correlation between Embarked and survival, but we agreed that there could be a problem with the number of 'S' values and causing the machine learning model to classify with a bias
**** Fare:- The test.csv file had some empty fare values and they were filled using the median of the corresponding pclass value
**** Cabin and PassengerID:- We dropped the cabin and passenger values 
**** In the test.csv file after the preprocessing, there was still one age value that was null, and we imputed that with the general media

* The code below is mostly similar for all of group 2's preprocessing:- the build_dict function is done for age and fare to build a dictionary with titles and pclass as the keys and their medians as the values. to fill missing ages we used the title's corresponding median, and for the fare, we used the pclass's corresponding median value.
[[files/Preprocessing Function.jpg|center|thumb|720x720px]]
* '''Meeting 2 (Finish preprocessing, splitting and, training model)'''
** The preprocessing of the data was done and there were no null data, all values were mapped to a number, and we were ready to start working on our machine learning models 
** We split the data using the following code:-
*** X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10) 
** We then looked at how machine learning models can be trained and tested using a simple example and split up individually to work on our machine learning model 

====  Individual Working (testing various machine learning models) ====
* After referring to some kaggle notebooks and the provided jupyter notebook I mainly considered the following the machine learning models
** K Neighbors
** Random Forest
** '''Passive Aggressive (Finally chose Passive Aggressive for Co-dominance)'''
* K nearest neighbors:- 
** I looked through the documentation to learn how k-nearest neighbors, and it seems to be a model finds a point's neighboring points and uses it to predict the label
** The algorithm seems to take a majority vote from these neighboring points
** The algorithm is defined as instance-based learning or non-generalizing learning according to sklearn's documentation. The model does not try to construct a generalized model, but stores instances of the training data.
** Parameters changed and observation:- 
*** n_neighbors:- The number of neighbors to algorithm takes into account - Increasing this value often led to better accuracy, but after a certain threshold it made the accuracy worse or just didn't make a difference
*** p and distance metric:- the distance metric were the values that made a noticeable change in the data, The p values can be 1 or 2 for Manhattan distance and Euclidean distance (Euclidean distance uses the distance formula (Pythagoras), and the Manhattan formula just adds the X and Y values instead of calculating the hypotenuse value)
[[files/K-nearest score metric.jpg|center|frameless]]
* Random Forest Classifier
** I worked with this the least and changed some of the parameters to see the effect on the result (Works by creating a lot of decision trees)
*** Criterion ("entropy"):- The criterion parameter had two modes:- "entropy" and "gini" the classifier defaults to gini, but I tried changing the value to entropy and got better results
*** random_state (5):- this is usually None, but I tried putting in integer values and got better results 
*** n_estimators (20):- This is the number of trees in the forest, changing this number did have some positive effect on the result
[[files/Random Forest Classifier.jpg|center|frameless|Score and (FPR, FNR) value for Random Forest]]
* Passive Aggressive 
** This is the machine learning model that I was able to use in-order to ensure co-dominance with my team
** This model had a good score, but there was a heavy trade-off between. I got a really low FPR value at the cost of a high FNR value
** This model is traditionally used with large amounts of data as it takes input data sequentially and the model is updated step-by-step. 
** Parameters:-
*** C(0.5):- I set the value of C to 0.5 - This value is called the regularization parameter and is used to make changes to a model when an incorrect prediction is made
*** random_state(3):- I set random_state to 3:- This is used to shuffle the data
*** The changes made to the parameters were done to test out different scenarios, and these values gave decent results and were co-dominant with my team
[[files/Passive Aggressive - metric.jpg|center|frameless|Score and (FPR, FNR) value for Passive Aggressive]]

====  Pareto Optimal Algorithms ====
* All the algorithms were graphed using FPR, and FNR (False Positive and False Negative).
* The Pareto optimality can be seen by the following link:- [[files/Pareto codominance.png]]
* The following link is my '''predictions.csv''' file:- https://drive.google.com/file/d/1-6gqtrxZqyDJA-hIlKPWlS8ikj2PLuwP/view?usp=sharing

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Lecture Notes - Titanic Machine learning
|Completed
|February 10, 2021
|February 17, 2021
|February 12, 2021
|-
|Run Titanic Lab on Jupyter Notebook and Understand it
|Completed
|February 10, 2021
|February 17, 2021
|February 12, 2021
|-
|Subteam Meeting 1
|Completed
|February 10, 2021
|February 17, 2021
|February 13, 2021
|-
|Subteam Meeting 2 
|Completed
|February 10, 2021
|February 17, 2021
|February 14, 2021
|-
|Finish Data Preprocessing 
|Completed
|February 10, 2021
|February 17, 2021
|February 14, 2021
|-
|Try different Machine Learning models
|Completed
|February 10, 2021
|February 17, 2021
|February 15, 2021
|-
|Achieve Co-dominance
|Completed
|February 10, 2021
|February 17, 2021
|February 16, 2021
|}

== February 3, 2021 ==

==== '''Self Evaluation''' ====
[[files/Self Graded Rubric.jpg|center|thumb|604x604px]]Link:- https://drive.google.com/file/d/11CdEV8gnKscFthL_4g0uLzAAYBpRvYP2/view?usp=sharing

==== Lecture Meeting Notes ====
* Multi-Objective genetic algorithms and genetic programming
** What do algorithms look for in their mate? 
*** It depends on what the algorithms want to improve on, and what tradeoffs can be made. Eg:- enery consumption, efficiency, accuracy, etc. 
** Gene Pool:- 
*** A gene pool is a set of genomes that will be evaluated in the current generation
*** In the example of the OneMax Problem, the genome would be the list of values for every individual, for genetic programming its the tree function
*** The <u>Search Space</u> consists of the set of all possible genomes, for the case of automated algorithm design, the search space contains all the algorithms
** Evaluation Process
*** Takes the gene pool (set of genomes) and evaluates them to get an objective score
*** Objectives:- Set of measurements each individual is scored against
*** The <u>Object Space</u> consists of the set of objectives
*** The evaluating function simply maps individuals from a location on the search space to a location on the objective space
*** The switch from search space to objective space is also referred to as a genotypic description to phenotypic description
** Confusion Matrix
*** A data set contains two types of samples:- positive and negative 
*** The confusion matrix is built after the data is classified by the algorithm as positive or negative
*** There are four possibilities of classification
**** True Positive:- A sample that is positive is correctly predicted as positive
**** False Negative (Type 2 Error):- A sample that is positive is wrongly predicted as negative
**** False Positive (Type 1 Error):- A sample that is negative is wrongly predicted as positive
**** True Negative:- A sample that is negative is correctly predicted as negative  {| class="wikitable" ! !Predicted Positive !Predicted Negative |- !Actual Positive |True Positive (TP) |False Negative (FN) |- !Actual Negative |False Positive (FP) |True Negative (TN) |}
*** Maximization Measures
**** We try to test out various algorithms to see which renders better results
**** The results are compared by the rate of true positives and true negatives (for maximization measures)
**** Sensitivity:- This is defined as the True Positive rate - TPR = TP/P = TP/(TP + FN)
**** Specificity:- This is defined as the True Negative rate - TNR = TN/F = TN/(FP + TN)
*** Minimization Measures
**** In order to minimize we consider the failure rates (false negative and false positive rates)
**** False Positive Rate - FP / N = TN / (FP + TN)
**** False Negative Rate - FN / P = FN / (TP + FN)
*** Accuracy
**** The Accuracy of an algorithm is also calculated from the true positive and true negative rate (how many the algorithm go correct)
**** ACC = (TP + TN) / P = (TP + TN) / (TP + TN + FP + FN)
*** Precision (Positive Predicted Value)
**** Out of the ones that were positively predicted how many were correct:-
***** PPV = TP / (TP + FP)
*** False Discovery Rate:-
**** Out of the ones that were positive how many were wrong:-
***** FP / (TP + FP)
***** FDR  = 1 - PPV (opposite of positive predicted value)
*** Negative predicted value:-
**** Out of the ones that were negative how many were correctly predicted:-
***** NPV = TN / (TN +FN)
** Objective Space and Pareto Optimality
*** Individuals are plotted on the objective space using their objective scores
*** N objectives can be used to plot the objectives, but for simplicity, we assume two (plotted on x and y)
*** The X-axis has the first objective and Y-axis has the second objective. (Objective score can be computed by functions like mean squared rate, true-positive rate, false-positive rate, etc.)
*** Pareto Optimality:-
**** An individual is considered Pareto optimal if there is '''no other individual that outperforms it on all objectives'''
**** The set of Pareto individuals is known as the Pareto frontier
**** Nondominated Sorting Genetic Algorithm 2 (NSGA2):-
***** The population is separation in different ranks, based on its nondomination
***** The rank 0 is the set of individuals that non dominated by others in the graph (The Pareto frontier)
***** Rank 1 is the set of individuals that are non dominated after removing the rank 0 individuals from consideration (Second Pareto Frontier)
***** This process goes on creating multiple ranks and the individuals belonging to the lower rank can beat the individuals on a higher rank
**** Strength Pareto Evolutionary Algorithm 2 (SPEA2):-
***** Each individual in the objective space (graph) is given a "strength" S
****** The value of S for an individual A is determined by how many other individuals does A dominate
***** Each individual is given a "rank" R
****** The value R for an individual A is determined by the sum of S's (strengths) of the individuals that dominate it

==== Lab 2 Multi-Objective Part Notes ====
* Genetic programming to get an individual with a particular function
* Goal:- We are trying to obtain a function (-x + sin(x^2) + tan(x^3) - (cos(x))^2) using genetic programming by minimizing two objectives, the mean squared error and the size of the tree
* Individuals and Objectives
** The fitness class:- creator.create("FitnessMin", base.Fitness, weights=(-1.0,-1.0))
** There are two inputs for weights in Fitness, as there are two objectives we need to minimize (tree size and mean squared error)
** The individuals are in functions represented in a tree form
* The primitive set (set of operators) contains the following functions:- add, multiplication, negative, sin, cos, tan, subtract
* The individual and population are created in an identical manner to the single objective example
* Evaluation
** The evaluation process is slightly different and returns two objective scores as opposed to one
** Each individual (function represented as a tree) is inputted to the evaluation function and the output is the mean-squared error (with the expected function) and the size of the tree
* Toolbox functions are registered:- mating is done by the cxOnePoint function in the gp module and mutations are done by the genFull (generates a full tree and replaces a random point in the individual tree with the full tree) function
* Pareto Dominance
** The concept of individuals being Pareto optimal is used here as there are two objectives that need to be optimized 
** The function Pareto Dominance takes in two individuals and returns true if the first individual dominates (better at both objectives) the second one
* Visualizing Pareto Dominance
** We create a population of 300 individuals and set aside one individual 'A' for comparing objectives
** All the individuals are then compared to individual A to check if they dominate A, are dominated by A, or neither (done using Pareto Dominance function)
** The results are plotted and as follows:-
[[files/Objective Space highlighting Pareto dominance.png|center|thumb]]
* Visualizing Pareto Dominance (cont.)
** The blue dot shows the individual that was set aside to compare with other individuals (Individual A)
** The Red dots show the individuals that dominate A (blue point)
** The green dots show the individuals that are dominated by A 
** The black dots don't fall in either category

* Evolutionary process
** The evolution is done using a function in the algorithms module of gp, and not using the traditional evolutionary loop
** The algorithm used is the Mu + Lambda algorithm (adds two parameters, Mu is the number of individuals to select for the next generation and Lambda is the number of children to produce at each generation)
** The probability of mating is 0.5 and mutation is 0.2 (same as last time)
** The "hof" variable (hall of fame) consists of the best individuals
** The results of running this segment is as follows:-
 Best individual is: negative(cos(multiply(add(cos(sin(cos(sin(cos(tan(x)))))), cos(x)), tan(x))))
 with fitness: (0.2786133308027132, 15.0)
[[files/The average, minimum of the two objectives (tree size and mean squared error).png|center|thumb]]
* Observations regarding the results
** The best individual's function is:-  -cos(tan(x) * (cos(sin(cos(sin(cos(tan(x)))))) + cos(x))
** The desired function was:- -x + sin(x^2) + tan(x^3) - (cos(x))^2
** The mean squared error was optimized but the tradeoff can be seen in the tree size, the tree size minimum and average values started off low but ended up increasing, as the mean squared error was decreasing
* Pareto Front Graphing
** The hof and the last population can be graphed with the objectives as the x-axis and y-axis to visualize the Pareto front
** The below image shows the scatter plot with the mean squared value is the x-axis and the tree size is the y-axis
** The red dots are the individuals in the hall of fame (Pareto optimal individuals)
** The blue dots are the individuals that belong to the last population
** The Area under the curve of this front is computed to be 2.384 approx
** The lower the Area under the curve is the better the Pareto front is 
[[files/Pareto Front Visualization.png|center|thumb]]
 Area Under Curve: 2.3841416372199005

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Lecture Notes for Multi-Objective Genetic Programming
|Completed
|February 3, 2021
|February 10, 2021
|February 6, 2021
|-
|Run Lab 2 Multi-Objective part Jupyter Notebook and Understand
|Completed
|February 3, 2021
|February 10, 2021
|February 7, 2021
|-
|Record Lab 2 Multi-Objective part Observations and Results in Notebook
|Completed
|February 3, 2021
|February 10, 2021
|February 7, 2021
|-
|Self Evaluation (VIP Notebook self-grading rubric)
|Completed
|February 3, 2021
|February 10, 2021
|February 10, 2021
|}

== January 27, 2021 ==

==== Lecture Meeting Notes ====
* Genetic Programming
** Key differences between Genetic Algorithms and Genetic Programming
*** Previously the individual was commonly a list of data that was passed through an evaluating function to get an objective score for evolution
*** The individual is a function that takes data as input and gives out the output
** The program or the function is represented using a tree
*** The tree's nodes are called "primitives" and they represent functions
*** The tree's leaves are "terminals" that contain the parameters to the function
*** The leaves can be seen as the input to the function and the root gives the output
*** The node's children are its inputs
**** Example:- 3 * 5 function can be represented with the root being * and its children being 3 and 5
*** The tree is converted to a lisp preordered parse tree
** The Crossover is done by randomly picking a point in each tree and everything below that point (the subtree) is switched
** There are many ways of mutation:- Adding a node, deleting a node, changing a node
** Example problem:- Evaluate sin(x) using primitives (+, -, *, /)
*** Things required to achieve this was the Taylor series and using variables in the tree
*** Including more primitive functions like power and factorial would make this easier

* Example of a tree portraying a function and converted to a list preordered parse tree
[[files/Example of function represented in a tree.jpg|center|thumb]]
* This tree in a preordered lisp format would be:- [+, -, *, 2, 3, 4, /, 4, 2]

* The tree written as a function as shown in lab 2 code:- add(sub(mul(2, 3), 4), div(4, 2))

* The function represented here is:- (((2 * 3) - 4) + (4 / 2)) = 4

==== Lab 2 Notes - Genetic Programming ====
* The individuals in this lab are considered functions and we are trying to get to a particular function using genetic programming which is x^4 + x^3 + x^2 + x

* gp module:
** The gp module is the primary package used in this exercise in addition to other deap modules such as tools, base, creator which were used in Lab 1 
** The module has classes such as PrimitiveTree, Primitive, Terminal, PrimitiveSet, which are essential data types used for creating a function and representing and it in a tree form 
** There are many useful functions for mating and mutations for the tree in the module 
* Start by creating an individual and FitnessMin class using creator (similar to lab 1), the objective is the error between the function we want, and the function we have as the individual and therefore we will try to minimize that error 
* Create a PrimitiveSet after that, The primitive set holds all the primitives  which in this case would be functions like add, multiply, subtract, negate
** Two functions I added to this were square and absolute (The square function added a bit of variety to multiplication) 
* Creating Individuals and Population
** We register functions to create an individual by using a special function in the gp module, genHalfAndHalf, which takes a primitive set, minimum depth and maximum depth as parameters and outputs a tree. 
** We use the initIterate function to call genHalfAndHalf which is registered as "expr" to create the individuals (which are different functions using the primitive set) 
** Using the initRepeat function we create the population by repeatedly creating individuals 
* Evaluation
** We evaluate by using the "compile" function in gp to compile the tree into a function (the individual is in tree form) 
** We call that function for a range of different values and compare their values with the function we want (x^4 + x^3 + x^2 + x) 
** We are trying to minimize the error of the function 
* The evolutionary functions
** registered functions using toolbox
*** evaluate function:- The evaluate function uses 1000 trial values from -1 to 1 and compares the result of the two functions (individual and target) 
*** select:- uses the selTournament with a size of 3 (similar to lab 1) 
*** mate:- uses the cxOnePoint function provided in the gp module - Does a mate between two children chooses a random point and exchanges subtrees 
*** expr_mut:- uses the genFull function provided in the gp module - Creates an expression where every leaf has the same depth (a full tree) 
*** mutate:- uses the mutUniform function provided in the gp module - Randomly chooses a point in the tree and replace subtree with the an expression (The expression is generated from the expr_mut function which generates a function in the form of a full tree) 
*** extra_mutate:- An extra mutation function I added which uses the nutNodeReplacement in the gp module - Choses a random primitive and replaces it with another with the same number of a arguments 
* Evolutionary Loop
** The evolutionary loop is nearly identical to Lab 1, the mates are done with a 0.5 probability and the mutations are added with a 0.2 probability 
** After running the loop the final generation gave the following individual:- 
-- End of (successful) evolution --
 Best individual is add(multiply(x, add(x, multiply(x, x))), add(absolute(square(multiply(x, x))), x)), (1.120518273795614e-16,)
* The function:- The function that was chosen is:- (x (x + (x * x))) + (|(x * x)^2| + x)
* After simplifying this we get:- x^2 + x^3 + x^4 + x
* This is identical to the function we desired
The plot below is the result of another evolution and records the minimum, maximum, and average values of the objective scores throughout the generations

[[files/Lab2 Statistics for Evolutionary Process.jpg|center|thumb]]
* After adding the mutation I added:-
** The mutation I added was:-
*** toolbox.register("extra_mutate", gp.mutNodeReplacement, pset=pset)
** The node replacement mutation randomly chooses a primitive node and replaces it with another primitive with the same number of arguments 
** Results:-
 Best individual is add(add(add(square(square(x)), x), multiply(x, square(x))), square(x)), (1.0918390848355592e-16,)
* The function above can be written as:- x^4 + x + x^3 + x^2
[[files/Statistical Data for Evolution with Node Replacement Mutation.jpg|center|thumb]]
* Observations:-
** The minimum objective score is reached rather quickly in these cases, mainly because the function is a simple one
** I ran the function a few times and almost halfway through the generation gets its minimized objective score (mean squared error) in the 10^-16 range
** Once this value is reached, it is safe to assume that there is an individual with the exact function we desire. 
** I also tested out adding primitives like division, and power, but they caused problems when the function ran
*** Division:- always poses the risk of division by 0
*** Power:- power can also have a similar problem if 0 and -1 are the 2 inputs, -> 0^-1, which would ultimately mean division by 0
** Ultimately most of the best individuals primarily used square, multiplication, and addition functions

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Lecture Notes for Genetic Programming
|Completed
|January 27, 2021
|February 3, 2021
|January 31, 2021
|-
|Run Lab 2 (Up to Multi-Objective) Notebook and Understand 
|Completed
|January 27, 2021
|February 3, 2021
|February 2, 2021
|-
|Record Lab 2 (Up to Multi-Objective) Observations and Results in Notebook
|Completed
|January 27, 2021
|February 3, 2021
|February 2, 2021
|}

== January 20, 2021 ==

==== Lecture Meeting Notes ====
* Introduction of Genetic Algorithms
** Core Concept:- Every generation is created by mating/mutation of '''individuals''' in the previous generation population, By evaluating their '''fitness''' every time eventually the best individual is produced
** Terminology:-
*** Individual - One specific candidate from the population, in our case it could be a particular solution to the problem
*** Population - A group of individuals
*** Objective - A value that characterizes individuals (eg:- Students take a test and Student A gets 80%, 805% is the objective value of that particular student)
*** Fitness - The relative comparison of objectives to other individuals (Eg:- If all students who took the test got a 100%, the fitness of all students would remain the same and this doesn't tell us who did better)
*** Evaluation - A function that takes individuals as an input and outputs an objective score which can be used later to obtain the fitness
** Selection - (akin to survival of the fittest) Two forms of selection strategies 
*** Fitness Proportionate: This method creates a wheel or a pie chart and the individual with the highest fitness gets the biggest chunk of the pie, and when the pie rotates the probability of the individual with high fitness being selected is greater
*** Tournament: Many tournaments are held among individuals and the individuals with higher fitness win and selected for mating
** Mate / Crossover
*** Single Point: Taking two individuals and splitting at one point, the first half of the first individual and the second half of the second individual make one offspring, and so on.
*** Double Point: Splitting at two points instead of one, but otherwise similar to single point mating
** Mutation
*** Making random modifications to parents to have diversity - eg (001001110 --> 001001111) - the mutation is called a bit flip

==== One Max Problem ====
* The problem starts with individuals that each has a list of 100 values either 0 or 1, the goal is the end up with an individual whose list has all 1's by evaluating fitness and mating 
* Deap tools:-
** The base module:-
*** the base module contains important classes such as Fitness and Toolbox - The Fitness class contains the value parameter which keeps track of the fitness value of any individual  
*** The toolbox contains the register method which is used to give functions different aliases to keep the evolutionary process consistent and simple
**** Eg:- The "individual" function is defined as repeatedly assigning random boolean values to the list and the "population" function repeatedly creates individuals.   
** The Creator module:-
*** This gives the capability to create object-oriented style classes with ease
**** Eg:- An example of this is   
** The Tools module:-
*** This consists of critical functions like initRepeat which was used to repeatedly create individuals to make up our population  
*** All the functions relating to mutation and mating also comes from this module such as cxTwoPoint, mutFlipBit, selTournament  
* The evolutionary loop runs 40 times creating a generation at each iteration  
* During each iteration, the offspring is created by using the selTournament function which selected the best individuals among  randomly chosen individuals  
* The offspring is cloned and mating occurs with a 50% chance and mutation occurs with a 20% chance   
* Resulting Final generation:-
** As the below scenario shows, the evolution process was successful as the individual has all 1's in the list 
-- Generation 39 --

Min 87.0 

Max 100.0

Avg 98.12666666666667

Std 2.024176101254541

-- End of (successful) evolution --

Best individual is [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], (100.0,)

==== The N Queens Problem ====
* Problem and Goal:- The goal is to determine a configuration of 'n' queens on an nxn chessboard such that no queen can take another
** Assumption:- Only one queen per row and column - So only need to check conflicts along the diagonal
* In this case, an individual is defined by one permutation - the list tells the position of the queen
** Eg:- [7, 0, 4, 6, 5, 1, 9, 2, 8, 3] - Meaning:- The first line has a queen on the 8th column (First value is 7 and zero indexing), the second line has a queen on the 1st column (Second value is 0), and so on
* The fitness, in this case, is the number of conflicts and we are trying to minimize this hence the parameter weights = (-1.0,)
* The evaluation function:- calculates the diagonal conflicts for each individual and that is the objective of each individual
* The cxTwoPoint and cxPartiallyMatched are written and the partially matched algorithm is used and '''registered''' as the mate function (Register:- done with the toolbox class's register function which simply gives a function an alias)
* The mutShuffleIndexes function is defined which shuffles the attributes of the individual with some probability denoted by the variable "indpb" and '''registered''' as the mutate function
* The evolutionary loop runs for 100 generations and still, there are times when the final generation does not reach 0
* Resulting best individual:-
-- Generation 99 --

Min 1.0

Max 11.0

Avg 1.9233333333333333

Std 2.0192710455893623

-- End of (successful) evolution --

Best individual is [2, 8, 11, 7, 0, 17, 19, 13, 3, 14, 4, 16, 9, 5, 12, 1, 15, 18, 6, 10], (1.0,)

==== Action Items ====
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Lecture Notes for Genetic Algorithm Slides
|Completed
|January 20, 2021
|January 27, 2021
|January 22, 2021
|-
|Install Jupiter Lab and Notebook 
|Completed 
|January 20, 2021
|January 27, 2021
|January 22, 2021
|-
|Open the Lab 1 notebook in Jupyter and complete OneMax Problem
|Completed
|January 20, 2021
|January 27, 2021
|January 24, 2021
|-
|Run and understand Queen Problem
|Completed
|January 20, 2021
|January 27, 2021
|January 24, 2021
|}