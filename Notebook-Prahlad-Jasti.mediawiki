== Team Members ==
Name: Prahlad Jasti

Major: Computer Science

Email: pjasti3@gatech.edu

Phone: 470-263-5595


Interests: Computer Vision, Computational Mathematics, Hiking, Airsoft
== April 26, 2021 ==

'''Scrum Notes:'''
* Finish up final presentations
* We will meet 2 days before final presentations to do a dry run

'''Subteam Notes:'''
* Talked with Cameron in order to determine cause of json error
** He suggested that we flatten the embedding layer, as there was a recent push to the nn branch which did this. (Final line of code)
 # Handle Keras applications
 elif isinstance(layer, dict):
   if layer['type'] == PretrainedModel.MOBILENET:
       s = data_pair.get_train_data().get_instances()[0].get_stream().get_data().shape
       new_layer = MobileNet(input_shape=s, include_top = False, weights = "imagenet")
       input_layers.append(new_layer.inputs)
       new_layer = new_layer.output
       curr_layer = new_layer
       if layer['type'] == PretrainedModel.INCEPTION:
           s = data_pair.get_train_data().get_instances()[0].get_stream().get_data().shape
           new_layer = InceptionV3(input_shape=s, include_top = False, weights = "imagenet")
           input_layers.append(new_layer.inputs)
           new_layer = new_layer.output
           curr_layer = new_layer
       if layer['type'] == PretrainedModel.VGG:
           s = data_pair.get_train_data().get_instances()[0].get_stream().get_data().shape
           new_layer = VGG16(input_shape=s, include_top = False, weights = "imagenet")
           input_layers.append(new_layer.inputs)
           new_layer = new_layer.output
         ->curr_layer = Flatten()(new_layer)


* This did not work, and it appeared that the concurrent.futures was more deep-rooted in EMADE than a json error
** As a result, I was told to stop working on this issue and focus on the presentation.
* I will talk about the basics of embedding layers, what the issue was, and our attempts to solve it. 

'''Individual Notes:'''
* Looking back, I should have chosen to help with getting NNLearners as a subtree or doing PACE runs in order to help generate nontrivial individuals.
** EMADE is not very helpful when helping to debug, as the stack trace is very generic.
** Would like to talk to some of the previous students in VIP who helped develop this branch.
* Would like to do more FPR/FNR runs. 
'''Action Items:'''
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Debug PretrainedEmbeddingLayer
|Incomplete
|4/19/2021
|4/29/2021
|
|-
|Finish and dry run presentation
|Complete
|4/19/2021
|4/30/2021
|4/29/2021
|}

== April 19, 2021 ==
'''Scrum Notes:'''
* Peer evals are released
* Start creating presentation for subteam final
'''Subteam Notes:'''
* Cameron pushed new update to nn-vip branch that would allow for more worker processes to run on EMADE, allowing an increase from 8 generations to 15 in 8 hours
** Cloned this new updated branch and shipped changes to PACE
* Analyzed potential solutions with Harris for json error.
* Cameron claimed that the error was occurring in the line json.loads(config)
** This line loads a json string for configurations, weights, and biases of a NN model in keras, which is stored as config. 
** Harris proposed alternatives to json.loads, such as json.stringify, in order to properly set input_dim. This did not work. 
 config = model.to_json()
 print("config: ",type(config))
 data = json.stringify(config)    #original: data = json.loads(config)
 for layer in data['config']['layers']:
 if layer['class_name']=='Embedding':
 if data_pair.get_datatype() == 'recdata':
     layer['config']['input_dim'] = numforembedding[i]
     i+=1
 elif data_pair.get_datatype() == 'textdata':
  layer['config']['input_dim'] = vocab_size


* I then put debug statements in the PretrainedEmbeddingLayer primitive method and the NNLearner method, and found that the standalone individual evaluator was never reaching them.
 def PretrainedEmbeddingLayer(data_pair, initializer, layerlist): 
    """Creates Embedding layer  
    Args:   
        empty_model: empty_model as terminal    
        data_pair: given dataset    
        out_dim: ouput dimension    
                layerlist: layerlist to append to 
    Returns:    
        Keras Embedding Layer   
    """ 
  ->print("Reached primitive")
    maxlen = MAXLEN 
    numwords=NUMWORDS   
    out_dim = abs(out_dim)  
    data_pair, vocab_size, tok  = tokenizer(data_pair, maxlen, numwords)
    out = {PretrainedEmbedding.GLOVE:100, PretrainedEmbedding.GLOVEFASTTEXT:501, PretrainedEmbedding.FASTTEXT:300, PretrainedEmbedding.GLOVETWITTER:200 }
    out_dim = out[initializer]   
    initializer = Constant(get_embedding_matrix(initializer, vocab_size, tok))
    layerlist.mylist.append(Embedding(vocab_size,out_dim , input_length=maxlen, embeddings_initializer=initializer))
    return layerlist

* This was the error that keeps occurring:
[[files/Concurrent Error.PNG]]

'''Individual Notes:'''
* Started doing EMADE runs on PACE
** Runs kept getting stuck at generation 1, even after seeding.
** Kept reaching a point where it would stop evaluating individuals, and the worker pool size was 0. 
** Generated 517 individuals, but the Pareto front had a very large AUC with many trivial individuals, so I scrapped the run. 
'''Action Items:'''
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Complete Peer Evals
|Completed
|4/19/2021
|4/27/2021
|4/27/2021
|-
|Debug PretrainedEmbeddingLayer
|Incomplete
|4/19/2021
|4/29/2021
|
|-
|Perform seeded run of EMADE
|Complete
|4/19/2021
|4/26/2021
|4/25/2021
|}

== April 12, 2021 ==
'''Scrum Notes:'''
* Met with team to discuss list of pending tasks
** Fix json error with Pretrained Embedding Layer primitive
** Make NNLearner a subtree in individuals
** Figure out why NNLearner individuals give trivial trees
** Merge PACE and MySQL errors into cache-v2 branch

'''Subteam Notes:'''
* Chose to work on json error with Harris
* The Pretrained Embedding Layer has an attribute called input_dim, which is the size of the vocabulary when learning word embeddings
* The json string for this layer is not being properly updated when setting input_dim to the vocab size
* Evaluated this individual in order to test fixes:
** NNLearner(ARG0,OutputLayer(DenseLayer(10, defaultActivation, 10, LSTMLayer(16, defaultActivation, 0, trueBool, trueBool,PretrainedEmbeddingLayer(ARG0,gloveWeights,InputLayer())))), 100, AdamOptimizer)
[[files/PEL0.png]]

'''Individual Notes:'''
* Fixed errors blocking me from starting EMADE
** MySQL connection refused [Errno 111]. Stagger the process starting mysql and the process starting EMADE.
** Output files not showing: I did absolutely nothing except rerunning EMADE a few days later, and it showed up. Still unable to reproduce error.
** Table 'history' already exists: Go to the amazon database with the query 
  USE DATABASE amazon
** Enter the query:
 DROP TABLE IF EXISTS history
* Installed remaining libraries that should have already been installed, e.g. transformers


'''Action Items:'''
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Setup PACE
|Completed
|3/29/2021
|4/5/2021
|4/13/2021
|-
|Perform EMADE runs
|Completed
|4/5/2021
|4/12/2021
|4/13/2021
|-
|Choose pending task
|Completed
|4/12/2021
|4/16/2021
|4/14/2021
|}

== April 5, 2021 ==
'''Scrum Notes:'''
* Statistics Lecture
* We need a way to make sense of randomness in EMADE metrics such as AUC, number of Pareto individuals, etc..
* Formulas:
** Mean (Expected value) = E[x] 
** Variance = sigma^2 = E[x^2] - E[x]^2
** Sampled mean = X_bar = s^2 = (n / (n - 1)) * sigma^2
** Standard deviation: sigma, s
* Hypothesis Testing
** Compute the probability of observing a sample given underlying truth
*** p(sample | hypothesis)
** Type I error (alpha): Probability we reject a null hypothesis when it is true (False Negative)
** Type II error (beta): Probability of accepting a hypothesis when it is not true (False Positive)
* Student's t-Test
** Testing hypothesis of small sample when standard deviation is unknown
** t-statistic = ((x - mu) / (s / sqrt(n)))
* Tails of t-test
** One tailed: alternate hypothesis is if one mean greater than another
*** H_0 = X_bar > mu_0
** Two tailed: alternate hypothesis is that distributions are not equal 
*** H_0 = X_bar != mu_0;

'''Subteam Notes (Neural Networks Lecture):'''
* Supervised Learning
** Assigning labels to training data, then determining labels for test data
* Unsupervised Learning
** No labels assigned, determine similarity within feature data instead
* Error function necessary to determine efficacy of current model
** e.g. RMSE (Root mean square error) between truth and prediction
* Neural Network (NN)
** Pipelined calculations of layers, given an input connecting to first layer, then next layer, etc..., all the way to output
* NN Layer
** Consists of operational calculation and activation function
*** Operation: Computation on input data
*** Activation: Normalizes the data using non-linearity
* NN Training
** Feedforward - Model evaluation by moving forward through the layers to get output
** Backpropagation - Set configurations (i.e. weights and biases) by reversing the feedforward calculation and determining how to maximize reduction in error (using gradient calculus)
* Data split:
** Training data: Feedforward with this data and backpropagate
** Validation data: Periodically feedforward with this data during training to ensure model isn't overfitting (Learning a data set too well, then performing poorly on test data)
** Test data: Assess what the model has learned with new data, only calculating error
** Typically split using 80/10/10 for train/test/validation
* NLP
** Training neural networks to understand written text
*** e.g. speech recognition, translation
** This VIP focuses on sentiment analysis, which determines whether opinionated text is either positive (truth label 0) or negative (truth label 1)
** Uses datasets with even splits of Amazon reviews
** NNs utilize word embeddings, which are vectorized representations of words, as models prefer numbers over words.
*** e.g. word2vec, GloVe, CBOW

'''PACE setup:'''
* Wiki is now fixed and we can follow guide on getting PACE set up
* Cameron put a video supplement to this guide on Youtube
** Clone nn-vip branch of EMADE and delete unnecessary datasets
** Use WinSCP or command line to move EMADE files to PACE
*** PACE server is pjasti3@pace-ice.pace.gatech.edu (Replace pjasti3 with gatech username)
** ssh into PACE server with GT credentials
** Create database folder in PACE and start MariaDB daemon. Add credentials and add new database amazon.
** Create conda environment and activate.
** Run pbsmysql.pbs and launchEMADE_amazon.pbs scripts, in order, to do a run of EMADE. 
* Issues:
** Output and error files were not showing up, indicating that EMADE never ran.
** Some libraries never installed, even though they were supposed to be in the conda environment.
** Table 'history' already existed whenever EMADE tried to populate database. 

'''Action Items:'''
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Setup PACE
|Incomplete
|3/29/2021
|4/5/2021
|
|-
|Perform EMADE runs
|Incomplete
|4/5/2021
|4/12/2021
|
|}

== March 29, 2021 ==
'''Scrum Notes:'''
* Assigned to NLP team
* Met the current members and learned about the team's goals about neural architecture search
'''Subteam Notes:'''
* EmadeDataPair:
** Datatype containing train/test data passed to individuals during the evaluation
*** Represented as ARG0
** Contains metadata (e.g. type of data, text, image, etc.)
* EmadeDataPair methods:
** data_pair.get_train_data() - (Return) Training Object - (Return Type) EmadeData
** training_object.get_numpy() - Feature Data - Numpy Array
** data_pair.get_datatype() - Type of feature data - String
** training_object.get_target() - Target Data - Numpy array
** data_pair.get_truth_data() - Target Data of test data - Numpy array
* Running EMADE
** python launchEMADE.py input.xml
*** Parses XML file to get information regarding database, datasets, evaluation metrics
** didLaunch.py
*** Runs after launchEMADE.py to load from XML and creates an EMADE object
** Master process
*** Instantiates database and initial status
*** Handles mating and mutation
*** handles queue of individuals evaluated by worker
** Worker process
*** Connects to database and retrieves individual for evaluation
* Task:
** Setup PACE computing environment to run EMADE on
** Cannot do this because VIP wiki is down and guide is on there.
'''Action Items:'''
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Resolved

|-
|Setup PACE
|Incomplete
|3/29/2021
|4/5/2021
|
|}

== March 22, 2021 ==
'''Bootcamp Notes:'''
* Presented our subteam's work on EMADE and compared results with ML and MOGP Pareto fronts
** [[Group 2|Subteam page with EMADE presentation]]
* Listened to presentations on topics we will work on after bootcamp: Stocks, NLP, ezCGP, Modularity
* Assignment: Rank subteam preferences on Canvas.

'''Action Items:'''
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Rank subteam preferences
|Completed
|3/24/2021
|3/28/2021
|3/26/2021
|}

== March 17, 2021 ==
'''Subteam Notes:'''
* Implemented preprocessing pipeline to feed into EMADE
** Used pipeline from first part of project where we imputed age and price fare for passengers in the data sets
** Additionally, this pipeline one-hot encoded gender of passenger, as ML models prefer raw numbers over parsing text
** Modified this code and pasted it into the data splitter file in the datasets folder.
* After dry run of worker processes, we noticed that we were getting the average number of false positives and false negatives over 5 folds of the Monte Carlo simulation, as opposed to their rates.
** We modified the evaluation functions, using the formulas FPR = (FP)/(FP + TN) and FNR = (FN)/(FN + TP), so that EMADE would display these rates in MySQL. 
 def false_positive(individual, test_data, truth_data, name=None):
    test_data = np.array([elem[0] for elem in test_data])
    truth_data = np.array(truth_data)
    if truth_data.shape != test_data.shape:
        return np.inf
    TN = np.sum(test_data[truth_data==0] == 0)
    FP = np.sum(test_data[truth_data==0] != 0)
    if FP + TN == 0:
        return 0
    return FP / (FP + TN)
    # return np.sum(test_data[truth_data==0] != 0) / len(test_data)
    # return np.sum(test_data[truth_data==0] != 0)
 def false_negative(individual, test_data, truth_data, name=None):
    # Put test data in the same form as truth data from buildClassifier
    test_data = np.array([elem[0] for elem in test_data])
    truth_data = np.array(truth_data)
    if truth_data.shape != test_data.shape:
        return np.inf
    # return np.sum(test_data[truth_data==1] != 1) / len(test_data)
    TP = np.sum(test_data[truth_data==1] == 1)
    FN = np.sum(test_data[truth_data==1] != 1)
    if FN + TP == 0:
        return 0
    return FN / (FN + TP)
    # return np.sum(test_data[truth_data==1] != 1)
* Used these evolutionary parameters for the mating, mutation, and selection processes
* Selection Algorithm: SELNSGA2
* Mutation Probabilities:
** Insert - 0.05
** Insert Modify - 0.10
** Ephemeral - 0.25
** Node Replace - 0.05
** Uniform - 0.05
** Shrink - 0.05
* Mating Probabilities
** Crossover - 0.50
** Crossover Ephemeral - 0.50
** Headless Chicken - 0.10
** Headless Chicken Ephemeral - 0.10
* Used 5 parallel evaluation processes, as I am using a PC. Use 2-3 if on laptop.
* Inputted all of these parameters in input_titanic.xml
* We let EMADE run for 25 generations, which was less than the ML and MOGP runs, because EMADE took too long to run each generation.
Ran this query in order to obtain FPR and FNR of each individual in the Pareto front:
 select * from individuals join paretofront on individuals.hash=paretofront.hash where paretofront.generation=(select max(generation) from paretofront);
The code paretofront.generation=(select max(generation) from paretofront); indicates the individuals on the Pareto front in the latest generation (max(generation))
* Pareto frontier:
[[files/Emade pareto.png|none|thumb|Pareto frontier of individuals generated by EMADE after 25 generations]]




'''Action Items:'''
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Modify EMADE pipeline
|Completed
|3/3/2021
|3/22/2021
|3/19/2021
|-
|Run EMADE with modified pipeline
|Completed
|3/3/2021
|3/22/2021
|3/20/2021
|}

== March 10, 2021 ==
'''Bootcamp Notes:'''
* No lecture, had opportunity to get help with EMADE
'''Subteam Notes:'''
* Assignment: Continued working on EMADE for Titanic problem
* Successfully made connection to remote database server for EMADE
** Connected to GT AnyConnect VPN
** Set Windows firewall inbound rule to allow port 3306 (MySQL default port)
** Set bind address in my.ini file of MySQL from 127.0.0.1 to 0.0.0.0 (Any IP address)
* Started a dry run of the worker process using this command 
** python src/GPFramework/launchGTMOEP.py templates/input_titanic.xml​ -w

'''Action Items:'''
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Connect to remote database server
|Completed
|3/3/2021
|3/22/2021
|3/12/2021
|-
|Run EMADE using a worker process
|Completed
|3/3/2021
|3/22/2021
|3/14/2021
|}

== March 3, 2021 ==
'''Bootcamp Notes:'''
* Listened to remaining presentations on ML and MOGP predictions
* Intro to EMADE
** Evolutionary Multi-Objective Algorithm Design Engine
** Uses MOGP to automate the process of choosing ML parameters to design ML algorithms
'''Subteam Notes:'''
* Assignment: Get EMADE working and connect to master database server on subteam
** Successfully ran EMADE locally by modifying input_titanic.xml and typing in 127.0.0.1 (localhost) and username and password
** Used MySQL workbench to attempt to connect to remote database, but connection refused

'''Action Items:'''
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Get EMADE to start working
|Completed
|3/3/2021
|3/22/2021
|3/7/2021
|-
|Connect to remote database server
|Incomplete
|3/3/2021
|3/22/2021
|
|}

== February 24, 2021 ==
'''Bootcamp Notes:'''
* Presented our subteam's work the past two weeks.
* Assignment: Install EMADE and MySQL

'''Action Items:'''
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Install EMADE and MySQL
|Completed
|2/24/2021
|3/3/2021
|2/28/2021
|}

== February 17, 2021 ==
'''Bootcamp Notes:'''
* Assignment: Use MOGP to create a Pareto frontier with a single algorithm, using the same preprocessed data for the Titanic Dataset last week. (Can't use Algorithms module from DEAP)
'''Meeting Notes:'''
* Meeting 1: 2/20/2021
** Built basic working pipeline of MOGP algorithm.
** Explored viable selection, mating, and mutation algorithms.
* Meeting 2: 2/23/2021
** Refined GP pipeline. 
*** Used SPEA2 selection algorithm and normalized evaluation function using a hyperbolic tangent function, as we want binary results. 
** Used teammate's results, as this one had the lowest area under curve in Pareto frontier.
** Created presentation with relevant images and notes from this week and last week.
** [[Group 2|Link]] to presentation and Pareto frontier.

'''Action Items:'''
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Build basic MOGP pipeline
|Completed
|2/17/2021
|2/24/2021
|2/20/2021
|-
|Optimize model and calculate AUC
|Completed 
|2/17/2021
|2/24/2021
|2/23/2021
|-
|Create presentation
|Completed
|2/17/2021
|2/24/2021
|2/23/2021
|-
|Update notebook
|Completed
|2/17/2021
|2/24/2021
|2/24/2021
|}

== February 10, 2021 ==
'''Bootcamp Notes:'''
* Continued discussion of multiple objective optimization.
* Assignment: Use the Titanic survival dataset on Kaggle and create individual algorithms in your subteam to minimize FPR and FNR in predicting whether a passenger survived or not.
** Results must be co-dominant with other results in the subteam
'''Meeting Notes:'''
* Meeting 1: 2/13/2021
** Began preprocessing data.
** Dropped features such as name, ticket number, and cabin. A person's name or ticket number should not affect survival chances in any way, and the cabin column had too many NaNs for it to be useful, even if a person's cabin can affect survival chances.
** Before dropping name, we extracted the title of the passengers name (e.g. Mr., Ms., Master, etc..) and imputed missing ages using the median age for each title. For example, all passengers with the title "Master" with a valid age had a median age of 4, so any passenger with this title with a missing age was given the age of 4. 
** We also one-hot encoded gender, between male and female, as this would be easier for an ML classifier to understand. 
* Meeting 2: 2/14/2021
** Created a general preprocessing function for the training and test sets for the dataframe.
** Evaluated classifier algorithms in order to predict survival and compared results using FPR and FNR.
*** Chose support vector machine as classifier, as this algorithm maps multidimensional data, such as passenger data, to higher dimensions, and creates clean boundaries in order to separate those who survived and those who didn't.
*** Results: (FPR, FNR): (0.06451612903225806, 0.22297297297297297)
*** This is Pareto-optimal with my teammates' results, as shown in the graph.

[[files/Pareto_codominance.png|none|thumb|FPR and FNR results for each teammate, each codominant with each other]]

'''Action Items:'''
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Set up Google Colab Notebook
|Completed
|2/10/2021
|2/17/2021
|2/13/2021
|-
|Preprocess Data with group
|Completed 
|2/10/2021
|2/17/2021
|2/14/2021
|-
|Tune SVM classifier
|Completed
|2/10/2021
|2/17/2021
|2/15/2021
|-
|Update notebook
|Completed
|2/10/2021
|2/17/2021
|2/17/2021
|}
== Self Asssessment ==
[[files/Jasti Prahlad VIP AAD notebook rubric.pdf|thumb]]
  
== February 3, 2021 ==
'''Bootcamp Notes:'''
* Gene Pool is the set of genomes to be evaluated in current generation.
** Genome is genetic description of individual, e.g. DNA, tree, array.
** Search space is the set of all possible genomes.
** Objective space is the set of all objectives, also known as the phenotype.
* Evaluation of genome is done by assigning a set of scores to an individual's genome.
** Confusion Matrix to determine accuracy of algorithm
{| class="wikitable"
!
!Predicted Positive
!Predicted Negative
|-
|'''Actual Positive (P)'''
|True Positive (TP)
|False Negative (FN) (Type II Error)
|-
|'''Actual Negative (N)'''
|False Positive (FP) (Type I Error)
|True Negative (TN)
|}
* Objective Functions with Confusion Matrix:
** Sensitivity or True Positive Rate (TPR): TP/P
** Specificity or True Negative Rate (TNR): TN/N
** False Negative Rate (FNR): FN/P = 1 - TPR
** False Positive Rate (FPR): FP/N = 1 - TNR
** Precision or Positive Predicted Value (PPV): TP/(TP + NP)
** False Discovery Rate (FDR): FP/(FP + TP) = 1- PPV
** Negative Predictive Value (NPV): TN/(TN + FN)
** Accuracy: (TN + TP) / (N + P)  

* Objective Space:
** Along with previous metrics, mean-squared error, cost, and complexity are also valid functions. 
** We can plot individuals on an n-dimensional objective space, or phenotype, where n is the number of objective functions. 

* Pareto Optimality:
** An individual is Pareto optimal if there is no other individual which dominates it among all objectives.
** The set of Pareto optimal individuals in the objective space is known as the Pareto frontier.

* Nondominational Sorting Genetic Algorithm II:
** We can assign Pareto individuals a rank of 0. If we "remove" the Pareto individuals from the objective space and find the new Pareto individuals, we can assign rank 1 to these Pareto points. We can repeatedly do this, incrementing rank by 1. 
** We can sort individuals based on rank.
** In order to break ties between individuals with the same rank, we use their crowding distance.
*** Crowding distance is sum of normalized Euclidean distances to all the points in its front. Higher crowding distance wins. 

* Strength Pareto Evolutionary Algorithm
** Each individual is given a strength S, which is the number of other individuals it dominates, and a rank R, which is the sum of strengths of other individuals that dominate it.
** Pareto individuals have a rank of 0.
** Distance to the Kth nearest neighbor is calculated to be σ. Fitness of the point is then R + 1/(σ + 2). 
'''Lab 2 Part 2 Multi-Objective Genetic Programming:'''
* We will try to determine an individual which mimics f(x) = -x + sin(x^2) + tan(x^3) - cos(x). This is done by minimizing the area under the Pareto frontier of the objective space.
* Using the original parameters, we will get the function f(x) = x + x^2 + x^3 + x^4 with fitness 8.812914293795791e-17.
[[files/Obj_space.png|none|thumb|Mean squared error of objective space]]
[[files/Lab3_0.png|none|thumb|Fitness graph with original parameters]]
[[files/Lab3_1.png|none|thumb|Pareto frontier with original parameters, AUC = 2.3841416372199005]]
By changing the tournament size to 10 and the mutation function to mutInsert, we can get an AUC reduction of just over 25%. 
[[files/Lab3_2.png|none|thumb|Fitness graph with new parameters]]
[[files/Lab3_3.png|none|thumb|Pareto frontier with new parameters, AUC = 1.7889732450245445]]
This yields a new function of f(x) = -cos(x + sin(sin(x))) with fitness 0.49416153493347187.
* Failed Attempts:
** Changing Mu and Lambda: Drastically increased the AUC, no matter which direction I changed it in.
** Using mutEphemeral for the mutation function: Slightly decreased the AUC, but nowhere near 25%
* mutInsert also did not decrease the AUC that much, and changing the tournament size was what drove the reduction over 25%, which shows that it is the most significant effect on the Pareto frontier. 

'''Action Items:'''
{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Finish Lab 2 Part 2
|Completed
|2/3/2021
|2/10/2021
|2/9/2021
|-
|Update Wiki
|Completed 
|2/3/2021
|2/10/2021
|2/9/2021
|-
|Complete Self-Evaluation
|Completed
|2/3/2021
|2/10/2021
|2/9/2021
|}

== January 27, 2021 ==
'''Bootcamp Notes:'''
* Besides binary arrays, we can represent individuals as objective functions themselves.
** Used in order to evaluate and minimize error of individual.
* We can represent functions as evaluation trees.
** Internal nodes are used to store primitives, which represent mathematical functions.
*** Binary functions with two input nodes: +, -, *, /
** External nodes, or leaves, store terminal nodes, which are inputs to the primitive nodes. 
* We can store the individual as the preorder traversal of this tree.
** Traverse root node, then recursively perform traversal on left subtree, then right subtree.
** Operators precede its inputs in representation: e.g. ((3 + 5) - 2) * 3 is represented as [*, -, +, 3, 5, 2, 3] 
* Crossovers among trees involve picking one node in each tree, and swapping their respective subtrees between the trees. 
* Mutations involve adding or deleting a node or subtree from the tree, along with changing a node. 
* In order to implement non-primitive functions such as sin(x), we can represent an approximation of the function with primitive functions using the Taylor series. 
** e.g. sin(x) = x - x^3/3! + x^5/5! - .... 
* In order to evaluate function, we can feed input points into our estimated function and the true function, and sum up their square error. 
'''Lab Notes: Symbolic Regression:'''
* Added tan(x) and cos(x) for primitive functions, and mutShrink as new mutation method
*Results:
** After 40 generations, the most fit function was tan^2(x) + tan(x)
** Fitness: 0.21309774609405807

[[files/lab2_0.png|none|thumb|Default parameters, adding tan(x) and cos(x)]]

[[files/lab2_2.png|none|thumb|Default parameters, adding mutShrink]]

[[files/lab2_1.png|none|thumb|Default parameters, adding tan(x) and cos(x) and mutShrink]]

[[files/lab2_3.png|none|thumb|Default parameters, increasing maximum depth to 5]]

{| class="wikitable"
!Task
!Status
!Date Assigned
!Due Date
!Date Completed
|-
|Finish Lab 2 Part 1
|Completed
|1/27/2021
|2/3/2021
|2/3/2021
|-
|Update Wiki
|Completed 
|1/27/2021
|2/3/2021
|2/6/2021
|}

== January 20, 2021 ==
'''Bootcamp Notes:'''
* Genetic Algorithm: Determining an optimal individual by creating a population and mating, mutating, and evaluating the new generation to generate improved individuals.
* Steps of Genetic Algorithm:
** Initialization: Generating individuals with properties such as having ones and zeroes at random in an array (e.g. [0,1,1,1,0,0,1,1,0])
** Evaluation: Computing the objective, or a function to maximize or minimize an individual towards a goal
** Selection: Randomly picks individuals for mating, with higher probability to pick individuals with higher fitness, or relative comparison to other individuals
** Crossover: Swapping properties between two fit individuals to create children, with mutations among the new children in order to encourage diversity
** Loop back to evaluation for new generation and repeat process until goal is achieved or individuals are good enough.
'''Lab Notes Part 1: One Max'''
* Objective: Obtain a bit vector of size 100 to have all ones using DEAP
* Run the main() function in the Python Notebook "Bootcamp - Lab 1.ipynb" 10 times after going through the function definition:
** Result after running through 40 generations: Success, Success, Fail (99.0), Success, Fail (99.0), Success, Success, Success, Success, Fail (99.0)
** Reached goal 70% of the time, but never scored below 99.0.
** Although it is not guaranteed, the crossover and mutations are able to successfully reach 100 ones in less than 40 generations.
'''Lab Notes Part 2: N-Queens'''
* Objective: Find an arrangement of N queens on an NxN chessboard such that no two queens can attack each other.
** This is achieved when each queen does not share its row or column with any other queens.
*Using default parameters, the algorithm reached the global minimum of 0 in 28 generations. 
*Using the custom mutation function (Swap queen in i^th column with the queen in the (n - i - 1)^th column), the algorithm never reached the global minimum of 0 (the farthest it got was 1.0) in 100 generations.
<code>
def mutReflect(individual, indpb):</code>
    size = len(individual)
    for i in range(size//2):
        if random.random() < indpb:
            individual[i], individual[size - i - 1] = individual[size - i - 1], individual[i]
    return individual,</code>
*Using the modified evaluation function (incrementing diagonal queen count by 1), the algorithm reached the global minimum of 0 in 39 generations. 
<code>def evalNQueens(individual):
   size = len(individual)
    #Count the number of conflicts with other queens.
    #The conflicts can only be diagonal, count on each diagonal line
    left_diagonal = [0] * (2*size-1)
    right_diagonal = [0] * (2*size-1)
    
    #Sum the number of queens on each diagonal:
    for i in range(size):
        left_diagonal[i+individual[i]] += 1
        right_diagonal[size-1-i+individual[i]] += 1
    
    #Count the number of conflicts on each diagonal
    sum_ = 0
    for i in range(2*size-1):
        if left_diagonal[i] > 1:
            sum_ += 1
        if right_diagonal[i] > 1:
            sum_ += 1
    return sum_,</code>
*Using both modifications, the algorithm never reached the global minimum of 0 (the farthest it got was 1.0) in 100 generations.
[[files/nqueens_default.png|none|thumb|Default parameters]]

[[files/nqueens_mut.png|none|thumb|Default parameters, using mutReflect]]

[[files/nqueens_eval.png|none|thumb|Default parameters, using custom evaluation function]]

[[files/nqueens_both.png|none|thumb|Default parameters, using both modifications]]
* Neither of my modifications made any improvements to the algorithm, which may suggest that random shuffles are an optimal choice for mutation.
{| class="wikitable"
!Task
!Status 
!Date Assigned
!Due Date
!Date Completed
|-
|Set up Notebook
|Completed
|1/20/2021
|1/27/2021
|1/31/2021
|-
|Install Jupyter Notebook and DEAP
|Completed
|1/20/2021
|1/27/2021
|1/20/2021
|-
|Finish One Max Lab
|Completed
|1/20/2021
|1/27/2021
|1/26/2021
|-
|Finish N Queens Lab
|Completed
|1/20/2021
|1/27/2021
|1/31/2021
|}