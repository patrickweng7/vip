'''Name:''' Pranav Pusarla

'''Email:''' [mailto:ppusarla3@gatech.edu]

'''Cell Phone:''' 408-728-3993

'''VIP:''' Automated Algorithm Design

'''Interests:''' Artificial Intelligence, Quantitative Finance, Volleyball, Soccer, Guitar

== August 25, 2021 ==
=== Class Notes: ===

'''<u>Concept of Genetic Algorithms</u>'''
*new generation created through mating/mutation of individuals
*fitness is evaluated then used again for new generation
**relative comparison to other individuals (Ex. how well does individual accomplish task relative to population?)

'''<u>Keywords</u>'''
*Objective: value used to characterize individuals (usually maximize objective)
*Selection: gives preference to better individuals therefore allowing them to pass on their genes
*Fitness Proportionate: greater the fitness value, the higher the probability of being selected for mating
*Tournament: several tournaments among individuals; winners selected for mating
*Mate/Crossover: represents mating between individuals
*Mutate: introduces random modifications; purpose to maintain diversity

'''<u>Algorithm</u>'''
#Randomly initialize population
#Determine fitness of population
#Repeat
##Select parents
##Perform crossover on parents
##Perform mutation
##Determine fitness of individuals

=== Lab 1 Notes: ===
'''One Max Problem'''
*Defined a fitness objective and individual classes with DEAP creator
*Defined individual as 100 boolean values and population as a list of individuals
**Evaluation function is just the sum of the ones
*Defined genetic operators such as mutate, mate, and tournament
*Start the main algorithm
**Evaluate each individual in population and map fitness with individual
**Select individuals for tournament
**Mate and mutate individuals with 50% and 20% chance respectively
**Re-evaluate fitness of new individuals and assign fitness to individuals
*Main algorithm was run 40 times to achieve best result

'''N Queens Problem'''
*Problem: Placing n queens on a nxn board so that none of the queens hit each other
*This time the weight of our fitness objective is negative since we want to minimize the number of conflicts between queens on the chessboard
*Defined a permutation function in the toolbox to represent locations of queens on the chessboard
*Defined evaluation function to count conflicts between other queens
**Since queens are placed on different rows and columns because of permutation, only need to check diagonals
**There are 2*n - 1 diagonals left and 2*n - 1 right
*Tested out two-point crossover function would two randomized permutations of 1-8
*Defined our mutation function as a shuffle function since we cannot change any of the numbers since they represent column values on board
**Made my own mutation function switching first and last if probability lower than indpb

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create VIP Notebook and add lecture notes
|Completed
|August 25, 2021
|September 1, 2021
|September 1, 2021
|-
|Finish Lab 1
|Completed
|August 25, 2021
|September 1, 2021
|September 1, 2021
|}

== September 1, 2021 ==
=== Class Notes: ===

'''<u>Genetic Programming</u>'''
*Instead of taking an individual and having a function evaluator to obtain objective scores, the individual is the function itself

'''<u>Tree Representation</u>'''
*We can represent a program as a tree structure
**Nodes are called primitives and represent functions
**Leaves are called terminals and represent parameters
*Tree can be converted to a lisp preordered parse tree
**Operator followed by inputs

'''<u>Crossover in GP</u>'''
*Crossover in tree-based GP is simply exchanging subtrees
*Start by randomly picking a point in each tree and switch subtrees of picked parents

'''<u>Mutation in GP</u>'''
*Can involve...
**Inserting a node or subtree
**Deleting a node or subtree
**Changing a node

'''<u>Evaluating a tree</u>'''
*We can feed a number of input points into the function to get outputs
*Run f(X)
*We can measure error between outputs and truth

=== Lab 2 Notes: ===
'''Symbolic Regression'''
*Creating fitness function with weight -1 so trying to find min
*Each individual represented by a tree by inheriting from Deap's PrimitiveTree class
*Created a primitive set that holds all mathematical functions and arguments
**Added two primitives (power and log)
*Defined toolbox with individual, population, and compile
**Also defined a function expr that creates a primitive tree between min and max depth
*Defined evaluation function that basically takes the mean squared error between the tree function and actual function based on inputted points
*Registered toolbox with genetic functions
**Added a mutation function - node replacement
*Ran 40 generations and got minimum of 8.77e-17 

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finish Lab 2 Part 1
|Completed
|September 1, 2021
|September 8, 2021
|September 8, 2021
|}

== September 8, 2021 ==
=== Class Notes: ===

'''<u>Genome</u>'''
*Gene pool is the set of genome to be evaluated
**Genome
***DNA
***GA: set of values
***GP: tree, string
**Search Space
***Set of all possible genome (algorithms)
*Evaluation of a genome -> genome with a score

'''<u>Maximization Measures</u>'''
*Sensitivity or True Positive Rate (TPR) 
**aka hit rate or recall
**TPR = TP / P = TP / (TP+FN)
*Specificity or True Negative Rate (TNR)
**TNR = TN / N = TN / (TN+FP)

'''<u>Minimization Measures</u>'''
*False Negative Rate (FNR)
**FNR = FN / P = FN / (TP+FN)
**FNR = 1 - TPR
*Fallout or False Positive Rate (FPR)
**FPR = FP / N = FP / (FP + TN)
**FPR = 1 - TNR

'''<u>Other Measures</u>'''
*Precision or Positive Predicted Value (PPV)
**PPV = TP / (TP+FP)
*False Discovery Rate
**FDR = FP / (TP+FP)
*Negative Predictive Value
**NPV = TN / (TN+FN)
*Accuracy
**(TP + TN) / (P+N)

'''<u>Objective Space</u>'''
*Each individual is evaluated using objective functions
**MSE
**Cost
**Complexity
**TPR, FPR
*Objective scores give each individual a point in objective space
*referred to as the phenotype of the individual

'''<u>Pareto Optimality</u>'''
*individual is Pareto optimal if there is no other individual in the population that outperforms the individual on all objectives
*the set of all Pareto individuals is known as the Pareto frontier
*we want to drive selection by favoring Pareto individuals
**but maintain diversity by giving all individuals some probability of mating

'''<u>Nondominated Sorting Genetic Algorithm II (NSGA II)</u>'''
*Population is separated into nondomination ranks
*Individuals are selected using binary tournament
*Lower Pareto rank beats higher Pareto rank
*Ties on the same front are broken by crowding distance
**Summation of normalized Euclidean distances to all points within the front
**Higher crowding distance wins

'''<u>Strength Pareto Evolutionary Algorithm 2 (SPEA 2)</u>'''
*Each individual is given a strength S
**S is how many others in the population it dominates
*Each individual receives a rank R
**R is the sum of S of the individuals that dominate it
**Pareto individuals are nondominated and receive an R of 0
*For ties, a distance to the Kth nearest neighbor is calculated and a fitness of R + 1/ (muk + 2)

=== Lab 2 Part 2 Notes: ===
*Working with two different objectives: minimizing two different mses
*Added sin, cos, tan as primitives to be used, reinitialized toolbox with seed to produce same results
*Defined new evaluation function 
*Created pareto dominance function that returns true if every value in individual 1 is less than individual 2
*Intialized population of 300 and selected one individual
**Separated population into dominated vs dominator with comparison of that one individual
*Blue point is the selected individual, red dots are the dominators, black is uncomparable, and green is dominated
*After running the main algorithm, we get AUC of 2.46
*To reduce the AUC by 25%, we have to understand what parameters to optimize
**For the eaMuPlusLambda function, we have mu, lambda, cxpb, mutpb, and ngen
***Mu - number of individuals selected for next generation
***Lambda - number of children to produce at each generation
***Cxpb - probability that an offspring is produced by a crossover
***Mutpb - probability that an offspring is produced by mutation
***Ngen - number of generation
**Changing lambda from 100->50 gives us an AUC of 2.46->2.39
**Changing lambda from 100->150 gives us an AUC of 2.46->6.08
**Increasing ngen from 50->100 gives us an AUC of 2.46->5.12
**Reducing ngen from 50->25 gives us an AUC of 2.46->2.35
**Increasing mu from 50->100 gives us an AUC of 2.46->6.94
**Decreasing mu from 50->25 gives us an AUC of 2.46->5.75
**Increasing cxpb from 0.5->0.8 gives us an AUC of 2.46->5.25
**Decreasing cxpb from 0.5->0.2 gives us an AUC of 2.46->6.05
**Increasing mutpb from 0.2->0.5 gives us an AUC of 2.46->6.40
*I do realize that the combination of different parameter changes does affect the performance
*For now, I'll stick with reducing ngen and lambda and see what happens
*Reducing ngen from 50->25 and lambda from 100->50 gives us an AUC of 2.39

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete self-evaluation rubric
|Completed
|September 8, 2021
|September 15, 2021
|September 15, 2021
|-
|Finish Lab 2 Part 2
|Completed
|September 8, 2021
|September 15, 2021
|September 15, 2021
|}

== September 15, 2021 ==
=== Class Notes: ===
*Went over example titanic data processing notebook

=== Group Project Notes: ===
*We started by going over the current pre-processing that was used in the example notebook
*We changed the encoding of the Embarked and Sex column to one-hot encoding so each category had its own column of 0s and 1s
*We also changed the random state to one that our whole group uses so we could get aligned results
*Once we finished the preprocessing, we each picked an algorithm and ran the model to get the designated results
**I picked gaussian process classifier and was able to tune the parameters (changing kernel) to get better results
'''Results:'''
{| class="wikitable"
!Name
!False Positives
!False Negatives
|-
|Eashan
|21
|31
|-
|Pranav
|19
|35
|-
|Elan
|25
|29
|-
|Jessi
|7
|79
|-
|Leul
|5
|93
|}

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Completed data processing and pareto optimal results with group
|Completed
|September 15, 2021
|September 22, 2021
|September 22, 2021
|}