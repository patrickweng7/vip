== Team Member ==
Team Member: Rishi Bhatnager

Email: rishibhatnager01@gatech.edu

Cell Phone: (908) 361-0455

Interests: Football (Giants, Seahawks), Basketball (Thunder), Music (Hip-hop), Machine Learning

Subteam: Market Analysis and Portfolio Optimization (Stocks)

Subteam Teammates:
* [https://github.gatech.edu/emade/emade/wiki/Notebook-Pranav-Shankar-Manjunath Pranav Manjunath]
* [https://github.gatech.edu/emade/emade/wiki/Notebook-Abhiram-Venkata-Tirumala Abhiram Tirumala]


=Fall 2021=
* [https://doi.org/10.1016/j.asoc.2015.10.030 Paper link]

==Self-eval==
* Sept 17: 95/100
** -1 Clarity of To-do items
** -2 Level of Detail
** -2 Usefulness to team

==Dec 10, 2021==
'''Individual / Subteam Meeting Notes'''
* We created [https://docs.google.com/presentation/d/1NEBKzjTDcTHCosAotFKmvSSGbmRxxeedptpNviGxse8/edit?usp=sharing our presentation]
* I modified [https://colab.research.google.com/drive/171NDrtM1QqIjpEcNtVye9O_abDdzNU_C?usp=sharing my seeding file notebook] from last week to aid with the presentation:
** Through SQL queries and Pandas data manipulation, I found the top 10 individuals (in terms of lowest loss percentage) from each experiment (aggregated among each of the 3 trials). I converted the loss percentages to profit percentages and then made a bar graph that averaged these top individuals' profit percentages to compare the performance of the best individuals from each set of objectives. The bar graph can be found on slide 20 of our presentation (linked above), in the notebook (also linked above), or below:
[[files/rbhat3/comp_exp_bar.png|thumb|50x50px]]
*** This bar chart indicate that our best objective sets seem to be [total loss, variance of profit per transaction] and [total loss, average loss per transaction]
*** It was surprising for us to not see an objective set that included cdf among the top experiment performances, though [total loss, cdf] was the third best performing experiment
*** Experiments with less objectives tended to perform better than those with larger objective sets
** I also added code to find the tree with the lowest overall loss percentage, as this was the individual we would use for analysis in our final presentation
*** In turned out to be a very long and complex tree: Learner(MyEVM(MyDeltaSTOCH(MyDeltaSTOCH(MyDeltaRSI(MyBollingerBand(MyEVM(MyDeltaMACD(MyEVM(MyDeltaSTOCH(MyDeltaSTOCH(MyBollingerBand(MyEVM(MyDeltaSTOCH(MyBollingerBand(MyEVM(MyEVM(MyDeltaSTOCH(MyEVM(MyDeltaSTOCH(MyDeltaSTOCH(MyBollingerBand(MyDeltaSTOCH(MyBollingerBand(MyEVM(MyBollingerBand(MyAROON(MyDeltaSTOCH(MyBollingerBand(MyEVM(MyBollingerBand(MyEVM(MyDeltaSTOCH(ARG0, 1, 32), 8), 5, falseBool, 10), 0), 128, greaterThan(myFloatIntDiv(myFloatIntSub(myFloatIntAdd(-0.5840088004334074, 10), lessThan(100.0, -4.695623339332281)), lessThanOrEqual(myFloatIntSub(0.1, 8), -4.695623339332281)), myFloatMult(-4.695623339332281, 100.0)), 980), 1, 14), 8), 7, falseBool, 10), 0), 10, greaterThan(myFloatIntDiv(myFloatIntSub(0.1, 50), lessThanOrEqual(myFloatDiv(myIntToFloat(lessThan(0.10145704993941784, 100.0)), myFloatSub(0.1, myFloatIntAdd(0.01, 5))), 10.0)), myFloatMult(3.693800388190727, 100.0)), -3), 1, 14), 6207, lessThanOrEqual(myFloatIntSub(myFloatIntSub(0.1, 8), passInt(0)), 10.0), 32), 1, 8), 1, 10), 8), 1, 32), 12), 0), 14, falseBool, 8), 1, 32), myFloatToInt(2.1890902574242297)), 6207, lessThanOrEqual(myFloatIntSub(0.1, 50), 10.0), 32), 1, 8), 1, 10), 8), 1, 32), 12), 14, falseBool, 10), 8), trueBool, myFloatToInt(-4.901541175503988)), trueBool, myFloatToInt(-4.901541175503988)), 0), LearnerType(‘ARGMIN’, {‘sampling_rate’: 1}), EnsembleType(‘BAGGED’, None))
**** It is tough to understand what exactly this individual is doing, but one interesting point is that the learner is not an ML model, but simply the argmin function
**** Abhiram hypothesized that since the trading algorithm only executes a trading decision if the signal is between 0 and 1, and since the only numbers within this range that the argmin function can return are 0 and 1, the algorithm simply does nothing unless the lowest signal is one of the two innermost TIs (MyDeltaSTOCH at index 0 or MyEVM at index 1)
**** I found it interesting that last semester our best performing individual was a very simple tree that just used the MyBollingerBand primitive as the only TI. Both that tree and this one are similar in that they are very confusing to understand and that it does not quite make sense how they work so well, but that is also why EMADE is helping with this problem: autoML finds the algorithms we could not arrive at by just using logical thought.
* I also added some code (pictured below) to the standalone tree evaluator that helps with comparing individuals' performance to that of the paper. It creates a DataFrame that has seven rows (one per stock) and four columns: the ticker, the profit percentage achieved by the given tree on the given ticker, the profit percentage achieved by the paper on the given ticker, and the difference between the two (i.e. how many percentage points higher the tree achieves compared to the paper). This DataFrame is then written to a csv.
** [[files/rbhat3/comp_code.png|thumb|50x50px]]
** For the presentation, we uploaded this csv to Google Sheets, and then added a conditional formatting to the last column (the one that shows the difference between our performance and the paper's) so that the cell is green if we outperform the paper and red otherwise. This table can be found in the lower right corner of  [https://docs.google.com/presentation/d/1NEBKzjTDcTHCosAotFKmvSSGbmRxxeedptpNviGxse8/edit#slide=id.gcfdc76cf8f_2_28 slide 24 of our presentation].
*** Using this table, we can see that we outperform the paper in 6/7 stocks. In fact, we outperform the paper by over 30 percentage points on 2/7 stocks, and by over 20 percentage points on 4/7 stocks. We found this to be surprisingly better than expected.
*** We will rerun this script and continue to look for individuals that outperform the paper in all stocks as we wrap up our experiments
* I was assigned to present slides 1-2, 20-27




'''Team Meeting Notes'''

''Final Presentation day, notes for each subteam below''

'''Our presentation'''
* Create trade signal visualization for complex individuals (especially for argmin/max trees where we don’t really know what’s going on)

Image Processing
* Baseline results:
** Objectives: ROC AUC, number of parameters
** Parteo front AUC: .219
* NSGA II performs better than Lexicase and NSGA III
* Geometric Mating and Mutation (new operators for sequence individuals): Partially matched, ordered crossover, uniform and multipoint
** Ran into errors with the former two
** Uniform and multipoint best ROC AUC: .365
* Contrast and sharpening filter gets lower ROC AUC than contrast and Sobel filter (but still worse than baseline)

NLP
* Goal: use autoML to improve Q&A systems
* Best results: .1% reduction in area of interest
** Both seeds and individuals performed poorly
** SOTA also doesn’t perform too well
* Future work:
** Improve low scoring results
** Implement multi output regression individuals
** Debug problems with already-implemented embeddings
** Lazy bug fixes

NAS
* Weight sharing:
** pros: best individual good for 1 epoch (64% test accuracy)
** cons: weights in database are too slow
* Adam optimizer performed well
* General next steps:
** Remove SGD or only use Adam
** Experiment with training callbacks

Modularity
* Experiment 1 takeaway: Size may not inherently imply usefulness
* Analysis of ARL effects on individuals’ objectives (experiment 2) takeaways:
** Most ARLs tended to follow the general distribution
** ARLs which differ from the general distribution were rare
* Future work: 
** ARL construction
** ARL selection experiments

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Analyze and compare performance of experiments
|In progress 
|Dec 3, 2021
|
|
|-
|Create and rehearse final presentation
|Complete 
|Dec 3, 2021
|
|Dec 10, 2021
|}



==Dec 3, 2021==
'''Individual Notes'''
* There wasn't much to be done last week due to Thanksgiving and us waiting on more experiments to be completed before we started the analysis
* Now that we have a good number of runs under our belts, I will start doing some of the analysis
* My idea for something that would be very useful would be to code a script that will check each of databases for an individual that beats the paper in as many stocks as possible
** Since this would require evaluating many trees, so the standalone tree evaluator may not be the most efficient way to do this
** Abhiram had the idea that we could just create a seed file with all of our best performing trees and then do a one generation run of EMADE with no evolution and seven objectives which are the loss percentage for each of the stocks we use
*** This would effectively evaluate all of our best individuals to find their profit percentage on each stock
*** This also does not require us to run the standalone tree evaluator many times
** Going on with this second idea, Abhiram asked me to create the seed file and he could conduct the EMADE run on PACE
*** I made [https://colab.research.google.com/drive/171NDrtM1QqIjpEcNtVye9O_abDdzNU_C?usp=sharing this notebook] that queries each of our experiments' databases for their top five individuals in terms of lowest loss percentage and then aggregates them into a seed file to which the notebook automatically writes. The file that is created can be found [https://drive.google.com/file/d/1r1RcyOjrWcPNi2ddpzOc0u1gwqALgu_U/view?usp=sharing here].
**** Updates (12/10):
***** Abhiram did not get the time to do this seven-objective EMADE run, so for our final presentation we just ran the standalone tree evaluator on our best overall individual, and we will do this seven-objective EMADE run over break
***** I slightly modified the notebook for our final presentation, so not everything directly applies to creating the seeding file (more on this in future entries)


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Analyze individuals
|Complete 
|Nov 11, 2021
|
|Dec 3, 2021
|-
|Analyze and compare performance of experiments
|In progress 
|Dec 3, 2021
|
|
|-
|Create and rehearse final presentation
|In progress 
|Dec 3, 2021
|
|Dec 10, 2021
|}


==Nov 19, 2021==
'''Subteam Notes'''
* Right now we're just running experiments. We're waiting to get some more done before we dive deep into performing all of the analysis, so we're waiting for that and just churning out runs. A tracker of our runs can be found [https://docs.google.com/spreadsheets/d/1PPvUCCXGK2Ni6-W26sQ39TONQPYYaP9ZAUq6xL0OGeY/edit?usp=sharing here]
* A good example of a relatively strong individual can be found [https://drive.google.com/file/d/13cH3K8wtKv-6ce0cwOn3MK-ogmOV8HM6/view here]. Here is how it compares to the paper:
{| class="wikitable"
!Stock
!Individual's Profit
!TS-Fuzzy Profit
!EMADE is better?
|-
|AAPL
|68.86% 
|45.16%
|Yes
|-
|BA
|8.11% 
|32.66%
|No
|-
|VZ
|15.14% 
| -1.24%
|Yes
|-
|XOM
|7.98% 
|14.78%
|No
|-
|JNJ
|.65% 
|6.31%
|No
|-
|CAT
|35.33% 
|17.01%
|Yes
|-
|S&P 500
|32.16% 
|11.9%
|Yes
|}
** So EMADE beats out the paper with 4/7 stocks, which is good, but there is obviously still room for improvement. But this is also just one individual and there is more analysis to be done. Also, it is definitely a plus that EMADE never produces a loss.
* Another analysis we have done is producing [https://drive.google.com/file/d/1-mkrx2bLEzR_NbkwhIfUasryAxRfUcMm/view this heatmap] summarizing the performance of individuals by the learners and primitives used.
** The numbers here are loss percentage, so darker blue is better. For example, -1.03% means 3% profit, -.97% means 3% loss, and -1 means no profit/loss.
** A blank box means there were no individuals with both that learner and primitive.
* Once we get a couple more runs done (most likely during or after Thanksgiving break) we will start conducting the bulk of our analysis.

'''Team Meeting Notes'''
* Dr. Zutty said:
** We're on a good pace, and our plan sounds good
** We should make it clear on the individuals' cdf charts that a profit of 1.68 signifies 68% profit not 168%

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Analyze individuals
|In progress 
|Nov 11, 2021
|
| Dec 3, 2021
|}

==Nov 11-12, 2021==
'''Subteam Meeting Notes'''
* We had a quick meeting to catch everyone up with what's going on with the team
* Basically, we're running all of our experiments, and that is what we'll primarily be doing for some time
* We can start analyzing individuals as runs finish up
** Specifically, we plan on using a graph similar to the one Dr. Zutty said was good from our Spring final presentation (heat map showing frequency of learner-primitive pairs in individuals)


'''Team Meeting Notes'''
* I was not able to go to the meeting (out sick), but Abhiram relayed to us that Dr. Zutty suggested we keep one objective throughout all of the runs so we can have some way to compare them. We decided to keep profit percentage (loss for the minimization) as this will also give all runs a comparison to the paper.


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Analyze individuals
|In progress 
|Nov 11, 2021
|
| Dec 3, 2021
|}


==Nov 3-5, 2021==
'''Individual Notes'''
* I tried debugging the PLR code. I was able to get our code closer to the paper's results but was unable to fully replicate these results
** I noticed that we were using close prices instead of adjusted closes, and making the change to use the latter got our PLR results to get closer to the paper's. Below is a side-by-side comparison, where my results are on the left and the paper's results. My results are missing one peak that appears in the paper, and that peak is circle in red on the right.
*** [[files/rbhat3/plr.png|thumb|50x50px]]
** It also looked like we were calculating trade signals using different formulas than in the paper (these might be from the old paper), but fixing this didn't change the PLR results. Specifically, I tried changing the method for how we generate signals to the following (this is the gen_labels method in [https://github.gatech.edu/rbhatnager3/emade/blob/stocks-pace/datasets/stocks2/scripts/gen_labels.py this file], my code isn't pushed so that link is for the existing code):
*** [[files/rbhat3/gen_signals.png|thumb|50x50px]]
* We are going to ask Dr. Zutty about this at our meeting tomorrow, and after that we should be able to start running our experiments 

'''Team Meeting Notes'''
* Since we can’t properly match the paper’s PLR results, we’re fitting to the a different function than the paper, which could be a problem
** Though it would be ideal to solve this before running our experiments, we can start the experiments since it is getting late and we aren’t too far off
* Abhiram got a weird error when turning our objectives to minimizations
** It turned out that this was just a bug in EMADE.py where a zero was hardcoded and it should be equivalent to the lower bound of the corresponding objective (as it is specified in the XML template)
* Parameters for objectives in template file:
** Lower and upper are lower and upper bounds on possible values
** Achievable and goal: lower bounds on values for the objective that should be immediately achievable and the ideal value we’re shooting for (individuals with values in this range we’ll be weighted higher)
*** Check the math on how these affect evolution for our objectives (since they’re pretty unique and haven’t been as robustly tested as the problems other subteams are exploring). Make sure this part of the code makes sense for our objectives.
* Next steps: do test runs of PACE-ICE and then begin running experiments


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read paper and begin replicating it
|Complete  
|Sept 17, 2021
|
|Nov 4, 2021
|}


==Oct 25, 2021==
'''Individual Notes'''
* I helped make and revise [https://docs.google.com/presentation/d/1lQG83vSyWI1dqOSJg99hHIOyeu1lj4umVpzzBjLG_cA/edit?usp=sharing our presentation]
* I also rehearsed my slides (1, 2, 16-19)

'''Team Meeting Notes'''
''Midterm presentation day. I had a time conflict before 6:30 and after 8, notes for the presentations for which I was present are below:''

Image Processing
* Dataset used: CheXNet (detecting pneumonia using chest X-rays)
* Baselines results:
** 30 generations
** Objectives: precision-recall AUC, Number of parameters
** Two trees: both received .094 AUC, one with 255 parameters and another with 30
* Updated selection methods (ex: NSGA III)
** Couldn’t run for multiple generations because of PACE issues) 
* Mating and mutation methods: simulated binary and blended crossover

Bootcamp 3
* ML models used: GNB, Random Forest, KNN, Logistic Regression, MLP

Bootcamp 4
* ML models used: MLP, Decision Tree, SVM, XGBoost
* Primitive set for MOGP: Boolean functions and basic mathematical operations
* MOGP Parameters:
** 300 genrations
** One point crossover mating
** Uniform mutation
* MOGP performance: .151 AUC
* EMADE: .26 AUC

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read paper and begin replicating it
|In progress 
|Sept 17, 2021
|
|
|-
|Prepare for presentation
|Complete 
|Oct 21, 2021
|Oct 25, 2021
|Oct 25, 2022
|}


==Oct 21-22, 2021==
'''Subteam Meeting Notes'''
* We tested some of our best trees from last semester to see how those results compared to the TS-Fuzzy paper
** We compared pretty well based on our profit percentages from the individual: Learner(MyBollingerBand(ARG0, 2, 61, falseBool), LearnerType('DECISIONTREE_REGRESSION', None), EnsembleType('SINGLE', None))
** Performance using code from last semester's paper: 
[[files/rbhat3/last_sem_performance.png|thumb|50x50px]]
** Performance using code from this semester's paper: 
[[files/rbhat3/this_sem_performance.png|thumb|50x50px]]
*** The decreased performance could be because this individual was optimized for the objectives from last semester, and some of those changed for this semester's paper (so this would be a seed individual rather than necessarily an end product)
** Performance from TS-Fuzzy model reported by paper: 
[[files/rbhat3/paper_perf.png|thumb|50x50px]]
*** Based on last year's performance, we're about better overall
*** Based on this semester's performance, we're slightly worse overall (but, again, we would still need to run EMADE using this individual as a seed instead of using it as our standard)
* We finished our experimentation design: 
** Objectives: profit, CDF, variance, error
*** In order to make profit a minimization, we'll test for loss (the negative of profit)
*** We'll test all possible combinations of two, three, or all of these
** Compute time: 768 hours (will take 8 hours on PACE)
*** Should result in about 430 generations
** Wall time: 8 hours
** Hosts: 3
** Workers per host: 4
** CPUs per host: 8
* We'll create and rehearse our presentation over the weekend

'''Team Meeting Notes'''
* Can train on last semester’s train data and test on this semester’s test data to see how well our algorithms from last semester generalize to new stocks 
** This would be a different type of analysis to using train and test data from this semester’s paper to do our main analysis
* PACE
** Processors per worker and workers per host should be the same
** Compute time equals number of jobs times number of cores per worker
* Objectives: to make profit a minimization we’ll take the negative of profit
** Problem that will arise is in AUC calculation, because we can’t use 0 as our lower bound, so we’ll have to search through all our experiments and find the lowest profit we’ve gotten and use something around that as our low bound


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read paper and begin replicating it
|In progress 
|Sept 17, 2021
|
|
|-
|Finalize experiment design
|Complete 
|Oct 15, 2021
|Oct 22, 2021
|Oct 21, 2022
|-
|Prepare for presentation
|In progress 
|Oct 21, 2021
|Oct 25, 2021
|Oct 25, 2022
|}



==Oct 14-15, 2021==
'''Subteam Meeting Notes'''
* Began process of replicating the PLR-SVR model from the paper
* Abhiram got functional code but couldn't match the paper's results, so we tried to fix this error
* I spent my time debugging the PLR code
** I found that we were using the wrong value for the threshold, but this fix alone didn't lead to us being able to duplicate the results from the paper
* We'll ask Dr. Zutty about this tomorrow and continue to try and match these results

'''Team Meeting Notes'''
* Dr. Zutty will look at our code and see why we can’t reproduce the paper’s results
* Contingency plan: just write the paper analyzing our process and the use of autoML in time series analysis and just compare our results to the paper’s
** Just use same datasets and say we couldn’t replicate the results, but we were able to beat them
** In this case, we could also compare to other paper’s results without having to replicate their results 
* In the paper, focus on how we’re searching the space, objective functions used, etc.
* Next steps:
** Continue to debug PLR-SVR code
** Design of experiments: full matrix of parameters (what info to collect, what we will be testing per run, what resources we’ll need, how long each run will be for, etc.)
** Maybe start writing the paper?


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read paper and begin replicating it
|In progress 
|Sept 17, 2021
|
|
|-
|Finalize experiment design
|In progress 
|Oct 15, 2021
|Oct 22, 2021
|Oct 21, 2022
|}


==Oct 6-7, 2021==
'''Individual Notes'''
* Taking a more in-depth dive into the paper
* Implementation notes:
** Don’t think we have the TS-Fuzzy algo implemented
* Concept Notes:
** TS Fuzzy model used to predict trading points, SVR predicts buy/sell
*** TS Fuzzy calculates the daily dynamic thresholds
** Input space: buy/sell points used as TIs
** Output space: trade signals  
** TS Fuzzy model controls dynamic threshold used to identify turning points
** Section 3.1.3 lists principles for making trading decisions 
** Equations 8 and 9 (page 7): formula for trading signal 
*** Important quote on trading signals: “The range of trading signal is 0.0–1.0. The turning points are the points at which the trading signal intersects the threshold. The threshold is equal to 0.5. If the downslope of the trading signal is lower than the intersection point, the intersection point is a sell point. On the contrary, if the down slope of the trading signal is over the intersection point, the intersection point is a buy point.”
** Table 1: 28 combinations of TIs, hyper parameters (8 distinct TIs)
*** I believe all are already implemented
*** SRA used to select with TIs to use for each stock (process detailed in section 4.2, page 7)
*** TIs normalized using min-max scaling (should be implemented already)

'''Subteam Meeting Notes'''
* SVR is implemented
* We need to figure out how to structure our experiments (i.e. figure out the best evolutionary parameters) 
** Objective functions:
*** CDF and total profit or profit per transaction are our two best 
*** Others that are possible: variance, MSE, MAE, 
*** Probably not, but maybe consider: num elements
** Compute time
* Learn fuzzy logic to better understand the paper ([http://cs.bilkent.edu.tr/~zeynep/files/short_fuzzy_logic_tutorial.pdf resource])
* Buy/sell decisions determined by Fuzzy logic 
** i.e. the fuzzy logic determines the dynamic threshold for whether the trading signal outputting by SVR is a buy/sell
* SRA used to determine features (i.e. select TIs)
** This isn’t important (can be handled by emade)
* Instead of replicating the fuzzy logic, it will probably be easier to replicate the the PLR-SVR and/or PLR-BPN approaches and then use that as a our basis for comparison 
** We need to fix profit percentage calculation to account for tax and commission rates
* Next steps: try to replicate the PLR-SVR

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read paper and begin replicating it
|In progress 
|Sept 17, 2021
|
|
|}



==Oct 1, 2021==
'''Team Meeting Notes'''
* Peer evals releasing on Monday
* Run many trials each with the same CPU hours to determine if we’ll end up with the same pareto front after each run 
** Basically we want to answer whether the pareto algorithms were found by random chance or will we find them whenever we use emade?
* Goals:
** Last semester: Used autoML to create an accurate stock market price predictor that returned a good profit (but we don’t know if our algorithm is better than state-of-the-art)
** This semester: we want to show that our algorithms are better than state of the art (the hope is that the novel strategies we implemented previously will give us better profits than state-of-the-art technology)
* Next steps: continue to solidify our understanding of the paper and ensure we have everything implemented properly in EMADE
** After that, we'll begin running experiments on the stocks to compare our results to the paper's

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read paper and begin replicating it
|In progress 
|Sept 17, 2021
|
|
|}


==Sept 30, 2021==
'''Individual Notes'''
* I was able to read some of the paper but had trouble understanding parts of it. I will discuss what I took from it at our subteam meeting later today.

'''Subteam Meeting Notes'''
* Sriram will be joining our subteam
* Abhiram explained parts of the paper that I did not understand and also showed us what he has been able to implement
** According to him, most of the implementation is very similar to what we did last semester, so there isn't much to be done
** He had some trouble implementing the paper's exact algorithm because it didn't do anything in EMADE. We will bring this up in tomorrow's team meeting.
** He did a run on the Apple data and got around 53% profit, compared to the paper, which got 45%
* We will more concretely determine next steps after tomorrow's meeting, but in general it will most likely be to continue to gain a better understanding of the paper as well as to continue replicating its logic in EMADE
* Interestingly, our best performing individual used ARGMIN as a learner: MyBollingerBand(Learner(adf_2(ARG0), ModifyLearnerList(LearnerType('ARGMIN', {'sampling_rate': 1}), [-6, -10], 23), ModifyEnsembleFloat(EnsembleType('ADABOOST', {'n_estimators': 50, 'learning_rate': 1.0}), 0.01, 3)), myAnd(myNot(falseBool), myOr(trueBool, falseBool)), passBool(ifThenElseBool(trueBool, trueBool, falseBool)), passBool(myAnd(trueBool, falseBool))) adf_2: MyDeltaBIASSMA(MyDeltaRSI(MyWILLR(ARG0, 7, falseBool), myFloatToInt(0.01)), myIntDiv(greaterThan(0.01, 3.7849898986862023), myIntMult(128, 7)))

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read paper and begin replicating it
|In progress 
|Sept 17, 2021
|
|
|}


==Sept 23, 2021==
'''Subteam Meeting Notes'''
* Abhiram and I explained how EMADE works to Pranav on a high level, specifically focusing on the stocks pipeline. This also helped us recollect our previous work.
* We decided on [https://doi.org/10.1016/j.asoc.2015.10.030 this paper], which is very similar to the paper from last semester but more clear and with slightly better results
** This is good because (hopefully) it keeps us from falling into the trap of spending much of the semester on implementation just to find that there are ambiguous parts to the paper that prevent complete replication
** Abhiram suggested the paper and has said that the explanations seem to be clear and the required implementation on top of what we already have does not seem to be too excessive
* We will all read the paper in full—and, in doing so, noting the differences between its implementation compared to last semester's paper—and then begin implementing its logic in EMADE

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Review last semester's work 
|Complete 
|Sept 10, 2021
|
|Sept 23, 2021
|-
|Decide on a paper
|Complete 
|Sept 17, 2021
|
|Sept 23, 2021
|-
|Read paper and begin replicating it
|In progress 
|Sept 17, 2021
|
|
|}


==Sept 16-17, 2021==
'''Individual Notes'''
* I did the self-eval (see above)
* I finished the outline that we started as a group (see below) -- I worked on the Background and Conclusion + Future Work sections

'''Subteam Meeting Notes'''
* Abhiram, Pranav, and I met and explored a couple of new papers that seem interesting as well as did as much of the outline as we could without finishing our results
** [https://paperswithcode.com/paper/predicting-the-direction-of-stock-market This] is the most interesting paper we found, though linked GitHub repo's noted there are some inconsistencies with the paper
** [https://docs.google.com/document/d/1LAEbFm8w2MzErkDiH4zPynyJY-T1snC1I0LQxd3svVU/edit?usp=sharing Current outline], which is just a pretty messy dump of info right now, but there is a pseudo-outline at the bottom of the doc, and we will be able to clean it up once we have a better gauge of what results we have/need later in the semester

'''Team Meeting Notes'''
* Our subteam logistics: we’re like a subteam of the subteam — Stocks (Paper)
** We’ll go to some of the Monday team’s meetings, and offer guidance/help when needed
* Space for PACE-ICE: /storage/home/hpaceice1/shared-classes/materials/vip/AAD
* The paper we found does not look too reputable, maybe try looking at its sequel
* For next week:
** Figure out which paper we’re using and think about how long it’ll take to reproduce their results: set a hard cap with some buffer as to how much time we’ll invest on this (don’t want to get too caught up in this)
*** Ideally we can reproduce the results and use EMADE to beat out these results (then there’s a clear path to paper)
** If we can’t reproduce the paper, then we’ll just try to beat out their results using our own methodology, which will still (hopefully) be good enough for a paper)
*** The big sticking point here is for this to be viable we still have to mimic the data preprocessing done by the paper, but worst case we use the paper from last semester and we’ve done a pretty good job replicating their data preprocessing

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Draft outline for paper 
|Complete 
|Sept 3, 2021
|
|Sept 17, 2021
|-
|Review last semester's work 
|In progress 
|Sept 10, 2021
|
|Sept 23, 2021
|-
|Notebook self-eval 
|Complete 
|Sept 10, 2021
|Sept 17, 2021
|Sept 17, 2021
|-
|Decide on a paper
|In progress 
|Sept 17, 2021
|
|Sept 23, 2021
|-
|Begin replicating the paper
|In progress 
|Sept 17, 2021
|
|
|}




==Sept 10, 2021==
'''Team Meeting Notes'''
* Add images by cloning wiki locally, creating a folder for your username, then add file online
* Self grade notebook by next week (don’t reupload Word file, just give total score and where points were lost)
* Dr. Zutty suggestions:
** Continue with outline so we get a good direction
** Most papers are not reproducible
** paperswithcode (website) to find a paper that gives code: try to find additional comparison points, but don't invest too much effort into this else we risk having another stagnant semester
** Can recruit non-time conflict VIP students to our subteam
** Try Pace-ICE for runs
* Spend the week getting reacquainted with what we did last semester, try to start outline, and maybe (if possible) get a better idea of what we can do for the semester to improve results (keep Spring paper, choose new one, do our own thing, etc)

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Draft outline for paper 
|In progress 
|Sept 3, 2021
|
|Sept 17, 2021
|-
|Pull CacheV2 
|Complete 
|Sept 3, 2021
|
|Sept 10, 2021
|-
|Review last semester's work 
|In progress 
|Sept 10, 2021
|
|Sept 23, 2021
|-
|Notebook self-eval 
|In progress 
|Sept 10, 2021
|Sept 17, 2021
|Sept 17, 2021
|}



==Sept 9, 2021==
'''Subteam Meeting Notes'''
* Attempted to pull CacheV2 into our branch but had some issues. I will try more today, but if that doesn't work we'll ask Dr. Zutty for help tomorrow.
** Update (9/10): I was able to execute the merge
* Discussed what we think should be the topic for our paper
** General topic: how autoML can benefit time series analysis
** Ideas of things to include:
*** Regression modeling in EMADE
*** Feature Extraction from Time Series Data (Technical Indicators, etc)
*** Implementation of the PLR pipeline from the previous paper
*** Monte Carlo approach
*** Discussion Section with comparative analysis of the TIs / other primitives
* Also discussed proposed direction for the semester:
** Get better results (right now we aren't matching the results from the paper, so we must first do that, and then try to beat them using EMADE)
** We are pretty concerned that we are setting ourselves up to not make that much progress by focusing too much on replicating last semester's paper, since we already tried to do that in the Spring and couldn't get it perfectly replicated because of confusing parts of the paper, so this is what we're thinking (but we'll talk to Dr. Zutty tomorrow and maybe change this):
*** We'll invest a light amount of effort to fully replicate the results of Spring's paper. If this works out, then we can use EMADE to (hopefully) beat those results and write our paper on that
*** If that doesn't work out, we'll try replicating a different, similar paper (one idea linked below) and do the same thing there
*** If neither of those work, then we'll probably just try using our own logic (not basing off of any paper) and see if our results can beat the papers we have looked at 
* Papers we're thinking about using for our research:
** [https://www.sciencedirect.com/science/article/pii/S1568494611000937#bib0230 Paper from Spring]
** [https://primo-pmtna03.hosted.exlibrisgroup.com/primo-explore/openurl?%3Fsid=Elsevier:Scopus&_service_type=getFullTxt&issn=15684946&isbn=&volume=38&issue=&spage=831&epage=842&pages=831-842&artnum=&date=2016&id=doi:10.1016%2Fj.asoc.2015.10.030&title=Applied%20Soft%20Computing%20Journal&atitle=A%20Takagi-Sugeno%20fuzzy%20model%20combined%20with%20a%20support%20vector%20regression%20for%20stock%20trading%20forecasting&aufirst=P.-C.&auinit=P.-C.&auinit1=P&aulast=Chang&vid=01GALI_GIT&institution=01GALI_GIT&url_ctx_val=&url_ctx_fmt=null&isSerivcesPage=true Another interesting paper]


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Draft outline for paper 
|In progress 
|Sept 3, 2021
|
|Sept 17, 2021
|-
|Pull CacheV2 
|In progress 
|Sept 3, 2021
|
|Sept 10, 2021
|}


==Sept 3, 2021==
'''Individual Notes'''
* Abhiram, Pranav, and I brainstormed some ideas of where we want to go this semester. We decided that we would like to take a research-oriented approach with an end goal of ultimately writing a paper.

'''Team Meeting Notes'''
* The time conflict team discussed the results of our brainstorming sessions and where each of us want to go for this semester
* At the very least, Abhiram, Pranav, and I will take the direction we proposed in our brainstorming session (to try and write a paper of autoML for time series data). We will also touch base with the non-time conflict stocks people to see what their plan is and whether they want to join us or if we will have two separate Stocks subteams.
** We can work in parallel to write the paper and get better results from EMADE
* The Modularity team will use Stock's pipeline so they can have a more complex problem to solve using their ADF methodology. The plan is for us to help them understand our code but still remain two distinct subteams. 
** My thought is that we will also take a look at their results at the end of semester to see whether their code is worth merging into our pipeline.
* For next week, we should write an outline for our paper, including the following sections: Abstract, Intro, Background (what’s been done in the field so far, general info about EMADE or autoML in general), methods (what we’re doing, our approach), results, conclusion and future work
** [https://dl.acm.org/doi/abs/10.1145/3449726.3463221 Anish's paper] (for reference) 
** Think about what experiments we are going to run to get the results we want
* We also need to pull CacheV2 into the stocks branch

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Brainstorm possible directions for this semester 
|Complete 
|Aug 27, 2021
|Sept 3, 2021
|Sept 3, 2021
|-
|Draft outline for paper 
|In progress 
|Sept 3, 2021
|
|Sept 17, 2021
|-
|Pull CacheV2 
|In progress 
|Sept 3, 2021
|
|Sept 10, 2021
|}

==Aug 27, 2021==
'''Team Meeting Notes'''
* Brainstorming session: goal is to get publishable results (and ultimately publish a research paper)
* Dr. Zutty suggested stocks and Modularity course conflict students work together to integrate their respective subteams’ work
* Another idea: create a validation dataset for stocks (probably unique stock in same time period as train/test data) and use that to show the cool algorithms we’ve built using autoML for time series data
* Abhiram, Pranav, and I might join the non-course conflict stocks subteam depending on how many people are already there, or just do our own sub-subteam (or join with Modularity) 

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Brainstorm possible directions for this semester 
|In progress 
|Aug 27, 2021
|Sept 3, 2021
|Sept 3, 2021
|}



=Spring 2021=
===Notebook Self-Evaluations===
* [[Media:Self-eval spring-2021.jpg|Self-eval]]
* [https://doi.org/10.1016/j.asoc.2011.02.029 paper]


==Apr 30, 2021==
'''Individual Notes'''
* I reviewed what I would say for the final presentation 
* Abhiram and I did a final run-through of the presentation

'''Team Meeting Notes'''
''Below are notes on each subteam's final presentation''

Our Presentation
* Be sure to study evolvability, focus on publishable results
** Statistical tests to study evolutionary ML in general using our case study (e.g. keeping everything constant, including two set objective functions and rotating out a third objective function and see which, if any, lead to a statistically-significant boost in how well the anchored two objective functions do)
* Visualization on slide 26 was well-done, can also look into how well or poorly certain indicators perform when combined with others (not just how well each primitive does on its own)
* Dr. Rohling: “I think you saw the [3] objective run AUC stagnate, yet the 4 objective did not stagnate.   This is likely because the solutions were creating ‘phenotypic diversity.’”

EzCGP
* Max pooling and drop out
** Significantly improved evolved architecture
** All individuals containing drop out had poor performance
* Dense Layer experiment: less overfitting, good performance 
** Issues: low diversity
* Study on crossover in CGP
** Process: Used problems in symbolic regression, measured fitness found after given number of fitness function evaluations, evolution was done using generational model
* Meta parameter search: implementing new way to run ezCGP (allows for small changes before runs)
* As mutation percentage increases, final individual fitnesses spread out
* Next semester: new mating methods, use existing CNN architectures, run full epoch if performance is above threshold

NLP
* Used PACE-ICE:
** Identical OS and resources
** Plenty of CPUs and GPUs
** Cons: 8 hour wall time, potentially long queue times, MySQL errors
* Documented code using Notion 
* Added Amazon product reviews dataset
* Used elapsed time of individuals as a metric for complexity on one run
* Takeaways: some questionable labels, struggles with longer reviews, will probably need  more model sophistication

Modularity
* Ongoing project has been allowing for more complex ARLs (increasing tree depth, allowing imbalanced trees)
* Potential objectives: precision score, recall score, F1 score, accuracy score, Cohen Kappa score
* Need more runs to determine effects of ARLs and comparing new architectures
** F1 scores may be correlated to Cohen Kappa
* Future work: new models, selection method, new dataset training

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Rehearse presentation 
|Complete 
|Apr 29, 2021
|Apr 30, 2021
|Apr 30, 2021
|}



==Apr 29, 2021==
'''Individual Notes'''
* Abhiram and I briefly discussed the results of our last run. What I noticed that I thought was very interesting was that the best individual was either identical or very similar to the best individual from one of our recent runs (the very good one with a CDF of .06).
** The best individual from this run (CDF = .12): Learner(MyBollingerBand(ARG0, 2, 61, falseBool), LearnerType('DECISIONTREE_REGRESSION', None), EnsembleType('SINGLE', None))

'''Subteam Meeting Notes'''
* Abhiram gave a quick briefing on our latest run: nothing crazy
** He seeded in the good Bollinger Bands individual, but had a CDF of .12 this time (double last time) since it didn’t do too well on XLP (and we didn’t have that dataset last run)
* Ran through [https://docs.google.com/presentation/d/1Ve_3G6xkq_y0QRFFXv0Mv7hAi-348cVPt48Zessnv_s/edit?usp=sharing our presentation] to figure out who’s doing which slides and what they will say
** I will do slides 1, 2, and 8

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Work on presentation 
|Complete 
|Apr 26, 2021
|Apr 30, 2021
|Apr 29, 2021
|-
|Rehearse presentation 
|In progress 
|Apr 29, 2021
|Apr 30, 2021
|Apr 30, 2021
|}



==Apr 26, 2021==
'''Individual Notes'''
* (Over the weekend) Abhiram and I discussed which stocks we should add to the dataset. We both like XLP (a Consumers Staples ETF) for its stability. I suggested VFH (a Financials ETF) to introduce some volatility (but nothing to  crazy since it's an ETF). Abhiram suggested SH (an S&P 500 short) for something almost completely downward trending. I concurred: we both think it'd be interesting to see how much EMADE can outperform random trading on a bearish stock and, while we don't expect an individual to make a profit, it would be interesting to see how much the best ones can minimize losses and maybe if a very good individual could pick up on the patterns and make a profit.
* I sent CSVs of both datasets to Abhiram, and he added them to emade
** We will use a train period of 2010-16 and a test period of 2017-2019 for both stocks
* Additionally, in planning how to execute the experiments in earlier entries (to investigate the effect of varying the quantity and complexity of seeds), I came to the conclusion that it would not be feasible to complete enough runs to be able to come to any sort of conclusions why still making it statistically-sound (maybe it would be possible if I did a few short runs, but what's the value in that?). As such, these experiments will be reserved for next semester when we have more time and can invest more resources into this endeavor.

'''Team Meeting Notes'''
* Final presentations this Friday 6-8:50, Stocks will go first
* Notebooks due Saturday 11:59 PM
* What we’re trying to show with the random experiment: emade has outperformed random trading by a statistically significant amount
* Dr. Zutty advised against using SH on our next run, since it's trend is so drastically different than our other datasets that it may skew our results

'''Subteam Meeting Notes'''
* I brought up my thinking regarding performing statistical experiments for the final presentation. The rest of the subteam agreed that we would not have enough time, and should just do this next semester.
* We’ll just include XLP in today’s run (we don’t want to change too many things between runs and make them incomparable; and SH might have an adverse effect on our results since it’s so much more bearish than any of our other datasets)
* Objectives we’re including:
** Average profit per transaction
** Variance of profit per transaction
** CDF
* Some of the first semesters created the backbone for our presentation. Everyone will work on their slides in the coming days.

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Think about possible experiments to include Final Presentation
|Complete 
|Apr 19, 2021
|
|Apr 26, 2021
|-
|Get new datasets
|Complete 
|Apr 22, 2021
|
|Apr 24, 2021
|-
|Work on presentation 
|In progress 
|Apr 26, 2021
|Apr 30, 2021
|Apr 29, 2021
|}



==Apr 22, 2021==
'''Individual Notes'''
* Began thinking about which experiments' results would be most interesting
* Thought that tinkering with the seeds would be interesting (e.g. see how the amount or complexity of the seeds would affect our results)

'''Subteam Meeting Notes'''
* Abhiram shared our best individual from past run, was interesting because of how simple it was and how well it performed
** [[files/random_experiment-2.png]]
** The individual: Learner(MyBollingerBand(ARG0, 2, 61, falseBool), LearnerType('DECISIONTREE_REGRESSION', None), EnsembleType('SINGLE', None))
* David W is looking into which TI tend to perform best and was looking for ways to test this. I suggested that instead of looking for a causal relationship (i.e. looking which TI’s cause the greatest increase in profit percentage) to look for a correlation (which TI’s tend to be in good performing individuals)
** Ex: could start out by querying the top 5-10% performing individuals and making a histogram of how frequently each TI appears in those individuals 
* Sriram shared his Fibonacci Retracement code with Abhiram, and he added it to emade
* We’ll probably use CDF as an objective function without using full profit percentage and without profit percentage variance on our next run
* We taught the first semesters how to add primitives to emade
* My tasks:
** Look for more datasets with wider timeframe and add them to emade
*** XLP (train 2010-16, test 2017-19)
*** Maybe a finance ETF if we want something a little more volatile?
** Start experimenting on how the number and complexity of seeds affects the performance of emade: 0 seeds, 50 seeds, 100 seeds; simple seeds (1-2 TIs) vs. complex seeds (many TIs)
*** In the future, we could also experiment which and how many objective functions perform best

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Think about possible experiments to include Final Presentation
|In progress 
|Apr 19, 2021
|
|Apr 26, 2021
|-
|Get new datasets
|In progress 
|Apr 22, 2021
|
|Apr 24, 2021
|}




==Apr 19, 2021==
'''Team Meeting Notes'''
* Final presentations on Apr 30 at 6PM (20-30 minute presentations)
** Should use material from stats lecture: either conclusive or inconclusive experiments are fine, but we should do some experiments to test the effectiveness of our runs
* Notebooks due night of May 1 
* Peer evals due next Tuesday
* Start thinking about what changes from our branch can be pushed onto Cache-V2
* Would be good if we can formulate the one stock optimization vs multi-stock optimization mini-experiment into a formal experiment
** i.e. when we were trying to determine whether emade would perform better if we only ran emade on one stock per time, we could try to conduct a formal experiment with that to come to a solid conclusion as to which is better 
* Next semester we might have more focused subteams: e.g. instead of having one big Stocks team working generally on optimizing AutoML for Stocks, we can have some people use the existing code as is and conduct many experiments to determine the effectiveness of the pipeline, how it performs best, etc
** i.e. the experimentation sub-subteam will freeze the existing code and then figure out whether the pipeline works best optimizing on one stock vs multiple stocks, bullish vs. neutral vs. bearish stocks, etc

'''Subteam Meeting Notes'''
* Sriram implementing Fibonacci Retracement
* Devesh continuing his experiment: working on smoothing the data to take more of a macro perspective than focusing on individual local extrema
* For today’s run, we will use the same eval functions as last run and add on CDF: profit percentage, average profit per transaction, variance of profit percentage, and CDF
* David will experiment with other evaluation functions and how to assess effectiveness of individual TIs
* We discussed increasing the window size: since our current dataset is fairly small, we will increase the window size from 30 to 40 (so we don’t make the number of windows too small), but on our next run when we have a larger dataset we will increase our window size further 
* Abhiram and I helped Sriram finish implementing the Fibonacci Retracement TI (mainly helping him with including lines to reduce errors and debugging small mistakes here and there)

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Think about possible experiments to include Final Presentation
|In progress 
|Apr 19, 2021
|
|Apr 26, 2021
|}


==Apr 15, 2021==
'''Individual Notes'''
* Reduced errors on MACD and RSI ([https://github.gatech.edu/rbhatnager3/emade/commit/229c4052f3ed86ec3f2ef2a3ca2cbe4933a10fbc commit])

'''Subteam Meeting Notes'''
* Kartik and Max coded eval function that compares individuals’ profit percentage compared to random trading (with the same amount of trades as the individual)
** Calculates CDF where 0 means individual is worse than all random experiments, .5 means individual is at the population mean, and 1 means individual outperformed all random trading experiments 
* Sririam suggested adding a couple of TIs: Fibonacci Retracement, Ichimoku Kinkō Hyō technical analysis (family of indicators)
* Devesh is trying to write code to analyze the lag between buy and sell points in individuals
* Abhiram ran quick experiments and found that running one stock at a time didn’t get much different results compared to when we run all the stocks together (as separate folds)
** He only tried this with Apple, so he’ll try with Verizon or Boeing next  to verify Apple wasn’t an anomaly (also because Verizon and Boeing typically don’t perform well on the conglomerate model)
* David wasn’t able to make the meeting but shared research about possible TIs to implement: Stochastic RSI, Aroon, VWAP, VWMA

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Improve error checking
|Complete
|Apr 12, 2021
|
|Apr 15, 2021
|}



==Apr 12, 2021==
'''Individual Notes'''
* Abhiram and I discussed how we can minimize errors:
** MACD error (type check parameters)
** Non-finite data (all traces include RSI, so will look into that)

'''Team Meeting Notes'''
* Pull latest CacheV2 and push onto our fork for bug fixes
* Once we solidify our pipeline, we can run experiments to determine the effect of various variables on our results, e.g.:
** Seeding (how much do the seeds help/hurt our runs): can try adding more/less seeds, more/less detailed seeds, or even no seeds
** Dataset: compare results when we add/remove some stocks (i.e. more/less folds) or use a longer/shorter timeframe on the dataset
** Evolutionary parameters: changing population size, etc. 
* Might be time to start adding more datasets, including ones for a longer period of time

'''Subteam Meeting Notes'''
* Will explore expanding the dataset (use more stocks, wider timeframe)
** Maybe include timeframes in different periods 
** Another Idea: running EMADE on one stock at a time (maybe we haven’t been getting good results because it’s too difficult to create a generalizable model)
* Abhiram created code to visualize how an individual tree’s results compare to Max’s random trading experiment 
[[files/z-score.png]]
** Red line in the image indicates the profit percentage of the individual 
* Tasking:
** Literature Review / TI group: research more TIs we can use; other datasets we can use
** Individual Analysis: z-score profit percentage evaluation function; add more metrics for comparing individuals
** EMADE implementation (me): determine cause of common errors in latest run and fix them (where possible)

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Improve error checking
|In progress
|Apr 12, 2021
|
|Apr 15, 2021
|}



==Apr 8, 2021==
'''Individual Notes'''
* (Yesterday) I fixed the seeds to reflect some primitives parameters having changed in the transition to ta-lib. I also fixed our ''delta_stoch()'' primitive to return the proper values. ([https://github.gatech.edu/rbhatnager3/emade/commit/a018b6f47ac5210803a49b81139d5447ef7beed7 commit])
* Abhiram and I discussed the possibility of introducing subgroups to our subteam. We'd still meet as usual, and these groups wouldn't be mutually exclusive, but each subgroup would just have more well-defined tasks (and this way we can assign tasks to subgroups instead of every individual person, which should be more efficient)

'''Subteam Meeting Notes'''
* Devesh suggested we try to implement an evaluation function that determines how close our buy and sell points are to the nearest local max and min points
** Interesting idea, not sure exactly how we would implement it
* Set up first semesters with colab and our SQL server
* Max’s random experiments found that we didn’t do that much better than random trading besides on AUO
** Potential solution: instead of having a profit percentage evaluation function, we compute the z-score of individuals’ profit percentage so we can normalize it relative to random trading (because a given profit percentage is  more impressive on some stocks compared to others, so this will give an unbiased way for emade to compare stocks)
* The idea of splitting up the team into subgroups seems to be well-received. The groups will be:
** Literature review, research, and TI implementation
** Data analysis (of EMADE runs, individuals, etc)
** EMADE implementation 
* I will work freely amongst all subgroups, but focus my efforts on the EMADE implementation group 

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Fix seeds for TA-lib primitives
|Complete
|Apr 1, 2021
|
|Apr 7, 2021
|}


==Apr 5, 2021==
'''Individual Notes'''
* Over the past couple days, I tied up many loose ends with ta-lib:
** (Yesterday) Finished unit tests for ta-lib primitives and [https://github.gatech.edu/rbhatnager3/emade/commit/93928cf65a2890cea00b4920a4563d724aa2fdc6 pushed code]. This commit also includes the formal primitives themselves (this commit added them emade).
** Fixed the MACD primitive on [https://github.gatech.edu/rbhatnager3/emade/commit/7dfaf7f6a6ce19a10d000e37952bf6578931d704 this commit] (previously, it would cause errors by returning NaN on small inputs)

'''Team Meeting Notes'''
* Good task to look into: analyzing individuals separately (like we did in midterm presentation)

''Stats lecture notes:''
* Using the various metrics we collect, we can do A/B: make a baseline run, then make a change and do another run (will help determine whether that change directly affected the results)
* Statistical measures: mean/expected value, variance, sample variance (used to correct for bias), standard deviation
* Hypothesis testing:
** We want to compute a probability of observing a sample at least as extreme as ours given our assumption of the underlying truth: P(sample | hypothesis), called p-value
** Significance value alpha usually set at .05 (the higher this value, the higher our confidence in our result)
** Beta parameter
** Can perform a t-test, which tells us notionally how far we are from the mean by steps of estimated standard deviation (standard error)
** One- vs two-tailed tests: are we trying to see if our value is greater/less than the population mean, or do we want to test equality?
*** We generally want to do a two-tailed test
** Our t-score must be greater than the t-table value to reject our null hypothesis
** Can use scipy to compute p-value from t-statistic
* We usually don’t know the true mean and standard deviation
** Can use Welch’s t-test (a.k.a. unequal variance t-test)
*** Computes the probability that two trials came from the same distribution
*** Gives us the t-statistic
** Can use scipy again to find the probability we would see these results again
* How to design experiments
** Greater t-value, the more unlikely the chance of observing the sample given the hypothesis
** If we want strong results, we either need differences in sample means that overwhelm the sample variance or a high number of samples to reduce the effect of the variance

'''Subteam Meeting Notes'''
* Could maybe conduct Welch’s test on indivduals’ profit percentage on various stocks
** Abhiram will run a small experiment to toy around with how we can use this

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Write unit tests for TA-lib primitives
|Complete
|Apr 1, 2021
|
|Apr 4, 2021
|-
|Fix seeds for TA-lib primitives
|In progress
|Apr 1, 2021
|
|Apr 7, 2021
|}



==Apr 1, 2021==
'''Individual Notes'''
* Finished coding ta-lib primitives. Only thing left to do before pushing them to emade is writing and running unit tests and fixing the seeds to reflect some primitives requiring different parameters.
* While doing this, I noticed that the existing Talib primitives: 
** Need documentation
** Some library calls do not use the time period parameters, intentionally?
** Need unit tests
* I don't know who wrote these, so I'll bring up these issues in the meeting

'''Subteam Meeting Notes'''
* First-semesters asked questions about the paper 
* I gave an update on ta-lib implementation 
* Max ran the random trading decision test: ran 10,000 experiments of randomly picking trading points and then alternating with buys/sells 
** Image: histogram of the results of the random experiment (profit percentages for each experiment)
[[files/random.png]]
[[files/random2.png]]
** Looks like AUO was the only stock for which emade substantially outperformed random trading
* Max will fix the issues I described above (in ''Individual Notes'')

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Implement TA-lib primitives
|Complete
|Mar 18, 2021
|
|Apr 1, 2021
|-
|Write unit tests for TA-lib primitives
|In progress
|Apr 1, 2021
|
|Apr 4, 2021
|-
|Fix seeds for TA-lib primitives
|In progress
|Apr 1, 2021
|
|Apr 7, 2021
|}



==Mar 29, 2021==
'''Individual Notes'''
* I began coding the ta-lib primitives, including integrating my earlier work into EMADE, updating documentation, and adding error mitigation tactics to my code
* I helped Abhiram make an [https://docs.google.com/presentation/d/1KIkFOwujn40HQH9y-SRVjWdLt03mrTGsESFNrLIzvFQ/edit?usp=sharing onboarding slideshow]

'''Team Meeting Notes'''
* Got our new team members
* Clarification on Monte Carlo approach: propose isn’t for EMADE evaluation, but when we have a stock that’s trending up, Monte Carlo could help us figure out what’s going on at peaks
** Right now we don’t have a baseline decision to base our results off  of, so we can take a coin flip approach to our trading decision at each time point to get how well we would do compared to a naive solution
*** i.e. at  each time point, we give some random chance on whether or not we take an action, and if we should take an action we take the next valid action (i.e. if our last action was a buy, then at the current time point we should sell, and vice versa)
** Could get the expected profit, variance, etc
** Will help us figure out if our model is just riding the trend of the stock or if we’re actually adding some value over a coin flip approach 
* Having competing objectives may help us get more diversity in our Pareto front
** Even if we have a select few objectives that we actually care about, adding more objectives could still help us get better individuals 

'''Subteam Meeting Notes'''
* Introduced our new members to our team: gave them more details on our goals, how we operate, what they'll be doing, etc.
** Abhiram and I gave the onboarding presentation (see ''Individual Notes''). I mainly talked about slides 4 and 5, focusing on how we give data to EMADE, how EMADE interprets our data, and how to write primitives  
* Maybe look into using TIs on an aggregate of stocks

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Implement TA-lib primitives
|In progress
|Mar 18, 2021
|
|Apr 1, 2021
|}



==Mar 25, 2021==
'''Individual Notes'''
* Abhiram and I discussed a change he made to EMADE to decrease errors: primitives will now be assumed to be in STREAM_TO_FEATURES mode ([https://github.gatech.edu/rbhatnager3/emade/commit/c41b8f28998c8abc2bd099f81e9979634bc9fa1b commit])

'''Subteam Meeting Notes'''
* Abhiram discussed the changes he made (see above)
* Discussed the Dr. Zutty's suggestion for adding Monte Carlo simulations as a baseline comparison to see how well an individual performs relative to random trading
** Interesting idea, but just comparing an individual's profit to the profit that would be made from a buy-and-hold strategy should achieve the same end goal but would probably be easier to implement 
* Discussed alternative fitness functions, since number of transactions and MAE don't really fit our goals 
** Abhiram suggested average profit per transaction and average variance per transaction
** Others will try and come up with other suggestions

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Implement TA-lib primitives
|In progress
|Mar 18, 2021
|
|Apr 1, 2021
|}


==Mar 22, 2021==
'''Individual Notes'''
''Note: includes some work from over the weekend''
* We created [https://docs.google.com/presentation/d/1xvS6nfHNZ9N56m4cDXoD4KzHeRoKkhfhsfIDHpysOUE/edit?usp=sharing our slideshow] for the presentation 
* I worked on and rehearsed my slides (slides 1, 2, 10, and 16)
* Abhiram and I cleaned up the presentation as a whole

'''Team Meeting Notes'''
''Notes on presentations:''
Our Presentation
* Idea I had: VZ and UMC could have had negative profit percentage because price trends downwards over almost the entire timeframe, so it’d be hard to make a profit without shorting or extending the time window
* What went well:
** Including specific individuals and analyzing them
* Suggestions:
** Monte Carlo decision-making for buy/sell decisions at the decision points (see if that performs well)
*** Paper used a random walk strategy
** Try to figure out why our best individuals are not using the TI  primitives
** Fix the “evolvability” of our primitives: how easy it is to get from one good solution to another
*** Ex: protect primitives from erroring as much as possible

Bootcamp 1
*  Feature selection methods used:
** Univariate selection 
** Feature importance
** Correlation matrix with heat map  
* ML Pareto front AUC:  .182
* MOGP Pareto front AUC:  .111
* 38 Pareto individuals in final generation of EMADE
* EMADE AUC: 2535
* Higher accuracy with emade

ezCGP
* Different framework, including not using DEAP, reusing nodes, active vs inactive nodes, fixed genome length, custom primitives and datatypes
* Objective: recreate CIFAR-10 results without relying on transfer learning
* CIFAR-10 experiment validation accuracy: 47.8%
** Training accuracy (97.2%) suggests overfitting —> plans to incorporate more primitives to make individuals more like vanilla CNNs

Bootcamp 2
* ML
** Models used: Support Vector Classifier, Random Forest, NN, Logistic Regression, Xgboost, Passive Aggressive Classifier
** AUC: .186
* MOGP
** Objectives: minimize false positive and negative rates 
** Mate: CXonePoint
** Mutate: mutUniform
** AUC: .127
* EMADE
** AUC: .245

Modularity
* Advantages: improve EMADE’s search optimization
* Using Sphinx for automatic documentation 
* In progress work:
** Time complexity analysis
** Improved documentation
* Concern: ARLs are inherently convergent
* Want to store ARLs in tree form instead of lambdas
** Allows for more complex operations between ARLs
* Future work
** Diversity measures
** Modify or expand upon selection method
** Optimize ARL genetic process
** MNIST experiments

Bootcamp 5
* ML:
** Algorithms used: KNN, NN, Decision tree
** AUC: .238
* GP:
** Primitives: add, subtract, multiply, sin, sigmoid, power of 2, cos
** AUC: .131
* EMADE
** AUC: .237

NLP
* NNLearner: similar to emade’s learners
* PACE-ICE for standard environment, free GPU
** Cons: databases errors, capped time, learning curve
* Documentation with Notion
* Starting using Amazon Product Reviews dataset for binary classification 
** 90% train, 10% test
** Balanced
* Got 1 non-seeded run but baad results —> emphasizes need to seed NNLearners 
** Seeded run got same results, with NNLearners failing (first priority is to fix this)
* Working on adding PyTorch functionality 
** Obstacles: transitioning from Keras to PyTorch
* Goals:
** Complete runs with NNLearners on Amazon dataset
** Remove frequently-failing individuals from runs

Bootcamp 3
* ML
** Algorithms used:  Gradient Boosting, Random Forest, Decision Tree, KNN, AdaBoost
*** Certain members focused on  FP vs FN
* MOGP:
** Mutation: mutUniform
** Mating: CXOnePointLeafBiased
** AUC: .11
** 448 pareto individuals 
* EMADE:
** Objectives: FP, FN (not tree size)
** AUC: .254
** 31 Pareto individuals 
* Could have gotten better emade results with stronger computer or more time

Bootcamp 4
* ML
** Used many boosting algorithms. Also used KNN, Random Forest, Logistic Regression, SVM, NN
** AUC: .294
** Accuracy (CatBoost): 84%
* GP
** Primitives: add, subtract, multiply, negative, max, min, square, sin, cos, tan
** AUC: .176
** Accuracy: 79%
* Emade: 
** AUC: 2200
** Using preprocessed data, AUC was down to 1838
** Modifying evaluation functions and using preprocessed data, AUC was .18

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Midterm Presentation slides
|Complete
|Mar 15, 2021
|Mar 22, 2021
|Mar 22, 2021
|-
|Implement TA-lib primitives
|In progress
|Mar 18, 2021
|
|Apr 1, 2021
|}


==Mar 18, 2021==
'''Individual Notes'''
* I began coding methods using the ta-lib functions to replace our current primitives. This [https://github.gatech.edu/rbhatnager3/emade/commit/2429d5011bce04869d7cbb78afeca87b24a86135 commit] includes a lot of work referenced in earlier entries (and some work from today):
** ''stock_methods.py'': bug fix on evm primitive (there was incorrect type checking that would throw an error on a valid input)
** ''talib_methods.py'': main functions are to acquire dataset used for testing and calculate ta-lib's values for TIs on this dataset, which is used to compare to the values calculated by our current primitives 
*** This is also the file where I began drafting how our new primitives would look once they used ta-lib instead of our own calculations
** ''ta-lib_compare.ipynb'': notebook I used as a sort of "playground" to look at exactly what we're getting from ta-lib to see what's relevant, what should be excluded, how the important stuff should be used, etc. I also used this notebook to compare the results of what ta-lib returned and what our primitives returned for the same data. For many indicators, there was little to no difference (SMA, EMA, and Bollinger Bands each had a 0% difference between our primitives and ta-lib), but for some there was a significant difference. We chose to go with ta-lib's results since it would probably be more reliable. For example, there was a 72.9% difference for RSI, but this process also led me to realize that our RSI was incorrect, as it returned 298.6 for this dataset, but RSI is supposed to fall between 0 and 100).
* Next step is to implement the methods in ''talib_methods.py'' as formal primitives (i.e. add them to EMADE's pset, code unit tests, update documentation and seeds to reflect changes, etc.)

'''Subteam Meeting Notes'''
* I discussed my work (see above)
* We discussed plans for the upcoming Midterm Presentation, a couple of people will create the backbone of our presentation and then everyone will fill in the slides that they will present

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look into TA-lib
|Complete
|Feb 25, 2021
|
|Mar 18, 2021
|-
|Complete Midterm Presentation slides
|In progress
|Mar 15, 2021
|Mar 22, 2021
|Mar 22, 2021
|-
|Implement TA-lib primitives
|In progress
|Mar 18, 2021
|
|Apr 1, 2021
|}



==Mar 15, 2021==
'''Individual Notes'''
* The biggest thing to note from our emade run was that a lot of individuals did not evaluate because they were using the incorrect mode (i.e. not stream-to-features). Abhiram and I did not know what could be causing this, so we'll ask Dr. Zutty in today's meeting
** Update (later today): solved (see Subteam Meeting Notes below)

'''Team Meeting Notes'''
* Midterm grades in progress
* Midterm presentations next week
* Abhiram explained issues with our emade run
** We’ll look into these issues in more detail in breakout session

'''Subteam Meeting Notes'''
* Dr. Zutty’s suggestions:
** Datatype affects how data is evaluated: in EMADE.py on line 1506, we need to check that there is a stream-to-feature primitive in the tree for it to be valid ([https://github.gatech.edu/rbhatnager3/emade/commit/f93eb5178430d6ddef41609d2aeb63e3b4d14f12 commit to fix])
** Queried database to look for odd errors:
*** <code>select * from individuals where error_string is NULL;</code>
**** Runtimes aren’t outrageous
*** <code>select * from individuals where tree like ‘%Learner%MyBollingerBand%’;</code>
**** Errors seem pretty standard
*** <code>select * from individuals where error_string is not NULL order by elapsed_time DESC;</code>
**** Looks for errored trees that take the longest to run
**** Most of our bad trees are taking less than two seconds to stop evaluating
** Suggestions:
*** Use multiple objectives (shoot for 3): could be tree size, MSE and MAE, or number of transactions 
*** Seed more individuals
*** Try cutting down population, launch, and queue sizes
*** Let emade run for more time
* Revised profit percentage calculation (including removing transaction cost), still confused why we’re getting different threshold values than the paper. We’ll try to find the issue but if not, we’re getting good enough results using our thresholds so we’ll move forward with those.

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look into TA-lib
|In progress
|Feb 25, 2021
|
|Mar 18, 2021
|-
|Complete Midterm Presentation slides
|In progress
|Mar 15, 2021
|Mar 22, 2021
|Mar 22, 2021
|}



==Mar 11, 2021==
'''Individual Notes'''
* Began to make sense of TA-lib and compare to our primitives (see commit discussed in March 18 entry, Individual Notes section for more)
* Pivoted focus towards getting TA-lib to work on colab, but Abhiram resolved issue while I was working on it

'''Subteam Meeting Notes'''
* We calculate profit differently than paper (bc we don’t account for taxes or transaction fees), which may explain why we have very different optimal thresholds than the paper
** Found what appears to be a typo in the profit percentage equation: it looks like taxes and transaction fees were swapped
** Decided to just not account for taxes: would make it too complicated and this just scales down profit anyways, so shouldn’t affect our results
** We will, however, account for transaction fees
*** Update (3/15): we removed the transaction fee from our calculation since it complicates the equation and we should not be executing enough trades for it to have a substantial effect on our results
* Made a database for our emade run and added configuration to template file
* We will run EMADE tomorrow morning


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look into TA-lib
|In progress
|Feb 25, 2021
|
|Mar 18, 2021
|}


==Mar 8, 2021==
'''Individual Notes'''
* I tried to find EPISTAR data but could not. We will move forward without this data (and just use the other five stocks mentioned in the paper).
* I continued working with ta-lib, including finishing building out the function calls for the necessary TIs, beginning to make sense of some of the return values that are distinct from what we currently have, and beginning to write code that can compare the results of the library with our primitives

'''Team Meeting Notes'''
* Dr. Zutty agrees with our decision to move on with using our own methodology to find the optimal threshold instead of investing a lot of effort into replicating the paper’s GA (we just have to make sure that however we do it, we get a consistent, reproducible result)
* Should try to get an emade run as soon as possible
* Midterm presentations in two weeks

'''Subteam Meeting Notes'''
* Kartik looked into TIs with less lag as well as leading indicators: some of them we already have (RSI, WILL %R, OBV, and stochastic oscillator), and others would require too many changes (e.g. larger windows on the time series)
* Abhiram implemented a simple model to find the optimal threshold
* He also prepared the xml and datasets: we should be ready to run EMADE on Thursday
* Interesting change to look into: increasing probability of mating/mutation, especially for successful individuals (so as to more quickly get rid of the bad individuals emade creates at the start of the evolution process)
** We will first try increasing the chances of crossover (I will contact Dr. Zutty about the best ways to do so)
* I’m going to try to finish up with ta-lib and write unit tests if necessary (existing test may be sufficient)


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look into TA-lib
|In progress
|Feb 25, 2021
|
|Mar 18, 2021
|-
|Try to find EPISTAR data
|Complete
|Mar 4, 2021
|
|Mar 8, 2021
|}


==Mar 4, 2021==
'''Individual Notes'''
* Fixed Taiwan stocks' datasets using Alpha Vantage ([https://github.gatech.edu/rbhatnager3/emade/commit/a5bb8bf820c36e3690bf2f31e059bc16b8ae3a94 commit])
* Krithik asked for help with how to implement indicators, so I hopped on a call with him to help him out
** We'll need to include high and low price data in our dataset for one of his indicators
* Spent a lot of time debugging the issues with my test file for the ta-lib library. I realized that I was getting import errors because I named my file talib.py, so python was trying to import functions from my file instead of the library.
** After figuring this out, I began to write code that would replace our existing primitives

'''Subteam Meeting Notes'''
* Resolution for threshold problem: we’ll just find the optimal threshold using our own method and move on towards running emade (the paper uses a GA to find the optimal threshold, we’re just going to use our own ideas towards doing so because we can’t figure out exactly how the paper does it, this way we can move on to other tasks)
* We explored the possibility of adding statistical values to our dataset. For example, we may look to add average volume, which does not need to be calculated (it's always going to be the same), so we'll probably just create a column that has the same value for every day. This is not an immediate need (still just an idea), so we'll have more concrete implementation/use-case ideas if/when we decide to use this.
* We will start trying to finish up adding TI primitives in the next week or two. In order to maximize efficiency, we will emphasize adding TIs included in ta-lib (though we will still need to code some for ourselves, especially volume indicators as ta-lib is lacking in these)
* My tasks:
** Try to find EPISTAR data using [https://www.tej.com.tw/ Taiwan Exchange Journal]
** Continue with ta-lib (replace current primitives with ta-lib implementation)


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look into TA-lib
|In progress
|Feb 25, 2021
|
|Mar 18, 2021
|-
|Fix Taiwan datasets
|Complete
|Mar 1, 2021
|
|Mar 4, 2021
|-
|Try to find EPISTAR data
|In progress
|Mar 4, 2021
|
|Mar 8, 2021
|}




==Mar 1, 2021==
'''Individual Notes
* I tried playing around with the ta-lib library
* I kept getting import errors whenever I tried to do anything with the library, even when I copied-and-pasted code from the documentation 
* Eventually I found out that running the same code from the emade top-level directory (instead of testCode/stocks), works (didn't get any import errors and was able to calculate SMA for our AAPL train data)
** I found this out pretty late, so I'll need to look more into this over the next couple days 

'''Team Meeting Notes'''
* Abhiram updated the team: we're mostly continuing our tasks so we can do a run of emade 

'''Subteam Meeting Notes'''
* There were some general updates of progress with everyone's tasks, including that PLR and exponential smoothing should be good to go
* There was some general confusion as to the purpose and use of the genetic algorithm in the paper. Namely, we are not sure what’s being modified, how the threshold is being represented, etc. As a general task, people without other tasks should try to look into this, including figuring out the evolution process used.
* My main tasks:
** Run the Alpha Vantage script on the Taiwan stocks
** Continue working with ta-lib

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look into TA-lib
|In progress
|Feb 25, 2021
|
|Mar 18, 2021
|-
|Fix Taiwan datasets
|In progress
|Mar 1, 2021
|
|Mar 4, 2021
|}



==Feb 25, 2021==
'''Individual Notes'''
[[files/Data script.png|thumb|400x400px|Main code used to get daily stock data from Alpha Vantage]]
* I was able to get data that lined up with that from the paper using the Alpha Vantage API ([https://github.gatech.edu/rbhatnager3/emade/commit/930b9fff585f1398d522897fcb6ab89dd2bdd674 commit])
** Created reusable script so we can easily get alternate data (other tickers, different time ranges, etc) if necessary in the future
** Created notebook to check Alpha Vantage's data: trends seem to match those in the paper; though exact close prices may be slightly off (based on eyeballing the graph in the paper), but values are in the same general range (definitely an improvement over Yahoo Finance's data)
** Notably, AV's close prices are what align with the paper, not the adjusted close. This is not what I expected, but the close values seem to consistently line up with the paper. [[files/AAPL closes.png|left|thumb|350x350px|AAPL closes from Alpha Vantage]][[files/AAPL data.png|center|thumb|350x350px|AAPL close prices from paper]]

'''Subteam Meeting Notes'''
* Everyone updated the subteam on the progress of their tasks. Most people were generally able to resolve the roadblocks 
** Kartik was able to make sense of and fix the PLR code
** Abhiram and David learned exponential smoothing and seem set to implement it in EMADE
* Max found [https://mrjbq7.github.io/ta-lib/index.html an interesting library] that can calculate TI values based on given data
** Him and I will look into how we can implement these in place of our current primitives


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Fix US datasets
|Complete
|Feb 22, 2021
|
|Feb 25, 2021
|-
|Look into TA-lib
|In progress
|Feb 25, 2021
|
|Mar 18, 2021
|}


==Feb 22, 2021==
'''Individual Notes'''
* Abhiram was able to fix the PLR code and asked if I could review it (in [https://colab.research.google.com/drive/1EtaQwCV_luXwZWII9NnR2HVjFFJcb-pm#scrollTo=6EBGYntrk4zN&forceEdit=true&sandboxMode=true this notebook])
** For some reason, when Abhiram ran the code it ran properly, but when I went to run it there was an error (even though I did not change any code). When I found this, Abhiram re-ran the code and got the same error.
** I debugged the code and found that the stockBuySell() method would return an empty list whenever it received a price list in descending order. Abhiram said he was planning cleaning up the notebook without this method anyways.

'''Team Meeting Notes'''
* Main focus for this week: preparing labeled dataset for EMADE run

'''Subteam Meeting Notes'''
* Abhiram said papers that cited our main paper helped explain how the trading signals are calculated: the points at which we calculate our trading signals should be the local max/min of the PLR (not every critical point)
* Lower thresholds lead to gaps in the trade signals, but we're not sure why. Me and Kartik will take a look at it.
* Issues with our AAPL, BA, VZ price data being inconsistent with what the paper shows in figures nine, ten, and eleven. Our best guess is because of stock splits and inconsistent calculations of the adjusted close price between the paper and Yahoo Finance. We'll see if getting our data from other sources solves this issue.
* Youssef, Krithik, and Kinnera will work on documentation and adding primitives
* David will look into exponential smoothing
* Kartik will continue to look at the PLR code
* I will fix US stock data (will try Alpha Vantage)

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look at PLR code
|Complete
|Feb 15, 2021
|
|Feb 22, 2021
|-
|Fix US datasets
|In progress
|Feb 22, 2021
|
|Feb 25, 2021
|}



==Feb 18, 2021==
'''Individual Notes'''
* I updated the US datasets so there were separate files for the train/test sets ([https://github.gatech.edu/rbhatnager3/emade/commit/2fd4f957cb8a373aff2705432f2a9918f9e42f5d commit link])
* I looked Abhiram's PLR code but could not find any errors. I will take a deeper dive before Monday.

'''Subteam Meeting Notes'''
* We further discussed the paper and one of its related papers (one common author and a citation of our main paper, [https://doi.org/10.1109/TSMCC.2008.2007255 link here])
* We tried to make sense of the methodology and see how we could fix the PLR code, but there was some common confusion with what was going on in both papers. It brought about similar feelings from last semester's paper, so we are going to spend the weekend trying to make better sense of the paper and then evaluate whether we should move forward with it. Dr. Zutty plans on reading the paper by next Monday, so we will see if he can help us out. Also, Krithik will begin looking for another paper in the event that we determine we should pivot our focus away from this one.
* I will continue to debug the PLR code


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Fix US datasets
|Complete
|Feb 15, 2021
|
|Feb 18, 2021
|-
|Look at PLR code
|In progress
|Feb 15, 2021
|
|Feb 22, 2021
|}


==Feb 15, 2021==
'''Individual Notes'''
* I read the paper ([https://doi.org/10.1016/j.asoc.2011.02.029 link]) we are planning on basing our research off of. 
* Some major differences compared to last semester’s paper (outside of obvious things like a different model) that I noticed:
** Uses multiple different time period for most TIs (e.g. instead of just using fifteen-day SMA, uses five-, six-, ten-, and twenty-day SMA)
** Uses a dynamic threshold for trading decision (other paper used a fixed threshold of .5 on their trading signal)
** Only uses one year of train data and six months of test data for each US stock, and close to three years of train data and about six months of test data per Taiwanese stock (I think the other paper used about five years worth of data)
* David downloaded the datasets for the US stocks before I had a chance to. I downloaded the data for the Taiwanese stocks which I could find on Yahoo Finance in [https://github.gatech.edu/rbhatnager3/emade/commit/9d8b7020f161f8980b32f58b606b8975beee24f8 this commit].
* Me and Abhiram discussed the dataset used by the paper. I expressed some reservations about the dataset being too short, and us not being able to learn enough based off of this. This may especially be true for the time period used by the paper (around the '08 recession), and since it uses train and test data splits in contiguous time periods. We decided that it would probably be best to initially use the paper's exact dataset for a one-to-one comparison, and then (assuming we get a good model from doing so) use our own dataset with a larger time period.


'''Team Meeting Notes'''
* Abhiram updated Dr. Zutty on the paper on which we are planning on basing our research for this semester
* Dr. Zutty had a couple of notes but for the most part said we could discuss further in our subteam breakout (see below for those notes)

'''Subteam Meeting Notes'''
* Dr. Zutty noted that there are two options for how we can deal with our data:
** Option A: Create individual Monte Carlo fold per stock, more scoped data, this tests the pipeline
** Option B: lump all stock data together and create random folds amongst the entire dataset, would yield a more generalizable model, this tests our model to find the rules of the market
* We’re planning on choosing Option A so that we can most effectively make trade decisions on a specific stock and create the best possible pipeline (option B seems too difficult to create a good model)
* We reviewed the changes Abhiram made over Winter Break to our primitives and considered whether we should add stream-to-stream primitives 
* To do:
** Split US stock datasets into separate train and test files
** Revise Abhiram’s PLR code (he couldn’t get the Euclidean distance to a point, so he just used vertical distance)
** Time permitting, look into exponential smoothing


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read paper
|Complete
|Feb 8, 2021
|Feb 15, 2021
|Feb 15, 2021
|-
|Get paper's dataset
|Complete
|Feb 11, 2021
|Feb 15, 2021
|Feb 15, 2021
|-
|Fix US datasets
|In progress
|Feb 15, 2021
|
|Feb 18, 2021
|-
|Look at PLR code
|In progress
|Feb 15, 2021
|
|Feb 22, 2021
|}



==Feb 11, 2021==
'''Subteam Meeting Notes'''
* I wasn't able to attend today's meeting, but I contacted Abhiram and he said to just read the paper and get the dataset used in the paper
* He also said people generally thought the paper was good and we could feasibly get it working soon

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read paper
|In progress
|Feb 8, 2021
|Feb 15, 2021
|Feb 15, 2021
|-
|Get paper's dataset
|In progress
|Feb 11, 2021
|Feb 15, 2021
|Feb 15, 2021
|}



==Feb 8, 2021==
'''Individual Notes'''
* I started looking into some new APIs we could use:
** [https://simfin.com SimFin] (suggested by Abhiram): seems to have everything we need, but free users can only get 12-month delayed data
** [https://github.com/ranaroussi/yfinance yfinance] (Python library): pulls data directly from Yahoo Finance
** [https://finnhub.io Finnhub] ([https://finnhub.io/docs/api documentation], [https://finnhub.io/pricing features]): looks like the best from quick review (has real-time price data; many TIs; financials; recommendations, price targets, etc. just to name a few features that could be of use)
*** Not sure if there are the following (probably) useful features: 
**** Historical price data (tons of other options if not in this library)
**** Number of outstanding shares (should be on balance sheet)
**** Tangible book value (should be on balance sheet)
* I also started learning some fundamental analysis:
** Book value = assets - liabilities
*** Stocks—especially banks—trading below their book value are pretty safe bets
**** Caveat: banks may also trade below book value when market expects loan losses (e.g. during COVID)
*** Associated indicator: price to book ratio
** Tangible book value = book value - intangible assets - goodwill
*** i.e. acquirable cash if a company sold off everything right now
*** Look for stocks that trade below tangible book value (this means that the market is valuing the company less than it's actually worth)
**** Can compare price to TBVPS (tangible book value per share) to price or total TBV to market cap (Either works: former is just the latter with both sides divided by number of shares)
*** TBVPS < price '''should NOT''' unilaterally be a huge buy signal: could either be indicative of a great investment or a broken company 
**** Accountants are better at valuing companies than the market, so the market may not always realize its undervaluing a company
**** Other factors that can lead to stocks trading below book value: high risk, poor prospects for growth, negative/low return on equity
** Equity: shareholders’ stake in the company
*** Total Equity = book value
** Can evaluate ETFs by evaluating the top 10 holdings
** Can use income statement and statement of cash flows to figure out where a company’s money is going: cash flows help us sort out the accounting jargon in the income statement 
** For metrics relative to shares, use ''total diluted shares''


'''Team Meeting Notes'''
* The direction we have right now seems really similar to last semester
* For whichever paper, compare it to last semester’s paper to ensure we’ll get better results this time around and we learned the right lessons to better accomplish our goals this semester
* Do self-evaluation by next week

'''Subteam Meeting Notes'''
* Kartik: unsupervised learning will be pretty difficult
* Kinnera shared three papers, but two used FOREX markets (and we want to stick to US). Third paper uses DNNs, but we might not want to do this since this isn’t yet fully integrated into emade (we’d have to work with NN subteam to update our fork, and this may be work than it’s worth)
* We discussed a couple of other papers. Most interesting was [https://doi.org/10.1016/j.asoc.2011.02.029 this one] suggested by Abhiram.
* We're all going to read Abhiram's paper and, if possible, try and find other papers to suggest


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Fundamental Analysis Research
|Complete
|Feb 1, 2021
|
|Feb 8, 2021
|-
|Read paper
|In progress
|Feb 8, 2021
|Feb 15, 2021
|Feb 15, 2021
|}



==Feb 4, 2021==
'''Subteam Meeting Notes'''
* Meetings on Thursdays at 5:30 seems to work for everyone
* Slow couple of days, most people are planning on doing their weekly work over the weekend
* Max will look into unsupervised clustering to find out how to treat trends
* A couple people will look for a new paper that we can base our research off of
* I will begin looking into how we can leverage fundamental analysis

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Fundamental Analysis Research
|In progress
|Feb 1, 2021
|
|Feb 8, 2021
|}

==Feb 1, 2021==
'''Team Meeting Notes'''

''Dr. Zutty gave us some suggestions based on our proposed direction:''
* He agrees we should move away from the paper we used last semester, but instead of completely straying away from heavily using literature, try to find a different paper
** Maybe look at the sources of the paper we used last semester
** Or we can look at papers that cited the paper we used
** Look for high citation count on Google Scholar
* Find a dataset that is commonly used for this problem
** Look for Kaggle competition datasets

'''Subteam Meeting Notes'''
* Seems like most people will be able to make Thursday at 5:30 PM for our weekly subteam meeting, but we'll need to confirm with Max and Joseph
* Goal for this semester: create a model capable of making a profit on test data
* Planned changes to dataset
** There’s a plan to include more securities in our dataset: instead of just using S&P, we are planning on trying to include other stocks/ETFs. Options on the table:
*** Blue-chip stocks in various industries
*** Sector ETFs/indices
*** Small cap stocks 
** We might try to go more granular than daily data: hourly or half-hourly
*** Doing this would make data more volatile, but it’d potentially help with lessening the impact of things TIs can’t account for, like company news, release of earnings reports, etc.
*** We could also look for abnormalities in volume data to account for these factors (e.g. technical analysis could not predict the huge spike in the prices of GME or AMC, but maybe we could infer something is going on based on the fact that their volumes also had a massive spike)
* Me and Abhiram will look into how we can make emade capable of performing fundamental analysis. A couple of necessary subtasks that come to mind:
** Become more acquainted with fundamental analysis and how we can automate it
** Being able to run emade on multiple datasets (so we can have price, volume, etc data and earnings reports, etc)
** Figure out how we can get the necessary data (maybe look into yfinance and related libraries/APIs for pulling earnings reports)

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Fundamental Analysis Research
|In progress
|Feb 1, 2021
|
|Feb 8, 2021
|}





==Jan 27, 2021==
'''Subteam Meeting Notes'''
* Intro meeting to discuss goals for the semester: ideally we would like to be able to make real-time trades (and build a model formidable to be worthy of doing so)
** Can use [https://alpaca.markets/algotrading Alpaca], which has a testing environment so we don't need to use real money
* Abhiram told us that over break he fixed our primitives: we had assumed that EMADE would give us all of the data, but Abhiram explained that instead we get a sliding window of the data (a list of lists). There are many different commits, but the updated file is [https://github.gatech.edu/rbhatnager3/emade/blob/stocks-experimental/src/GPFramework/stock_methods.py here].
* We expressed a couple of different ideas on where to go for the semester:
** We seemed to agree that we did not want to follow a paper as rigidly as we did last semester (it didn't go too well then, and now we also have a better idea of what we want to do, what we can do, how to do it, etc.)
** We might find another paper that looks interesting and (loosely) use it for ideas
** We'll definitely continue to add primitives to EMADE
** We'll continue looking for alternatives to the genetic labelling we used last semester
** If people have differing interests we might spilt into fluid groups temporarily 
* Not everyone could make the meeting, so we didn't make any concrete decisions (we'll do that on Monday when everyone should be in the meeting)

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Have intro meeting for subteam
|Complete
|Jan 25, 2021
|Feb 1, 2021
|Jan 28, 2021
|}


==Jan 25, 2021==
'''Team Meeting Notes'''
* Today was the first semester for the meeting, so we mainly went over logistical issues
* Abhiram and I expressed interest in continuing to co-lead the Stocks subteam

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Have intro meeting for subteam
|In progress
|Jan 25, 2021
|Feb 1, 2021
|Jan 28, 2021
|}

=Fall 2020=
===Notebook Self-Evaluations===
[[Media:VIP_AAD_notebook_rubric_self-eval.pdf|Self-eval 1]]


==Dec 2, 2020==
'''Individual Notes'''
* [https://github.gatech.edu/rbhatnager3/emade/commit/c3ec1cf7b0a1dfeef833b922ab373bce6f27a2a4 Updated default values and documentation]
** Changed default values to be invalid values wherever possible (e.g. for a TI with values ranging from 0-100, I made the default -1)
** Changed code to account for this (e.g. so an invalid -1 would be interpreted as a placeholder and not as a really low value)
** Updated documentation to reflect these changes 
** For EVM (which can be any real number), I made the default 0 (so it's a medium value and a little insight as possible is taken from it)
** Left MACD as is (current default is set to 10,000) because it can be any real number, but it doesn't have a clear medium (its values are relative to a security's baseline)
* Finished and rehearsed my slides (slides 1, 7, 8 on TI primitives in [https://docs.google.com/presentation/d/1arplCjluOGjVm58LiMHV2zVwXl0GCvCsvgb2Ou7nSN8/edit#slide=id.gadbc172287_0_95 powerpoint])
* Found some inconsistencies in how we are using some TIs vs. how they are supposed to be used.
** Mainly, some TIs are signals, but we're treating them as pure numbers.
*** Examples:
**** For MACD, the main insights are drawn when it crosses its signal line, and we do not account for that
**** For KD%, it is an important signal when the K% crosses the D%, but we treat them as independent numbers
*** Mine and Abhiram's idea: make a sort of “signal” primitive that acts as an alarm when these events happen
** Also, we added both the Stochastic KD% and the William's R% because both were included in the paper, but they are almost identical indicators (by both their math and their purpose)
** Will look into more over Winter Break


'''Subteam Meeting Notes'''
* We met right before the final presentations to run through our presentation 


'''Team Meeting Notes'''

''Final presentation notes:''

Our team (high-level feedback):
* Be sure to label all axes on graphs
* Give more context on details of paper
* Give mathematical formulas for TIs

NN Subteam
** Goal: evolutionary approach to neural architecture search 
** Paper used “Evolutionary Neural AutoML for Deep Learning”
** Multi-objective neuro-evolution
** Toxicity and X-Ray datasets
** Different objectives used: AUROC (paper) vs accuracy (NN subteam)
* NNLearner
** Layers are primitives, add Keras Layers to LayerList
* BERT: pre-trained language model
** BERTInputLayer: new primitive 
** Not included in current runs
* New mating and mutation functions
* Successfully able to do local style runs on PACE with CUDA enabled GPUs
* PACE issues, including with conda environments and space 
* New primitives:
** (CV) Adaptive Mean Thresholding 
** (CV) Adaptive Gaussian Thresholding
** (CV) Otsu’s Binarization
* Best individual (Toxicity): .0383 error
* Chest X-ray bounded box approach: YOLO NN architecture
** Only <1% data available with bounded box data
** Many diseases aren’t localizable to one particular region 
** Sacrifices accuracy for speed
* Best individual: ~46% accuracy
** Models were very simple 
* Chest X-Ray analysis, new hypothesis: EMADE starts off with small network and falls into local minima of guessing only one label
* Amazon Product Reviews: sentiment analysis dataset
** Foreign languages used, some use invalid characters: remove such reviews
* Future direction:
** Multi-task learning = good for multi-label problems
** Make BERT layer valid at any position
** More test datasets
** More complex adaptive mutation scheme
** Try coevolution

* '''<u>How this may help us</u>''': maybe to help with our incorrect use of using TIs as numbers as opposed to signals, we could create something analogous to NNLearner that can manage the TI primitives used, and look for signals/alarms in the primitives in a given individual


EzCGP
* Started using precision instead of accuracy
* Pip install works better than conda install with PACE-ICE GPU
* Genome seeding to work around PACE-iCE runtime constraints
* Initial results: .9788 precision, .975 recall, .977 F1
* State of the art results on CIFAR-10: 99.7% accuracy
** Next experiments: replicating results without transfer learning, new primitives 
* NAS Research code:
** Implementing super convergence, tensor flow cyclical learning rates
** Aging evolution (dying by old age)
** Early termination (terminate if current accuracies are worse than values by reference)
* Moving forward:
** Experiment on block structure (use similar primitives as with CIFAR-10 with MNIST)
** Continue neural architecture search experiment
** Explore ezCGP with EMADE



Modularity
* Current implementation:
** Search entire population for combinations of parent and children nodes
** Select some combination of nodes and abstract into a single node and add to pset as new primitive
* Current experiments:
** Differential Fitness
*** Only search individuals with positive differential fitness for ARLs
*** Takeaways of emade run:
**** Statistical significance in generations 16-19
**** Converged with baseline in later generations
** Alternate Selection Methods
*** Increases probability of getting ARLs, so new benchmark using runs with ARLs
*** Takeaways of emade run:
**** Statistical significance in generations 11-19
**** Converged with baseline in later generations
**** Bloat issues (some individuals had 40+ ARLs)
** Data Pair Restriction
*** Only allow for subtrees that return an EmadeDataPair to be used for ARLs
*** Takeaways of emade run:
**** No statistical significance
**** ARLs are more intuitively useful
**** Did not converge with baselines
**** Titanic dataset may be limited results
** Updated Selection 
*** Merged Data Pair Restriction with Alternate Selection Methods
*** Takeaways of emade run:
**** Not much bloat
**** Performs worse, but maybe due to smaller sample size
* Titanic and MNIST datasets, minimize false positives and negatives
** Compare to baseline without ARLs
* Overall takeaways
** Multiple runs have statistical significance in generations 10-20
** ARLs seem to be beneficial over baseline
* Future work
** MNIST
** Be sure to not to use trivial datasets 
** Evaluating diversity over time
** Integrating both ARLs and ADFs
** New heuristics
** Modifying creation method of ARLs
* MNIST performance
** About 80 valid individuals, 28 ARLs (about half are modify learners)
** AUC of about .088



'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finish assigned slide
|Complete
|Nov 30, 2020
|Dec 2, 2020
|Dec 2, 2020
|}


==Nov 30, 2020==
'''Individual Notes'''
* Me and Abhiram discussed an interesting finding from last emade run: most of the best-performing individuals did not include a TI primitive (even though we seeded individuals with those primitives)
** Best individual: Learner(StandardScaler(StandardScaler(ARG0, TriState.STREAM_TO_STREAM), TriState.STREAM_TO_FEATURES), ModifyLearnerFloat(LearnerType('SVM_REGRESSION', {'kernel': 0}), -1.9411225934041623, falseBool), EnsembleType('SINGLE', None))
*** Abhiram later changed the normalization primitives to only allow them to operate in FEATURE_TO_FEATURE mode because we now realize that STREAM_TO_STREAM and STREAM_TO_FEATURES don't make sense (we want to take in features and return normalized features)
** One idea we had was that since the TIs we've implemented so far are mainly just simple calculations off of the price data, emade could just be—in effect—"creating its own" TI using the price data that is just more effective than what we've included so far
*** Abhiram pointed out that since our TI features aren't independent of each other (they all rely on close prices), so emade could just be learning a calculation that circumvents needing to use each TI
* Took a look at Anshul's primitive and sent him revisions on Slack
* [https://github.gatech.edu/rbhatnager3/emade/commit/ff35a9caec76f1828cda96f73d79ecc4bdd051ed Cleaned up existing primitives] (mainly updated documentation and added max caps on periods)
* Kinnera, Abhiram, and I noticed that the first-semesters seem to be having trouble completing their tasks, so we started a Slack PM with them to check in on their progress, see whether they need help with anything in particular, and see whether they would like a special meeting devoted just to helping them. It seems like they made good progress on a primitive and just had a couple of lingering questions that we answered on the PM.

'''Team Meeting Notes'''
* Final presentations on Wednesday, will have about double the time we had for midterm presentations

'''Subteam Meeting Notes'''
* Discussed findings of latest emade run regarding individuals not using TI primitives (see '''Individual Notes''' above for more details)
* Will continue to work on presentation, try to meet on Wednesday before Team Meeting call to run through the slides
** I will do the slide on TI primitives
* Discussed plans moving forward, including plans to continue exploratory research over winter break and desire to continue with subteam next semester 

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Revise Anshul's, existing primitives
|Complete
|Nov 28, 2020
|
|Nov 30, 2020
|-
|Finish assigned slide
|In progress
|Nov 30, 2020
|Dec 2, 2020
|Dec 2, 2020
|}



==Nov 28, 2020==
'''Individual Notes'''
* Kartik and Max helped me finish up the emade run prep

'''Subteam Meeting Notes'''
* Mainly an organizational meeting, little slower than usual (less work/attendees because of Thanksgiving)
* I will help Anshul ensure he has properly wrote his primitive and touch-up documentation and implementation of our TI primitives

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Correctly add primitives to emade pset & emade run prep
|Complete
|Nov 17, 2020
|
|Nov 28, 2020
|-
|Revise Anshul's, existing primitives
|In progress
|Nov 28, 2020
|
|Nov 30, 2020
|}


==Nov 23, 2020==
'''Subteam Meeting Notes'''
* Dr. Zutty tried to fix issue with Abhiram’s normalization primitives, but they were unable to figure it out during the meeting
* Input csv for stream to feature should be of format: t,p,t,p,t,p,val (t=time, p=price, val=value to predict)
* According to Dr. Zutty, our genetic labeling approach is an “oracle” that may not translate to real applications very well, and the reason we’re seeing good results is because are trees are optimized on this oracle over this time period.
** To test if this is an issue, Dr. Zutty suggested that we take some data points, don’t use them for the genetic labeling notebook or for emade, and then our best emade individual over that time period to see how our results translate
** Update (11/24): Abhiram tried running our genetic labeling algorithm once on the normal time period, and once on the same period but shifted twenty days in the future. His goal was to see if the algorithm would produce similar trade signals at the same day, and it seems like they did:
*** [[files/Trade Signals.jpg|center|thumb|500x500px|Side-by-side view of trade signals two distinct time periods, one of which is offset twenty days in the future compared to the second. The graph lines up identical days vertically.]]
*** I suggested that Abhiram sees if these results hold for larger offsets, and we will do so once we start working with a larger dataset (which will happen when we start using our own dataset rather mirroring that used by the paper).
'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Correctly add primitives to emade pset & emade run prep
|In progress
|Nov 17, 2020
|
|
|}

==Nov 19, 2020==
'''Subteam Meeting Notes'''
* This meeting was mainly getting ourselves organizing and catching up the rest of the group on our individual tasks
** Most notably, Kartik said he will conduct an evaluation of our most recent emade run, after which we’ll analyze it as a group
** Kinnera, Tanishq, David, and maybe others will begin making unit tests for our existing primitives
** Others will continue implementing TI primitives, and then write a unit test for the primitive they coded
** Me, Abhiram, and Max will prep for next emade run (for which we will just pass in price data, and use the 6 TI primitives used by the paper ideally as stream-to-feature). My tasks for this preparation:
*** Change None returns so TIs return some default value
*** Cap periods of TIs so emade does not give some unreasonable hyperparameter (e.g. don't allow a 500-day SMA)
*** Make TI changes to correctly add them to emade (fixes described in more detail on 11/16 ''Subteam Meeting Notes'')
* I will also try to figure out the rule for calculating the trend signal
** In order to do this, I will see if I can find the time period used in the table of example data in our dataset (i.e. find the dates for these examples). Doing so will also help us figure out whether the MA in the table is over a 15- or 25-day period.
** This won't be necessary if we hear back from the paper's authors, but they haven't respond to us yet (Kartik emailed them on 11/16)


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Correctly add primitives to emade pset & emade run prep
|In progress
|Nov 17, 2020
|
|
|}



==Nov 16, 2020==
'''Individual Notes'''
''Pre-Meeting:''
* I tried to find a rule that could get us the trade signal as described the paper, but I wasn't successful

''Post-Meeting:''
* I worked on assembling the first csv that needs to be created for an emade run (see Subteam Meeting Notes below)
* While working on this, I discovered a couple of issues with our primitives, mainly that the output lists were not of the correct size for some primitives (some were longer than the input and some were shorter)
** I fixed the primitives and verified they still line up with the paper using my notebook from earlier in [https://github.gatech.edu/rbhatnager3/emade/commit/3d47fd2b56e10efd96663c1738388338c7b1ce74 this commit]
* I [https://github.gatech.edu/rbhatnager3/emade/commit/9639e72c35509c3cecbb6542c1f8ddd4b8bdf52a created a notebook] that assembles a Pandas DataFrame of the data required for the csv using the corrected primitives. This sets us up for an emade run once Abhiram trims out unneeded days and adds his genetic labels of correct trade decisions

'''Team Meeting Notes'''
* Midterm grades will be posted on Canvas shortly

'''Subteam Meeting Notes'''
* Try to find snippet of data from Table 1 in full dataset (can help us determine whether that table uses a 25- or 15-day MA, and maybe help us find the rule for trade signal)
** We can also try emailing authors about the equation/rule for finding the trade signal
* Remove None’s from TI returns (replacement can differ by TI, can be -1 for TIs that can’t be negative, but it's a design choice for other TIs)
* Want TriState.Stream_to_Feature, can also include Stream_to_Stream (as possible modes when adding primitives to registry)
* Write unit tests in place of placeholders (can find examples in GPFramework/UnitTests directory)
* Changes required to finish adding TI primitives to emade pset:
** Import stock method's registries and add them to pset in gp_framework_helper (important methods: ''addPrimitives()'' and ''get_primitive_registry()'')
** Standalone tree evaluator file: can be used to test our primitives 
*** Example tree that uses both the SMA and EMA (over 7-day periods): Learner(MyEMA(MySMA(ARG0, TriState.STREAM_TO_FEATURES, Axis.FULL, 7), TriState.STREAM_TO_FEATURES, Axis.FULL, 7), LearnerType(‘RAND_FOREST’, {’n_estimators’: 100, ‘criterion’:0, ‘max_depth’: 3, ‘class_weight’:0}), EnsembleType(‘SINGLE’, None))
**** ARG0 is a placeholder for EMADE data pair, which will contain price data that will be passed into SMA/EMA methods
**** This will use both SMA and EMA as features. If we wanted to find a 7-day EMA of the SMAs, then we would change the mode of SMA to TriState.STREAM_TO_STREAM (and keep the EMA as TriState.STREAM_TO_FEATURES)
* csv files we need to create for upcoming planned emade runs:
# Columns being price, 6 TI values (from paper), last column (to predict) is genetic trade labels
## Comment out lines that add TI primitives to pset in emade (we don't want to use these yet)
# Price and genetic trade label (using stream-to-feature)
## Comment out lines that add EMA and OBV (and other primitives not used by paper) to pset
# Price, volume, and genetic trade label (using stream-to-feature and everything else we have at our disposal)
## This is the run to really see how much better we can perform with emade compared to the paper (using our own strategy)


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Prep for emade run
|Complete
|Nov 12, 2020
|
|Nov 17, 2020
|-
|Correctly add primitives to emade pset
|In progress
|Nov 17, 2020
|
|
|}




==Nov 12, 2020==
'''Subteam Meeting Notes'''
* Still can’t figure out calculation for trend signal (but we have gotten everything else to line up with the paper)
* Max tried making LSTM, didn’t perform too well
* Abhiram shared findings me and him discussed individually yesterday, including:
** Passing in a window of data to the model (i.e. instead of just giving current day’s data, give all data for past five days) —> didn’t lead anywhere
** Adding more TIs as input to model (no real reason to specific TIs, just added popular ones)
** Normalization over past 200 days
* Task: prep for emade run

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Prep for emade run
|In progress
|Nov 12, 2020
|
|Nov 17, 2020
|}



==Nov 11, 2020==
'''. Individual Notes'''
* I [https://github.gatech.edu/rbhatnager3/emade/commit/bb78b1877bafd40227bdb3dae162ffab7fb3a6c8 made some changes] to our code
** I completed the sanity check on the TI data. All of our methods' results line up with those of the paper except for the William's R% (I can't figure out why, Abhiram will take a look soon).
**[[files/TI Errors.png|center|thumb|300x300px|The percent error between our primitives' results and those of the paper. Most metrics are well within 1% error, except WILLR, which still needs to be looked at (the NaN's/infinities are just because of division by zero)]]
** Update (later today): [https://github.gatech.edu/rbhatnager3/emade/commit/bccf4f5b1eab56e60fce8159ad4a52f300dc57ee Abhiram fixed the William's R% indicator]
** I also corrected some changes to our TI primitives, including fixing bugs and redesigning the methods to make them more effective. Examples:
*** Instead of inputting a list of prices and outputting one SMA, output a list of each SMA so we don't have to keep calling the method and can instead just call it once
*** Make output lists the same size as input lists
* Me and Abhiram called and discussed some of his recent results. He has been able to improve upon the genetic labeling notebook and even train basic ML models using the genetic labels. The models have been very successful, even using completely new test data (around 64% profit over 3 years)
** We still have to figure out how/if we want to normalize our TI data, which is difficult since stream data is constantly changing. Leading idea is to always normalize over the past n days (n=100 maybe?)
** In the near future we plan on modeling this code in emade to see if we can get better results


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Perform sanity check on TI data
|Complete
|Nov 3, 2020
|
|Nov 11, 2020
|}



==Nov 9, 2020==
'''Subteam Meeting Notes'''
* We are going to start putting a couple more minds towards the sanity checks to try and get some new ideas. We will contact Dr. Zutty if we still run into issues.
* We are also going to continue to develop the genetic labeling notebook, and we may also explore talking an unsupervised learning approach

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Perform sanity check on TI data
|In progress
|Nov 3, 2020
|
|Nov 11, 2020
|}



==Nov 6, 2020==
'''Individual Notes'''
* I started writing the code for the sanity check of the TI data. As I was doing this, I made some fixes/improvements to my TI methods. (not yet committed but listed some examples below)
** Made primitives' uses more intuitive: I had previously designed the methods so that they take in a list and return a single number representing the TI value for the most recent data in the list, but I changed this so that the methods take in a list and then return a list of the same size as the input, where each element corresponds to the TI value of that input date. Dates where the TI cannot be calculated will be returned as None in the corresponding index (so that the output and input lists are of the same length). Ex:[[files/Prev.png|center|thumb|old method]][[files/Curr.png|center|thumb|new method]]
** Revised other code to account for these changes
* I will continue working on this between today and Monday's meeting
* Abhiram continued working on building a basic model for trading predictions using the genetic labels. We discussed some interesting results he got in an independent run:
[[files/At results2.jpg|center|thumb|500x500px|33% profit over 100 days of unseen data]]


'''Subteam Meeting Notes'''
* Abhiram and Max have been working on a [https://github.gatech.edu/rbhatnager3/emade/blob/stocks-base/testCode-stocks/genetic_labeling.ipynb genetic labeling notebook] that they're using to create a list of labeled data that we can use to train emade on.
** This will allow us to perform supervised learning (the notebook will produce an array of the correct trading decisions, so we will use emade to make a model that predicts these decisions)
** Max had run into some issues, but Abhiram had been able to fix most of them before/during the meeting
** We also improved upon the notebook. Ex: the notebook had previously optimized on funds remaining, but this caused the model to just sell all of its stocks at the end of the time period (which is obviously not what it should do). This probably wouldn't be a problem if the model were deployed, because there would not be a defined end bound to the time period on which the model operates (it would just continually operate), but in this case there is (the end bound of the paper's time period). Regardless, we changed the optimization to net worth, which yielded similar profit percentage while still making reasonable trade decisions towards the end of time period.
* Max and Kartik tried to perform check a sanity check to ensure our formula for calculating the trade signal is consistent with examples in the paper. Their results were not lining up with those of the paper. If I have time before Monday's meeting after finishing the sanity check on our TI methods I will try and help them out.


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Perform sanity check on TI data
|In progress
|Nov 3, 2020
|
|Nov 11, 2020
|}

==Nov 3, 2020==
'''Subteam Meeting Notes'''
* We discussed our issues with out data/results with Dr. Zutty. He suggested we do a "sanity check" (i.e. compare our data with the paper's to make sure they're the same) and to then assess our situation after that. Me and Kartik will perform the comparisons with the paper.
* The first semesters got a chance to read the paper and we returning students answered their questions about it.
* Moving forward, we're going to:
** Resume adding TI primitives to EMADE
** Continue looking for faults/improvements in our model, explantations for the anamolies

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Perform sanity check on TI data
|In progress
|Nov 3, 2020
|
|Nov 11, 2020
|}


==Oct 31, 2020==
'''Individual Notes'''
* Tanishq helped me with adding the TI primitives to emade's pset ([https://github.gatech.edu/rbhatnager3/emade/commit/61032bac75d2d21e548a8c2eee1d57402e7bff56 commit])

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Make primitives for TIs
|Complete
|Oct 8, 2020
|
|Oct 31, 2020
|}


==Oct 29, 2020==
'''Subteam Meeting Notes'''
* Nothing much has happened so far this week, as most people have been pretty busy, so everyone will just carry on with their current tasks

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Make primitives for TIs
|In progress
|Oct 8, 2020
|
|Oct 31, 2020
|}



==Oct 26, 2020==
'''Team Meeting Notes'''
* Dr. Zutty gave the subteam assignments for the first semester students

'''Subteam Meeting Notes'''
* Everyone introduced themselves, and Abhiram and I gave the first semesters a quick rundown of the team and told them to read the paper by next meeting
* We continued to explore explanations for anomalies in our data. We are still a little confused about the trading suggestions and relevance of the trading signals. We will continue to examine whether there are mistakes in our code, if there is some other explanation, or if we should explore alternative trading suggestions.

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Make primitives for TIs
|In progress
|Oct 8, 2020
|
|Oct 31, 2020
|}



==Oct 23, 2020==
'''Individual Notes'''
* I [https://github.gatech.edu/rbhatnager3/emade/commit/2e05d6ec8bf667839157fada1f5216e3390466a0 coded methods] for all 6 TIs used by the paper 
* Still to do: add to emade pset 
* Abhiram did an independent run and found some interesting results, attaining 24% profit over 200 days: 
[[files/At results.jpg|center|thumb|500x500px]]

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Make primitives for TIs
|In progress
|Oct 8, 2020
|
|Oct 31, 2020
|}



==Oct 22, 2020==
'''Subteam Meeting Notes'''
* We looked over data/graphs that Abhiram compiled. There were some anomalies:
** The SMA was almost always below the close price (for SPY). He tried doing the same analysis on GOOGL and the issue persisted (though not as drastically). Maybe this is something wrong with the data supplied by AlphaVantage, we will conduct similar analysis using actual S&P 500 data.
***[[files/SPY Data Anomaly.jpg|center|thumb|Example graph with anomaly. MA is always below close price (but, for example, should be above the close price during down trends)]]
** Changing hyperparameters (namely when to buy vs. sell) created large changes in profit percentage. What was most confusing was that encoding trading decisions that were the opposite of orthodox logic/what the paper said gave a huge profit percentage, compared to encoding orthodox logic leading to losses or modest gains. Again, we will revisit this once we get actual S&P data
* I will code TIs as primitives so we can build a dataset based off of S&P prices

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Make primitives for TIs
|In progress
|Oct 8, 2020
|
|Oct 31, 2020
|}


==Oct 19, 2020==
'''Individual Notes'''
* I rehearsed my assigned slides (slides 1, 2, and 11)

'''Team Meeting Notes'''
* Our subteam presented first
* Here are some notes on other presentations:
''Bootcamp:''
* Got .1998 AUC
* Had database connection issues —> solved by creating GCP MySQL instance 
* Some challenges/future improvements: one-hot encoding, bias towards dominant individuals
* Takeaways: GP had lowest AUC but used single objective, EMADE balanced objectives better
''Modularity:''
* Using ARLs to improve a population’s fitness, among other benefits
* Two current experiments:
** Data pair limitations (need EMADE data pair)
** Alternate selection method
''Bootcamp:''
* Best individuals in GP used Age, Sex, and PClass more often
* EMADE:
** Best accuracy on Pareto front was 96.5%
** AUC = .0052
''NN:''
* Rebranding to focus on NNs and not solely NLP
* NNLearner: Adding Keras layers as Layer primitives
* Past: Using Adam optimizer and various activations depending on layer. Now: adding terminals and terminal mutations to introduce variation in those areas
* Added Concatenate Layers
* Adaptive Mutation Function: reduce mutation of good individuals

'''''Note: I had to leave the meeting early because I had class'''''

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Make primitives for TIs
|In progress
|Oct 8, 2020
|
|Oct 31, 2020
|}

==Oct 15, 2020==
'''Individual Notes'''
* I completed my slides on our [https://docs.google.com/presentation/d/1Fm_pXaKLDFHDEsk-T1rXW1p2UGS8ax2Nvd8k8TIstqs/edit?usp=sharing Midterm Presentation]. As of right now, slide 8 (''CEFLANN vs. EMADE'') is my only slide, but I also made changes to slide 11 (''Currently Working On'')

'''Subteam Meeting Notes'''
* Organized team for midterm presentations on Monday 
* Analyzed past colab run, will do another
** Weird issue with many individuals evaluating to have 0 error. Max will look into this, but for right now we will ignore these individuals. 

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Make primitives for TIs
|In progress
|Oct 8, 2020
|
|Oct 31, 2020
|-
|Complete and rehearse my slides
|Complete
|Oct 12, 2020
|Oct 19, 2020
|Oct 15, 2020
|}





==Oct 13, 2020==
'''Individual Notes'''
* I fixed the errors in the script that extracts SPY stock data so we can do a colab run on the correct data

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Make primitives for TIs
|In progress
|Oct 8, 2020
|
|Oct 31, 2020
|-
|Complete and rehearse my slides
|In progress
|Oct 12, 2020
|Oct 19, 2020
|Oct 15, 2020
|-
|Fix SPY data
|Complete
|Oct 12, 2020
|
|Oct 13, 2020
|}



==Oct 12, 2020==
'''Individual Notes'''
* I started writing primitives for technical indicators

'''Team Meeting Notes'''
* Midterm presentations next week 

'''Subteam Meeting Notes'''
* We discussed the plan for making the slides for our midterm presentation (who would focus on which slides) and what should be done before then
** I will focus on the data preprocessing slides and some of the research slides 
* We went over our past colab runs and found that there is an error in how I preprocessed the data (so I will work on fixing that)

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Make primitives for TIs
|In progress
|Oct 8, 2020
|
|Oct 31, 2020
|-
|Complete and rehearse my slides
|In progress
|Oct 12, 2020
|Oct 19, 2020
|Oct 15, 2020
|-
|Fix SPY data
|In progress
|Oct 12, 2020
|
|Oct 13, 2020
|}




==Oct 8, 2020==
'''Subteam Meeting Notes'''
* Most of the meeting was organizational and confirming that we are going to pivot our focus towards making better use of EMADE (based on Dr. Zutty's suggestions from 10/5/20 subteam meeting)
* We also conducted a run of emade on our old data so we have material to work with for the midterm presentations
* I will start transferring my old work into the EMADE framework

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Make primitives for TIs
|In progress
|Oct 8, 2020
|
|Oct 31, 2020
|}


==Oct 5, 2020==
'''Individual Notes'''
* I ran the script (fixed one minor error) and created a csv with the data we need ([https://github.gatech.edu/rbhatnager3/emade/commit/e69eb42f29dc17eced6d9955a244907007e48f5f commit])

'''Team Meeting Notes'''
* Notebooks are due tonight
* Midterm presentations will be in two weeks (10/19)
* Best way to test feasibility of a tree/architecture: in the input xml and under the “datasets” tag, there is usually one nested “dataset” tag, but you can create a second tag (but put it before the existing dataset) and name it something like “SmallDataset”
** The SmallDataset should only have a couple individuals. Obviously the model will overtrain on these individuals but this will just test whether the tree errors or not
** There will be two sets of objective scores: one for each dataset, and, assuming the tree works, the score for the full dataset is the relevant one


'''Subteam Meeting Notes'''
* Max and Kartik Colab runs: boosted model was good, but only because it just predicted prices close to the current price (when trying to predict prices 15 mins in future)
* Load data in using stream-to-feature (instead of as feature data)
** TIs should be constructing features from stream data
** Make primitives for each TI, pass data in for past t days. Implement TIs used in paper
*** Do so with as many hyperparameters as possible to increase evolvability (e.g. MACD primitive uses a parent MACD and EMA used in intermediate steps —> 2 primitives)
*** Data that’s taken in is past 26 days close prices (or whatever the max amount of historical data we need is, it’s 26 for they hyperparameters used by paper) and then the actual next day’s close price (not used for training, used to evaluate success, since this is a supervised problem)
**** Instead of next day’s close price, we can use another data point (e.g. the actual next day’s trade signal for a more direct comparison to paper). Basically this last number should be the actual value of whatever we’re comparing
** Data is inputting as a list with alternating values of label (timestamp) and data
*** Ex: [0, price[0], 1, price[1], …, 25, price[25], actual] where 0, 1, … 25 is the day, price[i] is the price at day i, and actual is the actual value of what we’re predicting
** Read in using load_many_to_one_from_file() in src/GPFramework/data.py
** Need to edit XML to specify StreamData instead of FeatureData


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Clean up code and export S&P data to csv
|Complete
|Sept 28, 2020
|
|Oct 5, 2020
|-
|Research CEFLANN architecture
|Completed by others
|N/A
|N/A
|N/A
|}



==Oct 1-2, 2020==
'''Individual Notes'''

''Pre-meeting:''
* I tried running the script again and still got the too many calls error
** Update (10/2): Abhiram noticed that this is because there is 5 calls/min limit and I call the API 6 times in the script. He [https://github.gatech.edu/rbhatnager3/emade/commit/34f318bd531008e31f095add027c74304ee46dc3#diff-1ae26b39b13f9861f0e75777571d3d21 added delays] after each API call to circumvent this issue

''Post-meeting:''
* I made many [https://github.gatech.edu/rbhatnager3/emade/commit/2ea63c69797a1b5483cd1a9d3a27c553d48d1208#diff-1ae26b39b13f9861f0e75777571d3d21 changes] to the script I've been making to get stock data:
** I reworked how I store the data to make the data more readable and easier to export to a csv. Previously, I had a dictionary that mapped date strings to the price at that day as well as an indicators dictionary with keys being a string denoting the indicator and the value being a dictionary that mapped date strings to the value of that indicator on that day. I consolidated this into one dictionary that maps date strings to a dictionary that describes the security's data from that day (i.e. each day has a dictionary that maps "Price" to the price on that day, "SMA" to the simple moving average of that day, etc)
** I added columns in the stockData dictionary (described above) for the trend and trading signals on each day. This included keeping track of the extrema for each indicator and making a couple guesses at times due to ambiguity in the paper (if "rising" meant strictly increasing on non-decreasing; what the trend signal should be if there's no trend; what the equation for the trading signal should be if there's no trend; and what the trading signal should be when the specified equation causes a ZeroDivisionError)
** Before exporting the TI data to a csv, I normalized the values, using the same equation as that in the paper
* These were all of the remaining equations from the paper I had left to incorporate, so all there is left to do for the data to be ready to use is clean up some code and incorporate a couple of suggestions from the team
* I put all these changes (as well as transferring my old work on CacheV2) onto the stocks-base branch 


'''Subteam Meeting Notes'''
* Abhiram add a ModifiedLearner for regression and more regression primitives
* Will use stocks-base branch from now on (instead of CacheV2)
* Will do an emade run on colab once I finish getting the S&P data

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Encode equations from paper
|Complete
|Sept 24, 2020
|Oct 1, 2020
|Oct 1, 2020
|-
|Clean up code and export S&P data to csv
|In progress
|Sept 28, 2020
|
|Oct 5, 2020
|-
|Research CEFLANN architecture
|In progress
|Sept 24, 2020
|
|
|}



==Sept 29-30, 2020==
'''Individual Notes'''
* I [https://github.gatech.edu/rbhatnager3/emade/commit/e3dbb929a8015a1d086c9097c52cfc098768a4c3 committed changes] which add support for all the TIs used in the paper and includes code to export the data to a csv file
* I am having issues with AlphaVantage oftentimes not returning data because I exceed the call limit. This could be because I called the API so many times during testing, but it still seems unlikely that I called the API that many times. Will address in subteam meeting (maybe get a different API key or have someone else try to run the script)
** In order to catch when this issue happens (and not get an unrelated error or write bad data to the csv), I included a method that checks the data for when the script exceeds the call limit
** Update (10/2): Abhiram noticed that this is because there is 5 calls/min limit and I call the API 6 times in the script. He [https://github.gatech.edu/rbhatnager3/emade/commit/34f318bd531008e31f095add027c74304ee46dc3#diff-1ae26b39b13f9861f0e75777571d3d21 added delays] after each API call to circumvent this issue

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Implement more Technical Indicators from AlphaVantage
|Complete
|Sept 24, 2020
|Oct 1, 2020
|Sept 30, 2020
|-
|Encode equations from paper
|In progress
|Sept 24, 2020
|Oct 1, 2020
|Oct 1, 2020
|-
|Export S&P data to csv and send to Max/Kartik
|In progress
|Sept 28, 2020
|
|Oct 5, 2020
|-
|Research CEFLANN architecture
|In progress
|Sept 24, 2020
|
|
|}



==Sept 28, 2020==
'''Individual Notes'''
* I [https://github.gatech.edu/rbhatnager3/emade/commit/f72bb2fd127a618e8f08c09462b22a095f574096 committed changes] which add support for more TIs to the Python script on which I have been working
** I was unsure if I implemented MACD and RSI correctly since the paper was a little ambiguous as to which parameters it used, so I just used the default values and will address this in the subteam meeting

'''Team Meeting Notes'''
* Each subteam gave updates on their progress for the week

'''Subteam Meeting Notes'''
* Each member gave updates on their weekly progress
* CEFLANN research: 
** Differs from MLP in that CEFLANN more computationally efficient bc no hidden layers
** Usually trains with ELM instead of backwards propagation
** CEFLANN seems to just be a type of FLANN that is specific to the paper
** Seems like we can implement, but we need to do it ourselves (don’t think it’s in emade yet)
*** Update (later today): I DM'd Anish and he said FLANNs are not implemented in emade
* Max and Kartik performed regression on colab, but there were minor issues with there not being a timeout for evaluating individuals 

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Implement more Technical Indicators from AlphaVantage
|In progress
|Sept 24, 2020
|Oct 1, 2020
|Sept 30, 2020
|-
|Encode equations from paper
|In progress
|Sept 24, 2020
|Oct 1, 2020
|Oct 1, 2020
|-
|Export S&P data to csv and send to Max/Kartik
|In progress
|Sept 28, 2020
|
|Oct 5, 2020
|-
|Research CEFLANN architecture
|In progress
|Sept 24, 2020
|
|
|}



==Sept 24, 2020==
'''Individual Notes'''
* I [https://github.gatech.edu/rbhatnager3/emade/commit/e9e48984b922d7489b73e15a1b1ed8c0ed9a19fb committed changes] to the Python file I created yesterday to allow us to pull technical indicator values from the AlphaVantage API

'''Subteam Meeting Notes'''
* Everyone gave updates on their individual progress. Most notably, Kartik and Max figured out regression in emade and Abhiram fixed some bugs in our colab notebook
* Some of the tasks that were divided up:
** Integrating regression problems into our Colab notebook
** Beginning to model some of the paper's code (I will be helping with this, focusing on the code related to the technical indicators and calculating the trade signal)
** Researching CEFLANNs to see what parts are already implemented in EMADE and what we must implement (I will help out with this if I have time after the previous task)

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Implement more Technical Indicators from AlphaVantage
|In progress
|Sept 24, 2020
|Sept 28, 2020
|Sept 30, 2020
|-
|Encode equations from paper
|In progress
|Sept 24, 2020
|Sept 28, 2020
|Oct 1, 2020
|-
|Research CEFLANN architecture
|In progress
|Sept 24, 2020
|
|
|}


==Sept 23, 2020==
'''Individual Notes'''
* I researched the best ways we can get financial data
** [https://www.alphavantage.co AlphaVantage] is a good API for getting data to the nearest minute (but we only need closing prices anyways). Small problem: it only gets data for securities (i.e. not actual indices)
** YahooFinance no longer has an API (from what I've seen), but you can manually download a csv of historical data (so lots of data available, but can automate it through code). But this seems like the best way to get data on indices. [https://finance.yahoo.com/quote/%5EGSPC/history?period1=1262476800&period2=1420070400&interval=1d&filter=history&frequency=1d Daily S&P data] in same time interval as the paper we're using
* I created a Python file that tests getting financial data. I included code for parsing the csv from Yahoo Finance (if we want data from the actual S&P 500) as well as using AlphaVantage (I took data from SPY, a popular S&P index fund).
* [https://github.gatech.edu/rbhatnager3/emade/commit/1e22f39e08196fe3aff8b255cbe187a0015bada2 My commit], which includes the Yahoo Finance csv and the Python file in a new directory for test code for our subteam
** For both data, I made a dictionary of daily close prices to mirror the data used by the paper

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Research APIs that can get stock data
|Complete
|Sept 21, 2020
|Sept 24, 2020
|Sept 23, 2020
|}


==Sept 21, 2020==
'''Team Meeting Notes'''
* Dr. Zutty suggested we start looking into getting the exact data used by the paper and looking at which primitives we will be implementing vs which are already implemented
* The modularity subteam had some trouble with Colab, so they might get into contact with us since we were able to get that working

'''Subteam Meeting Notes'''
* Will use AWS MySQL database on our Colab runs (Kartik set this up last week)
* We caught everyone on where we stand and where we plan on moving
** Will continue to progress towards replicating the paper we had selected last week
* Tasks vary by person. I will look into APIs that can get us the stock data we need to replicate the paper (S&P 500 close prices). [https://www.alphavantage.co/#about Alpha Vantage] might work, but I'll look into it more later in the week

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look into performing regression with EMADE
|(Max is going to do this now)
|N/A
|N/A
|N/A
|-
|Research APIs that can get stock data
|In progress
|Sept 21, 2020
|Sept 24, 2020
|Sept 23, 2020
|}




==Sept 19-20, 2020==
'''Individual Notes'''
* Me and Kartik began setting up Colab on our subteam's fork ([https://colab.research.google.com/drive/1xKxJLNskjXpUuxLogr1wORpu3nt7Q5xI?usp=sharing notebook link])
** We had some trouble setting up a remote MySQL database. We had planned to use [https://remotemysql.com remotemysql.com], but it was pretty buggy.
** We used Anish's notebook (from last semester, used for the nlp-nn subteam and shared with the rest of the VIP) and changed the code to apply to our fork and to use the titanic dataset
** I had to leave for another meeting, but Kartik will run the notebook once he gets the database set up
*** Update (9/20): Kartik set up a database using AWS (free for twelve months) and will run the Colab notebook
* As assigned, I conducted a short run of the titanic dataset using our subteam's fork


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look into performing regression with EMADE
|In progress
|Aug 28, 2020
|
|
|-
|Conduct test run of EMADE
|Complete
|Sept 4, 2020
|Sept 21, 2020
|Sept 20, 2020
|-
|Get Colab working
|Complete
|Sept 17, 2020
|Sept 21, 2020
|Sept 20, 2020
|}

==Sept 17, 2020==
'''Individual Notes'''
* I read through [https://doi.org/10.1016/j.jfds.2016.03.002 the paper] we will be focusing on in-depth. Some high-level notes:
** Treats trading as a classification problem (final result is one of three signals: buy, sell or hold), but uses a continuous trading signal during training (signal is a value from 0-1, which can be more descriptive than a discrete class value).
** Objective of paper: generate profitable trades using a computational efficient functional link artificial neural network (CEFLANN) and trading rules motivated by technical analysis
** Performance metric: profit percentage during some period of time, and a comparison to conventional classification models (SVM, etc)
** Technical Indicators used:
**# 25-day Simple MA
**# MACD
**# Stochastic KD (K% relates current closing price to others in a given time range, D% is 3-day MA of K%)
**# RSI
**# Larry William's R% (stochastic oscillator)
** MA used to determine trend
** TIs are fed into the CEFLANN, which determines the trading signal. This is kind of confusing because shortly after the paper says the trading signal is solely determined by a simple formula using closing prices. Will discuss in meeting
*** Maybe: closing prices determine the current trading signal and the CEFLANN is used to predict the next trading signal
** Trading decision comes from a simple conditional:
*** If predicted next trend is up, then buy
*** If predicted next trend is down, then sell
*** Else, hold
** Profit is calculated and then compared to performance of other ML models. Trades conducted on S&P 500 and BSE SENSEX
*** CEFLANN model outperforms other models on both indices (but Naive Bayes comes close on both)

'''Subteam Meeting Notes'''
* Talked through paper and tried to figure out what’s going on
** Others are similarly confused on role of the TIs in the paper (except MA, whose role is clear to determine trend)
* Will follow general direction of paper and implement in EMADE 
* Hold off on trying to add CEFLANN functionality in EMADE (will do that later). Will start with more basic 
* Tasks for Monday:
** Titanic run of emade on fork (everyone)
** Get colab working (Me and Kartik)

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look into performing regression with EMADE
|In progress
|Aug 28, 2020
|
|
|-
|Conduct test run of EMADE
|In progress
|Sept 4, 2020
|Sept 21, 2020
|Sept 20, 2020
|-
|Read CEFLANN paper
|Complete
|Sept 14, 2020
|Sept 17, 2020
|Sept 17, 2020
|-
|Get Colab working
|In progress
|Sept 17, 2020
|Sept 21, 2020
|Sept 20, 2020
|}



==Sept 14, 2020==
'''Team Meeting Notes'''
* Each team gave updates on their progress
* Assignment for next week: self-evaluation of notebooks
* May have an issue with working on fork of emade instead of branch, but we should be fine for now

'''Subteam Meeting Notes'''
* We caught up the entire subteam on the progress of both groups
* We started to solidify the direction of our team:
** We will read [https://doi.org/10.1016/j.jfds.2016.03.002 this paper] and focus on mimicking its techniques in EMADE to see if EMADE can produce better results. We will then add on some of our own ideas to see if we can improve upon this model.
** The EMADE group will begin conducting test runs of EMADE on pre-loaded and custom datasets later this week.
** We plan on using Colab for our runs of EMADE
* For next meeting: read paper linked above

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look at past Stocks subteam's work
|Complete
|Aug 24, 2020
|
|Sept 14, 2020
|-
|Look into performing regression with EMADE
|In progress
|Aug 28, 2020
|
|
|-
|Conduct test run of EMADE
|In progress
|Sept 4, 2020
|
|Sept 20, 2020
|-
|Read CEFLANN paper
|In progress
|Sept 14, 2020
|Sept 17, 2020
|Sept 17, 2020
|}



==Sept 10, 2020==
'''Individual Notes'''
* I did my assigned research, which was to look for at least 3-4 interesting/applicable research papers and read 1-2 (I found 7 and read 3)
* Here are the papers I found with some notes:
*[[files/Paper notes 1.png|center|thumb]][[files/Paper notes 2.png|center|thumb]][[files/Paper notes 3.png|center|thumb]]
* Update (later in day): The bolded papers are the ones we decided (as a group) were the most applicable, I linked them below 
** [https://doi.org/10.1016/j.jfds.2016.03.002 ''A hybrid stock trading framework integrating technical analysis with machine learning techniques'']
** [https://doi.org/10.1016/S0305-0548(03)00063-7 ''Generating trading rules on the stock markets with genetic programming'']

'''Subteam Meeting Notes'''

''Research group meeting''
* Each member discussed their findings from the papers they reviewed
* We decided which papers would be most relevant for our purposes (a list of around 5 papers)
* We will narrow down to 1-2 papers on our Monday meeting because the EMADE group meeting (tomorrow) will be important towards deciding our preferred direction for our model and because the whole subteam should be at the Monday meeting (today was just the research group)
* Assignment for next week will be to start reading the paper(s) we decide to focus on


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look at past Stocks subteam's work
|In progress
|Aug 24, 2020
|
|
|-
|Look into performing regression with EMADE
|In progress
|Aug 28, 2020
|
|
|-
|Find 3-4 relevant research papers
|Complete
|Sept 3, 2020
|Sept 10, 2020
|Sept 10, 2020
|-
|Read 1-2 papers in-depth
|Complete
|Sept 3, 2020
|Sept 10, 2020
|Sept 10, 2020
|-
|Conduct test run of EMADE
|In progress
|Sept 4, 2020
|Sept 11, 2020
|Sept 20, 2020
|}


==Sept 4, 2020==
'''Subteam Meeting Notes'''

''EMADE group meeting''
* We reviewed possible designs for our trading algorithm. We had an in-depth discussion for each possibility, including considering what each model would take in and output and generally considered how each would work at a high level
* For next week, we will try to familiarize ourselves with how to complete a run of EMADE, namely for a dataset and xml configuration document that we create from scratch (as opposed to one that is already in EMADE)
** Time permitting: we will also try to run a regression problem on EMADE before next week

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look at past Stocks subteam's work
|In progress
|Aug 24, 2020
|
|
|-
|Look into performing regression with EMADE
|In progress
|Aug 28, 2020
|
|
|-
|Find 3-4 relevant research papers
|In progress
|Sept 3, 2020
|Sept 10, 2020
|Sept 10, 2020
|-
|Read 1-2 papers in-depth
|In progress
|Sept 3, 2020
|Sept 10, 2020
|Sept 10, 2020
|-
|Conduct test run of EMADE
|In progress
|Sept 4, 2020
|Sept 11, 2020
|Sept 20, 2020
|}



==Sept 3, 2020==
'''Individual Notes'''

''Completed my research for today's meeting:''
* Exponential MAs (EMAs) 
** Can compare EMAs of different time periods to identify trends
*** Ex: if a 50 day (medium-term) EMA drops below a 200 day (long-term) EMA, that likely signifies a bearish trend
** Source: [https://www.investopedia.com/articles/active-trading/011815/top-technical-indicators-rookie-traders.asp#trend-50-and-200-day-ema Investopedia: Top Technical Indicators for Rookie Traders]
* Average Directional Index, ADX (requires some more research)
** Lagging trend strength indicator
** Does not give insight on trend direction
** Good complement to other trend indicators I’ve researched, which only indicate the direction of a trend, not its strength 
** Shows when a trend is gaining or losing momentum (which is the velocity of price): a series of higher ADX peaks means momentum is increasing (and vice versa)
** We should focus on trading during strong trends, and ADX can help us identify those situations
** Source: [https://www.investopedia.com/articles/trading/07/adx-trend-indicator.asp Investopedia: ADX]
* Moving Average Convergence and Divergence, MACD (requires some more research)
** Lagging indicator to reveal strength, direction, momentum, and duration of a trend
** Calculated by subtract the 26- and 12-period EMAs
** Signal line: 9-day EMA of the MACD
*** Buy when MACD crosses above signal line
*** Sell/short when MACD crosses below signal line
** Source: [https://www.investopedia.com/terms/m/macd.asp#:~:text=Moving%20Average%20Convergence%20Divergence%20(MACD)%20is%20a%20trend%2Dfollowing,from%20the%2012%2Dperiod%20EMA. Investopedia: MACD]

'''Subteam Meeting Notes'''

''Research group meeting:''
* Each member shared their research on the technical indicators they were assigned to look at
* We decided that it was best to take Dr. Zutty's suggestion to look into more research papers, so we shifted the focus of this group: we will focus on using already-researched methods of using ML for trading. First, we will try to replicate one paper's process for creating a trading bot and see if we can outperform it using EMADE and some of our own research. Everyone's assignment is to find some interesting papers and read a couple. 


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look at past Stocks subteam's work
|In progress
|Aug 24, 2020
|
|
|-
|Research at least 2-3 Trend Analysis technical indicators
|Complete
|Aug 27, 2020
|Sept 3, 2020
|Sept 3, 2020
|-
|Look into performing regression with EMADE
|In progress
|Aug 28, 2020
|Sept 4, 2020
|
|-
|Find 3-4 relevant research papers
|In progress
|Sept 3, 2020
|Sept 10, 2020
|Sept 10, 2020
|-
|Read 1-2 papers in-depth
|In progress
|Sept 3, 2020
|Sept 10, 2020
|Sept 10, 2020
|}


==Aug 31, 2020==
'''Individual Notes'''
* Looked more into what exactly technical indicators are and how they should be used
* Began the assigned research for the next group meeting on technical indicators which measure trends in stock data:
** Parabolic Stop and Reverse (Parabolic SAR)
*** Leading indicator
*** Finds the direction of a security’s price and draws attention to when this direction is changing
*** Indicators appear as series of dots placed either above (price trending downwards) or below (price trending upwards) the price bars on a chart
*** Dots flipping means there may be a change in price direction coming
*** Works well during a trend, but can give false signals when price is moving sideways or if market is choppy
*** Conclusion: this will likely be an effective indicator for our purposes (especially for index funds) for the long-term
**** May be bad to use right now since the market has been choppy for the past couple months (though past week or so would indicate choppiness is over, but could be too soon to tell)
**** Hopefully by the time we have an implementable model the market will smooth out and Parabolic SAR will be useful
*** Source: [https://www.investopedia.com/trading/introduction-to-parabolic-sar/#:~:text=The%20parabolic%20SAR%20is%20a,SAR%20was%20developed%20by%20J. Investopedia: Parabolic SAR]
** Moving Average (MA)
*** Lagging indicator. We said we would focus on leading indicators, but so far I could not find any leading trend indicator (besides Parabolic SAR). However, I will bring up in the subteam meeting later today that we could benefit from having a lagging indicator to confirm the predictions of our leading indicator(s)
**** Update (from later today): the rest of the subteam agreed with my proposed use for lagging indicators, although we will still focus on leading indicators
*** Pretty simple TI and suitable for our goals: smooths out price data by averaging out the prices over some interval of time (can be any interval the trader desires). The average is continuously updated
*** Helps filters out noise from short-term fluctuations
**** Good for choppy markets (like right now)
*** General guideline: if prices are above MA, trend is up (and vice versa)
**** Can help identify supports/resistances in up/downtrends: a long-term MA (50-200 day interval) can act as a floor (support) or ceiling (resistance) during an up- or downtrends (respectively), meaning that in the uptrend, for example, the prices will seldom move below the floor (i.e. the moving average will “support” the upwards trend), and instead “bounce” up off of the MA. A deviation from this pattern may signify a change in the trend
*** There are different types of MAs, including differing time intervals and simple (linear) vs exponential (biased towards more recent prices), so we’ll have to see what’s best for us
*** Sources: [https://www.investopedia.com/articles/active-trading/052014/how-use-moving-average-buy-stocks.asp#:~:text=Key%20Takeaways,days%20for%20the%20averaging%20interval. Investopedia: How to Use a Moving Average]


'''Team Meeting Notes'''
* Abhiram and I gave the updates for the Stocks subteam
* Other subteams updated the team on what they have been doing this past week
** Most subteams were doing organizational things (including us)


'''Subteam Meeting'''
* We decided that we're going to put the Technical Indicator group on hold until we get more progress out of the other two groups
* Research group updated subteam on what they've found so far about their assigned technical indicators
** My updates are above under "Individual Notes"
** Volume indicators and Relative Strength Indicator also discussed
* Notes from Dr. Zutty:
** Branch off of Cache_V2
** Look through research on trading ML. What did other people used to predict their data. Maybe even mimic another paper’s methodology but do it through automated ML with EMADE and add some of our own TIs and stuff and see if we get better results
*** Try Google Scholar. Look for papers with a lot of citations
*** If you find a good paper, look at the papers it cited, and which papers cited that paper
*** Focus on what they’re predicting: change in stock price, volume traded, a binary buy/sell, etc
*** Big question: how can AAD improve ML for trading
* For next research group meeting: finish research into technical indicators, but then rethink direction of the group and maybe focus on more on looking at articles and research papers  for building trading AIs, replicating those in EMADE, and then improving upon those methodologies with our own research


'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Research how to implement ideas discussed in meetings
|Complete
|Aug 24, 2020
|Aug 31, 2020
|Aug 31, 2020
|-
|Look at past Stocks subteam's work
|In progress
|Aug 24, 2020
|
|
|-
|Research at least 2-3 Trend Analysis technical indicators
|In progress
|Aug 27, 2020
|Sept 3, 2020
|Sept 3, 2020
|-
|Look into performing regression with EMADE
|In progress
|Aug 28, 2020
|Sept 4, 2020
|
|}



==Aug 28, 2020==
'''Individual Notes'''
* I completed the [https://docs.google.com/document/d/1Z5-M1qqlk3JoX_r8eFkATcFduHT50abJoimD6PpiMi4/view Google Doc] consolidating the resources for our subteam

'''Subteam Meeting Notes'''

''EMADE Group meeting:''
* Looked through ways we could get stock data and technical indicators
** Max experimented with getting data from technical indicators on Trading View
** Alpha Vantage: stock data (minute-by-minute prices, company data, financial statements, etc)
* Created [https://github.gatech.edu/rbhatnager3/emade fork] of emade repo

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Research how to implement ideas discussed in meetings
|In progress
|Aug 24, 2020
|Aug 31, 2020
|Aug 31, 2020
|-
|Look at past Stocks subteam's work
|In progress
|Aug 24, 2020
|Aug 31, 2020
|
|-
|Schedule/attend meetings for Groups A,B,C
|Complete
|Aug 24, 2020
|Aug 28, 2020
|Aug 28, 2020
|-
|Create and share Google Doc consolidating resources for the subteam to explore
|Complete
|Aug 27, 2020
|
|Aug 28, 2020
|-
|Research at least 2-3 Trend Analysis technical indicators
|In progress
|Aug 27, 2020
|Sept 3, 2020
|Sept 3, 2020
|-
|Look into performing regression with EMADE
|In progress
|Aug 28, 2020
|Sept 4, 2020
|
|}



==Aug 27, 2020==
'''Individual Notes'''
* I compiled a list of securities that our model could trade. As discussed in the last meeting, I aimed to create a list that included blue-chip stocks from different sectors. The list was:
** Apple
** Proctor and Gamble
** Exxon-Mobil
** JPMorgan Chase
* I also listed some metrics (aside from technical indicators) that we could include in our model, including:
** Open/close prices
** Volume (and comparing a given day's volume to a security's average volume)
** Beta (based on whether we'd target stable or volatile securities, or a mix of the two)
** Overall market health
* I began looking through an [https://www.visualcapitalist.com/12-types-technical-indicators-stocks/ article] Tanishq posted in the Slack which describes what a technical indicator is, how they're used, types of indicators, and examples of popular TIs within each type.
** Based on previous discussions, it seemed like we would be focusing on leading indicators (these try to predict a stock's price)

'''Subteam Meeting Notes'''

''Research Group meeting:''
* We came with a new idea on how to design our model: a primary regression model that would aim to predict a security's price and a secondary model that would determine whether it should be bought, sold, or held based on the predicted price
* One potential obstacle is that none of us have experience using EMADE for regression, so we will need to look into that before building our model 
* We decided that (at least to start) the model will only trade an S&P index fund in order to:
** Limit volatility that may complicate the problem 
** Minimize the effect of operations of individual companies (so we don't have to worry about, for example, news sentiment analysis regarding a specific stock)
** Represent whole-market performance
* Since we are focusing on an S&P index-fund ETF (which is relatively nonvolatile), we will not look into volatility indicators to start, and will instead focus on trend, momentum, and volume indicators 
** At the end of the meeting, we were all assigned a category of indicators to look into by the next group meeting
** I was assigned Trend Indicators, so I will be looking into Parabolic Stop and Reverse (a popular trend indicator from the article linked above) and indicators in the [https://www.tradingview.com/ideas/movingaverage/ Moving Average] and [https://www.tradingview.com/scripts/trendanalysis/ Trend Analysis] tabs on Trading View


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Research how to implement ideas discussed in meetings
|In progress
|Aug 24, 2020
|Aug 31, 2020
|Aug 31, 2020
|-
|Look at past Stocks subteam's work
|In progress
|Aug 24, 2020
|Aug 31, 2020
|
|-
|Schedule/attend meetings for Groups A,B,C
|In progress
|Aug 24, 2020
|Aug 28, 2020
|Aug 28, 2020
|-
|Create and share Google Doc consolidating resources for the subteam to explore
|In progress
|Aug 27, 2020
|
|Aug 28, 2020
|-
|Research at least 2-3 Trend Analysis technical indicators
|In progress
|Aug 27, 2020
|Sept 3, 2020
|Sept 3, 2020
|}


==Aug 24, 2020==
'''Individual Notes'''
* In order to prepare for today's subteam and team meetings, I did some research into trading securities using AI
* This research gave me an introductory understanding of the topic so I could better participate in today's meetings' discussions, including noting some clarifying questions to help guide and narrow our focus and long-term vision for this project and make suggestions on how to best achieve these goals
* Here are some sources/findings:
** [https://alpaca.markets Alpaca] is a popular (and free) API for making trades
** [https://www.alphavantage.co Alpha Vantage] can be used to get data
** [https://medium.com/swlh/build-an-ai-stock-trading-bot-for-free-4a46bec2a18 Medium article] for getting started with making an AI trading bot

'''Subteam Meeting Notes'''

We had two subteam meetings today, one before and one after the team meeting

''Before Team Meeting''
* This was a meeting with Dr. Zutty to discuss the viability of this subteam. He said we had an interesting vision and seemed to be on the right track
* As we had discussed at our previous subteam meeting, Dr. Zutty had suggested to look at the past Stocks subteam's work
* We discussed using an ensemble learner and how this would fit into EMADE. Dr. Zutty said this would be a suitable problem for EMADE (in terms of building the model's tree and automatically tuning hyperparameters) and gave a couple of notes:
** Our data will likely come into EMADE as Stream Data
** We should take in Time Series data (maybe consisting of open/close prices, trading volume, etc) and feed that into an ensemble method, consisting of, for example, three Technical Indicators, each of which independently make a buy/sell suggestion (or even a continuous value of confidence to buy vs sell), and then make a determination based on what those indicators say
** In all, we would have 4 primitives (in this example):  3 Technical Indicators (stream to features) and 1 ML primitive 
** Sample tree: EnsembleLearner(TI3(TI2(TI1(LPF(data), stream_to_feature, [hyperparameters]), stream_to_feature, [hyperparameters]), stream_to_feature, [hyperparameters], adaboost, random_forest)

''After Team Meeting''
* In this meeting we tried to use Dr. Zutty's suggestions and our growing comfort with the topic to establish a clear direction and long-term goals for the project.
** We decided to make a model that will issue buy/sell recommendations using various metrics such as security's open/close prices, trading volume, EPS, etc. Eventually we would like to make automated trades (with fake money)
** Our model's goal will be for short-term gains (so day-trading will be a highlight)
** We will look at 6 securities (specific ones TBD): 5 blue-chip stocks (each from a different sector) and 1 index-fund (ETF)
* We also handled logistical issues, such as finalizing a weekly meeting time (Mondays after the team meeting) and appointing subteam leaders (Abhiram and I volunteered as co-leaders)
* Since we have a large subteam, we decided it would be best to split into three groups: one which will research financial metrics to focus on for trading and the requisite ML for a trading bot, another which will figure out how to evolve the features/technical indicators, and a final one which will work on integrating this work into EMADE (Groups A, B, and C, respectively)
** I volunteered to focus my efforts on Group A, but I will also be involved in the other groups' work
** At the end of the meeting we decided that each group should meet individually. After the meeting, I set up a LettuceMeet to organize these meeting times, making them later in week so members could have a couple days to do some research and come to the meeting with ideas on how the group should get started and could formulate long-term goals for that group

'''Team Meeting Notes'''
* Dr. Zutty created a "Weekly Reports" page for each of the subteams to maintain
* Each of the subteams summarized their progress for this week, but their wasn't much to say other than organizational details

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Devise suggestion for subteam's vision/big idea task and conduct necessary research
|Complete
|Aug 21, 2020
|
|Aug 24, 2020
|-
|Meet with subteam and Dr. Zutty (preferably before next team meeting)
|Complete
|Aug 21, 2020
|Aug 24, 2020
|Aug 24, 2020
|-
|Research how to implement ideas discussed in meetings
|In progress
|Aug 24, 2020
|Aug 31, 2020
|Aug 31, 2020
|-
|Look at past Stocks subteam's work
|In progress
|Aug 24, 2020
|Aug 31, 2020
|
|-
|Schedule/attend meetings for Groups A,B,C
|In progress
|Aug 24, 2020
|Aug 28, 2020
|Aug 28, 2020
|}


==Aug 21, 2020==
'''Subteam Meeting Notes'''
* Today the stocks subteam had its first meeting
* The meeting was primarily spent trying to establish our vision and determine whether it would be a viable subteam
** Given that everyone in the meeting was only in their 2nd semester of the VIP, we were unsure if we had a good enough understanding of EMADE, so we explored the possibility of becoming a subdivision of the nlp-nn subteam
** This option didn't pan out, but we decided that we would still try to pursue this effort and get any necessary help with EMADE in the future
* We also explored the possibility of more or less continuing the efforts of the past "portfolio optimization" subteam, as well as looking at individual companies' stocks' metrics
* We concluded the meeting looking to met again before the next team meeting so we could do a bit more research and ask for Dr. Zutty's help

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Establish and attend a meeting for those interested in stocks subteam
|Complete
|Aug 17, 2020
|Aug 23, 2020
|Aug 21, 2020
|-
|Devise suggestion for subteam's vision/big idea task and conduct necessary research
|In progress
|Aug 21, 2020
|
|Aug 24, 2020
|-
|Meet with subteam and Dr. Zutty (preferably before next team meeting)
|In progress
|Aug 21, 2020
|Aug 24, 2020
|Aug 24, 2020
|}


==Aug 17, 2020==
'''Team Meeting Notes'''
* Today was the first meeting of the semester, so Dr. Zutty mainly reviewed how the semester would (especially in the remote format) and discussed some organizational details
* We also discussed subteam assignments, and I was one of a handful of people who expressed interest in forming a stocks subteam


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Establish and attend a meeting for those interested in stocks subteam
|In progress
|Aug 17, 2020
|Aug 23, 2020
|Aug 21, 2020
|}


=Spring 2020=

==Apr 20, 2020==
'''Team Meeting Notes'''

Today each subteam gave a presentation on their progress this semester. Below are notes from the other subteams' presentations
* ADFs:
** Most effective in middle generations (gens 15-20)
** Problem: some ADFs are (in part) comprised of other ADFs
*** ADFs with other ADFs as the root node tended to not perform well
** Finding: selecting ADFs based on frequency isn’t a good option
** Datasets used: titanic
* Research Fundamentals:
** Neat GP: control bloat naturally
** Higher speciation thresholds led to changes in hypervolume
** Datasets used: titanic
* NLP (time conflict):
** Text summarization (whereas we do text classification)
** Got PACE working (e.g. includes sample pbs scripts)
** Primitives added:
*** Num named entities: greater number of named entities implies more info
*** TFISF (Term Frequency-Inverse Sentence Frequency): assigns a ranks to each sentence based on how important it is in the context of the text as a whole
**** Words that appear less frequently are counted as more significant  and thus more important to incorporate into a summary
* EzCGP:
** Had incompatibility issues when switching from tensorflow 1.0 to 2.0
** Found that Google Cloud GPU becomes significantly faster than CPU as num processes inc
*** Colab helpful
* Common Points:
** Many other subteams only use (or, at least, heavily rely on) the titanic dataset (we use a couple of different datasets, though we do focus on the movie reviews dataset)
** A lot of teams still have issues with PACE


==Apr 18, 2020==
'''Individual Notes'''
* I finished my slide for the final presentation and rehearsed what I would say
[[files/Final slide.png|center|thumb|400x400px|The final version of my slide]]

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finish my assigned slide
|Complete
|Apr 17, 2020
|Apr 19, 2020
|Apr 18, 2020
|}


==Apr 17, 2020==
'''Subteam Meeting Notes'''
* We created a Google Slides presentation and everyone was told to complete their slide by Sunday


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finish my assigned slide
|In progress
|Apr 17, 2020
|Apr 19, 2020
|Apr 18, 2020
|}


==Apr 16, 2020==
'''Individual Notes'''
* I worked on adding my primitive to the nlp-nn branch of emade
* Since I am new to neural networks, I spent some time researching ReLU (the layer I am tasked with adding) so that I could get a foundational understanding of what relu is. I came across the following links:
** [https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7 https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7]
** [https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0 https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0]
** [https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6 https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6]
* I also used the [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation tf.keras api] to help me understand how to work with relu
* Adding the layer to emade went smoothly for the most part, except for a couple of things I was a bit unsure about that Anish quickly cleared up
* Mohan had previously said in the slack that first semesters do not have permission to push to the repo, so we should fork it and submit a pull request to the nlp-nn branch ([https://github.gatech.edu/emade/emade/pull/160/files here's my pull request])


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Add ReLU layer to pset
|Complete
|Apr 10, 2020
|Apr 16, 2020
|Apr 16, 2020
|}


==Apr 13, 2020==
'''Subteam Meeting Notes'''
* The returning students gave the first semesters instructions on how to add their primitives to emade (essentially by adding it to the LayerList in neural_network_methods.py and then adding it to the pset in gp_framework_helper.py)
* The returning students updated the group on their progress, including planning for the final presentation:
** Returning to PACE seems like the best option since we cannot get a long enough run on colab
** Everyone should continue working on their to-do list tasks until Wednesday/Thursday. We will then begin working on the final presentation during the Friday meeting

'''Individual Notes'''
* I was able to successfully run emade on colab using a remotemysql database, but I did run into an error with deap. Cameron had brought up this same error in a subteam meeting, and Anish had suggested uninstalling deap through conda and manually cloning the repo (this worked)
* I volunteered to add a ReLU activation layer as a primitive


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run emade on colab
|Complete
|Apr 6, 2020
|Apr 13, 2020
|Apr 11, 2020
|-
|Self-assign task from to-do list
|Complete
|Apr 10, 2020
|Apr 13, 2020
|Apr 11, 2020
|-
|Add ReLU layer to pset
|In progress
|Apr 10, 2020
|Apr 16, 2020
|Apr 16, 2020
|}


==Apr 10, 2020==
'''Subteam Meeting Notes'''
* I brought up how, due to my bugs with MySQL (others cannot connect to my server), I could not get colab to run. Anish suggested creating a database on [https://remotemysql.com remotemysql.com]
* The returning members discussed a to-do list of tasks they made and told the first semesters to self-assign themselves a task

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run emade on colab
|In progress
|Apr 6, 2020
|Apr 13, 2020
|Apr 11, 2020
|-
|Self-assign task from to-do list
|In progress
|Apr 10, 2020
|Apr 13, 2020
|Apr 11, 2020
|}


==Apr 6, 2020==
'''Subteam Meeting Notes'''
* First semesters went over the results of their notebooks, and others reported having very high accuracies
** *UPDATE* Mohan eventually discovered that this high accuracy was due to an incorrect evaluation function, as emade does not yet support multilabel classification
* Anish said he finished implementing emade on colab. He is finalizing a sample notebook with documentation that the first semesters should run through using a remotely-accessible MySQL server

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run emade on colab
|In progress
|Apr 6, 2020
|Apr 13, 2020
|Apr 11, 2020
|}


==Apr 5, 2020==
'''Individual Notes'''
* Since I have little experience with neural networks, I spent some time self-teaching myself the basics, using:
** A Google ML crash course [https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/anatomy article] and [https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/video-lecture video]
** A Crash Course YouTube video on [https://www.youtube.com/watch?v=oV3ZY6tJiA0 the basics] and another on [https://www.youtube.com/watch?v=lgKrup5oi_A training neural nets]
** A 3Blue1Brown [https://www.youtube.com/watch?v=aircAruvnKk YouTube video]
* I looked over the introductory materials posted in the slack for the first semesters, including:
** A pdf made by Anish about the basics of emade and the nlp-nn branch in particular
** An article on [https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/ multilabel text classification], and another on [https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/ multiclass, multilabel classification]
** A sample notebook made by Pulak (some screenshots shown below)  [[files/Keras intro.png|center|thumb|400x400px|The notebook's tokenizer]][[files/Keras intro 2.png|center|thumb|400x400px|Building the neural net]]

* I also ran through [https://github.gatech.edu/emade/emade/blob/nlp-nn/first_sem/toxic/Toxicity-keras.ipynb the toxicity notebook]
** I was only able to run 1 epoch since it started to take up too much of my computer's processing power, but I still got around 98% accuracy without changing the notebook
** I did not modify the notebook since it already had such high accuracy, but I did take a deep dive into how the notebook preprocessed the data, constructed the neural net, and examined its success. Below are a couple of code blocks of code I found particularly useful:   [[files/Toxic.png|center|thumb|400x400px|Preprocessing the data]][[files/Toxic 2.png|center|thumb|400x400px|Creating the network's layers]]

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Join slack and review posted materials
|Complete
|Mar 23, 2020
|
|Apr 5, 2020
|-
|Go through toxicity/x-ray notebook
|Complete
|Mar 23, 2020
|Apr 6, 2020
|Apr 5, 2020
|}


==Apr 3, 2020==
'''Subteam Meeting Notes'''
* First semesters reported their progress on the toxicity/x-ray notebook
** I had not gotten a chance to start, so I said I would complete it over the weekend
** Some first semesters reported having very high accuracies (95+) without modifying the notebook. The returning students were unsure of the reason for this but they suspected that the GLOVE embeddings had something to do with it and that they would look into it more over the weekend.
* Returning students discussed their progress, including Anish having gotten close to implementing EMADE on Google Colab

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Join slack and review posted materials
|In progress
|Mar 23, 2020
|
|Apr 5, 2020
|-
|Go through toxicity/x-ray notebook
|In progress
|Mar 23, 2020
|Apr 6, 2020
|Apr 5, 2020
|}


==Mar 30, 2020==
'''Subteam Meeting Notes'''
* First semesters were told to choose and work on the toxicity/x-ray data set individually instead of with a partner
** I chose to work on the toxicity dataset (NLP)
* Returning students gave a quick overview of EMADE (how it works, where to find important files, etc)
** First semesters were told to checkout the nlp-nn branch
** We can seed individuals by using a seeding script located at emade/src/GPFramework/seeding_from_file.py

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Join slack and review posted materials
|In progress
|Mar 23, 2020
|
|Apr 5, 2020
|-
|Go through toxicity/x-ray notebook
|In progress
|Mar 23, 2020
|Apr 6, 2020
|Apr 5, 2020
|}


==Mar 23, 2020==
'''Team Meeting Notes'''
* We transitioned to online classes on Blue Jeans
* I was assigned to the NLP-Neural Networks subteam

'''Subteam Meeting Notes'''
The returning students introduced new students to the subteam, including what they've been doing and what we'll be doing:
* Two groups: neural networks and stats
* First semesters will probably work on adding new primitives
* We'll be introduced to neural networks and the Keras API by working on a notebook for either the toxicity (NLP) or x-ray dataset (CV)
** Task: dive into the notebook, look at the preprocessing, how Keras is used, create a neural net with at least 85% accuracy
** We'll be assigned partners to do this with

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Join slack and review posted materials
|In progress
|Mar 23, 2020
|
|Apr 5, 2020
|-
|Go through toxicity/x-ray notebook
|In progress
|Mar 23, 2020
|Apr 6, 2020
|Apr 5, 2020
|}


==Mar 22, 2020==
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Rank subteams
|Complete
|Mar 9, 2020
|Mar 23, 2020
|Mar 22, 2020
|}


==Mar 9, 2020==
'''Team Meeting Notes'''

Each of the subteams, both from the first semester and returning students' classes gave a presentation on their progress. Below are my notes from the latter's presentations:
* ADFs (Automatically Defined Functions):
** Common subtrees are used as single primitives in future trees to decrease complexity
** Looking to implement ADFs into EMADE
** Future plans:
*** Continued analysis of common primitives/ADFs, when ADFs are most effective, caching and evaluation time
*** Differential fitness heuristic: fitness difference between child and best parent
*** Evolving ADFs, using ADFs in evolutionary process
*** Entropy Informed ADF Creation (use more ADFs for more diverse populations, and vice versa)
** Meets on Thursdays from 4:30-5:30
** Data analysis, cloud computing, ML
* NLP-Neural Network:
** Uses IMDb movie reviews dataset
** Prior semesters:
*** Text classification
*** Primitives added:
**** Vectorizers (convert text to numeric vectors)
**** Sentiment (converts text into vector decoding sentiment)
**** Stemmatizer (converts words to their base form using established word stems)
** Goal: genetically evolve neural networks, incorporate Keras API into EMADE
** Meets at 4:30 on Fridays
** For those interested in neural networks or NLP models
* Research Fundamentals (Bloat Control):
** Bloat: increase in the size of tree without improvement in fitness
** How it happens:
*** Fitness Causes Bloat Theory: bigger trees often result in increased fitness because they can explore more of the solution space
*** Crossover Bias Theory: smaller trees aren't selected for crossover as often as larger trees since they tend to be less fit
** Bloat is more time- and memory-intensive and results in less effective breeding
** Bloat = normalized size change / normalized fitness change
** Use speciation to maintain complexity and diversity
** Future plans:
*** Altering speciation distance threshold
*** Restricted vs unrestricted mating
*** Investigate drop in num of individuals by changing rate of crossover and mutation
*** Integrate fitness sharing and crossover changes
** Pitch:
*** Research on core parts of EMADE
*** Fundamental research about evolutionary algorithms
*** Meetings at 1:15 on Fridays
* ezCGP
** 3-step Process:
*** Data augmentation
*** Preprocessing
*** Training
** Uses CIFAR-10 dataset 60,000 images evenly divided across 10 classes
** Testing flatten primitive
** Focus on ReLU and ELU (popular activation functions)
** Future plans:
*** Add data augmentation
*** Support for tensorflow gpu
*** Optimize evolutionary process
*** Organize best run results and visualization to work more seamlessly

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Rank subteams
|In progress
|Mar 9, 2020
|Mar 23, 2020
|Mar 22, 2020
|}


==Mar 8, 2020==
'''Subteam Meeting Notes'''
* We had complications with the overnight run as our database was erased for some reason (even though our reuse was set to 1), so we stuck with our original, 32 generation run
** It is important to note that the original run that we ended up using for our presentation did not include the same preprocessing of the data that we used for the earlier stages of the project. We tried to do this with the overnight run that ended up not working out but, due to time complications, we could not try another extended run.
** Below is a graph depicting our final results for all three phases of the project: [[files/FP, FN, AUC for All 3 Final Runs.png|center|thumb|500x500px|This graph depicts the number of false positives and false negatives as well as the AUC for the ML, MOGP, and EMADE runs on the titanic dataset]]

'''Individual Notes'''
* I finished my assigned slides and rehearsed what I would say during the in-class presentation

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run EMADE on Titanic dataset
|Complete
|Feb 19, 2020
|Mar 9, 2020
|Mar 8, 2020
|-
|Finish Midterm Presentation
|Complete
|Feb 19, 2020
|Mar 9, 2020
|Mar 8, 2020
|-
|Rehearse for Midterm Presentation
|Complete
|Mar 7, 2020
|Mar 9, 2020
|Mar 8, 2020
|}


==Mar 7, 2020==
'''Subteam Meeting Notes'''
* We met and ran emade on the titanic dataset for a more extended period of time
* We only made it to 32 generations, which we did not think was enough, so we decided to save this run (i.e. we exported the table of pareto individuals to a csv) and run it again overnight
* For the new run we decided to use the same preprocessing of the data that we did for the ML and MOGP stages of the project
* We created and began working on [https://docs.google.com/presentation/d/1HKAtfxUGHAqXmvGPzLQAk5f3CI09wXYGTkR6E8MKtmQ/edit?usp=sharing our Google Slides presentation], but we could not finish and decided to complete our assigned slides individually (I worked on the "Data Preprocessing" and "What is EMADE?" slides)

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run EMADE on Titanic dataset
|In progress
|Feb 19, 2020
|Mar 9, 2020
|Mar 8, 2020
|-
|Finish Midterm Presentation
|In progress
|Feb 19, 2020
|Mar 9, 2020
|Mar 8, 2020
|-
|Rehearse for Midterm Presentation
|In progress
|Mar 7, 2020
|Mar 9, 2020
|Mar 8, 2020
|}


==Mar 4, 2020==
'''Subteam Meeting Notes'''
* All members were able to connect to Karthik's master process, though some workers experienced new, individual issues
* Those with worker-process issues worked on solving those, while the others (myself included) ran emade on the titanic dataset
** We did not get through enough generations, so we decided to meet over the weekend to resolve any lingering worker issues and run emade for more generations

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run EMADE on Titanic dataset
|In progress
|Feb 19, 2020
|Mar 9, 2020
|Mar 8, 2020
|-
|Finish Midterm Presentation
|In progress
|Feb 19, 2020
|Mar 9, 2020
|Mar 8, 2020
|}

==Mar 3, 2020==
'''Individual Notes'''
* I spent today learning some basic MySQL commands from the [https://www.w3schools.com/sql/ same w3schools link] as mentioned in a previous entry
* I also created dummy databases on my own and played around with Sequel Pro

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Learn Basic MySQL
|Complete
|Feb 19, 2020
|
|Mar 3, 2020
|-
|Run EMADE on Titanic dataset
|In progress
|Feb 19, 2020
|Mar 9, 2020
|Mar 8, 2020
|}


==Feb 29, 2020==
'''Subteam Meeting Notes'''
* Today was the Hackathon, so subteams worked by themselves to prepare their run of emade on the titanic dataset
* Our subteam continuing playing around with MySQL and emade, including connecting to each other's databases and running emade with each person trying to be the master
** Some people had trouble connected to someone else's database (including myself, see "Individual Notes" section below for more info)
** We decided Karthik would run the master process and the rest of us would be workers for our final run of emade that we'd use for our presentation

'''Individual Notes'''
* I revisited [https://askubuntu.com/questions/735601/how-to-completely-delete-mysql-for-clean-install an old forum] on deleting MySQL and it worked (the issue was that I mistakenly was inputting my root password for sudo commands instead of my computer password)
* After removing MySQL 8, I installed MySQL 5, this time using homebrew and [https://gist.github.com/operatino/392614486ce4421063b9dece4dfe6c21 these instructitons]
* I still was unable to connect to other people's databases. Dr. Zutty made various suggestions, including deleting and reinstalling MySQL through multiple different channels, but none worked. I also tried looking up solutions, but that was futile as well
* I also tried using MariaDB instead of MySQL (again, I installed it using homebrew). That did not work either, but I left MariaDB on my computer instead of deleting it and reinstalling MySQL
* I ultimately decided to just leave what I had on my computer and just not be the master (I could still connect to others' databases, so I was still able to be a worker)

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Learn Basic MySQL
|In progress
|Feb 19, 2020
|
|Mar 3, 2020
|-
|Run EMADE on Titanic dataset
|In progress
|Feb 19, 2020
|Mar 9, 2020
|Mar 8, 2020
|-
|Change root password
|Complete
|Feb 21, 2020
|
|Feb 29, 2020
|-
|Uninstall MySQL 8 and install MySQL 5
|Complete
|Feb 26, 2020
|Feb 29, 2020
|Feb 29, 2020
|}


==Feb 26, 2020==
'''Team Meeting Notes'''
* Dr. Zutty told us to downgrade from MySQL 8 to MySQL 5, as some features of emade would not work properly on the former
* Deleting MySQL requires the root password, so I continued trying to change my root password using the following links (still had little progress, though): 
** [https://blog.runcloud.io/retrieve-your-root-password-and-other-invaluable-mysql-commands/ blog.runcloud.io]
** [https://stackoverflow.com/questions/50691977/how-to-reset-the-root-password-in-mysql-8-0-11 stackoverflow.com]
** [https://www.tecmint.com/change-mysql-mariadb-root-password/ tecmint.com]

'''Subteam Meeting Notes'''
* Those who had MySQL installed and working began creating dummy databases and practiced connecting to each others' (I could not since my MySQL was not working, so I continued to work on that)

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Learn Basic MySQL
|In progress
|Feb 19, 2020
|
|Mar 3, 2020
|-
|Run EMADE on Titanic dataset
|In progress
|Feb 19, 2020
|Mar 9, 2020
|Mar 8, 2020
|-
|Change root password
|In progress
|Feb 21, 2020
|
|Feb 29, 2020
|-
|Uninstall MySQL 8 and install MySQL 5
|In progress
|Feb 26, 2020
|Feb 29, 2020
|Feb 29, 2020
|}

==Feb 21, 2020==
'''Lab Notes'''
* I installed MySQL and Sequel Pro on my computer
** Note: the suggested link on the emade github repo was for a Windows download, so I used the DMG Archive link on [https://dev.mysql.com/downloads/mysql/ the "MySQL Community Downloads" page] for Mac
* I began learning basic MySQL commands from [https://www.w3schools.com/sql/ w3schools]
* I tried to set up a test database to play around with MySQL, but I could not access my root account (probably because I entered the password wrong when downloading MySQL). I began researching how to access or change my root password but didn't come up with much. The following are some links I tried using:
** [https://www.howtoforge.com/setting-changing-resetting-mysql-root-passwords howtoforge.com]
** [https://www.techrepublic.com/article/how-to-set-change-and-recover-a-mysql-root-password/ techrepublic.com]
** [https://support.rackspace.com/how-to/mysql-resetting-a-lost-mysql-root-password/ support.rackspace.com]

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install MySQL
|Complete
|Feb 19, 2020
|
|Feb 21, 2020
|-
|Learn Basic MySQL
|In progress
|Feb 19, 2020
|
|Mar 3, 2020
|-
|Run EMADE on Titanic dataset
|In progress
|Feb 19, 2020
|Mar 9, 2020
|Mar 8, 2020
|-
|Change root password
|In progress
|Feb 21, 2020
|
|Feb 29, 2020
|}

==Feb 19, 2020==
'''Team Meeting Notes'''
* In lecture we were introduced to EMADE, including learning how to install it, how to run it, and some basics on how it works:
** EMADE (Evolutionary Multiobjective Algorithm Design Engine) uses to GP to optimize ML models, such as by automating hyperparameter tuning
*** In addition to basic boolean and mathematical functions as primitives, EMADE uses scikit and other ML models as primitives
** EMADE is run with the following command, with an optional <code>-w</code> parameter at the end to run as a worker: <code>python src/GPFramework/launchGTMOEP.py templates/input_file.xml</code> 
** Input file:
*** Configures python 
*** Includes necessary parameters for connecting to a MySQL database, including option to reuse individuals already in database from a previous run 
*** Specifies datasets (typically cross-folded into mulitple Monte Carlo trials)
**** Corresponding train and test files comprise a DataPair object 
*** Sets objectives, evaluation function, crossover/mutation functions, etc 
** Repo structure:
*** src/GPFramework: houses main code
**** gtMOEP.py: main engine, contains most of the evolutionary loop, including the evaluation function 
**** gp_framework_helper.py: builds the pset 
*** datasets/: houses some test datasets 
*** templates/: houses input files 

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install MySQL
|In progress
|Feb 19, 2020
|
|Feb 21, 2020
|-
|Learn Basic MySQL
|In progress
|Feb 19, 2020
|
|Mar 3, 2020
|-
|Run EMADE on Titanic dataset
|In progress
|Feb 19, 2020
|Mar 9, 2020
|Mar 8, 2020
|}

==Feb 17, 2020==
'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Download EMADE
|Completed
|Feb 12, 2020
|
|Feb 17, 2020
|}

==Feb 12, 2020==
'''Lab Notes'''
* Completed my slide for the presentation (which is on the preprocessing phase, particularly feature selection)
* This included creating an extra graph to illustrate how we created bins for the Age feature
** This was a histogram that demonstrated how certain age groups had a higher survival rate than others
'''Team Meeting Notes'''
* Each subteam gave a short presentation on their ML and GP models
* A couple of interesting notes were how different groups designed their models:
** Feature set: 
*** A couple of other groups used the Pclass feature, whereas we strongly considered using it but ultimately decided to cut it out
*** One other group combined the SibSp and Parch features into one "family size" feature like we did
** Other ML models used include voting ensemble, stochastic gradient descent, GNB, multi-layer perceptron
** We only used logical primitives, but other groups expanded beyond this, using primtives such as add, subtract, mulitply, negative, trig functions, and sigmoid
*** In hindsight, sigmoid probably would have been a good addition to our primitive set to normalize our data
** Group 5 did something unique with their data cleaning: instead of replacing NaN ages with the mean age (as the example from class did), they replaced the NaN values with the mean age corresponding to that passenger's title (since that would most likely be a more accurate estimation of that passenger's age)

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete and rehearse my slide
|Completed
|Feb 10, 2020
|
|Feb 12, 2020
|-
|Download EMADE
|In progress
|Feb 12, 2020
|
|Feb 17, 2020
|}

==Feb 10, 2020==
'''Subteam Meeting Notes'''
* Fixed the eval function by adding a third objective which aimed to minimize the square of the sum of the number of false positives and false negatives (we squared the sum so as to more heavily penalize larger sums because the sum by itself was not producing the desired results)
* Fixed the Pareto dominance function to take in a single individual and a list of other individuals thought to be Pareto and returns whether the given individual is Pareto

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create slides
|Completed
|Feb 5, 2020
|
|Feb 10, 2020
|-
|Fix eval function and evolutionary loop
|Completed
|Feb 8, 2020
|
|Feb 10, 2020
|-
|Complete and rehearse my slide
|In progress
|Feb 10, 2020
|
|Feb 12, 2020
|}

==Feb 8, 2020==
'''Subteam Meeting Notes'''
* Completed eval function to compare individuals based on number of false negatives and false positives
* Wrote preliminary Pareto dominance function and evolutionary loop
* We ran into a problem because our Pareto individuals were almost entirely those which had no false positives and a lot of false negatives
** We reserved this problem for our next meeting
* Below are the completed eval and Pareto dominance functions:
[[files/Parto Dominance Function.png|thumb|The two functions we used to identify Pareto individuals|center|750x750px]]
[[files/Eval Function.png|thumb|The function we used to evaluate individuals|center|750x750px]]

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create slides
|In progress
|Feb 5, 2020
|
|Feb 10, 2020
|-
|Complete eval function
|Completed
|Feb 6, 2020
|
|Feb 8, 2020
|-
|Fix eval function and evolutionary loop
|In progress
|Feb 8, 2020
|
|Feb 10, 2020
|}

==Feb 6, 2020==
'''Subteam Meeting Notes'''
* Created new notebook for genetic program for Titanic problem
* Used same setup for DEAP as we had in previous labs (main model was Genetic Programming lab and Multi-Objective lab)
* Created primitive set comprised of various boolean operators, such as np.logical_and np.greater
* Performed same data cleaning as last week's lab
* Trimmed feature set to same as used for the ML models in last week's labs (Age, Sex, and family_size)
[[files/GP Setup.png|center|thumb|700x700px|How we defined our primitive set and toolbox]]

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with team to discuss future plan for constructing genetic program
|Completed
|Feb 5, 2020
|
|Feb 6, 2020
|-
|Create slides
|In progress
|Feb 5, 2020
|
|Feb 10, 2020
|-
|Complete eval function
|In progress
|Feb 6, 2020
|
|Feb 8, 2020
|}

==Feb 5, 2020==
'''Team Meeting Notes'''
* In lecture we discussed common practices for delivering effective presentations in a research setting. These included:
** Having a clear and effective title slide
** Numbering all of our slides
** Making graphs easy to interpret by having a title, axes labels, and a readable font size
** Including a takeaway point on relevant slides
** Making slides with enough detail to be understood by themselves

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Pick a scikit model and develop Pareto frontier with group
|Completed
|Jan 29, 2020
|
|Feb 5, 2020
|-
|Meet with team to discuss future plan for constructing genetic program
|In progress
|Feb 5, 2020
|
|Feb 6, 2020
|-
|Create slides
|In progress
|Feb 5, 2020
|
|Feb 10, 2020
|}

== Jan 29, 2020 ==
'''Team Meeting Notes:'''
* Importing data: <code>pd.read_csv('fileName.csv')</code>
* Data cleaning:
** A couple of helpful methods for pandas data frames: <code>df.drop()</code>, <code>df.set_index()</code>, <code>df.loc[]</code>
** Checking for columns with NaN values: <code>df.columns[df.isna().any()].tolist()</code>
** Potential methods for dealing with NaN values:
*** Drop the column
*** Replace the NaN values with some value (such as the mean, median, or mode of the column): <code>df.fillna(value=nan_map, inplace=True)</code> where <code>nan_map</code> is a dictionary with keys equal to the columns with NaN values and their associated values are the values with which to replace the NaN values in that column
**** Note: <code>df.replace()</code> works similar to <code>df.fillna()</code> and can be used to replace qualitatitve data with quantitative data
* Training models:
** Most models already built in sklearn
** To train models, import it from sklearn, instantiate an object for it, and then call the fit method
* Evaluating the model:
** There are methods to score (by accuracy) models and get the predictions made by the model
** sklearn has pre-built methods for confusion matrices

'''Subteam Meeting Notes:'''
* Beginning of lab modeled the in-class jupyter notebook:
** Imported data
** Dropped unnecessary columns (Name, Ticket, and Cabin)
** Set the Passenger ID column as the indices for the data frame
** Filled NaN values with mean for numeric columns and mode for categorical data
** Converted categorical in Embarked and Sex columns to numeric data (numbers corresponding to each category)
* Creating our feature set:
** Consolidated the Sibsp and Parch columns into one column called family_size
** Grouped ages into bins
** Used a heatmap to determine which features best correlated to survival rate
** Ultimately decided to use the following columns: Sex, Age, and family_size
* Training our models:
** Everyone in group picked a model and trained it
** We all shared our number of false negatives and false positives to ensure no model was dominated by another one
* Final steps:
** Used model to make predictions on data from test.csv
** Converted predictions to a data frame, which I used to create a csv (the deliverable) listing all of my predictions [[files/Data cleaning.png|center|thumb|650x650px|Data cleaning]][[files/Heatmap code.png|center|thumb|700x700px|The heatmap we used to select our features as well as the code we used for grouping our ages and creating our family_size feature]]

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finish Multi-Objective GP lab
|Completed
|Jan 22, 2020
|
|Jan 29, 2020
|-
|Pick a scikit model and develop Pareto frontier with group
|In Progress
|Jan 29, 2020
|
|Feb 5, 2020
|}

== Jan 22, 2020 ==
'''Team Meeting Notes:''' 

GP cycle:
# New gene pool: set of genomes to be evaluated in this generation
## Genome: genotypic description of individuals (set of values for GA, set of values, tree structure for GP)
## Search space (genotypic description): set of all possible genomes
# Evaluation: associates genome (individual) with a set of scores
## Objectives: set of measurements each genome/individual is scored against
## Objective space (phenotypic description): set of objectives
## Evaluation maps a genome/individual from a location in the search space (all possible genomes) to a location in the objective space (the set of objectives)
### Genotypic to phenotypic description
# Genes w/ scores
# Fitness computation: each individual is evaluated using objective functions (MSE, TPR, ACC, etc)
## Objective scores give each individual a location in objective space (a.k.a. the genome’s phenotype)
# Genes w/ fitness
# Selection
## Should favor pareto individuals (but must still maintain randomness and diversity by giving all individuals a chance at mating)
# Parent genes
# Mating
# Child genes
# Mutation
# Repeat
## Among points on the pareto frontier, it is up to the algorithm to decide which are the most preferable
### It’s common to pick points closest (by Euclidean distance) to the ideal location (e.g. for a minimization problem for both objectives the ideal location is the origin)
### Again, if there are multiple points equidistant from this location then the algorithm decides how to break this tie
* Stat measures:
** True Positive Rate (TPR) -- a.k.a. sensitivity, hit rate, recall
*** TP/P = TP/(TP+FN)
** True Negative Rate (TNR) -- a.k.a. specificity
*** TN/N = TN/(FP+TN)
** False Negative Rate (FNR)
*** FN/P = FN/(TP+FN) = 1 - TPR
** False Positive Rate (FPR) -- a.k.a. fallout
*** FP/N = FP/(FP+TN) = 1 - TNR
** Positive Predictive Value (PPV) -- a.k.a. Precision
*** TP/(TP+FP)
** False Discovery Rate
*** FP/(TP+FP) = 1 - PPV
** Negative Predictive Value (NPV)
*** TN/(TN+FN)
** Accuracy (ACC)
*** (TP+TN)/(P+N) = (TP+TN)/(TP+FN+FP+TN)
Pareto Optimality
* An individual is pareto if there is no other point that outperforms it on all objectives
* Pareto frontier: the set of all pareto individuals (which represent unique contributions)
Sorting Algorithms
* Nondominated Sorting Genetic Algorithm II
** Population separated into non-domination ranks
** Individuals selected using binary tournament
** Higher pareto ranks (i.e. lower number) beat lower ranks
*** Ties are broken by crowding distance, sum of Euclidean distance to all other points on the front. Points with less points around it--i.e. higher crowding distance--are favored so that the model will explore lesser-explored algorithms
* Strength Pareto Evolutionary Algorithm 2
** Each individual given a strength S = number of points it dominates
** Each individual then receives a rank R = sum of strengths of individuals that dominate it
*** Pareto individuals have rank 0 (they aren’t dominated)
** A distance to the kth nearest neighbor (σk) is calculated and a fitness is obtained
*** Fitness = R + (σk+2)^(-1)

'''Lab Notes'''
* I ran through the DEAP documentation in order to figure out what was going on in the lab. Among the things that I discovered were:
** How the Pareto frontier is calculated
** How to find the area under the Pareto frontier
** How to define a target function and minimize the error and number of primitives used
* I also ran into trouble when trying to use other DEAP algorithms to lessen the AUC (area under the Pareto frontier), which I plan on resolving by asking Dr. Zutty for potential solutions

'''Action Items'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finish Multi-Objective GP lab
|In Progress
|Jan 22, 2020
|
|Jan 29, 2020
|}


== Jan 15, 2020 ==
'''Team Meeting Notes:'''
** Genetic Programming: Instead of taking an individual and having a function evaluation obtain objective scores, the individual is the function
** Tree Representation: representing program with tree structure made up of primitives and terminals
*** Nodes are called primitives and represent functions
*** Leaves are called terminals and represent params
**** Input can be thought of as a particular type of terminal
**** Output is produced at the root (top) of the tree
**** Ephemeral: in place of a constant terminal, an ephemeral is used, which is a function that returns some constant used as a terminal
*** Work from bottom up
*** Not stored as a tree → it’s converted to a lisp preordered parse tree
**** operator followed by inputs
**** Uses root first then expands: [+, input1, input2]  [+, *, input3, input4, 1]  [+, *, 3, 4, 1]  = 3*4 + 1
*** Arity: the number of arguments a function takes
** Crossover in GP
*** Exchanging subtrees
*** Start by randomly picking a point in each tree → this point and everything below create subtrees
** Mutation in GP
*** Can involve inserting/removing/changing a node or subtree
*** One way to change a node is to recall an ephemeral so that you’ll get a different value back
** Ex: symbolic regression for y=sin(x)
*** Primitives: +, -, *, /
*** Terminals: integers, x
*** Using 3rd order Taylor series: x-x3/3!
*** [-, x, /, *, x, *, x, x, *, 3, 2]
*** Evaluating a tree:
**** Feed a series of outputs: x = [0, 2*pi)
**** Run f(x)
**** Calculate MSE
'''Lab Notes'''
* Along with filling in the blank cells on the lab, I used various other methods to increase my understanding of genetic programming:
** I looked at the deap.gp documentation on certain methods (including genHalfAndHalf, PrimitiveSet, compile, etc)
** I used pprint() to print out various calls to gp.genHalfAndHalf with a range of min/max values to observe the different formats of trees returned
* A point made in the lab that I found noteworthy: "[genetic programming relies] a lot less about data than machine learning and care[s] more about objectives
* Among the blank cells I filled in the lab were adding np.sin and np.cos as primitives and using mutNodeReplacement as a secondary mutation function
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Week 2 Lab (up to section on multi-objective GP)
|Completed
|Jan 15, 2020
|
|Jan 21, 2020
|}


== Jan 8, 2020 ==
'''Team Meeting Notes:'''
* Dr. Zutty introduced the VIP to the class, including the general procedures of the course, the difference between Monday and Wednesday meetings, the role of the team wiki, the importance of our notebooks, and examples of current projects
* Lecture Notes:
** <u>Genetic algorithms</u>: evolving a population over generations
** Created through mating/mutation of individuals in the prev population → their fitness is then evaluated
** Iterative approach (search heuristic) to find the best algorithm of the options
** Keywords:
*** Individual: one specific candidate in the population (w/ properties such as DNA)
*** Population: a group of individuals whose properties will be altered
*** Objective: a value used to characterize individuals that you are trying to maximize or minimize (usually the goal is to increase objective through the evolutionary algorithm)
*** Fitness: relative comparison to other individuals; how well does the individual accomplish a task relative to the rest of the population
*** Evaluation: a function that computes the objective of an individual
*** Selection: represents “survival of the fittest”; gives preference to better individuals, therefore allowing them to pass on their genes
**** Fitness proportionate: the greater the fitness value, the higher the probability of being selected for mating
**** Tournament: several tournaments among individuals (num of individuals in each tournament independent on tournament size); winners are selected for mating
*** Mate/crossover: represents mating btw individuals
*** Mutate: introduces random modifications (purpose is to maintain diversity)
** Algorithms: various evolutionary algorithms to create a solution or best individual; process:
**# Randomly initialize population
**# Determine the fitness of the population
**# Repeat the following until the best individual is good enough:
**## Select parents from population (selected randomly)
**## Perform crossover on parents creating population
**## Perform mutation of population
**## Determine fitness of population
'''Lab Notes:'''
* Before beginning the lab I used various resources to familiarize myself with the DEAP framework:
** Read through online walkthrough of DEAP framework ([https://towardsdatascience.com/intro-to-evolutionary-computation-using-deap-618ca974b8cb link])
** Important points from DEAP documentation ([https://deap.readthedocs.io/en/master/overview.html Overview section] and [https://deap.readthedocs.io/en/master/examples/ga_onemax.html One-Max problem walkthrough]):
*** Creating custom and default types using <code>creator.create(name, baseClass, otherArgs)</code>
*** Initializing functions, fields (e.g. individuals, a population, etc), and operators (e.g. mate, mutate, etc) using <code>base.Toolbox().register(name, otherArgs)</code>
*** Create <code>main()</code> that carries out the evolution:
**** Evaluate the fitnesses of the initial population
***** Note: must create and register a function that evaluates the fitness of an individual
**** Iterating over the number of generations:
***** Clone population
***** Mate children based on pre-defined probability
***** Mutate individuals based on pre-defined probability
***** Recalculate fitnesses of individuals that were the product of a crossover or mutation
**** Calculate stats of final population
** Ran through Jupyter Notebook from class (similar to DEAP documentation of One-Max problem)
* One-Max section of the lab:
** Similar to DEAP documentation 
** Note: the probability specified when registering the mutate function refers to the probability of flipping each individual bit, whereas the probability in the if statement is the probability that we will call the mutate function on an individual
* N-Queens section of the lab:
** For my custom mutate function I simply shuffled the individual using the following method: <code>random.shuffle(individual)</code>
*** Our choices of how to mutate individuals were limited since we could not have duplicate bits in an individual (so bit-flipping would not be practical)
** This section demonstrated the limitations of genetic algorithms because we could not get a global minimum of 0 (which was the sought-after value) even after running through 100 generations (my best run yielded a minimum of 1)
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Setup wiki
|Completed
|Jan 8, 2020
|
|Jan 13, 2020
|-
|Join Slack channel
|Completed 
|Jan 8, 2020
|
|Jan 8, 2020
|-
|Run through Jupyter Notebook from class on genetic algorithms
|Completed
|Jan 8, 2020
|
|Jan 14, 2020
|-
|Go through introductory DEAP documentation
|Completed
|Jan 8, 2020
|
|Jan 13, 2020
|-
|Complete Week 1 Lab
|Completed
|Jan 8, 2020
|
|Jan 15, 2020
|}