'''Name:''' Rohan Batra

'''Email:''' [mailto:rbatra34@gatech.edu]

'''Cell Phone:''' 470-909-8524

'''VIP:''' Automated Algorithm Design

'''Interests:''' Artificial Intelligence, Machine Learning, Soccer, Guitar

= Fall 2021 =
== Week 15: November 29th - December 6th(2021) == 
===Outline of class notes===
'''In-class Notes:'''
* Went through the SCRUM for each group

'''Lecture Notes:'''
 




== Week 14: November 22nd - November 29th(2021) == 
===Outline of class notes===
'''In-class Notes:'''
* Went through the SCRUM for each group

'''Lecture Notes:'''
* We discussed how our data is almost ready and everyone from the bootcamp is almost setup with PACE now. We also talked about what we did over the week, such as running new mutation methods and finalising hyperfeatures.
* We then went over the work of every other group, discussed multiple bugs that everyone is facing, and brainstormed on finding the apt solutions to debug any problem.
* We worked on fixing multiple PACE issues (mainly with MySQL).

'''Individual Notes:'''
* Fixed the issue of 'Device or resource is busy' by using the command 'lsof +D' and the kill() command to stop all open mysql files from running. This allowed me to reinstall mysql that I needed to do to setup PACE.
* Currently, I am facing "Can't connect to server" issues.
* Working on adding the pbs scripts and fixing the MySQL problems.
* I am mentioning all the errors and issues I am facing in getting PACE setup to ensure I do not repeat them in the future and can work through them successfully.

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|November 22nd, 2021
|November 29th, 2021
|November 22nd, 2021
|-
|Review lecture notes
|Completed
|November 22nd, 2021
|November 29th, 2021
|November 22nd, 2021
|-
|Set up PACE
|In progress
|November 22nd, 2021
|November 29th, 2021
|November 29th, 2021
|-
|Individual Research 
|Completed 
|November 22nd, 2021
|November 29th, 2021
|November 25th, 2021
|}

== Week 13: November 15th - November 22nd(2021) == 
===Outline of class notes===
'''In-class Notes:'''
* Went through the SCRUM for each group

'''Lecture Notes:'''
* We discussed how each of the groups is working towards developing the final presentations for the end of the semesters and obtaining the desired objectives.
* Worked on multiple objectives such as data pre-processing and sharpening features for the hyperfeatures team.
* Bootcamp students worked on setting up PACE. It is almost done but many issues are being encountered, which are being solved as we move forward.

'''Sub-team meeting Notes:'''
* Workday
* Helped each other with multiple processes like processing data, look for the apt research papers for selection methods, mating and mutation methods etc.
* We also tried resolving other PACE issues. 

'''Individual Notes:'''
* Tried fixing the disk storage issue by removing all previous conda environments. It didn't fix the issue entirely.
* I asked Professor Zutty why I am facing 'Disk Quota Exceeded' issues. With his help and after using scripts like, "du -h ../" and "ls -larth" to find which dataset is taking uo most of the memory space. 
* Turns out there was one extremely large dataset that hadn't been removed because of which I was facing conda environment issues as PACE only offers a certain fixed amount of memory to each individual.
* Currently facing some issues while installing tensorflow in the conda environment.
* All other packages have been installed.
* I read few papers and did research on how sharpening hyperfeature can be improved.
* I worked on setting up PACE. 
* Faced multiple issues towards the end and now I am deciding to reinstall my MySQL and then run the steps again.
* The current error that I am getting is, "rm: cannot remove ‚Äòdb/.nfs000000007ed4a9f2000006d5‚Äô: Device or resource busy", when I try and remove all MySQL files.
* I am trying to fix this error by using the internet and taking help from sub-team members on Slack. 

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|November 15th, 2021
|November 22nd, 2021
|November 16th, 2021
|-
|Review lecture notes
|Completed
|November 15th, 2021
|November 22nd, 2021
|November 15th, 2021
|-
|Sub-team meeting
|Completed
|November 15th, 2021
|November 22nd, 2021
|November 17th, 2021
|-
|Set up PACE
|In progress
|November 15th, 2021
|November 22nd, 2021
|November 22nd, 2021
|-
|Individual Research 
|In progress
|November 15th, 2021
|November 22nd, 2021
|November 22nd, 2021
|}

== Week 12: November 8th - November 15th(2021) == 
===Outline of class notes===
'''In-class Notes:'''
* Went through the SCRUM for each group

'''Lecture Notes:'''
* Our team mentioned our objectives for the rest of the semester. We decided to create some new developments for improving hyperfeatures such as creating primitives for checking/improving the sharpening characteristics of the images. Also, we are trying to develop multi-class algorithms to classify images as per the diseases and check whether a disease is present.

'''Sub-team meeting notes:'''
* Workday 
* Tried understanding more about hyperfeatures and seeting up PACE.
* Faced the following issue: ERROR 2002 (HY000): Can‚Äôt connect to local MySQL server through socket ‚Äòscratch/db/mysqldb.sock‚Äô (2)

'''Individual Notes:'''
* Solved this issue by using SQL directly through the login node and then ssh into the compute node.
* Solved login issues by making sure I was logged into SQL through the root user.
* Faced issue: [Errno 122] Disk quota exceeded
* Currently working on solving this issue

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|November 8th, 2021
|November 15th, 2021
|November 10th, 2021
|-
|Review lecture notes
|Completed
|November 8th, 2021
|November 15th, 2021
|November 8th, 2021
|-
|Sub-team meeting
|Completed
|November 8th, 2021
|November 15th, 2021
|November 10th, 2021
|-
|Set up PACE
|In progress
|November 8th, 2021
|November 15th, 2021
|November 15th, 2021
|}
== Week 11: November 1st - November 7th(2021) == 
===Outline of class notes===
'''In-class Notes:'''
* Got assigned to sub-teams
* Went through the SCRUM for each group

'''Lecture Notes:'''
* I was assigned to the image processing sub team.
* Got familiar with what the team is working with and introduced myself to the team members.
* Told to clone the forked EMADE repository, read a research paper, and try to get PACE to work on my individual system

'''Sub-team meeting Notes:'''
* Had a round table discussion about what each group (out of Selection method group, data preparation group, mating and mutation method group, and hyperfeatures group) in the sub team has accomplished in this week.
** Selection: Lexicase vs NSGA II
** Hyper Features: Going through papers to understand implementing primitives.
** Mating and mutation: Creating new methods
** Data Preparation/infrastructure: Fixing bugs and prepping EMADE.
* Discussed possible solutions to every issue discussed. One issue faced is regarding the fact the trees are difficult to be converted to binary, which is leading to problems in creating datasets.
* The main branch in the sub-team's EMADE is not master and is cachev2.
* We are using nn-vip branch(Neural Architecture Search) through which we use the main Image-Processing(nn-vip).
* No deep learning in this main cachev2 branch.
* Path to our shared directory on PACE to access all datasets and be a part of the image processing group: root/storage/home/hpaceice1/shared-classes/materials/vip/AAD/ip_group
* You may need to seed your data with NN learners before running a fresh EMADE run on PACE. -- seeding_from_file.py xml.
* Assigned to the hyperfeatures subgroup.

'''Individual Notes:'''
* Cloned the forked EMADE repository.
* Read a part of the "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning" (https://arxiv.org/pdf/1711.05225v3.pdf) and the "Concurrent Neural Tree and Data Preprocessing AutoML for Image Classification" papers.
* I read through how each of small features in multiple images makes a major difference in classification later on and did my own research to understand the difference between multi-class and multi-label.
[[files/ChexnetImage2.png|height= 500px]] [[files/ChexnetPaperImage1.png|height=500px]]
* Did some research about PACE. Couldn't work too much with PACE as it was under maintenance. 
* Tried to configure PACE. Faced some issues with SCP transferring
* I fixed the SCP transferring issue by changing the default port number to my own custom port number in the cnf file. 

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|November 1st, 2021
|November 8th, 2021
|November 3rd, 2021
|-
|Review lecture notes
|Completed
|November 1st, 2021
|November 8th, 2021
|November 1st, 2021
|-
|Sub-team meeting
|Completed
|November 1st, 2021
|November 8th, 2021
|November 3rd, 2021
|-
|Cloned forked EMADE
|Completed
|November 1st, 2021
|November 8th, 2021
|November 1st, 2021
|-
|Read papers and worked with PACE
|In progress
|November 1st, 2021
|November 8th, 2021
|November 7th, 2021
|}

== Week 10: October 25th - November 1st(2021) == 
===Outline of class notes===
'''In-class Notes:'''
* Final Presentation Day

'''Lecture Notes:'''
* Understood more about the subteams like NLP, NAS, Image processing etc.
* Presented our work and findings on ML, MOGP, and EMADE models.
* Answered questions relating to why we were only able to run 10 generations.
* Mentioned our mistakes and how we could improve them.
* Talked about the multiple graphs we used in our presentation.

'''Individual notes:
* Talked about MOGP results and EMADE installation/difficulties.
* Mentioned how we could have improved our findings and answered questions regarding the graphs and generation issues.

'''Presentation Link:'''https://docs.google.com/presentation/d/1ShDz-7hPoor3ExWA9BKqiSzqn-G4ufgBYWor-mtlzdU/edit?usp=sharing

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|October 25th, 2021
|November 1st, 2021
|October 25th, 2021
|-
|Review lecture notes
|Completed
|October 25th, 2021
|November 1st, 2021
|October 25th, 2021
|-
|Presented our findings
|Completed
|October 25th, 2021
|November 1st, 2021
|October 25th, 2021
|}

== Week 9: October 20th - October 25th (2021) == 
===Outline of class notes===
'''In-class Notes:'''
* Workday with VIP almuni

'''Lecture Notes:'''
* Configured how to run all processes (master and workers)
* Joined the master server

'''Sub-Team meeting Notes:'''
* Worked out how to join worker processes and speed up the generation process.
* Plotted graphs for the different results we had.
* Prepared the presentation slides and accumulated the results for ML, MOGP, and EMADE.

'''Individual Notes:'''
* Gave the idea for using a virtual conda environment to fix the invalid python version issue.
* Helped others run EMADE and join the worker processes.
* Wrote the code for plotting graphs along with two other team members.
* Worked on the presentation slides and did more research on EMADE.

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|October 20th, 2021
|October 25th, 2021
|October 24th, 2021
|-
|Review lecture notes
|Completed
|October 20th, 2021
|October 25th, 2021
|October 20th, 2021
|-
|Worked on multiple issues (Worker Process)
|Completed
|October 20th, 2021
|October 25th, 2021
|October 24th, 2021
|-
|Created virtual conda environment
|Completed
|October 20th, 2021
|October 25th, 2021
|October 23th, 2021
|-
|Sub-Team Meeting
|Completed
|October 20th, 2021
|October 25th, 2021
|October 24th, 2021
|}


== Week 8: October 13th - October 20th (2021) == 
===Outline of class notes===
'''In class:'''
* Work on trying to get EMADE to run.

'''Lecture Notes:'''
* Workday.
* Join a common MySQL server made by one team member. Use the following command to do so: "mysql -h hostname -u username -d database_name -p"
* Dr. Zutty answered questions and helped resolve difficulties relating to running EMADE.

'''Sub-Team meeting notes:'''
* We were able to successfully get EMADE to recognize the MySQL database we were using.
* Worked on further methods and steps to start with the generation process.
* Faced a major issue such that the fitness values for individuals were always infinity.
* As we were facing issues with the data types for our primitives as well, we believe that the issue may be related to the false positive and false negative evaluation functions that are defined in evalFunctions.py.

'''Individual Notes:'''
* Did a lot of research on how to fix the (inf, inf, inf) issue. Tried asking research alumnis for any advice as well.
* Helped develop the evaluation functions with the team.
* Worked on the process of joining the MySQL server for worker processes. Faced some issues with python version
* Attempting to fix this python version issue by creating a virtual conda environment. Got this idea after doing some research.

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|October 13th, 2021
|October 20th, 2021
|October 17th, 2021
|-
|Review lecture notes
|Completed
|October 13th, 2021
|October 20th, 2021
|October 14th, 2021
|-
|Worked on multiple issues
|Completed
|October 13th, 2021
|October 20th, 2021
|October 20th, 2021
|-
|Tried creating virtual conda environment
|In progress
|October 13th, 2021
|October 20th, 2021
|October 20th, 2021
|-
|Sub-Team Meetings
|Completed
|October 13th, 2021
|October 20th, 2021
|October 16th - 19th, 2021
|}

== Week 7: October 6th - October 13th (2021) == 
===Outline of class notes===
'''In class:'''
* Reminded of notebook completion for mid term grading.
* Introduction to EMADE and MySQL.
* Final Presentation is on October 25th, 2021.

'''Lecture Notes:'''
* Introduced to EMADE (Evolutionary Multi-Objective Algorithm Design Engine) and understood the conceptual working of this engine.
* It combines/integrates the Multiple objective evolutionary search with advanced primitives to automate the designing of Machine Learning algorithms.
* Understood more about the input .xml file that is required for fulfilling our objectives of minimization.
* Discussed importance of evolution of parameters.
* Comprehended and went over the EMADE structure, headless chicken crossover, and final output of EMADE.
* Assigned tasks for the week:
** Install EMADE, using the instructions given in the README.md file
** After installation of all requirements, we have to run EMADE on the Titanic dataset as a group.
** Important to connect worker process to peer using "python src/GPFramework/launchGTMOEP.py templates/input_titanic.xml -w".
** Understand the usage of SQL and make sure to plot graphs and diagrams for final presentation on October 25th, 2021.


'''Sub-Team meeting Notes:'''
* All team members were able to successfully install EMADE and MySQL on their laptops
* Configured the input XML file for EMADE so that we can run the Titanic dataset.
* Worked on getting familiar with MySQL and run EMADE on titanic dataset.
* One-hot encoded "Embarked" feature in dataset to improve the algorithm and reflect changes under our MOGP assignment as well.
* Dealt with some pythonic issues, which we planned on fixing in the next meeting. Issue was related to evaluating individuals in queue.


'''Individual Notes:'''
* Completed all required mentioned in the README document before the sub-team meeting.
* Already had git and MySQL installed.
* Cloned the emade repository.
* Researched more on the working of MySQL and practiced using it with MySQL workbench.
* Tried to solve issues faced during sub-team meeting and debug the algorithm we were developing on the Titanic dataset. Reached out to Dr. Zutty to resolve the issue with XML.


'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|October 6th, 2021
|October 13th, 2021
|October 12th, 2021
|-
|Review lecture notes
|Completed
|October 6th, 2021
|October 13th, 2021
|October 8th, 2021
|-
|Installed all required softwares
|Completed
|October 6th, 2021
|October 13th, 2021
|October 11th, 2021
|-
|Sub-Team Meeting
|Completed
|October 6th, 2021
|October 13th, 2021
|October 12th, 2021
|}

== Week 6: September 29th - October 6th (2021) == 
===Outline of class notes===
'''In class:'''
* Presentation Day

'''Lecture Notes:'''
* Listened to presentations on MOGP and ML findings of other groups and asked questions.
* Presented our own analysis of the ML and MOGP findings. We compared these observations.
* Had a discussion about the changes and improvements we could have made in our algorithm. We should have used one-hot encoding for columns like 'Embarked'. This would have helped us convert alphabetical values into binary numerical values rather than non binary values. This could have increased our algorithms accuracy.
* Sub Team #2 Presentation: https://docs.google.com/presentation/d/1E5DIPJOt7uBeqUeYklg6TE7X7PTdOsaFdUjTDCrttkU/edit?usp=sharing

'''Individual Notes:'''
* Presented and explained the slides including Data Pre-Processing Procedure and the final comparison between MOGP and ML findings.
* Wrote the code for generating a csv file containing the best MOGP algorithm results and submitted it as a group.
* Completed Mid-Term Peer Evaluation for my teammates.

CSV File: https://drive.google.com/file/d/1U3DTjQCR9zZx5LvaO1SqspM6T-hH7Dig/view?usp=sharing

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|September 29th, 2021
|October 6th, 2021
|October 4th, 2021
|-
|Review lecture notes
|Completed
|September 29th, 2021
|October 6th, 2021
|September 30th, 2021
|-
|Peer Evaluations
|Completed
|October 4th, 2021
|October 6th, 2021
|October 8th, 2021
|}

== Week 5: September 22nd - September 29th (2021) == 
===Outline of class notes===

'''In class:'''
* Discussed last week's (Titanic Dataset) project.
* Got notified of this week's task.
* Learned how to give presentations.
* Had a discussion with group members as to when to schedule upcoming meetings.

'''Lecture Notes:'''
* While discussing last week's project, we found out that changing the hyper parameters was an important factor required for obtaining co-dominant algorithms.
* Sometimes, this change in hyper parameters led to a reduction in the Pareto optimal solutions/fitness scores.
* Completing the objective of obtaining co-dominant solutions required hit and trial and a few iterations for finding the apt models to achieve a successful outcome.
* For next week we have to use multiple objective genetic programming to find the Pareto optimal solutions for the same Titanic Dataset. This time we don't have any constraints such as finding a set of co-dominant algorithms. 
* Our aim is to improve the accuracy of our algorithm to find the best optimal solutions.
* We have to use basic primitives, such as logical and mathematical, to generate desired solutions.
* We are supposed to write our own algorithms for selection, crossover, mutator functions and not use tournament selection.
* Finally, we have to compare the Pareto frontier values (ML and GP) and then submit a csv file.
* Generation of own genetic loop is important and presentation of work and algorithms will take place next week.
* <b>Presentation about how to give a presentation:</b>
** Important to follow mentioned organized structure.
** Should present required schedule tables and graphs that have been plotted.
** Explain how you came up with the solution.
** Text can be on the slides but try not to read off the text directly.

'''Sub-team Meeting Notes:'''
* Discussed the steps required to attain desired output through the python notebook.
* Used primitives from Lab 2 to fulfill the requirements of this Lab.
* Experimented multiple models and algorithms to find the apt one for selection as we were not allowed to use tournament selection. We used SPEA2.
* Used symbolic regression evaluation function.
* Decided content to be added to our presentation.
* Programmed our own genetic loop.

[[files/VIPgraph.PNG|center|thumb]] [[files/GraphImage.png|height=300px]] 

'''Individual Notes:'''
* Did a lot of research on working of symbolic regression evaluation function and difference between NSGA II and SPEA2 for selection algorithms. Finalized evaluation function and data selection.
* Worked on programming the genetic loop. As it had to be completely original, coding one took a while and required each one of us to do our own research for the same.
* I designed the layout of the presentation and was responsible for verifying the code and information to be included.
* Added code for area under the curve to ensure an apt comparison of ML and MOGP.

'''Presentation:''' https://docs.google.com/presentation/d/1E5DIPJOt7uBeqUeYklg6TE7X7PTdOsaFdUjTDCrttkU/edit?usp=sharing

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 26th, 2021
|-
|Review lecture slides
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 25th, 2021
|-
|Held team meeting #1
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 26th, 2021
|- 
|Held team meeting #2
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 28th, 2021
|- 
|Completed Python notebook
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 29th, 2021
|- 
|Made Presentation
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 28th, 2021
|}

== Week 4: September 15th - September 22nd (2021) == 
===Outline of class notes===

'''In class:'''
* Sub-Teams assigned
* Discussion of Titanic dataset
* Overview of Machine learning libraries, especially Sci-kit Learn

'''Lecture Notes:'''
* Got assigned into sub-teams using our Pareto optimality (decided by the Professor)
** I have been assigned to Sub-Team #2 consisting of me, Manas, Aditya Kumaran, Adhitya Gurunathan, and Yisu Ma.
* Introduced to Kaggle, an online community based platform for data science and machine learning purposes.
* Discussed the Titanic Dataset on Kaggle.
** The Titanic Dataset:</u> The titanic problem is used for designing a model which can accurately predict whether or not a given passenger will survive on the basis of the data. Our task is to come up with co-dominant algorithms and submit each of our predictions in the form of a csv file on Canvas.
** Codominant Algorithms: Algorithms in which one does not dominate another on every objective. For this particular challenge, we have to classify survivors and minimize false positives (FP) and false negatives (FN).
* Went over the sample code/Python notebook related to this dataset. The notebook used sci-ket learn for modeling.
* Towards the end of the lecture, we broke out into our sub-teams, noted one another's contacts, and set up a meeting outside of lecture time.

===Titanic Dataset (ML)===
'''Sub-Team meeting Notes:'''
* Created a Discord Chat to keep in touch, share ideas, and schedule meetings.
* Scheduled a virtual meeting at 2:45 pm on 18th September. 
* Set up the Python notebook for the Titanic Challenge and opened it in Jupiter using the Anaconda Navigator.
* Discussed the parameters/features that our not vital for improving the accuracy of our Machine Learning algorithm.
* Removed the following parameters, as they did not affect the passenger survival rate significantly:
** Name
** PassengerID
** Ticket Number
** Fare
* We kept the 'Embarked' and 'Sex' parameter and mapped the alphabetical values to numerical ones for easier algorithmic comprehension using a dictionary (column map). This parameter was important as the port you boarded from would affect which cabins were available. Hence, this would have an impact on where one was when the ship sank.
* The impact of the parameters on the survival rate were reasons like not willing to separate from their family, class, age, gender, etc.
* We then created a NaN_map to fill in the missing Age and Embarked values.
* We tried using different models using the Sci Kit learn documentation. We used the RandomForestClassifier and the MLP classifier initially, which were co-dominant algorithms. We changed their parameters constantly to find the best optimal solutions. We imported the 'ensemble' class to run RandomForestClassifier.
* Then went through the documentation more to find co-dominant algorithms. After trying various algorithms we finally found three others, AdaBoostClassifier, SVM, and DecisiomTreeClassifier. We changed the parameters around in order to ensure that the algorithms were co-dominant. These changes led to a change in the algorithms accuracy, but the change was not large enough to affect the objective of the confusion matrix.
* Wrote multiple functions to plot scatter plot graphs and saved 5 different csv files (one for each member) for each model.
* Pareto Optimal Frontier using varying classifiers: (change in parameters mentioned in brackets)
**  Rohan = DecisionTreeClassifier (min_samples_leaf=30). False Positive = 9, False Negative = 45
**  Manas = RandomForestClassifier (n_estimators = 100, max_depth = 5, min_samples_leaf = 5, criterion = entropy, random_state = 2). False Positive = 18, False Negative = 29. 
**  Aditya = AdaBoostClassifier. False Positive = 32, False Negative = 21. 
**  Adithya =  MLP Classifier. False Positive = 26, False Negative = 26.
**  Yisu = SVM (used svm.SVC, sigmoid = kernel). False Positive = 0, False Negative = 104. 

[[files/GrpahPresentation.png|center|thumb]] 

'''Individual Contribution:'''
* I decided to use the DecisionTreeClassifier as each one of us were responsible for implementing our own algorithms.
* Made changes to its parameter, min_samples_leaf=30, to ensure it has an optimal value and co-dominance with other algorithms.
* This then got me a good accuracy with an algorithm that fit well with the other four algorithms we planned on using.
* Learned more about different parameters of this model from the Sci Kit learn documentation. I had initially tried different models like more kernel functions and nearest neighbor algorithms, but none fit as well in with the other algorithms we planned to choose.
* I explored multiple functions and learned more about their parameter usage as well.
* Two other members and I thought of mapping alphabetical values to numerical ones to ensure optimized and accurate supervised learning algorithmic values for the final solution.

CSV File: https://drive.google.com/file/d/1ZWLsUhp5cQVxyHYLrqaqe6KKr5hG-_Yl/view?usp=sharing

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|September 15th, 2021
|September 22nd, 2021
|September 18th, 2021
|-
|Review lecture slides
|Completed
|September 15th, 2021
|September 22nd, 2021
|September 17th, 2021
|-
|Held team meeting #1
|Completed
|September 15th, 2021
|September 22nd, 2021
|September 18th, 2021
|- 
|Held team meeting #2
|Completed
|September 15th, 2021
|September 22nd, 2021
|September 19th, 2021
|- 
|Completed Python notebooks with 5 different co-dominant algorithms
|Completed
|September 15th, 2021
|September 22nd, 2021
|September 19th, 2021
|}
== Week 3: September 8th - September 15th (2021) == 
===Self-Evaluation Form=== 
https://drive.google.com/file/d/1nv79npHVXmHw1cMCW4aEE8cNPSBdqY_t/view?usp=sharing

===Outline of class notes===

'''In class:'''
* Discussed 'Multiple Objectives ‚Äì The MO in MOGA and MOGP'
* Overview of the Pareto Optimality 
* Rated our Python and ML Skills

'''Lecture Notes:'''
* Had an open discussion about what an algorithm looks for in a mate. There are a variety of objectives that we aim to achieve by generating specific algorithms to the get the best solution. Specificity, precision, accuracy, etc. are few important characteristics/goals that we aim to achieve.
* <b>Genetic Programming Cycle:</b> New Gene Pool ‚Üí Evaluation ‚Üí Genes with scores ‚Üí Fitness Computation ‚Üí Genes with fitness ‚Üí Selection ‚Üí Parental Genes ‚Üí Mating ‚Üí Child Genes ‚Üí Mutation ‚Üí New Gene Pool
* Keywords:
** Gene pool: The set of genome to be evaluated during the current generation
** Genome: Genotypic description of an individuals (DNA, GA = set of values, GP = tree structure, string)
** Search Space: Set of all possible genome. For Automated Algorithm Design it's the set of all possible algorithms
** The Evaluation of a Genome: Associates a genome/individual (set of parameters for GA or string for GP) with a set of scores.
*** From a location in search space: Genotypic description
*** To a location in objective space: Phenotype description
* <b>Important measures for multi objectivity: </b>
** True Positive ‚Äì TP: How often are we identifying the desired object
** False Positive ‚Äì FP: How often are we identifying something else as the desired object
** True Negative - TN: How often are we identifying the non-desired object
** False Negative - FN: How often are we identifying the desired object as something else
** Objectives: Set of measurements each genome (or individual) is scored against
** Objective Space: Set of objectives 
* Confusion Matrices use TP, FP, TN, FN to visualize outcomes.
* Other measures that we use for optimization and prediction:
** Sensitivity or True Positive Rate (TPR): AKA hit rate or recall, TPR = TP/P = TP/(TP+FN)
** Specificity (SPC) or True Negative Rate (TNR): TNR = TN/N = TN/(TN+FP)
** False Negative Rate (FNR):FNR = FN/P = FN/(TP+FN), FNR = 1 - TPR
** Fallout or False Positive Rate (FPR): FPR = FP/N = TN/(FP+TN), FPR = 1 ‚Äì TNR = 1 - SPC
** Precision or Positive Predictive Value (PPV), PPV = TP / (TP + FP), Bigger is better
** False Discovery Rate, FDR = FP/(TP + FP), FDR = 1 - PPV, Smaller is better
** Negative Predictive Value (NPV), NPV = TN / (TN + FN), Bigger is better
** Accuracy (ACC), ACC = (TP+TN) / (P+N), ACC = (TP+TN) / ( TP + FP + FN + TN),  Bigger is better
** Objective Space:Each individual is evaluated using objective functions like Mean squared error, Cost, Complexity, True positive rate etc.
** Objective scores give each individual a point in objective space
** This may be referred to as the phenotype of the individual 
* All these measures help us in selecting the right algorithms, mating, fitness computation etc.
* <b>Pareto Optimality</b>
** An individual is Pareto optimal if there is no other individual in the population that outperforms the individual on all objectives
** The set of all Pareto individuals is known as <i>the Pareto frontier.</i>
** These individuals represent unique contributions. We want to drive selection by favoring Pareto individuals (But maintain diversity by giving all individuals some probability of mating).
* Two types of Multi objectivity algorithms:
** Non-dominated Sorting Genetic Algorithm II (NSGA II)
*** Population is separated into non-domination (nothing over performs it at all individuals and that is why it called Pareto optimal) ranks. Individuals are selected using a binary tournament and a lower Pareto ranks beat higher Pareto ranks.
*** Ties on the same front are broken by crowding distance (more diversity is better) (its better if points are pareto optimals but a little far from one another). Summation of normalized Euclidian distances to all points within the front. Higher crowding distance wins
** Strength Pareto Evolutionary Algorithm 2 (SPEA2)
*** Each individual is given a strength S and receives a rank R. S refers to the number of others in the population that it dominates and R refers to the sum of S‚Äôs of the individuals that dominate it. Pareto individuals are nondominated and receive an R of 0.
*** A distance to the kth nearest neighbor (ùõîk) is calculated and a fitness of R + 1/(ùõîk + 2) is obtained

===Lab 2 (Part 2): Multi-Objective Genetic Programming===
* Followed instructions given in the python notebook and tried to minimize the mean squared error along with the tree size.
* I also plotted the Pareto frontier. The main reason why we are trying to minimize area under the curve is to reduce distance of the Pareto curve with the axes and increase the probability of finding optimized and better individuals to minimize the two objectives.
* The area under the curve with the original parameters was: 2.463792426733847
* Best individual is: negative(cos(multiply(add(cos(sin(cos(multiply(add(cos(cos(x)), cos(add(multiply(x, x), sin(x)))), tan(x))))), cos(x)), tan(x)))) with fitness: (0.27530582924947056, 25.0)
[[files/2.2.1.png]] [[files/2.2.2.png]] [[files/2.2.3.png]]
* Now, our goal is to reduce AUC by at least 25% by changing the parameters.
* To achieve this goal, I attempted to remove the three trigonometric functions.
* First, when I just removed the tan function, the area under the curve increased; however, once I removed all three functions, the area under the curve reduced and became 0.6912744703006891.
* Best individual is: subtract(x, x) with fitness: (0.7223441838209306, 3.0). This was the best individual in this case.
[[files/2.2.2.3.png]] [[files/2.2.2.2.png]] [[files/2.2.2.1.png]]

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|September 8th, 2021
|September 15th, 2021
|September 11th, 2021
|-
|Review lecture slides
|Completed
|September 8th, 2021
|September 15th, 2021
|September 10th, 2021
|-
|Complete Self-Evaluation Form
|Completed
|September 8th, 2021
|September 15th, 2021
|September 10th, 2021
|- 
|Complete Lab 2 (Part 2)
|Completed
|September 8th, 2021
|September 15th, 2021
|September 12th, 2021
|}

== Week 2: September 1st - September 8th (2021) == 
===Outline of class notes===

'''In class:'''
* Overview of Genetic Programming.
* Learnt about Tree Representation.
* Briefly discussed Crossover and Mutation in Genetic Programming and Symbolic Regression.

'''Lecture Notes:'''
* Did a brief review of last week's material regarding Genetic Algorithms.
* Instead of taking an individual and having a function evaluator to obtain objective scores, the individual will be the function itself.
* Discussed one of the most common program structures in genetic programming, <b>Tree Representation</b>. Went over its uses and how to traverse over this tree. Parts of tree representation:
** Nodes are called primitives and represent functions, eg: +, -, x, /.
** Leaves are called terminals and represent parameters. The input can be thought of as a particular type of terminal. The output is produced at the root of the tree.
* To store the tree, the tree is converted to a lisp preordered parse tree and the order is determined by the inputs, so we use the root first and then go down.
* For example, if we use the following representation, we can determine how to traverse through the tree to determine the function by going top down and left to right.
[[files/TreeRepresentation1.png]]

So for this tree representation the function will be, f(x) = 3 * 4 + 1 and the parse would be [+,*,3,4,1].
* Crossover in tree-based GP is simply done by exchanging subtrees. Start by randomly picking a point in each tree and then these points and everything below create subtrees.
* The function with the best output mate and then produce the next generation of children.
* We also use the root sum (mean) square error method to calculate the optimized output and algorithms.
* Mutation can involve and is done by:
** Inserting a node or subtree
** Deleting a node or subtree
** Changing a node
* Learned more about the example of Symbolic Regression.
* Symbolic Regression is an example involving the mutation of GP that is used to evolve solutions using primitives.
* For instance: We use primitives like sin function, factorial, cos function, tan function, exponential, summation to evolve a solution to y = sin(x) by using the Taylor Series formula for sin(x).
* Finally we discussed that to evaluate the tree, we can measure the difference between the truth and the output that we obtain after feeding the a number of input points into the function to get outputs and then running f(x).

===LAB 2: Symbolic Regression===
* Tried to derive/inherit individuals from DEAP's PrimitiveTree instead of lists.
* Added primitives to the primitive set and chose a new mutation to find the most optimized solution.
* Compiled the primitive tree to generate the evaluation function, which finds the squared sum of the difference between our desired function's output and our individual's output. In this case we are trying to find minimum values that approaches zero faster.
* I tried modifying the code in three different ways:
** <u>The original program</u>:
[[files/Original Lab 2.png|center|thumb]]

*** First Attempt: Best individual is add(add(negative(negative(multiply(x, x))), multiply(subtract(multiply(x, x), negative(x)), multiply(x, x))), x), (1.0123748708878107e-16,)
*** Second Attempt: Best individual is add(add(add(multiply(add(multiply(multiply(x, x), x), multiply(x, x)), x), multiply(x, x)), x), subtract(x, x)), (8.59033944318508e-17,)
*** Depicts requirement for a more optimized and consistent solution to obtain minimum efficiently.
** <u>Program with added mutation</u>

*** First Mutation (mutInsert): Best individual is add(multiply(x, add(add(multiply(x, multiply(x, x)), x), multiply(x, negative(negative(x))))), x), (1.1585916677755867e-16,)

[[files/part2Lab2.png|center|thumb]] 

Even after running the code for a number of times the error is lesser and the solution is more consistent and optimized than the original program.

*** Second Mutation (mutShrink): Best individual is add(add(multiply(x, x), multiply(x, multiply(x, x))), add(x, multiply(x, multiply(multiply(x, x), x)))), (9.522005638492831e-17,)

[[files/part2lab21.png|center|thumb]]

Values were usually close to zero, but in some cases the value was very high (for maximum) and not as consistent. 
** <u>Program with only added primitives</u>

[[files/extralab2.png|center|thumb]]

*** Best individual is add(add(multiply(x, x), multiply(multiply(add(square(x), x), x), x)), x), (1.0172711918255375e-16,)

** <u>Program with both added primitives and mutation</u>
*** We used the mutInsert mutation as it seemed more consistent.

*** First pair of primitives (sin and square): Best individual is add(x, add(multiply(add(x, square(x)), multiply(x, x)), multiply(x, x))), (1.0123748708878107e-16,)
[[files/lab2primitive1.png|center|thumb]]

*** Second pair of primitives (absolute, cos): Best individual is add(add(cos(add(cos(subtract(x, x), cos(add(negative(x), subtract(x, negative(multiply(x, x)))), x)), x), x), cos(subtract(x, x), x)), add(x, x)), (0.0,)
[[files/finallab2.png|center|thumb]]

<b>In this lab, I attempted to reduce error to bring it almost to zero. This can be seen best in the graph where I used the absolute and cos primitives with the mutInsert mutation.</b>

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes (GP)
|Completed
|September 1st, 2021
|September 8th, 2021
|September 2nd, 2021
|-
|Review lecture slides
|Completed
|September 1st, 2021
|September 8th, 2021
|September 1st, 2021
|- 
|Complete Lab 2 (Part1)
|Completed
|September 1st, 2021
|September 8th, 2021
|September 4th, 2021
|}

== Week 1: August 25th - September 1st (2021) ==

===Outline of class notes===

'''In class:'''

* Overview of Automated Algorithm Design Wiki, syllabus, notebooks, etc.
* Started lecture on Genetic Algorithms.
* Briefly discussed the One Max Problem.

'''Lecture Notes:'''

* <u> The Concept </u>
** With genetic algorithms, each new generation is created through mating/mutation of individuals in the previous population  
** Their fitness is evaluated before mating/mutation.
** This fitness evaluation is done through the Fitness Proportionate method (randomization) or tournament method.
** Through numerous operations of the different processes, it will eventually produce the best individual 

* <u>Important Keywords</u>
** Algorithms: various evolutionary algorithms to create a solution or best individual.
** Individual: One specific candidate in the population (with properties such as DNA). In terms of programming this can be seen as a single solution to a problem.
** Population: Group of individuals whose properties will be altered. This refers to a number of solutions (set) for a problem.
** Objective: A value used to characterize individuals that you are trying to maximize or minimize. Usually the goal is to increase objective through the evolutionary algorithm.
** Fitness: Relative comparison to other individuals of the population.
** Evaluation: A function that computes the objective of an individual.
** Selection: This represents ‚Äòsurvival of the fittest'. Preference given to better individuals, therefore allowing them to pass on their genes.
*** Fitness Proportionate: The greater the fitness value, the higher the probability of being selected for mating.
*** Tournament: Several tournament style competitions among various individuals. Winner selected for mating.
** Mate/Crossover: Represents mating between individuals.
** Mutate: Introduce random modification. The purpose is to maintain diversity.

* <u>The Process:</u>
** Randomly initialize population
** Determine fitness of population
** Select parents from population
** Perform crossover on parents creating population
** Perform mutation of population
** Determine fitness of population
** Repeat until best individual is good enough

===Lab 1: Genetic Algorithms with DEAP===

'''One Max Problem:''' This is a simple genetic algorithm problem and its objective is to convert all 0's and 1's in a vector/bitstring to all 1's. The outline and process completed is as follows (as per steps followed in the DEAP Python notebook):
* Imported the required modules (mostly DEAP tools)
* Defined individual classes, the fitness objective, and functions using DEAP's ToolBox.
* Represented bit string individuals as Booleans.
* Evaluated the population, ran the code repeatedly, performed tournament selection, mating, and mutation.
** Mating: Two-point crossover function
** Mutate: Flipping a bit in our bit string to either 1 or 0 respectively with an independent probability of flipping each individual bit of 5%.
* Ran the algorithm for 40 generations.
* <u>Conclusion:</u> Global maximum fitness of 100.0 was usually achieved in 40 generations but sometimes this was not the case; however, the maximum fitness obtained was always very close to 100.0. This method of tournament selection among 3 individuals is better than random search.

'''The N Queens Problem:''' The problem is to determine a configuration of n queens on a nxn chessboard such that no queen can be taken by one another. In this version, each queen is assigned to one column, and only one queen can be on each line. The outline and process completed is as follows (as per steps followed in the DEAP Python notebook):
* Imported the required modules (mostly DEAP tools)
* Defined individual classes, the fitness objective, and functions using DEAP's ToolBox.
* The fitness objective is to minimize the number of conflicts between 2 queens on the chessboard. Used to range function as well.
* Defined a function called "permutation" to help create our individuals and population and others like Evaluation (minimize diagonal conflicts), Selection (tournament selection of 3 individuals), Crossover (partially matched), and Mutation (shuffle indexes) functions.
* Ran main evolutionary loop for 100 generations. 
* Current algorithm did not always achieve the global minimum (0.0) but was consistently close.
* Visualization done to achieve more efficiency and speed using graphs as follows (100 generations graph):
[[files/FirstLab.png|center|thumb]]
* Iterations can be reduced with change is parameters and functions. This is clear as we can see that the minimum is easily obtained around the 25th generation. I am trying to work on making few changes in the algorithm for improved results by changing the mutation and mating probabilities.

For both problems we also found the basic statistics such as mean, minimum, maximum, and standard deviation, which helped to show that the final required output could be obtained in earlier generations itself with some tweaks made in the code.


'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install and set up DEAP library for Python
|Completed
|August 25th, 2021
|September 1st, 2021
|August 26th, 2021
|-
|Begin Notebook
|Completed
|August 25th, 2021
|September 1st, 2021
|August 25th, 2021
|- 
|Review Slides
|Completed
|August 25th, 2021
|September 1st, 2021
|August 26th, 2021
|-
|Complete Lab 1
|Completed
|August 25th, 2021
|September 1st, 2021
|August 29th, 2021
|}









