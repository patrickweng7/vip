'''Name:''' Rohan Batra

'''Email:''' [mailto:rbatra34@gatech.edu]

'''Cell Phone:''' 470-909-8524

'''VIP:''' Automated Algorithm Design

'''Interests:''' Artificial Intelligence, Machine Learning, Soccer, Guitar

= Fall 2021 =
== Week 6: September 29th - October 5th (2021) == 
===Outline of class notes===
'''In class:'''
* Presentation Day

'''Lecture Notes:'''
* Listened to presentations on MOGP and ML findings of other groups and asked questions.
* Presented our own analysis of the ML and MOGP findings. We compared these observations.
* Had a discussion about the changes and improvements we could have made in our algorithm. We should have used one-hot encoding for columns like 'Embarked'. This would have helped us convert alphabetical values into binary numerical values rather than non binary values. This could have increased our algorithms accuracy.
* Sub Team #2 Presentation: https://docs.google.com/presentation/d/1E5DIPJOt7uBeqUeYklg6TE7X7PTdOsaFdUjTDCrttkU/edit?usp=sharing

'''Individual Notes:'''
* Presented and explained the slides including Data Pre-Processing Procedure and the final comparison between MOGP and ML findings.
* Wrote the code for generating a csv file containing the best MOGP algorithm results and submitted it as a group.
* Completed Mid-Term Peer Evaluation for my teammates.

CSV File: https://drive.google.com/file/d/1U3DTjQCR9zZx5LvaO1SqspM6T-hH7Dig/view?usp=sharing

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|September 29th, 2021
|October 5th, 2021
|October 4th, 2021
|-
|Review lecture notes
|Completed
|September 29th, 2021
|October 5th, 2021
|September 30th, 2021
|-
|Peer Evaluations
|Completed
|October 4th, 2021
|October 5th, 2021
|October 8th, 2021
|}

== Week 5: September 22nd - September 29th (2021) == 
===Outline of class notes===

'''In class:'''
* Discussed last week's (Titanic Dataset) project.
* Got notified of this week's task.
* Learned how to give presentations.
* Had a discussion with group members as to when to schedule upcoming meetings.

'''Lecture Notes:'''
* While discussing last week's project, we found out that changing the hyper parameters was an important factor required for obtaining co-dominant algorithms.
* Sometimes, this change in hyper parameters led to a reduction in the Pareto optimal solutions/fitness scores.
* Completing the objective of obtaining co-dominant solutions required hit and trial and a few iterations for finding the apt models to achieve a successful outcome.
* For next week we have to use multiple objective genetic programming to find the Pareto optimal solutions for the same Titanic Dataset. This time we don't have any constraints such as finding a set of co-dominant algorithms. 
* Our aim is to improve the accuracy of our algorithm to find the best optimal solutions.
* We have to use basic primitives, such as logical and mathematical, to generate desired solutions.
* We are supposed to write our own algorithms for selection, crossover, mutator functions and not use tournament selection.
* Finally, we have to compare the Pareto frontier values (ML and GP) and then submit a csv file.
* Generation of own genetic loop is important and presentation of work and algorithms will take place next week.
* <b>Presentation about how to give a presentation:</b>
** Important to follow mentioned organized structure.
** Should present required schedule tables and graphs that have been plotted.
** Explain how you came up with the solution.
** Text can be on the slides but try not to read off the text directly.

'''Sub-team Meeting Notes:'''
* Discussed the steps required to attain desired output through the python notebook.
* Used primitives from Lab 2 to fulfill the requirements of this Lab.
* Experimented multiple models and algorithms to find the apt one for selection as we were not allowed to use tournament selection. We used SPEA2.
* Used symbolic regression evaluation function.
* Decided content to be added to our presentation.
* Programmed our own genetic loop.

[[files/VIPgraph.PNG|center|thumb]] [[files/GraphImage.png|height=300px]] 

'''Individual Notes:'''
* Did a lot of research on working of symbolic regression evaluation function and difference between NSGA II and SPEA2 for selection algorithms. Finalized evaluation function and data selection.
* Worked on programming the genetic loop. As it had to be completely original, coding one took a while and required each one of us to do our own research for the same.
* I designed the layout of the presentation and was responsible for verifying the code and information to be included.
* Added code for area under the curve to ensure an apt comparison of ML and MOGP.

'''Presentation:''' https://docs.google.com/presentation/d/1E5DIPJOt7uBeqUeYklg6TE7X7PTdOsaFdUjTDCrttkU/edit?usp=sharing

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 26th, 2021
|-
|Review lecture slides
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 25th, 2021
|-
|Held team meeting #1
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 26th, 2021
|- 
|Held team meeting #2
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 28th, 2021
|- 
|Completed Python notebook
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 29th, 2021
|- 
|Made Presentation
|Completed
|September 22nd, 2021
|September 29th, 2021
|September 28th, 2021
|}

== Week 4: September 15th - September 22nd (2021) == 
===Outline of class notes===

'''In class:'''
* Sub-Teams assigned
* Discussion of Titanic dataset
* Overview of Machine learning libraries, especially Sci-kit Learn

'''Lecture Notes:'''
* Got assigned into sub-teams using our Pareto optimality (decided by the Professor)
** I have been assigned to Sub-Team #2 consisting of me, Manas, Aditya Kumaran, Adhitya Gurunathan, and .
* Introduced to Kaggle, an online community based platform for data science and machine learning purposes.
* Discussed the Titanic Dataset on Kaggle.
** The Titanic Dataset:</u> The titanic problem is used for designing a model which can accurately predict whether or not a given passenger will survive on the basis of the data. Our task is to come up with co-dominant algorithms and submit each of our predictions in the form of a csv file on Canvas.
** Codominant Algorithms: Algorithms in which one does not dominate another on every objective. For this particular challenge, we have to classify survivors and minimize false positives (FP) and false negatives (FN).
* Went over the sample code/Python notebook related to this dataset. The notebook used sci-ket learn for modeling.
* Towards the end of the lecture, we broke out into our sub-teams, noted one another's contacts, and set up a meeting outside of lecture time.

===Titanic Dataset (ML)===
'''Sub-Team meeting Notes:'''
* Created a Discord Chat to keep in touch, share ideas, and schedule meetings.
* Scheduled a virtual meeting at 2:45 pm on 18th September. 
* Set up the Python notebook for the Titanic Challenge and opened it in Jupiter using the Anaconda Navigator.
* Discussed the parameters/features that our not vital for improving the accuracy of our Machine Learning algorithm.
* Removed the following parameters, as they did not affect the passenger survival rate significantly:
** Name
** PassengerID
** Ticket Number
** Fare
* We kept the 'Embarked' and 'Sex' parameter and mapped the alphabetical values to numerical ones for easier algorithmic comprehension using a dictionary (column map). This parameter was important as the port you boarded from would affect which cabins were available. Hence, this would have an impact on where one was when the ship sank.
* The impact of the parameters on the survival rate were reasons like not willing to separate from their family, class, age, gender, etc.
* We then created a NaN_map to fill in the missing Age and Embarked values.
* We tried using different models using the Sci Kit learn documentation. We used the RandomForestClassifier and the MLP classifier initially, which were co-dominant algorithms. We changed their parameters constantly to find the best optimal solutions. We imported the 'ensemble' class to run RandomForestClassifier.
* Then went through the documentation more to find co-dominant algorithms. After trying various algorithms we finally found three others, AdaBoostClassifier, SVM, and DecisiomTreeClassifier. We changed the parameters around in order to ensure that the algorithms were co-dominant. These changes led to a change in the algorithms accuracy, but the change was not large enough to affect the objective of the confusion matrix.
* Wrote multiple functions to plot scatter plot graphs and saved 5 different csv files (one for each member) for each model.
* Pareto Optimal Frontier using varying classifiers: (change in parameters mentioned in brackets)
**  Rohan = DecisionTreeClassifier (min_samples_leaf=30). False Positive = 9, False Negative = 45
**  Manas = RandomForestClassifier (n_estimators = 100, max_depth = 5, min_samples_leaf = 5, criterion = entropy, random_state = 2). False Positive = 18, False Negative = 29. 
**  Aditya = AdaBoostClassifier. False Positive = 32, False Negative = 21. 
**  Adithya =  MLP Classifier. False Positive = 26, False Negative = 26.
**  Yisu = SVM (used svm.SVC, sigmoid = kernel). False Positive = 0, False Negative = 104. 

[[files/GrpahPresentation.png|center|thumb]] 

'''Individual Contribution:'''
* I decided to use the DecisionTreeClassifier as each one of us were responsible for implementing our own algorithms.
* Made changes to its parameter, min_samples_leaf=30, to ensure it has an optimal value and co-dominance with other algorithms.
* This then got me a good accuracy with an algorithm that fit well with the other four algorithms we planned on using.
* Learned more about different parameters of this model from the Sci Kit learn documentation. I had initially tried different models like more kernel functions and nearest neighbor algorithms, but none fit as well in with the other algorithms we planned to choose.
* I explored multiple functions and learned more about their parameter usage as well.
* Two other members and I thought of mapping alphabetical values to numerical ones to ensure optimized and accurate supervised learning algorithmic values for the final solution.

CSV File: https://drive.google.com/file/d/1ZWLsUhp5cQVxyHYLrqaqe6KKr5hG-_Yl/view?usp=sharing

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|September 15th, 2021
|September 22nd, 2021
|September 18th, 2021
|-
|Review lecture slides
|Completed
|September 15th, 2021
|September 22nd, 2021
|September 17th, 2021
|-
|Held team meeting #1
|Completed
|September 15th, 2021
|September 22nd, 2021
|September 18th, 2021
|- 
|Held team meeting #2
|Completed
|September 15th, 2021
|September 22nd, 2021
|September 19th, 2021
|- 
|Completed Python notebooks with 5 different co-dominant algorithms
|Completed
|September 15th, 2021
|September 22nd, 2021
|September 19th, 2021
|}
== Week 3: September 8th - September 15th (2021) == 
===Self-Evaluation Form=== 
https://drive.google.com/file/d/1nv79npHVXmHw1cMCW4aEE8cNPSBdqY_t/view?usp=sharing

===Outline of class notes===

'''In class:'''
* Discussed 'Multiple Objectives – The MO in MOGA and MOGP'
* Overview of the Pareto Optimality 
* Rated our Python and ML Skills

'''Lecture Notes:'''
* Had an open discussion about what an algorithm looks for in a mate. There are a variety of objectives that we aim to achieve by generating specific algorithms to the get the best solution. Specificity, precision, accuracy, etc. are few important characteristics/goals that we aim to achieve.
* <b>Genetic Programming Cycle:</b> New Gene Pool → Evaluation → Genes with scores → Fitness Computation → Genes with fitness → Selection → Parental Genes → Mating → Child Genes → Mutation → New Gene Pool
* Keywords:
** Gene pool: The set of genome to be evaluated during the current generation
** Genome: Genotypic description of an individuals (DNA, GA = set of values, GP = tree structure, string)
** Search Space: Set of all possible genome. For Automated Algorithm Design it's the set of all possible algorithms
** The Evaluation of a Genome: Associates a genome/individual (set of parameters for GA or string for GP) with a set of scores.
*** From a location in search space: Genotypic description
*** To a location in objective space: Phenotype description
* <b>Important measures for multi objectivity: </b>
** True Positive – TP: How often are we identifying the desired object
** False Positive – FP: How often are we identifying something else as the desired object
** True Negative - TN: How often are we identifying the non-desired object
** False Negative - FN: How often are we identifying the desired object as something else
** Objectives: Set of measurements each genome (or individual) is scored against
** Objective Space: Set of objectives 
* Confusion Matrices use TP, FP, TN, FN to visualize outcomes.
* Other measures that we use for optimization and prediction:
** Sensitivity or True Positive Rate (TPR): AKA hit rate or recall, TPR = TP/P = TP/(TP+FN)
** Specificity (SPC) or True Negative Rate (TNR): TNR = TN/N = TN/(TN+FP)
** False Negative Rate (FNR):FNR = FN/P = FN/(TP+FN), FNR = 1 - TPR
** Fallout or False Positive Rate (FPR): FPR = FP/N = TN/(FP+TN), FPR = 1 – TNR = 1 - SPC
** Precision or Positive Predictive Value (PPV), PPV = TP / (TP + FP), Bigger is better
** False Discovery Rate, FDR = FP/(TP + FP), FDR = 1 - PPV, Smaller is better
** Negative Predictive Value (NPV), NPV = TN / (TN + FN), Bigger is better
** Accuracy (ACC), ACC = (TP+TN) / (P+N), ACC = (TP+TN) / ( TP + FP + FN + TN),  Bigger is better
** Objective Space:Each individual is evaluated using objective functions like Mean squared error, Cost, Complexity, True positive rate etc.
** Objective scores give each individual a point in objective space
** This may be referred to as the phenotype of the individual 
* All these measures help us in selecting the right algorithms, mating, fitness computation etc.
* <b>Pareto Optimality</b>
** An individual is Pareto optimal if there is no other individual in the population that outperforms the individual on all objectives
** The set of all Pareto individuals is known as <i>the Pareto frontier.</i>
** These individuals represent unique contributions. We want to drive selection by favoring Pareto individuals (But maintain diversity by giving all individuals some probability of mating).
* Two types of Multi objectivity algorithms:
** Non-dominated Sorting Genetic Algorithm II (NSGA II)
*** Population is separated into non-domination (nothing over performs it at all individuals and that is why it called Pareto optimal) ranks. Individuals are selected using a binary tournament and a lower Pareto ranks beat higher Pareto ranks.
*** Ties on the same front are broken by crowding distance (more diversity is better) (its better if points are pareto optimals but a little far from one another). Summation of normalized Euclidian distances to all points within the front. Higher crowding distance wins
** Strength Pareto Evolutionary Algorithm 2 (SPEA2)
*** Each individual is given a strength S and receives a rank R. S refers to the number of others in the population that it dominates and R refers to the sum of S’s of the individuals that dominate it. Pareto individuals are nondominated and receive an R of 0.
*** A distance to the kth nearest neighbor (𝛔k) is calculated and a fitness of R + 1/(𝛔k + 2) is obtained

===Lab 2 (Part 2): Multi-Objective Genetic Programming===
* Followed instructions given in the python notebook and tried to minimize the mean squared error along with the tree size.
* I also plotted the Pareto frontier. The main reason why we are trying to minimize area under the curve is to reduce distance of the Pareto curve with the axes and increase the probability of finding optimized and better individuals to minimize the two objectives.
* The area under the curve with the original parameters was: 2.463792426733847
* Best individual is: negative(cos(multiply(add(cos(sin(cos(multiply(add(cos(cos(x)), cos(add(multiply(x, x), sin(x)))), tan(x))))), cos(x)), tan(x)))) with fitness: (0.27530582924947056, 25.0)
[[files/2.2.1.png]] [[files/2.2.2.png]] [[files/2.2.3.png]]
* Now, our goal is to reduce AUC by at least 25% by changing the parameters.
* To achieve this goal, I attempted to remove the three trigonometric functions.
* First, when I just removed the tan function, the area under the curve increased; however, once I removed all three functions, the area under the curve reduced and became 0.6912744703006891.
* Best individual is: subtract(x, x) with fitness: (0.7223441838209306, 3.0). This was the best individual in this case.
[[files/2.2.2.3.png]] [[files/2.2.2.2.png]] [[files/2.2.2.1.png]]

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes
|Completed
|September 8th, 2021
|September 15th, 2021
|September 11th, 2021
|-
|Review lecture slides
|Completed
|September 8th, 2021
|September 15th, 2021
|September 10th, 2021
|-
|Complete Self-Evaluation Form
|Completed
|September 8th, 2021
|September 15th, 2021
|September 10th, 2021
|- 
|Complete Lab 2 (Part 2)
|Completed
|September 8th, 2021
|September 15th, 2021
|September 12th, 2021
|}

== Week 2: September 1st - September 8th (2021) == 
===Outline of class notes===

'''In class:'''
* Overview of Genetic Programming.
* Learnt about Tree Representation.
* Briefly discussed Crossover and Mutation in Genetic Programming and Symbolic Regression.

'''Lecture Notes:'''
* Did a brief review of last week's material regarding Genetic Algorithms.
* Instead of taking an individual and having a function evaluator to obtain objective scores, the individual will be the function itself.
* Discussed one of the most common program structures in genetic programming, <b>Tree Representation</b>. Went over its uses and how to traverse over this tree. Parts of tree representation:
** Nodes are called primitives and represent functions, eg: +, -, x, /.
** Leaves are called terminals and represent parameters. The input can be thought of as a particular type of terminal. The output is produced at the root of the tree.
* To store the tree, the tree is converted to a lisp preordered parse tree and the order is determined by the inputs, so we use the root first and then go down.
* For example, if we use the following representation, we can determine how to traverse through the tree to determine the function by going top down and left to right.
[[files/TreeRepresentation1.png]]

So for this tree representation the function will be, f(x) = 3 * 4 + 1 and the parse would be [+,*,3,4,1].
* Crossover in tree-based GP is simply done by exchanging subtrees. Start by randomly picking a point in each tree and then these points and everything below create subtrees.
* The function with the best output mate and then produce the next generation of children.
* We also use the root sum (mean) square error method to calculate the optimized output and algorithms.
* Mutation can involve and is done by:
** Inserting a node or subtree
** Deleting a node or subtree
** Changing a node
* Learned more about the example of Symbolic Regression.
* Symbolic Regression is an example involving the mutation of GP that is used to evolve solutions using primitives.
* For instance: We use primitives like sin function, factorial, cos function, tan function, exponential, summation to evolve a solution to y = sin(x) by using the Taylor Series formula for sin(x).
* Finally we discussed that to evaluate the tree, we can measure the difference between the truth and the output that we obtain after feeding the a number of input points into the function to get outputs and then running f(x).

===LAB 2: Symbolic Regression===
* Tried to derive/inherit individuals from DEAP's PrimitiveTree instead of lists.
* Added primitives to the primitive set and chose a new mutation to find the most optimized solution.
* Compiled the primitive tree to generate the evaluation function, which finds the squared sum of the difference between our desired function's output and our individual's output. In this case we are trying to find minimum values that approaches zero faster.
* I tried modifying the code in three different ways:
** <u>The original program</u>:
[[files/Original Lab 2.png|center|thumb]]

*** First Attempt: Best individual is add(add(negative(negative(multiply(x, x))), multiply(subtract(multiply(x, x), negative(x)), multiply(x, x))), x), (1.0123748708878107e-16,)
*** Second Attempt: Best individual is add(add(add(multiply(add(multiply(multiply(x, x), x), multiply(x, x)), x), multiply(x, x)), x), subtract(x, x)), (8.59033944318508e-17,)
*** Depicts requirement for a more optimized and consistent solution to obtain minimum efficiently.
** <u>Program with added mutation</u>

*** First Mutation (mutInsert): Best individual is add(multiply(x, add(add(multiply(x, multiply(x, x)), x), multiply(x, negative(negative(x))))), x), (1.1585916677755867e-16,)

[[files/part2Lab2.png|center|thumb]] 

Even after running the code for a number of times the error is lesser and the solution is more consistent and optimized than the original program.

*** Second Mutation (mutShrink): Best individual is add(add(multiply(x, x), multiply(x, multiply(x, x))), add(x, multiply(x, multiply(multiply(x, x), x)))), (9.522005638492831e-17,)

[[files/part2lab21.png|center|thumb]]

Values were usually close to zero, but in some cases the value was very high (for maximum) and not as consistent. 
** <u>Program with only added primitives</u>

[[files/extralab2.png|center|thumb]]

*** Best individual is add(add(multiply(x, x), multiply(multiply(add(square(x), x), x), x)), x), (1.0172711918255375e-16,)

** <u>Program with both added primitives and mutation</u>
*** We used the mutInsert mutation as it seemed more consistent.

*** First pair of primitives (sin and square): Best individual is add(x, add(multiply(add(x, square(x)), multiply(x, x)), multiply(x, x))), (1.0123748708878107e-16,)
[[files/lab2primitive1.png|center|thumb]]

*** Second pair of primitives (absolute, cos): Best individual is add(add(cos(add(cos(subtract(x, x), cos(add(negative(x), subtract(x, negative(multiply(x, x)))), x)), x), x), cos(subtract(x, x), x)), add(x, x)), (0.0,)
[[files/finallab2.png|center|thumb]]

<b>In this lab, I attempted to reduce error to bring it almost to zero. This can be seen best in the graph where I used the absolute and cos primitives with the mutInsert mutation.</b>

'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Update lecture notes (GP)
|Completed
|September 1st, 2021
|September 8th, 2021
|September 2nd, 2021
|-
|Review lecture slides
|Completed
|September 1st, 2021
|September 8th, 2021
|September 1st, 2021
|- 
|Complete Lab 2 (Part1)
|Completed
|September 1st, 2021
|September 8th, 2021
|September 4th, 2021
|}

== Week 1: August 25th - September 1st (2021) ==

===Outline of class notes===

'''In class:'''

* Overview of Automated Algorithm Design Wiki, syllabus, notebooks, etc.
* Started lecture on Genetic Algorithms.
* Briefly discussed the One Max Problem.

'''Lecture Notes:'''

* <u> The Concept </u>
** With genetic algorithms, each new generation is created through mating/mutation of individuals in the previous population  
** Their fitness is evaluated before mating/mutation.
** This fitness evaluation is done through the Fitness Proportionate method (randomization) or tournament method.
** Through numerous operations of the different processes, it will eventually produce the best individual 

* <u>Important Keywords</u>
** Algorithms: various evolutionary algorithms to create a solution or best individual.
** Individual: One specific candidate in the population (with properties such as DNA). In terms of programming this can be seen as a single solution to a problem.
** Population: Group of individuals whose properties will be altered. This refers to a number of solutions (set) for a problem.
** Objective: A value used to characterize individuals that you are trying to maximize or minimize. Usually the goal is to increase objective through the evolutionary algorithm.
** Fitness: Relative comparison to other individuals of the population.
** Evaluation: A function that computes the objective of an individual.
** Selection: This represents ‘survival of the fittest'. Preference given to better individuals, therefore allowing them to pass on their genes.
*** Fitness Proportionate: The greater the fitness value, the higher the probability of being selected for mating.
*** Tournament: Several tournament style competitions among various individuals. Winner selected for mating.
** Mate/Crossover: Represents mating between individuals.
** Mutate: Introduce random modification. The purpose is to maintain diversity.

* <u>The Process:</u>
** Randomly initialize population
** Determine fitness of population
** Select parents from population
** Perform crossover on parents creating population
** Perform mutation of population
** Determine fitness of population
** Repeat until best individual is good enough

===Lab 1: Genetic Algorithms with DEAP===

'''One Max Problem:''' This is a simple genetic algorithm problem and its objective is to convert all 0's and 1's in a vector/bitstring to all 1's. The outline and process completed is as follows (as per steps followed in the DEAP Python notebook):
* Imported the required modules (mostly DEAP tools)
* Defined individual classes, the fitness objective, and functions using DEAP's ToolBox.
* Represented bit string individuals as Booleans.
* Evaluated the population, ran the code repeatedly, performed tournament selection, mating, and mutation.
** Mating: Two-point crossover function
** Mutate: Flipping a bit in our bit string to either 1 or 0 respectively with an independent probability of flipping each individual bit of 5%.
* Ran the algorithm for 40 generations.
* <u>Conclusion:</u> Global maximum fitness of 100.0 was usually achieved in 40 generations but sometimes this was not the case; however, the maximum fitness obtained was always very close to 100.0. This method of tournament selection among 3 individuals is better than random search.

'''The N Queens Problem:''' The problem is to determine a configuration of n queens on a nxn chessboard such that no queen can be taken by one another. In this version, each queen is assigned to one column, and only one queen can be on each line. The outline and process completed is as follows (as per steps followed in the DEAP Python notebook):
* Imported the required modules (mostly DEAP tools)
* Defined individual classes, the fitness objective, and functions using DEAP's ToolBox.
* The fitness objective is to minimize the number of conflicts between 2 queens on the chessboard. Used to range function as well.
* Defined a function called "permutation" to help create our individuals and population and others like Evaluation (minimize diagonal conflicts), Selection (tournament selection of 3 individuals), Crossover (partially matched), and Mutation (shuffle indexes) functions.
* Ran main evolutionary loop for 100 generations. 
* Current algorithm did not always achieve the global minimum (0.0) but was consistently close.
* Visualization done to achieve more efficiency and speed using graphs as follows (100 generations graph):
[[files/FirstLab.png|center|thumb]]
* Iterations can be reduced with change is parameters and functions. This is clear as we can see that the minimum is easily obtained around the 25th generation. I am trying to work on making few changes in the algorithm for improved results by changing the mutation and mating probabilities.

For both problems we also found the basic statistics such as mean, minimum, maximum, and standard deviation, which helped to show that the final required output could be obtained in earlier generations itself with some tweaks made in the code.


'''<i>Action Items</i>'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install and set up DEAP library for Python
|Completed
|August 25th, 2021
|September 1st, 2021
|August 26th, 2021
|-
|Begin Notebook
|Completed
|August 25th, 2021
|September 1st, 2021
|August 25th, 2021
|- 
|Review Slides
|Completed
|August 25th, 2021
|September 1st, 2021
|August 26th, 2021
|-
|Complete Lab 1
|Completed
|August 25th, 2021
|September 1st, 2021
|August 29th, 2021
|}









