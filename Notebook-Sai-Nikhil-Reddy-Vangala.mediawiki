'''Name:''' Sai Nikhil Vangala

'''Email:''' [mailto:svangala3@gatech.edu]

'''Cell Phone:''' 502-600-3086

'''VIP:''' Automated Algorithm Design

'''Interests:''' Artificial Intelligence, Machine Learning, Cloud Computing, Tennis, Watching Sports

= Fall 2021 =

== Week 1 (8/23 - 8/27) ==

=== Lecture Notes (8/25) ===
* With genetic algorithms, each new generation is created through mating/mutation of individuals in the process.
* Keywords:
** Individual: One specific candidate in the population
** Population: group of individuals whose properties will be altered.
** Objective: a value used to characterize individuals that you are trying to maximize or minimize
** Fitness:  relative comparison to other individuals.
** Evaluation: a function that computes the objective of an individual. 
** Selection: represents 'survival of the fittest'; gives preference to better individuals therefore allowing them to pass on their genes.
*** Fitness Proportionate: Great fitness value means higher the probability of being selected for mating.
*** Tournament: Several tournaments among individuals; winners are selected for mating.
** Mate/Crossover: represents mating between individuals
** Mutate: introduces random modifications; purpose is to maintain diversity
** Algorithms: various evolutionary algorithms to create a solution or best individual
*** Randomly initialize population
*** Determine fitness of population
*** Repeat‚Ä¶.
**** Select parents from population
**** Perform mutation of population
****Determine fitness # of population
*** Until best individual is good enough.


===Lab 1: Genetic Algorithms with DEAP===
* One Max Problem: Very simply genetic algorithm problem in which we search for a 1 filled solution, basically to find a bit string containing all 1s with a set length.
** Imported all of the required modules such as DEAP.
** Defined the classes (fitness objective and individual) using DEAP's Creator.
** Defined the bit string individuals as a list of Booleans represented by 1s and 0s.
** Defined the genetic algorithm's genetic operators: evaluate, mate, mutate, select.
*** Evaluated the population
*** Ran the evolutionary process, 40 generations
*** Added selection
*** Used tournament selection on the population
*** Two-point crossover mating function
*** Flipping a bit in our bit string to either 1 or 0 respectively with an independent probability of flipping each individual bit of 5% for mutation
** 100.0 for the global maximum fitness was the norm in most of 40 generations, however due to the population's initial randomization, crossover and mutation probabilities, and the bit flips, there were clear discrepancies. In addition, this method is more effective than random search as there is optimization of the search space using respective fitness objectives.


* N Queens Problem: The problem is to determine a configuration of n queens on a nxn chessboard such that no queen can be taken by one another. In this version, each queen is assigned to one column, and only one queen can be on each line.
** Imported all of the required modules such as DEAP.
** Defined the classes (fitness objective and individual) using DEAP's Creator.
** Defined the toolbox and the evaluation function.
** Defined the crossover function: Two-Point.
** Defined the mutation function: partially matched crossover because it represents swapping around pairs of queen positions between two parent individuals: More effective. 
** Defined a new function called "" and tested it.
** Defined and ran evolutionary loop, 100 generations. 
** Although 100 generations were looped, the algorithm was not able to always achieve a global minimum of 0.0
** Generated a visualization to speed up the algorithm improvement process and plotted the average, minimum, and maximum over 100 generations.

===Action Items===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
| Lab 1: Genetic Algorithms with DEAP
|Completed
|August 25, 2021
|September 1, 2021
|August 30, 2021
|-
|Notebook
|Completed
|August 25, 2021
|September 1, 2021
|August 30, 2021
|-
|Review Notes
|Completed
|August 25, 2021
|September 1, 2021
|August 30, 2021
|-
|Set-Up Slack
|Completed
|August 25, 2021
|September 1, 2021
|August 30, 2021
|}



== Week 2 (8/30 - 9/3) ==

=== Lecture Notes (9/1) ===
* Reviewed material on Genetic Algorithms form last week's meeting.
* Tree Representation
** We can represent a program as a tree structure
*** Nodes are called primitives and represent functions
*** Leaves are called terminals and represent parameters.
**** The input can be thought of as a particular type of terminal
**** The output is produced at the root of the tree.

* How is the Tree Stored?

** The tree is converted  into a '''lisp preordered parse tree'''
** The operator would be followed by inputs
** The tree for f(X) = 3 * 4 + 1 can be written as: [+,*,3,4,1]
** We do a depth-first traversal in this pre-ordered parse tree

* Crossover in Genetic Programming
** Basically exchanging subtrees
** Start by randomly picking a point in each tree
** These points and everything below create subtrees
** The subtrees are exchanged to produce children
** We take what's left of parent 1 and what's left of parent 2 and swap and create 2 child algorithms out of that

* Mutation in GP
** Mutation can involve‚Ä¶
*** Inserting a node or subtree
*** Deleting a node or subtree
*** Changing a node
** When we delete a node or subtree, we can perform a shrink operation to fill up that tree
** Any change we make locally to a tree, that is called a mutation
** Example: Symbolic Regression
*** Using simple primitives, use genetic programming to evolve a solution to y = sin(x)
*** Primitives include: +, *, -, /
*** Terminals include integers and X
** We solve this using a Calc 1 Concept of Taylor Series
*** Taylor Series for sin(x)

* Evaluating a tree
** We can feed a number of input points into the function to get outputs 
*** X = [0..2ùúã]
** We can measure error between outputs and truth.


===Lab 2: Symbolic Regression===
* Imported the libraries needed for GP.
* Created the fitness and individual classes

===Action Items===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Notebook
|Completed
|September 1, 2021
|September 8, 2021
|September 8, 2021
|-
|Review Notes
|Completed
|September 1, 2021
|September 8, 2021
|September 8, 2021
|-
|Complete Lab 2: Symbolic Regression
|Completed
|September 1, 2021
|September 8, 2021
|September 8, 2021
|}



== Week 3 (9/6 - 9/10) ==

=== Lecture Notes (9/8) ===
* Genetic Programming Cycle
** New Gene Pool
** Evaluation
** Genes with scores
** Fitness Computation
** Genes with fitness
** Selection
** Parental Genes
** Mating
** Child Genes
** Mutation
** New Gene Pool
* Gene pool is a set of genome to be evaluated during the current generation.
** Genome
*** Genotypic description of an individuals
*** DNA
*** GA = set of values
*** GP = tree structure, string
** Search Space: Set of all possible genome. For AAD, set of all possible algorithms.
* Evaluation of a Genome assocites a genome/individual (set of parameters for GA or string for GP) with a set of scores.
* What are these scores
** True Positive - TP
*** How often are we identifying the desired object
** False Positive - FP
*** How often are we identifying something else as the desired object.
* Objectives
** set of measurements each genome is scored against
** phenotype
* Objectives Space: set of objectives
* Evaluation: Maps an genome/individual 
** From a location in search space
*** Genotypic description
** To a location in objective space
*** phenotype description

* Classification Measures
** Data Set: Positive Samples (P) and Negative Samples (N)
** Classifier
** Confusion Matrix: True Positive, False Negative (Type II Error), False Positive (Type I Error), True Negative

* Maximization Measures
** Sensitivity or True Positive Rate (TPR)
*** AKA hit rate or recall
*** TPR = TP/P = TP/(TP+FN)
** Specificity (SPC) or True Negative Rate (TNR)
*** TNR = TN/N = TN/(TN+FP)

* Minimization Measures
** False Negative Rate (FNR)
*** FNR = FN/P = FN/(TP+FN)
*** FNR = 1 - TPR
** Fallout or False Positive Rate (FPR)
*** FPR = FP/N = TN/(FP+TN)

* Other Measures
** Precision or Positive Predictive Value (PPV)
*** PPV = TP/(TP+FP)
*** Bigger is better
** False Discovery Rate
*** FDR = FP/(TP+FP)
*** FDR = 1 - PPV
*** Smaller is better
** Negative Predictive Value (NPV)
*** NPV = TN/(TN+FN)
*** Bigger is better
** Accuracy (ACC)
*** ACC = (TP+TN)/(P+N)
*** ACC = (TP+TN)/(TP+FP+FN+TN)
*** Bigger is better

* Objective Space
** Each individual is evaluated using objective functions
*** Mean Squared Error
*** Cost
*** Complexity
*** True positive rate
*** False positive rate
** Objective scores give each individual a point in objective space
** This may be referred to as the phenotype of the individual

* Pareto Optimality
** An individual is Pareto optimal if there is no other individual in the population that outperforms the individual on all objectives.
** The set of all Pareto individuals is known as the Pareto frontier.
** These individuals represent unique contributions
** We want to drive selection by favoring Pareto individuals
*** But maintain diversity by giving all individuals some probability of mating.

* Nondominated Sorting Genetic Algorithm II (NSGA II) 
** Population is separated into nondomination ranks
** Individuals are selected using a binary tournament
** Lower Pareto ranks beat higher Pareto ranks

* Strength Pareto Evolutionary Algorithm 2
** Each individual is given a strength S
*** S is how many others in the population it dominates
** Each individual receives a rank R
*** R is the sum of S's of the individuals that dominate it
*** Pareto individuals are nondominated and receive an R of 0.
** A distance to the kth nearest neighbor (ùõîk) is calculated and a fitness of R + 1/(ùõîk + 2) is obtained

===Lab 2 (Part II): Multi-Objective Optimization===

===Self-Grading===
https://drive.google.com/file/d/1O1iChfPIhI4IgQ1qM6bHCb4pRmuOaBxQ/view?usp=sharing

===Action Items===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Notebook
|Completed
|September 8, 2021
|September 15, 2021
|September , 2021
|-
|Review Notes
|Completed
|September 8, 2021
|September 15, 2021
|September , 2021
|-
|Self-Grading
|Completed
|September 8, 2021
|September 15, 2021
|September , 2021
|-
|Lab 2: Part 2
|Completed
|September 8, 2021
|September 15, 2021
|September , 2021
|}
