'''Name:''' Sai Nikhil Vangala

'''Email:''' [mailto:svangala3@gatech.edu]

'''Cell Phone:''' 502-600-3086

'''VIP:''' Automated Algorithm Design

'''Interests:''' Artificial Intelligence, Machine Learning, Cloud Computing, Tennis, Watching Sports

= Fall 2021 =

== Week 1 (8/23 - 8/27) ==

=== Lecture Notes (8/25) ===
* With genetic algorithms, each new generation is created through mating/mutation of individuals in the process.
* Keywords:
** Individual: One specific candidate in the population
** Population: group of individuals whose properties will be altered.
** Objective: a value used to characterize individuals that you are trying to maximize or minimize
** Fitness:  relative comparison to other individuals.
** Evaluation: a function that computes the objective of an individual. 
** Selection: represents 'survival of the fittest'; gives preference to better individuals therefore allowing them to pass on their genes.
*** Fitness Proportionate: Great fitness value means higher the probability of being selected for mating.
*** Tournament: Several tournaments among individuals; winners are selected for mating.
** Mate/Crossover: represents mating between individuals
** Mutate: introduces random modifications; purpose is to maintain diversity
** Algorithms: various evolutionary algorithms to create a solution or best individual
*** Randomly initialize population
*** Determine fitness of population
*** Repeatâ€¦.
**** Select parents from population
**** Perform mutation of population
****Determine fitness # of population
*** Until best individual is good enough.


===Lab 1: Genetic Algorithms with DEAP===
* One Max Problem: Very simply genetic algorithm problem in which we search for a 1 filled solution, basically to find a bit string containing all 1s with a set length.
** Imported all of the required modules such as DEAP.
** Defined the classes (fitness objective and individual) using DEAP's Creator.
** Defined the bit string individuals as a list of Booleans represented by 1s and 0s.
** Defined the genetic algorithm's genetic operators: evaluate, mate, mutate, select.
*** Evaluated the population
*** Ran the evolutionary process, 40 generations
*** Added selection
*** Used tournament selection on the population
*** Two-point crossover mating function
*** Flipping a bit in our bit string to either 1 or 0 respectively with an independent probability of flipping each individual bit of 5% for mutation
** 100.0 for the global maximum fitness was the norm in most of 40 generations, however due to the population's initial randomization, crossover and mutation probabilities, and the bit flips, there were clear discrepancies. In addition, this method is more effective than random search as there is optimization of the search space using respective fitness objectives.


* N Queens Problem: The problem is to determine a configuration of n queens on a nxn chessboard such that no queen can be taken by one another. In this version, each queen is assigned to one column, and only one queen can be on each line.
** Imported all of the required modules such as DEAP.
** Defined the classes (fitness objective and individual) using DEAP's Creator.
** Defined the toolbox and the evaluation function.
** Defined the crossover function: Two-Point.
** Defined the mutation function: partially matched crossover because it represents swapping around pairs of queen positions between two parent individuals: More effective. 
** Mutation that I added
    def mutUniform(individual, expr, pset):
    """Randomly select a point in the tree *individual*, then replace the
    subtree at that point as a root by the expression generated using method
    :func:`expr`.
    :param individual: The tree to be mutated.
    :param expr: A function object that can generate an expression when
                 called.
    :returns: A tuple of one tree.
    """
    index = random.randrange(len(individual))
    slice_ = individual.searchSubtree(index)
    type_ = individual[index].ret
    individual[slice_] = expr(pset=pset, type_=type_)
    return individual,
** Defined and ran evolutionary loop, 100 generations. 
** Although 100 generations were looped, the algorithm was not able to always achieve a global minimum of 0.0
** Original:
*** Best individual is [1, 6, 4, 11, 0, 18, 13, 17, 19, 12, 3, 9, 16, 5, 2, 15, 10, 8, 7, 14], (1.0,)
** Added Mutation Effect:
*** Best individual is [7, 12, 0, 5, 13, 11, 16, 8, 1, 9, 18, 3, 15, 17, 19, 4, 10, 14, 6, 2], (0.0,)
** Generated a visualization to speed up the algorithm improvement process and plotted the average, minimum, and maximum over 100 generations.

===Action Items===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
| Lab 1: Genetic Algorithms with DEAP
|Completed
|August 25, 2021
|September 1, 2021
|August 30, 2021
|-
|Notebook
|Completed
|August 25, 2021
|September 1, 2021
|August 30, 2021
|-
|Review Notes
|Completed
|August 25, 2021
|September 1, 2021
|August 30, 2021
|-
|Set-Up Slack
|Completed
|August 25, 2021
|September 1, 2021
|August 30, 2021
|}



== Week 2 (8/30 - 9/3) ==

=== Lecture Notes (9/1) ===
* Reviewed material on Genetic Algorithms form last week's meeting.
* Tree Representation
** We can represent a program as a tree structure
*** Nodes are called primitives and represent functions
*** Leaves are called terminals and represent parameters.
**** The input can be thought of as a particular type of terminal
**** The output is produced at the root of the tree.

* How is the Tree Stored?

** The tree is converted  into a '''lisp preordered parse tree'''
** The operator would be followed by inputs
** The tree for f(X) = 3 * 4 + 1 can be written as: [+,*,3,4,1]
** We do a depth-first traversal in this pre-ordered parse tree

* Crossover in Genetic Programming
** Basically exchanging subtrees
** Start by randomly picking a point in each tree
** These points and everything below create subtrees
** The subtrees are exchanged to produce children
** We take what's left of parent 1 and what's left of parent 2 and swap and create 2 child algorithms out of that

* Mutation in GP
** Mutation can involveâ€¦
*** Inserting a node or subtree
*** Deleting a node or subtree
*** Changing a node
** When we delete a node or subtree, we can perform a shrink operation to fill up that tree
** Any change we make locally to a tree, that is called a mutation
** Example: Symbolic Regression
*** Using simple primitives, use genetic programming to evolve a solution to y = sin(x)
*** Primitives include: +, *, -, /
*** Terminals include integers and X
** We solve this using a Calc 1 Concept of Taylor Series
*** Taylor Series for sin(x)

* Evaluating a tree
** We can feed a number of input points into the function to get outputs 
*** X = [0..2ðœ‹]
** We can measure error between outputs and truth.


===Lab 2: Symbolic Regression===
* Imported the libraries needed for GP.
* Created the fitness and individual classes
* Initialized primitive sets and all the primitives that the tree can use. 
** Best individual is add(add(x, multiply(x, multiply(add(x, multiply(x, x)), x))), multiply(x, x)), (1.1690561362729958e-16,)
* After adding the two new primitives (Sin and Cos) & new mutation (mutEphemeral): 
** Best individual is multiply(sin(add(x, x), cos(negative(subtract(cos(x, subtract(negative(x), x)), x)), x)), sin(multiply(subtract(sin(cos(sin(x, x), x), cos(x, x)), x), x), x)), (0.0,)
* Defined the toolbox, individual, population, and compiler.
* Registered genetic operators and added a new mutation and expanded it in the evolutionary loop as well. 
* Programmed the main evolutionary algorithm which prints out the individuals in a tree format being read from left to right.

===Action Items===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Notebook
|Completed
|September 1, 2021
|September 8, 2021
|September 8, 2021
|-
|Review Notes
|Completed
|September 1, 2021
|September 8, 2021
|September 8, 2021
|-
|Complete Lab 2: Symbolic Regression
|Completed
|September 1, 2021
|September 8, 2021
|September 8, 2021
|}



== Week 3 (9/6 - 9/10) ==

=== Lecture Notes (9/8) ===
* Genetic Programming Cycle
** New Gene Pool
** Evaluation
** Genes with scores
** Fitness Computation
** Genes with fitness
** Selection
** Parental Genes
** Mating
** Child Genes
** Mutation
** New Gene Pool
* Gene pool is a set of genome to be evaluated during the current generation.
** Genome
*** Genotypic description of an individuals
*** DNA
*** GA = set of values
*** GP = tree structure, string
** Search Space: Set of all possible genome. For AAD, set of all possible algorithms.
* Evaluation of a Genome assocites a genome/individual (set of parameters for GA or string for GP) with a set of scores.
* What are these scores
** True Positive - TP
*** How often are we identifying the desired object
** False Positive - FP
*** How often are we identifying something else as the desired object.
* Objectives
** set of measurements each genome is scored against
** phenotype
* Objectives Space: set of objectives
* Evaluation: Maps an genome/individual 
** From a location in search space
*** Genotypic description
** To a location in objective space
*** phenotype description

* Classification Measures
** Data Set: Positive Samples (P) and Negative Samples (N)
** Classifier
** Confusion Matrix: True Positive, False Negative (Type II Error), False Positive (Type I Error), True Negative

* Maximization Measures
** Sensitivity or True Positive Rate (TPR)
*** AKA hit rate or recall
*** TPR = TP/P = TP/(TP+FN)
** Specificity (SPC) or True Negative Rate (TNR)
*** TNR = TN/N = TN/(TN+FP)

* Minimization Measures
** False Negative Rate (FNR)
*** FNR = FN/P = FN/(TP+FN)
*** FNR = 1 - TPR
** Fallout or False Positive Rate (FPR)
*** FPR = FP/N = TN/(FP+TN)

* Other Measures
** Precision or Positive Predictive Value (PPV)
*** PPV = TP/(TP+FP)
*** Bigger is better
** False Discovery Rate
*** FDR = FP/(TP+FP)
*** FDR = 1 - PPV
*** Smaller is better
** Negative Predictive Value (NPV)
*** NPV = TN/(TN+FN)
*** Bigger is better
** Accuracy (ACC)
*** ACC = (TP+TN)/(P+N)
*** ACC = (TP+TN)/(TP+FP+FN+TN)
*** Bigger is better

* Objective Space
** Each individual is evaluated using objective functions
*** Mean Squared Error
*** Cost
*** Complexity
*** True positive rate
*** False positive rate
** Objective scores give each individual a point in objective space
** This may be referred to as the phenotype of the individual

* Pareto Optimality
** An individual is Pareto optimal if there is no other individual in the population that outperforms the individual on all objectives.
** The set of all Pareto individuals is known as the Pareto frontier.
** These individuals represent unique contributions
** We want to drive selection by favoring Pareto individuals
*** But maintain diversity by giving all individuals some probability of mating.

* Nondominated Sorting Genetic Algorithm II (NSGA II) 
** Population is separated into nondomination ranks
** Individuals are selected using a binary tournament
** Lower Pareto ranks beat higher Pareto ranks

* Strength Pareto Evolutionary Algorithm 2
** Each individual is given a strength S
*** S is how many others in the population it dominates
** Each individual receives a rank R
*** R is the sum of S's of the individuals that dominate it
*** Pareto individuals are nondominated and receive an R of 0.
** A distance to the kth nearest neighbor (ð›”k) is calculated and a fitness of R + 1/(ð›”k + 2) is obtained

===Lab 2 (Part II): Multi-Objective Optimization===
* Created new fitness and individual classes as it is a new problem. 
* The two objectives that will be minimized: Mean Squared Error & Size of tree.
* The three new primitives of sin, cos, tan were added and set a seed for randomization. 
* Reinitialized the rest of the toolbox functions.
* Defined the pareto dominance function: returns true if the first individual dominates the second individual. 
* Initialized a random population of 300 individuals and one seperate individual for comparison.
* The population was sorted by pareto dominance in comparison to the sperate individual defined. 
* The objective space plotted using the sorter population.
** Blue = selected individual
** Red = dominators
** Black = uncomparable 
** Green = dominated
* Defined and ran the main evolutionary algorithm that omitted:
** negative(cos(multiply(add(cos(sin(cos(multiply(add(cos(cos(x)), cos(add(multiply(x, x), sin(x)))), tan(x))))), cos(x)), tan(x)))) with fitness: (0.27530582924947056, 25.0)
* Visualized the objective space and pareto front which omitted:
** Area Under Curve: 2.463792426733847
* Implemented a version that produces a 25% or more decrease in the AUC using changes in the following factors:
** NGEN
*** Decrease --> Decrease and Increase --> Increase
** MU
*** Decrease --> Increase and Increase --> Increase
** LAMBDA
*** Decrease --> Decrease and Increase --> Increase
** CXPB
*** Decrease --> Increase and Increase --> Increase
** MUTPB 
*** Increase --> Increase
** Removing one of operator functions: add, subtract, multiply, sin, cos, tan

===Self-Grading===
https://drive.google.com/file/d/1O1iChfPIhI4IgQ1qM6bHCb4pRmuOaBxQ/view?usp=sharing

===Action Items===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Notebook
|Completed
|September 8, 2021
|September 15, 2021
|September 15, 2021
|-
|Review Notes
|Completed
|September 8, 2021
|September 15, 2021
|September 15, 2021
|-
|Self-Grading
|Completed
|September 8, 2021
|September 15, 2021
|September 15, 2021
|-
|Lab 2: Part 2
|Completed
|September 8, 2021
|September 15, 2021
|September 15, 2021
|}

== Week 4 (9/13 - 9/17) ==

=== Lecture Notes (9/15) ===
* Assigned into Bootcamp groups for the rest of the 10 weeks.
** Jordan Stampfli
** Austin Peng
** David Zhang
* Introduced to new resources:
** scikit
** pandas
** numpy
* Introduced and explained the kaggle titanic data set and provided instructions for the upcoming assignment.

=== Sub-team Meeting (9/18) ===
* Began by sharing code via Google Colab
* Used same split as Jupyter Notebook example: 33% in testing
* 4 codominant models: 
**perceptron
** random forest
** logistic regression
** gaussian naive bayes
* Predictions were found using all of the 4 models and saved as 4 seperate csv files

{| class="wikitable"
!Model
!False Negative
!False Positive
|-
|Perceptron
|30
|27
|-
|Random Forest
|30
|30
|-
|Logistic Regression
|31
|25
|-
|Gaussian
|40
|21
|}

===Action Items===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Notebook
|Completed
|September 15, 2021
|September 15, 2021
|September 15, 2021
|-
|Review Notes
|Completed
|September 15, 2021
|September 23, 2021
|September 22, 2021
|-
|Meet with group
|Completed
|September 15, 2021
|September 23, 2021
|September 18, 2021
|-
|Submit CSV file of titanic test predictions to canvas
|Completed
|September 15, 2021
|September 23, 2021
|September 22, 2021
|}

== Week 5 (9/20 - 9/24) ==

=== Lecture Notes (9/22) ===

===Action Items===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Notebook
|Completed
|September 22, 2021
|September 29, 2021
|September 28, 2021
|-
|Review Notes
|Completed
|September 22, 2021
|September 29, 2021
|September 28, 2021
|-
|Meet with group
|Completed
|September 22, 2021
|September 29, 2021
|September 28, 2021
|-
|Complete and Practice Presentation
|Completed
|September 22, 2021
|September 29, 2021
|September 28, 2021
|}

== Week 5 (9/27 - 9/31) ==

=== Lecture Notes (9/29) ===
