== Team Member ==
Name: Sanket Manjesh

Phone Number: 6187514050

Email: smanjesh3@gatech.edu

Interests: Machine Learning, Computer Vision, Artificial Intelligence

Teammates on CV sub-team for NLP: Anuraag Govindarajan, Tusheet Goli, Tushna Eduljee 

== December 2, 2020 ==

=== General Meeting Notes (Final Presentation, No Subteam Meeting) ===
* Final Presentations are today
* Stocks
** Had inconsistencies with base research paper
** The way in which trading signal was calculated was inconsistent with research paper
** Found solutions to maximize profits over time
** Implemented many Technical Indicator Primitives within EMADE (Exponential Moving Average, Moving Average Convergence/Divergence, Ease of Movement)
** On Balance Volume - takes in opening/closing prices per day and returns OBV value, utilizes EMADE's string to feature
** Paper's predicted trading score was much higher than truth score while EMADE run gave a lower predicted score than truth value (maybe caused by underfitting?)
** Moved on using ten days at a time for predictions
** Maybe run new STREAM_TO_FEATURES on data or test larger time ranges, find more literature to base research off of
NLP 
*Final Presentation Link: https://docs.google.com/presentation/d/1Z8PjzaP_LjhyycpGSeXX7rIHdVYkxHzqg8vaKVEYhdU/edit#slide=id.gaec8ee0d3b_15_0
*Here are the slides that I individually presented:
[[files/Screen Shot 2020-12-03 at 10.27.50 PM.png|center|thumb|322x322px|Intro to what CoDeepNeat is]]
[[files/Screen Shot 2020-12-03 at 10.27.59 PM.png|center|thumb|332x332px|Results Obtained with CoDeepNeat Implementation]]
[[files/Screen Shot 2020-12-03 at 10.28.08 PM.png|center|thumb|364x364px|Graph of Results Obtained with CoDeepNeat]]
* With these slides, I gave an introduction to what CoDeepNeat is and explained how it evolves both the overall neural net architecture and the subcomponents inside each layer separately before combining them for evaluation
* I made sure to go over the results from both implementations with CoDeepNeat and explained how I got a low accuracy with the first implementation for CIFAR-10 as I only ran 3 generations while I was able to observe 40 generations of runs in the second implementation and obtain around a max of 92.7 percent accuracy for CIFAR-10
* Also briefly went over the graphs obtained from the second CoDeepNeat and how the average accuracy for CIFAR-10 hovered around 0.75 - 0.85 with the "MMS DA " version of CoDeepNeat giving the best accuracy
* More Primitives were added to EMADE from CV subteam with Otsu's Binarization
* Needed new 2D implementation of this binarization method in EMADE
* Chest Xray baseline run got a bad accuracy and Area Under the Curve result (AUC stayed around 0.5 - 0.6)
* Future goals revolve around making BERT Layer valid, Unit testing new primitives, trying Coeveolution with CoDeepNeat and taking it further, and improving on our adaptive mutation

* EZCGP
** Preprocessed with OpenCV primitives
** Objective Scores utilized were False Positive vs. False Negative and Recall vs. the objective of accuracy previously used
** Utilized genome seeding to reduce run time on EMADE
** Saved 20 individuals as pickle files for future runs
** Got 9 generations, best individual had precision of 0.98 and recall of 0.97
** Try to increase randomness, add more primitives in the future
** AUC was already close to 0.98 with the model and it might've been harder to improve upon
** Added cyclical learning rate to maybe run less epochs, get most out of every training set
** Aging - prune individuals and stop individuals from dying from old age
** Might marry EZCGP with EMADE in the future
** Also look into new mating methods, take out transfer learning
* Modularity
** ARL - Finding combinations of nodes that are useful
** Search the entire population for combinations of children/parent nodes based on their cumulative distribution functions
** ARLs can grow by wrapping around each other
** Minimize False Positive/Negative
** Compare results with no ARL baseline
** Differential fitness - difference of fitness between parent and child
** Only consider individuals with positive differential fitness and find which nodes make biggest impact
** Alternate Selection - Increases probability of ARLs being selected for individuals with scaling
** Had bloat issues with too many ARLs for individuals
** Data Pair Restriction: Only create ARLs that were impactful with EMADEPairTree
** ARL needed to return DataPair to be considered useful
** ARL decreases false positive/negative but never reaches statistical significance
** Did not converge with baseline along later generations (maybe use a more complex datasets than Titanic Dataset)
** Alternate Selection: reduce bloat
** AUC is actually higher for Alternate Selection (might be due to variance with low sample size)
** ARLs limited standard deviation, might be limiting diversity
** Want to add MNIST dataset to EMADE, can leverage more primitives
** Ran 49 generations on MNIST, had issues with creating valid individuals
** Individuals were also very similar
** Lost of overtraining occurred on MNIST dataset, AUC was .088 which is extremely low

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Present slides for final presentation
|Complete
|December 2, 2020
|December 2, 2020
|December 2, 2020
|-
|Complete Notebook for today
|Complete
|December 2, 2020
|December 2, 2020
|December 2 , 2020
|}

=== Goals For Future Semesters ===
* Create more unit tests for newly added CV primitives
* Try co-evolution with CoDeepNeat and integrate chest Xray dataset with it
* Make adaptive mutation more complex
* Find more datasets to test/train on for results and analysis
* Make BERT Layer valid for any position in the tree

== November 30, 2020 ==

=== General Meeting Notes ===
* Last general meeting before final presentations on December 2nd

* Stocks
** Finalizing for final presentation
* EZCGP
** Also finalizing results for final presentation
* NLP
** Focusing on gettin results for final presentations
** Getting more runs on PACE, but had issues with packages being installed, disk space issues
* Modularity
** Also focusing on finalizing for presentations

=== Subteam Meeting Notes ===
* Now focusing in on getting all of our slides ready for the final presentation on Wednesday
* Compared all of the runs from PACE, IceHammer with each other and are going to put results in the slides
* Also planning to rehearse the presentation an hour before the presentations begin

=== Individual Notes ===
* I had finished my work with getting CoDeepNeat implementations run
* When comparing to the current results of EMADE, I saw that the accuracy achieved for EMADE on chest Xray was around 0.93 while the results for CoDeepNeat on CIFAR-10 achieved a Mac accuracy of around 92.7
* Of course, while both got a high accuracy on their respective datasets, we still cannot fully compare until we can run CoDeepNeat on chest X-ray and measure its accuracy which is a future step
* In addition, we should consider maybe coding in more objectives, such as false positive/negative rate or f1 score to the implementations found of CoDeepNeat that can also be used as metrics to measure against CoDeepNeat
* Other than this comparison, I worked on finishing my part for the final presentation and organized my slides

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Work on getting my slides finished for final presentation
|Complete
|November 23, 2020
|December 2, 2020
|December 1, 2020
|-
|Finish peer evaluations
|Complete
|November 23, 2020
|December 2, 2020
|December 1, 2020
|-
|Complete Notebook for this week
|Complete
|November 30, 2020
|December 2, 2020
|December 1, 2020
|}

== November 23, 2020 ==

=== General Meeting Notes ===
* Peer Evaluations are out
* Midterm grades were transferred to Canvas

* Stocks
** Discussed results of attempting to figure out trend scores
** Tried to figure out how to keep model/added rules for genetic labelling
** Created a set of labels for SMP 500 and ran on EMADE
* EZCGP
** Need to fix more greedy hyperparameters
** Working on final presentation
** Were allowing each block to produce many children for mating/mutation instead of moving some of them
* NLP
** Focus on getting runs in/getting results for our paper
** Get two baseline runs for each dataset we're working on, get runs for mating/mutation functions
** Complete trials runs with lovely/amazon review dataset
** Utliize feature extraction methods for dealing w/ primitives, image data
* Modularity
** Got more runs in IceHammer, results look more promising with more generations
** Try to get 10 samples in before presentations
** Got more runs with differential fitness
** Try to merge in differential function work with ARM fixes that were done

=== Subteam Meeting Notes (Monday and Friday) ===
* NLP subteam is still performing runs on PACE and IceHammer and attempting to get results
* Tejas ran into issues with PACE involving memory and disk quote exceeded errors
* Also had issues with removing conda environment
* EMADE runs are on this document: https://github.gatech.edu/pagarwal80/EMADEResults
* I also was able to find a new CoDeepNeat implementation that I could perform more generations of runs on and get more valid results

=== Individual Work ===
* Found a new implementation of CoDeepNeat to run and analyze: https://github.com/sash-a/CoDeepNEAT
* I initially tried to run the code myself, but as with the first implementation, it was taking very long to run each generation
* Luckily though, I was able to see that the creators of the implementation had already performed a thorough run with the CIFAR-10 dataset themselves and I was able to analyze this

=== Analysis of Implementation Run ===
* This implementation went through around 40 generations which was much more than I was able to go through myself with the last implementation I found
* The run-through was able to obtain a fairly high train/test max accuracy of around 0.92
* The implementation also tested out four different versions of CoDeepNeat to see how they would perform against each other
* The average accuracy of different versions hovered around the 0.75 - 0.85 range with the highest accuracy being obtained by the "MMS DA" version (perhaps can use this for comparison against EMADE)
* The next step is to compare against EMADE results and maybe see if we can incorporate CoDeepNeat
[[files/Screen Shot 2020-12-02 at 4.43.49 PM.png|center|thumb|353x353px|Max/Average accuracies for 40 generations with this implementation of CoDeepNeat with CIFAR-10]]

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Compare Results of CoDeepNeat with EMADE
|Complete
|November 16, 2020
|November 30, 2020
|November 29, 2020
|-
|Make sure to organize information in slides for final presentation
|Ongoing
|November 23, 2020
|December 2, 2020
|
|-
|Make sure to complete peer eval
|Ongoing
|November 23, 2020
|December 2, 2020
|
|-
|Complete Notebook for this week
|Complete
|November 23, 2020
|November 30, 2020
|November 29, 2020
|}

== November 16, 2020 ==

=== General Meeting Notes ===
* Stocks 
** Had differences between results found in research paper and the trading signal formula they used
** Are still implementing more Technical Indicators for EMADE
** Are also working on making/updating the genetic labeling notebook
* EZCGP
** Still were running into PACE issues that need to be resolved
** implementation team had Baseline Runs with 10 hours on PACE (will try to reduce runtime for future)
* NLP
** Subteams are now focusing on getting more runs on IceHammer, PACE
** Will attempt to further utilize Amazon Review dataset for Sentiment Analysis
** CV subteam was adding more Primitives for EMADE
** Also try and run CoDeepNeat comparative studies against EMADE
* Modularity
** Still doing runs using DataPairLimit
** Also starting to train with MNIST dataset

=== Subteam Meeting Notes (Monday and Friday) ===
* Tusheet and Tushna coded/added more Primitives to EMADE
* We need to have all of the primitives in signal, spatial, and extraction feature files
* IceHammer, PACE runs will be split up and done by Anuraag, Tusheet, Pulak
* Was able to find an implementation for CoDeepNeat that implemented CIFAR-10 and MNIST datasets as will be discussed below

=== Individual Work ===
* Found a GitHub implementation of CoDeepNeat: https://github.com/sbcblab/Keras-CoDeepNEAT
* I attempted to incorporate chest Xray dataset, but was getting errors when attempting to split into train/test dataset, might've needed to perform more data preprocessing for this specific implementation
* Instead, I decided to just utilize the CIFAR-10 and MNIST datasets as they had already been loaded in using Keras within the implementation's code

=== Analysis of CoDeepNeat Run ===
* For both the CIFAR-10 dataset and MNIST dataset, I noticed strange results
* Both CIFAR-10 and MNIST got their highest accuracy of runs in the first generation, but as the generations moved on, the accuracy dropped to only 0.1
* This might have been due to a limitation on my end as I was only able to run around 4 generations for each dataset, and many more generations of training might be needed to get more valid results (runs were taking a very long time)
* The highest accuracy I achieved with CIFAR-10 was around 0.4 while the highest I achieved with MNIST was around 0.95
* I am not sure on the discrepancy between the accuracy of the datasets although I believe it might be due to the fact that I just need more generations within my runs to gain higher accuracy 
* The next steps will be to find another implementation of CoDeepNeat that will take less time to train or will already have a run completed with a substantial amount of generations so I can get valid metrics of accuracy for comparison
[[files/Screen Shot 2020-12-03 at 9.10.25 PM.png|center|thumb|586x586px|Highest Accuracy Obtained for CIFAR-10]]
[[files/Screen Shot 2020-12-03 at 9.26.59 PM.png|center|thumb|554x554px|Highest Accuracy for MNIST Dataset]]

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Find new implementation of CoDeepNeat to run many more generations on
|Complete
|November 16, 2020
|November 23, 2020
|November 21, 2020
|-
|Compare Results of CoDeepNeat run with EMADE
|Ongoing
|November 16, 2020
|November 30, 2020
|
|-
|Complete Notebook for this week
|Complete
|November 16, 2020
|November 23, 2020
|November 22, 2020
|}

== November 9, 2020 ==

=== General Meeting Notes ===
* Stocks
** Add more TI primitives into EMADE
** Tested trading signal formula and discovered disparities from the paper
** Need to resolve issues that team was getting from datasets they were utilizing
* ezCGP
** lots of people were having PACE issues, set up a meeting to resolve them
** Hopefully can have baseline scores in by next meeting
** Fixed methods on CV2 that were failing
* NLP
** With bounding box, could only obtain data from 8 of 15 classes and couldn't make good predictions
** Use pretained VGG model in the future for CV subteam
** Use Amazon Datset review for Sentiment Analysis tasks for a novel problem
** Some workers on ICE Hammer were not able to utilize the GPU
* Modularity
** Gave first semesters an intro to colab and got them working on new experiments
** Get MNIST dataset working with npz format
** With Selection Method, the new selection method and benchmark selection methods seem to be very similar

=== Subteam Meeting Notes (Monday and Friday) ===
* One of our teammates, Maxim, updated the documentation for PACE
* PACE documentation link: https://www.notion.so/maximgeller/Configuring-EMADE-on-PACE-60aedf065abc445096617c3cec875a11
* Common errors with PACE that we were having throughout the semester were updated with solutions in the document
* For the CV subteam, we started looking at more Primitives that could be added to EMADE to perhaps improve our accuracy and other objectives
* I was also able to find research papers for CoDeepNeat to learn about and later compare against the Neural Architecture Search methods being utilized in EMADE

=== Individual Work ===
* Found research papers corresponding to CoDeepNeat that I read through with links below:
* https://arxiv.org/pdf/1703.00548.pdf
* https://arxiv.org/pdf/2002.04634.pdf

=== Takeaways and Analysis From Papers ===
* CoDeepNeat is a type of Neural Architecture Search that focuses on both an overall neural net architecture along with the subcomponents of each individual layer 
* CoDeepNeat considers the overall architecture and the subcompnonets of each layer as two different populations
* Each population is evolved separately in order to optimize both the overall structure of the neural net along with the nodes that make up each layer
* At the end, for the evaluation stage, the fitness of the assembled networks are averaged out to find the final fitness
* Perhaps CoDeepNeat could serve as a solution to one of the problems with EMADE where it was always searching for the neural architecture with the lowest number of parameters
* The next steps will be to find implementations of CoDeepNeat to test with Chest Xray dataset or other image datasets and compare with how EMADE performs

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look for GitHub implementations for CoDeepNeat
|Complete
|November 2, 2020
|November 16, 2020
|November 12, 2020
|-
|Attempt to integrate a popular image classification dataset with CoDeepNeat implementation
|Complete
|November 9, 2020
|November 16, 2020
|November 12, 2020
|-
|Run and analyze objective scores for CoDeepNeat implementation
|Complete
|November 9, 2020
|November 16, 2020
|November 14, 2020
|}

== November 2, 2020 ==

=== General Meeting Notes ===
* Stocks
** Started to get first semesters introduced to the subteam
** Will start tasking first semesters soon 
** Still also looking into more research papers/literature to utilize and compare results with
* EzCGP
** Still fixing issues involving baseline with the implementation 
** Also working on integrating first semesters and tasking them
** Research team is working on performing code reviews
* NLP
** CV subteam is still working on running chest Xray dataset with YOLO model
** Also working on implementing Adaptive Mutation
** Still need to integrate BERY word embedding model with EMADE
** Also will try to get more runs in on PACE/Ice Hammer for CV subteam
* Modularity
** Are integrating first semesters and working on tasking them
** Working on maybe getting MNIST dataset integrated to run on
** Still refactoring Arguments, are also presenting the results from their runs

=== Subteam Meeting Notes (Monday and Friday) ===
* Looking into utilizing amazon review dataset for sentiment analysis
* Anuraag, Tusheet, and Tejas were able to get results using the YOLO approach and Bounded Box
* They trained for 150 epochs
* Ran into some problems with Bounded Box
* Only about 1,000 images out of the total 100,000+ images actually had bounded box data
* Also, many diseases were not localizable to one region, meaning the 1000 images only have 8 of the total 15 diseases
* YOLO/Bounded box was not able to detect most of the diseases

=== Individual Work ===
* I was able to find a GitHub to run the chest Xray dataset utilizing VGG for a run and MobileNet for another run
* Link to GitHub: https://github.com/paloukari/NIH-Chest-X-rays-Classification
* For these models, the True Positive and True Negative rate seemed to get closer and closer to 1 as more generations were ran
* Also will start looking into CoDeepNeat to further compare how it does against Neural Architecture Search implemented into EMADE by NLP team

=== Comparison to YOLO and Bounded Box ===
* In comparison to YOLO and Bounded Box, the VGG and mobile net models seemed to do much better than the YOLO model
* VGG and MobileNet were able to get an AUC of around 0.7- 0.8 for most diseases and were able to actually classify all 15 diseases unlike with the Bounded Box approach
* Both VGG and MobileNet seemed to converge to an accuracy of around  0.88 although VGG took 10 minutes less to do so
* From this analysis, it seems that VGG is for now the go-to model to use for our chest Xray Dataset
[[files/Screen Shot 2020-12-03 at 7.11.07 PM.png|center|thumb|AUC Curve for VGG/MobileNet]]
[[files/Screen Shot 2020-12-03 at 7.13.46 PM.png|center|thumb|AUC Curve for Models Tested (Convergence for VGG and MobileNet around 0.88) ]]

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Start looking into CoDeepNeat and research papers for comparative study
|Complete
|November 2, 2020
|November 9, 2020
|November 8, 2020
|-
|Compare VGG results and MobileNet results with each other
|Complete
|November 2, 2020
|November 9, 2020
|November 6, 2020
|-
|Also start looking for GitHub implementations for CoDeepNeat
|Ongoing
|November 2, 2020
|November 16, 2020
|
|}

== October 26, 2020 ==

=== General Meeting Notes ===
* Stocks
** Truth data produced a negative profit percentage
** Moving Average from AlphaVantage API was inconsistent with price values
** Went over what first semester students would do 
* ezCGP
** Research/Implementation teams are doing a pseudo merge
** First semester students will be tasked
** Might add more primitives in the future
* NLP
** Have a run using YOLO with chest X-ray dataset
** Still working on adding word embedding/implementation layers
** Look at Co-evolution to boost performance on datasets with EMADE
* Modularity
** Give first semester students new tasks

=== Subteam Meeting Notes (Monday and Friday) ===
* Worked on doing further Research with Bounded Box and YOLO problem
* YOLO model - does only pass through of image, takes a reduction in time for training
* Also looked into VGG/MobileNet for chest Xray dataset on my part to compare with results gained from YOLO and Bounded Box problem

=== Individual Work ===
* Looked into this research paper for MobileNet: https://arxiv.org/pdf/1704.04861.pdf
* Also looked into this article for VGG: https://neurohive.io/en/popular-networks/vgg16/

=== Analysis of Research Papers/Articles ===
* MobileNet focuses on building smaller, more efficient networks and aims to optimize for latency
* Built from depth wise separable convolutions to reduce computations in the first few layers
* The core layers of MobileNet are built around depth wise separable convolutions that factorize a standard convolution into a depth wise convolution
* MobileNet could perhaps be utilized with Chest Xray dataset to reduce computation time, build up more complicated layers through the depth wise separable convolutions that MobileNet is built off of
* Can then possibly lead to reducing training time for chest Xray dataset along with getting higher score on objectives such a accuracy or false positive/negative rate
* VGG replaces large-sized kernel filters with multiple smaller 3 X 3 kernel filters
* Adds more complexity to the filters, with more depth and more fully connected nodes, but takes longer to train
* Could use on chest Xray dataset to test on in conjunction with MobileNet
* Next steps would be to find implementation of Mobile Net or VGG to utilize with chest Xray data

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Find Github implementation of Mobile Net or VGG
|Completed
|October 26, 2020
|November 2, 2020
|October 30, 2020
|-
|Compare Results of YOLO run with Mobile Net or VGG
|Completed
|October 26, 2020
|November 2, 2020
|November 2, 2020
|-
|Complete Notebook entry for this week
|Completed
|October 26, 2020
|November 2, 2020
|October 28, 2020
|}

== October 19, 2020 ==

=== General Meeting Notes ===
* Midterm Presentations
* Stocks
** Focused on how to use EMADE to perform regression on time series data
** Want to optimize stock market trading data/improve on existing data
** Research Team - create CEFLAN architecture from paper they read
** Implementation - ran EMADE with regression, integrate TIs
** Technical Indicators used:
*** Heuristic used by traders
*** Use high value to have theoretical buy-in power, reduce volatility with stable stocks
** Added more regression learners to EMADE - Multi-Layer Perception, Gaussian Process
** Ran experimental data for 85 generations
** Seeded run w/ vanilla model of scikit
** Future Work:
*** Utilize EMADE's stream-to-feature primitive 
*** Integrate TI indicator primitives in EMADE
*** Utilize more recurrent neural networks and LSTMs
*EzCGP
**Added new primitives such as Gaussian Noise, Convolutions for CIFAR-10 dataset
**Working on block evaluation/refactoring
**When training CIFAR-10, all individuals are trained in full, takes lots of computation time, can't initialize weights of a network in a systemic way
**Attempting to address diversity with Noisy/Aging Training
**Also attempting to reduce computation time
***Looked into early termination based on reference curve
***Terminate if current accuracies are worse than values of reference curve by N consecutive times
**Looked into super-convergence to reduce more expensive regularization techniques
*NLP
*Link to midterm presentation: Â https://docs.google.com/presentation/d/1Z8PjzaP_LjhyycpGSeXX7rIHdVYkxHzqg8vaKVEYhdU/edit#slide=id.ga3dcb7a2bf_6_2
*Individual Slides I presented (I am on the CV subteam)
*[[files/Screen Shot 2020-12-03 at 12.57.25 PM.png|center|thumb|Slide I presented]]
*Some key takeaways
**For the future in this semester, we should focus on finding more papers about different Neural Architecture methods to compare against and possible utilize
**Also want to attempt to add in more Primitives for the CV subteam
**Can also try and train Chest Xray dataset on more varied models to see what kind of results we obtain and compare
* Bootcamp Group 3
** Predict whether a person survived on the Titanic
** Minimize False positive and False Negative rates
** Used multiple models for co-dominant Pareto Front - MLP NN, Logistic Regression, Random Forest, SVM
** Had 3 runs with 30 generations and 5821 total evaluated individuals
** Analyzed elapsed time - summed up how many seconds it took to evaluate individuals with null fitness vs non-null fitness
** Problems w/ NSGA II as it truncated individuals
** Problems with using on-hot encoding, created bias
** Couldn't determine total positive and negative results
** Genetic Programming had lowest AUC but tended to optimize for single objective
** EMADE balanced objectives better than GP
*Modularity
**Exploring ways to abstract parts of individuals
**Can allow us to reuse blocks of code
**ARL - introduce modularity to EMADE, dynamically evolves and forms a new primitive from a section of a tree
**Refactored arguments with ARLs
**ARLs creating wrappers around ARLs but not adding any complexity
**Had check to only allow for subtrees that return EmadeDataPair
**Rab 2nd experiment with second selection method
**Increases probability of ARL getting selected through linear scaling of probabilities
*Bootcamp Group 1
**Dropped name and ticketID Values
**Changed non-numerical values to numerical
**Used Naive Bayes, Random Forest, Logistic Regression
**No models and Pareto fronts really outperformed the other
**Best individuals had age, sex, and class features
**On EMADE, ran 17 generations, 311 final valid individuals, 235 on the Pareto front
**Average accuracy of Pareto individual of 95.2 percent
**EMADE used 5-fold cross-validation on the dataset
**EMADE produced better results in terms of AUC even though it took more processing power
**EMADE had better accuracy compared to ML and GP
*Bootcamp Group 2
**Created heat map to discover covariance in data
**Analyze survival rates in data to look for predictable outcomes
**Cabin section was dropped due to skewing of data, dropped embarked, fare w/ no predictive capabilities
**Some models were co-dominant while others were not
**Had some issues with EMADE initially , but connected later with internal ip and ifconfig command
**Headless chicken runs - 50 percent increase to both HC hyperparameters, 20 percent increase to crossover
**AdaBoost combines weak classifiers into strong classifiers, created individuals w/ more optimized results
**Headless chicken run created a wider tree, more constants at the leaves
**EMADE performed the worst on the AUC
**GP had slightly higher AUC than ML

=== Subteam Meeting (Monday and Friday) ===
* Consider testing the Chest Xray dataset using YOLO method and Bounded Box
* Utilizing YOLO could possibly allow for images to have multiple diseases detected on them
* Also looking into CoDeepNeat paper to compare results of EMADE against
* Possibly implement attention layer, adaptive mutation layer for the future
* Since Anuraag and Tejas were working on YOLO, I decided to also take a look at other methods, such as mobile net/VGG to train the Chest Xray dataset and get results from

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Present midterm presentation/take notes from other groups
|Completed
|October 19, 2020
|October 19, 2020
|October 19, 2020
|-
|Research more into mobile net/VGG for chest Xray for chest Xray data
|Completed
|October 19, 2020
|October 26, 2020
|October 24, 2020
|-
|Research more into YOLO to also compare results for chest Xray data
|Completed
|October 19, 2020
|October 26, 2020
|October 24, 2020
|}

== October 12, 2020 ==

=== General Meeting Notes ===
* Stocks
** Performed a big data run on Emade
** evolve means of calculating technical indicators w/ Emade
** Ran 500 gens on Emade for S and P 500 with error around 10.3
* ezCGP
** Research Team is building up presentation[[files/Screen Shot 2020-12-03 at 12.04.28 PM.png|thumb|Pareto Front Graph from Colab Run]]
** Implementation team is trying to get baseline runs before midterm presentation/have metrics set up
** Merge PR for convolution2D primitive and added Conv2D transpose
* Modularity
** Fixed issue with args in ARLs
** Focus on doing runs with arg changes and observe results

=== Subteam Meeting Notes (Monday and Friday) ===
* Were still working on getting colab to work with EMADE, although must of us fixed our SQL issues
* Subteam was able to perform a run on colab once we had EMADE set up, here are some of the results when we ran EMADE

=== Individual Work ===
* [[files/Screen Shot 2020-12-03 at 12.04.43 PM.png|thumb|Parameters in graph decrease with increase in generations]]Was finally able to get mySQL working with running EMADE on colab
* Turns out that I did not need to worry about Port Forwarding in the first place and could have just used remotemysql for database hosting
* I also ran into a problem but that happened because I utilized the wrong seeding file, but fixed it by using input_image.xml

=== Analysis ===
* Results showed a relatively low AOC curve (low error), maybe could be improved further on chest Xray dataset through more models that we will try out in the future
* Interesting to note that the number of parameters found within the model start to decrease as time moves on

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Fix MySQL issues to get colab runs working with EMADE
|Complete
|September 28, 2020
|October 12, 2020
|October 10, 2020
|-
|Analyze initial results from colab run with EMADE
|Complete
|October 12, 2020
|October 19, 2020
|October 14, 2020
|-
|Make sure to have slides ready for the midterm presentation
|Complete
|October 12, 2020
|October 19, 2020
|October 19, 2020
|}

== October 5, 2020 ==

=== General Meeting Notes ===
Make sure to update notebook by 11:59 pm tonight 

VIP Midterm presentation are in 2 weeks
* Stocks
** Added sklearner functions for regression such as stochastic gradient descent
** Will try to add functions to EMADE
** Want to implement neural nets into EMADE w/ keras
* ezCGP 
** Setting up baseline runs on PACE
** Maybe will incorporate feature freeze in the future after having initial runs
* NLP 
** Ran EMADE with mating functions, with two different number of generations
** number of generations didn't affect score
** CV subteam is working on getting smaller runs on colab/setting up mySQL 
* Modularity
** having problems with arg[0]
** Need tow work on restructuring ARLs to not face the same issues

=== Sub-Team Meeting Notes (Still need to meet this Friday also) ===
* Make sure to continue working on getting our models run on colab
* Finish setting up SQL on colab also w/ port-forwarding
* Tejas already has a setup framework for colab and EMADE, just need to focus on mySQL

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get mySQL working with colab using port-forwarding
|Ongoing
|September 28, 2020
|October 12, 2020
|
|-
|Make sure to update notebook for mid-term grading
|Complete
|September 28, 2020
|October 5, 2020
|October 5, 2020
|-
|Run model for Chest X-ray dataset on colab 
|Ongoing
|September 28, 2020
|October 5, 2020
|
|}

== September 28, 2020 ==

=== General Meeting Notes ===
* Stocks
** Fixed bugs on colab
** Were in the process of implementing functions within research papers
** Were able to make a regression run on EMADE w/ colab
* NLP
** Tested mating functions on EMADE
** Best individual score .035 on first run, got it to .037 on second run
** Mating function didn't help improve score much
* ezCGP
** Research team will make and put high-performance individuals for seeding
** Implementation Team:
*** Working on getting basic primitives
*** Shifting old primitives over to new framework
*** Finish data loading w/ primitives
* Modularity
** Got colab up and running, are going to try new selection method
** Importing sep python package produces an issue (same issues with NLP subteam)
** Individuals are not being properly created
** Might look at digits (0 - 10) dataset to load into EMADE

=== Sub-Team Meeting Notes (Monday/Friday of this week) ===
* PACE was running into too many issues w/ importing packages, decided that for now we should just switch to colab to perform our runs
* Need to figure out getting Colab integrated with EMADE
* Also be on the lookout for any other clusters that could be usable, maybe talk to Dr. Zutty about this
* Focus on incorporating more primitives into layer list (other subteam)
* Now we will try to port forward/ get SQL database set up with colab using port forwarding

=== Individual Work ===
* I am now working on getting mySQL setup to work with colab thru port-forwarding
* I began looking at the document provided on the wiki page to integrate colab with EMADE

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get mySQL working with colab
|Ongoing
|September 28, 2020
|October 12, 2020
|
|-
|Make sure conda environment/dependencies work with colab
|Completed
|September 28, 2020
|October 5, 2020
|October 1, 2020
|-
|Finish Peer Evals
|Completed
|September 28, 2020
|October 2, 2020
|October 2, 2020
|}

== September 21, 2020 ==

=== General Meeting Notes ===
*Stocks
** Focusing on reading up on research papers
** Want to compare EMADE's results with paper results
** Will perform these runs on colab by integrating EMADE
* NLP
** Tested mating functions on EMADE
** CV subteam is still trying to have initial runs of models on PACE
** New optimizers were added to the NLP-NN branch
* ezCGP
** Focusing on cifar-10 dataset
** Also looking into more research papers and discussing results
* Modularity
** Still running into PACE issues that need to be resolved
** mySQL still needs to be set up w/ colab and are running into issues
** Doing runs on EmadeDataPair to observe the results

=== Sub-Team Meeting Notes (Monday/Friday of this week) ===
* For the CV subteam, we are now running our pre-trained models on PACE and seeing if we are running into any errors
* Anuraag said he had some issues with conda packages when attempting to run his model
* I also ran into issues when running my model on PACE, although mine might center around the PBS script that I had created and will try to see if the script needs any fixes throughout the week

=== Individual Work ===
* I attempted to run a pre-trained model I had found on PACE and see what errors I might run into
* Link to repo: https://github.com/vinaya8/Transfer-Learning-on-Chest-X-Ray-Images
* I used scp to get the model into my local PACE directory and created a PBS script to tray and run the model on PACE with my conda environment
* PBS Script:
[[files/Screen Shot 2020-09-27 at 7.53.05 PM.png|center|thumb|507x507px|PBS Script to run model]]
* However, when I went to submit a job request using my PBS script, I got the error shown below:
[[files/Screen Shot 2020-09-27 at 7.53.43 PM.png|center|thumb|599x599px|Error when trying to run model on PACE]]
* I believe that I might have an error somewhere within my PBS script but I will try and contact my other teammates to figure it out

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run models on PACE/resolve issues (Have now decided to just switch to colab)
|Ongoing
|September 7, 2020
|October 5, 2020
|
|-
|Fix mySQL errors with PACE (Have now decided to just switch to colab)
|Ongoing
|September 7, 2020
|October 5, 2020
|
|}

== September 14, 2020 ==

=== General Meeting Notes ===
* Make sure to fill out self-evaluation and put it in notebooks by next Monday
* Self-evaluation:
*[[files/Screen Shot 2020-09-28 at 12.21.04 PM.png|center|thumb|804x804px|Self-evaluation]]
* NLP
** Still running preliminary tests on EMADE for CV subteam to make sure the runs works
** Trying to also run pre-trained models on PACE
** Still need to resolve issues with PACE in terms of mySQL
** Looking into more academic papers also to help w/ future endeavors
* Modularity
** Looking to improve selection methods for individuals
** Improve up tournament selection modification
* Stocks
** Subteams are still reading up on more research papers
** They are trying to learn more information about Technical Indicators
** Are also going more into depth in incorporating ML with stock trading
*ezCGP
**Still looking through research papers and deciding which ones are best to use
**Also finished converting their primitives

=== Sub-Team Meeting Notes (Monday/Friday of this week) ===
* We are now attempting to try and run our found pre-trained models on PACE
* Some people were encountering Disk Quota Exceeded error until we decided to delete the .git folder 
* Still should be on the lookout for any more pre-trained models we can find to run on PACE
* Also need to do seeded runs for chest X-ray dataset on PACE
* Also look for research papers related to image classification

=== Individual Work ===
* I decided for this week to do some more research on the VGG convolutional neural net model and how it could be utilized for the CV subteam
* This was due to the fact that many of the GitHub repos I had found for the chest X-ray dataset utilized VGG in their models
* Links for research:
** https://towardsdatascience.com/vgg-neural-networks-the-next-step-after-alexnet-3f91fa9ffe2c
** https://neurohive.io/en/popular-networks/vgg16/
** Takeaways:
** VGG has significant performance improvement over the previous AlexNet neural net for image classifiaction
** One of the biggest reasons for this improvement is because VGG utilizes very small receptive fields and contains 3 ReLU units, allowing it to be more discriminative in its classification
** The small-size convolution filters allows VGG to have more weight layers, leading to improved performance
** However, VGG still does take a very long time to train
** VGG's network architecture weights are also very long
** Perhaps we can utilize VGG for more of our own models in the future to improve our image classification and how many categories it is able to find or to improve the accuracy of our classifier models

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run pre-trained models on PACE
|Ongoing
|September 7, 2020
|September 21, 2020
|
|-
|Complete self-evaluation
|Completed
|September 14, 2020
|September 21, 2020
|September 18, 2020
|-
|Fix mySQL issues on PACE
|Ongoing
|September 7, 2020
|September 21, 2020
|
|}

== September 7, 2020 ==

=== General Meeting Notes ===
* It was Labor Day so there was no general meeting today

=== Sub-Team Meeting Notes (Monday/Friday of this week) ===
* Worked on attempting to set up mySQL on PACE, but we were still running into errors involving the paths in the .my.cnf file
* We were still continuing to find pre-trained models from GitHub repos for X-ray dataset
* Other team members were doing preliminary runs of a small portion of the chest X-ray dataset on PACE although I was still waiting to find more models to run mine
* Also working on seeding individuals for future runs on PACE

=== Individual Work ===
* Now that I had PACE set up with a conda environment, I decided to try and attempt to run mySQL on PACE with the updated .my.cnf file
* [[files/Screen Shot 2020-09-27 at 10.41.36 PM.png|thumb|387x387px|Queue not found error run into when setting up mySQL on PACE]]I was able to create a db file in the scratch directory and modify the .my.cnf file in terminal
* However, I ran into "queue not found" errors when I attempted to run the command "qsub -I -q iw-shared-6 -l nodes=1:ppn=1,walltime=01:00:00" in order to get an allocation on a compute node
* I decided to fix the SQL issues a little later on as I did not need SQL to initially run my pre-trained models
* I found two more pre-trained models with links below:
** https://github.com/zdmc23/nih-chest-xrays
** Interestingly, this model utilized a much simple binary classification model compared to many of the more complicated Convolutional Neural Networks utilized in earlier models
** The model did a great job of separating chest X-rays with Pulmonary Fibrosis, but it would also be more interesting to further test and see whether binary classification could work as more categories of diseases are added
** https://github.com/vinaya8/Transfer-Learning-on-Chest-X-Ray-Images
** This model also utilized a VGG convolutional network like the first model I had found, meaning that VGG might be a very viable model for CV 
** Had a very easy to follow read.me to implement, so I might start by testing this model out first on PACE 

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run Pre-Trained models on PACE
|Ongoing
|September 7, 2020
|September 21, 2020
|
|-
|Fix mySQL issues I was facing on PACE
|Ongoing
|September 7, 2020
|September 21, 2020
|
|-
|Find out more about VGG Model and how it's used for CV
|Completed
|September 7, 2020
|September 21, 2020
|September 13, 2020
|}

== August 31, 2020 ==

=== General Meeting Notes ===
* Stocks
** The newly formed stocks team decided to split into 3 subteams
** One group focused on integrating w/ EMADE, the other focused on utilizing ML with stock trading, the last one focused on making technical indicators
* NLP
** For the CV subteam, we are still looking for pre-trained models for the chest X-ray dataset
** Wanted to add other word-embedding models such as ELMO
**[[files/Screen Shot 2020-09-27 at 9.37.08 PM.png|thumb|New update for the .my.cnf file]]Wanted to add new mutation functions that could potentially be used with EMADE
** Make sure that PACE is up and working for everybody
* ezCGP
** Split into two sub teams to focus on research fundamentals and maintaining the actual code
*Modularity
**Focusing on more research by looking at papers, also are attempting to fix bugs found from the prior semester

=== Sub-Team Meeting Notes (Monday/Friday of this week) ===
* We had to modify the .my.cnf file to utilize later in order to be able to run mySQL on PACE
* Still needed to make sure that PACE was up and running for everyone
* We were still on the lookout for pre-trained models for our X-ray dataset to run on PACE

=== Individual Work ===
* Now that I was able to get on PACE, I decided to test out and make sure that I could set up a Conda environment for my required packages
* At first, when I attempted to install tensorflow, Keras with Conda, I was getting a disk quota exceeded error
* I eventually found out that to fix this error, I needed to delete the ./git repo in Emade as it was taking up quite a bit of memory
* I continued to look for pre-trainned models on GitHub for the chest X-ray dataset and found a decent one
** Link to repo: https://github.com/paloukari/NIH-Chest-X-rays-Classification 
** The model utilizes a Convolutional Neural Network (commonly used as I searched through other models for the x-ray dataset) 
** Found the interesting result that some diseases were easier to identify/classify than others within the dataset 
** Found that utilizing VGG neural networks kept the error rate to somewhat underneath 9 percent and performed very well 

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Search for more pre-trained models for Chest X-ray dataset
|Ongoing
|August 17, 2020
|September 7, 2020
|
|-
|Get PACE login working and conda environment set up
|Completed
|August 24, 2020
|August 31, 2020
|August 29, 2020
|-
|Make sure mySQL can run with PACE
|Ongoing
|August 31, 2020
|September 7, 2020
|
|}

== August 24, 2020 ==

=== General Meeting Notes ===
* Stocks
** Newly formed team as of last week and are still going to continue throughout the remainder of VIP
* NLP
** For the CV subteam, we are attempting to start looking for pre-trained models for the chest X-ray dataset
** CV subteam also needs to figure out how to incorporate/run these models with PACE
** Make sure PACE is up/running for everyone
* ezCGP
** Split into two sub teams to focus on research fundamentals and maintaining the actual code

=== Sub-Team Meeting Notes (Monday/Friday of this week) ===
* The major focus for the CV subteam was to find pre-trained models for the Kaggle Chest X-Ray dataset we are exploring to run on PACE and train with EMADE
* We decided on using PACE instead of Google Colab to run our models
* We also looked at two other CV datasets that were similar to the Chest X-ray Dataset from Kaggle we were already exploring:
** https://stanfordmlgroup.github.io/competitions/chexpert/
** https://www.kaggle.com/c/cifar-10

* The Stanford X-ray dataset also happens to be another large dataset utilized for screening X-ray images for diseases while the second cigar-10 dataset is a more general dataset of various images

=== Individual Work ===
* The first task for myself was to ensure that I could log onto PACE so I could utilize it for future runs
* Initially, I was getting an error about my SSH key being invalid, but I went into the .ssh/known_hosts file in my home directory and updated it with the new key
* I was still having trouble logging into PACE as I kept getting a "remote side unexpectedly closed network connection" error
* I also started looking at GitHub repos online for pre-trained models for the Kaggle X-ray dataset (Ongoing)

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get PACE working for future runs
|Completed
|August 24, 2020
|August 31, 2020
|August 25, 2020
|-
|Find Pre-trained models for X-ray dataset
|Ongoing
|August 17, 2020
|September 7, 2020
|
|-
|Look at/familiarize my self with cifar-10/Stanford x-ray dataset
|Ongoing
|August 24, 2020
|September 7, 2020
|
|}

== August 17, 2020 ==

=== General Meeting Notes ===
* Met with everyone in the VIP to discuss the start of the fall semester/course of action to take throughout
* Went over the sub teams that already existed within the VIP to decide if we wanted to stick with our previous subteam or switch
* I decided to stick with my previous NLP subteam
* Dr. Zutty also discussed the possible generation of new sub teams and the idea of a new stocks/finance prediction subteam also came up
* Dr. Zutty also clarified the remote status of the class and mentioned that notebooks would be more important than ever now

=== Sub-Team Meeting Notes ===
* There was no subteam meeting today
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Decide on which VIP subteam to be on for semester
|Completed
|August 17, 2020
|August 24, 2020
|August 17, 2020
|-
|Make sure to set new goals for the NLP subteam
|Ongoing
|August 17, 2020
|August 31, 2020
|
|-
|Resolve any issues with PACE we might face
|Ongoing
|August 17, 2020
|August 31, 2020
|
|}

== April 20, 2020 ==

=== General Meeting Notes ===
* Final Presentations are today
* Made sure to take notes on each sub team's presentations

=== Individual Notes on SubTeams ===

==== ADFs ====
* Want to improve EMADE by reusing useful subtrees of individuals 
* Identify what parts of individuals are useful across the population
* Was evidence of improvement from previous findings as there were lower overall mean/median AUC values
* ADFs might limit the amount of exploration done but they didn't significantly alter standard deviations
* By analyzing ADF composition, can determine how they impact a population's fitness and if they are being used often enough by the population
* From primitive analysis, it was found that many ADFs do contribute to the Pareto Front relative to the number of individuals
* Differential Fitness - measures the change in fitness between an individual and a parent, children who improve over their parents may have useful components for rest of population

==== Research Fundamentals ====
* The main goal of research fundamentals was to reduce Bloat in evolutionary algorithms
* Reducing bloat would help the algorithms run faster as Bloat is just unnecessary code that doesn't contribute to fitness
* The main technique used to control was Neat-GP
* Neat-GP attempts to control bloat utilizing a defined set of heuristics
* Fitness sharing penalizes individuals that come from highly populated species
* Internal nodes, leaves, branches are swapped with each other in a process known as crossover
* It was found that with a speciation threshold of 0.6, there was a large increase in hyper volume but not bloat
* With a speciation threshold of 0.3, there was a large increase in both hyper volume and bloat

==== NLP (Time-Conflict) ====
* Running MySQL and EMADE on PACE produced many issues (similar to our NLP subteam for non time-conflict)
* Assigned each member a unique port to run MySQL as they couldn't all run it on the same default port
* Num Named Entities Primitive: The more named entities in a paragraph, the more information it contains, meaning it has greater importance
* Primitive takes in a 2D numpy array where the inner array is an array of sentences and the outer array is an array of paragraphs
* Primitive uses spaCy library to find the named entities
* Tried to make the primitive more efficient by writing unit tests for it 
* TS-ISF Primitive: Assigns numbers to a sentence in a document to label how important that sentence is within the document
* Each world is analyzed and assigned a numerical values to represent its rarity[[files/Screen Shot 2020-04-27 at 3.11.48 PM.png|thumb|My slide about the SeLU primitive]]
* NumNamedEntities primitive ended up taking about only 10 minutes while the TSIF primitive took greater than 2 hours to evaluate (needed to cut this time down)
* In terms of statistical trends, the average accuracy for NumNamedEntities went down while that of TS-ISF ended up going up as more generations passed

==== EZGCP ====
* The dataset used was CIFAR-10 due to its increased complexity compared to MNIST and its wide variety of models to use for comparison
* Ran compared models on Tesla P100 GPU on PACE for 39 generations
* Got better test accuracies than models that were similar that did not use augmentation
* For Select Individuals, the average approximate accuracy was 84 percent while the best test accuracy after 30 epochs was 84.7 percent after run 2
* For 3-Block VGG with dropout, the test accuracy after 200 epochs was 84.31 percent
* Data Augmentation: generate new samples from an existing small dataset
* Data Augmentation is implemented by specifying a pipeline of transformations applied randomly to images in order
* Transfer Learning uses pre-trained and validated neural networks from an existing dataset to use on a new dataset by retraining the last layer
* Found that training multiple models in tensorflow-gnu causes large slowdowns over time
* In comparing speedup of CPU vs GPU, found that GPU is faster than CPU, CPU scales worse with increased number of processes

=== Subteam Meeting Notes/Individual Work ===
* Presentation Slides link: https://docs.google.com/presentation/d/1sfyO-eB262HKiVnPvO8vDu_6n4wR-Q1RJrIfldWo810/edit#slide=id.g6bd390a1d0_1_6

* Subteam presented our final presentation slides
* I created my slide discussing my SeLU primitive that I added and defined and presented it
* Talked about internal/batch normalization provided by SeLU and how mean of values can be set to 0, standard deviation can be set to 1, and values can be preserved between layers
* Discussed the code I added to neural_networks_methods.py and gp_framework_helper.py to define and add my primitive
* Also discussed the graph of SeLU and how it can shift the mean by allowing both positive/negative values for y
* Also discussed how SeLU allows for increasing/decreasing the variance of values through scaling with a lamb parameter in its equation
* Subteam discussed future goals to improve/add new primitives, include computer vision primitives, concatenate layers of neural net, and incorporate ADFs to save permutations of working layers
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Present our sub teams results for final presentation
|Complete
|April 20, 2020
|April 20, 2020
|April 20, 2020
|-
|Take notes on information/results other sub teams presented
|Complete
|April 20, 2020
|April 20, 2020
|April 20, 2020
|-
|Complete Peer Evaluations
|Complete
|April 9, 2020
|April 22, 2020
|April 21, 2020
|-
|Complete Notebook
|Complete
|April 20, 2020
|April 27, 2020
|April 27, 2020
|}

== April 13, 2020 ==

=== Subteam Meeting Notes ===
* [[files/SeLU definition.jpg|thumb|Defining SeLU primitive]][[files/Adding SeLU.jpg|thumb|Adding SeLU as a primitive]]Colab and EMADE ended up working for our subteam
* Needed to make sure we finished defining/adding our tasked primitive before Thursday to get it ready for the final presentation
* We started to put content on the slides for our final presentation coming up in a week
* [[files/Adding Activation Model.jpg|thumb|Adding Activation Model for SeLU]]Needed to make sure to finish peer evals which were due by the 22nd

=== Individual Work ===
* I was able to define and add my primitive to neural_networks_methods.py as shown in the pictures to the right
* I made sure to define my primitive as a separate function in neural_networks_methods.py
* I also added the actual primitive using "pset.addprimitive" in gp_framework_helper.py
* I also made sure to add the SeLU model as an Activation model since SeLU is an Activation Function
* Mohan told me I didn't need to have an input parameter
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Define and add the SeLU primitive
|Complete
|April 6, 2020
|April 20, 2020
|April 15, 2020
|-
|Complete Peer Evaluations
|Ongoing
|April 9, 2020
|April 22, 2020
|
|-
|Get ready for final presentation and add content to slides
|Ongoing
|April 6, 2020
|April 20, 2020
|
|}

== April 6, 2020 ==

=== Subteam Meeting Notes ===
* Went over results we had with Toxicity dataset/X-ray Chest imaging dataset
* We all found that we had very high accuracy with our results after training the data
* We were also starting to be tasked with adding new primitives in our neural_networks_methods.py

=== Individual Work ===
* The primitive that I was assigned to work on was the SeLU function
* I started to do some research on the SeLU function and found more information on this website: https://towardsdatascience.com/gentle-introduction-to-selus-b19943068cd9
* I learned about the advantages that SeLU had over the ReLU activation function
* I learned that SeLU can add batch normalization functionality where values are transformed so their mean is zero and standard deviation is one between each layer of a neural network
* I also learned about how SeLU adds internal normalization where each layer in a neural net is able to preserve the mean and variance from the previous layer
* The SeLU function is able to shift the mean for values by having both negative and positive values for y unlike the ReLU activation function
* The formula behind SeLU also utilizes a scaling parameter to either increase or decrease its gradient and, therefore, the variance of values
{| class="wikitable"
!'''Task'''
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Research more about how the SeLU function works
|Complete
|April 6, 2020
|April 13, 2020
|April 10, 2020
|-
|Define and add the SeLU primitive to neural_networks_methods.py
|Ongoing
|April 6, 2020
|April 20, 2020
|
|-
|Start adding to slides as Final Presentation is coming up
|Ongoing
|April 6, 2020
|April 20, 2020
|
|}

== March 30, 2020 ==

=== Subteam Meeting Notes ===
* Mohan talked more about using EMADE and its major features to first semesters
* [[files/Screen Shot 2020-04-27 at 4.13.33 PM.png|thumb|Changine epochs to 3 and train/test split to 0.82/0.18 to train Toxicity Dataset]]I am still continuing to work on the toxicity dataset and training it
* Also discussed starting to find and add primitives that could be run on EMADE
* Also looked further into the original code given to us in the toxicity dataset and how to eliminate overfitting from the original code that trained the dataset

=== Individual Work ===
* While working on training the toxicity dataset, I decided to use a single layer for multi-label classification as other sub team members were using multiple layers
* I decided to reduce the number of epochs run on the dataset from 5 to 3 as the larger number of epochs might have contributed to the overfitting in the model
*[[files/Screen Shot 2020-04-27 at 4.44.17 PM.png|thumb|Accuracy results after training Toxicity Dataset with new epoch and train/test split values]]Reducing the epochs brought the accuracy down from around 98.1 to 97.8, which was not a large reduction in accuracy, and that overfitting might still be occurring 
* I also attempted to increase, decrease the test/train split
* On my first run where I increased the test split to .25 and decreased the train split to .75 and still received an accuracy of around 97 percent
* On my second run, I decreased the test split to .18 and increased the train split to .82 and also still received an accuracy of around 97 percent
* From my results, I still could not figure out the right range of values for the epoch and test/train split to solve the overfitting problem, I might need to look into other models to train the data on
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Train on Toxicity Dataset/Look into overfitting
|Complete
|March 2, 2020
|March 30, 2020
|March 30, 2020
|-
|Look into more models dealing with Single output layers and multi-label classification
|Ongoing
|March 30, 2020
|April 27, 2020
|
|-
|Make sure to keep my notebook updated
|Complete
|March 30, 2020
|March 30, 2020
|March 30, 2020
|}

== March 23, 2020 ==

=== Subteam Meeting Notes ===
* This was the first meeting that we had which was held over Blue Jeans
* The main goal for today was to go over general information about what the NLP subteam does to the first semesters
* Tasks were assigned to look into either the Toxicity dataset or Chest X-ray Imaging dataset and utilize ML models to train the data
* I asked to work on the Toxicity dataset to gain some more experience working with ML models and Neural Nets

=== Individual Work ===
* I started to look into the Toxicity dataset and the features it was comprised of in order to better understand how to train it
* Link for information found: https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/
* I saw that the dataset contained comments from Wikipedia's talk page edits and that there were six categories under which these toxic comments could fall under
* The main goal of training this dataset is to predict  the image category and description given an image as input
* Can utilize either single or multiple output layers for multi-label classification of our data
* Using a single dense layer, there will be six outputs with a sigmoid activation function and binary cross entropy loss functions
* If a neuron's output value is greater than or equal to 0.5, it is assumed that the comment belongs to the category represented by that neuron
* I was going to try to change the train/test split and models used to train this data in order to see what accuracy I could get
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look into Toxicity Dataset and train it
|Ongoing
|March 2, 2020
|March 30, 2020
|
|-
|Explain to first semesters more about the goals/tasks of the NLP subteam
|Complete
|March 23, 2020
|March 23, 2020
|March 23, 2020
|-
|Learn more about multi-label classification and using single/multiple output layers
|Complete
|March 23, 2020
|March 30, 2020
|March 25, 2020
|}

== March 16, 2020 ==
* There is no subteam meeting today as it is Spring Break

== March 9, 2020 ==

=== General Meeting Notes ===
* Today is the day of Midterm presentations
* Link to our subteam's presentation: https://docs.google.com/presentation/d/1cmuV6Awxr-s7rA8fMaNm9yOOKJuXTESOuNley5uLd_g/edit#slide=id.p[[files/Screen Shot 2020-04-26 at 11.13.27 PM.png|thumb|Main slide I presented]]
* Made sure to listen to presentations of other groups, discuss how problems they might be facing could impact us with my own group

=== Subteam Meeting/Individual Work ===
* The picture to the right is the information for the main slide I presented for the group presentation
* I talked about example learners we had in relation to our Movie Reviews Dataset
* Explained how our NNLearner that we initially had as a seed evolved
* The evolution of our NNLearner ended up spanning two generations
* Made sure to reiterate how MMLearner worked by taking in a LayerList and Data Pair and creating a neural net, train, store. and test predicitons
* Also explained that the two generations the NNLearners evolved to were not Pareto optimal as we could not run them utilizing multiple GPUs
* Our sub team made sure to clarify goals and expectations for future first semesters who were looking to join our team after this presentation
{| class="wikitable"
!'''Task'''
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Explain NNLearner examples in presentation and why they weren't Pareto optimal
|Completed
|March 9, 2020
|March 9, 2020
|March 9, 2020
|-
|Went through goals for future first semesters looking to join our team
|Completed
|March 9, 2020
|March 9, 2020
|March 9, 2020
|-
|Look into Chest X-ray, Toxicity Datasets
|Ongoing 
|March 2, 2020
|March 30, 2020
|
|}

== March 2, 2020 ==

=== Sub-Team Meeting Notes ===
* Continued to work on and add to the slides on our presentation: https://docs.google.com/presentation/d/1cmuV6Awxr-s7rA8fMaNm9yOOKJuXTESOuNley5uLd_g/edit#slide=id.p
* Decided to incorporate Chest X-ray image, toxicity datasets that we found in literature review paper into our own work with EMADE
* I was also tasked with editing a script for job submission in launchEmade.py using the flags I had found within PACE-ICE

=== Individual Work ===
* Utilizing the flags I had found for job submission on PACE-ICE from the previous week, I attempted to add on to a general script for job submission in launchEmade.py
* I edited the code for creating a script to run on the son of the Grid Engine Cluster
* I only added on the "-m abe" flag and the "-l" flag for potential memory usage of the job as the other flags for the script were already filled 
* I also tried to decipher how the other flags worked in the context of this code
*[[files/Screen Shot 2020-04-26 at 10.57.44 PM.png|thumb|Edits with new flags to launchEmade.py sons of Grid Cluster code]]I was able to see that with the "-l wall time" command, the expected wall clock time for the specific script was around 480:00:00
* I was able to see the engine_queue variable that this job was using to actually run the job on
* I added the "-l mem" flag to say how much memory the code might use and gave it a value of 2GB although I wasn't completely sure if this was the right value
* I also added the "-m abe" flag to specify how the job should send an email to notify if it has just begin executing or has terminated
{| class="wikitable"
!'''Task'''
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Continue working on midterm presentation slides
|Complete
|February 24, 2020
|March 9, 2020
|March 9, 2020
|-
|Edit Grid Engine Cluster script on launchEmade.py with new flags
|Complete
|March 2, 2020
|March 9, 2020
|March 5, 2020
|-
|Look into Chest X-Ray, Toxicity Datasets
|Ongoing
|March 2, 2020
|March 30, 2020
|
|}

== February 24, 2020 ==

=== Sub-Team Meeting Notes ===
* Started to discuss our midterm presentation, the content that was going to go into the slides, and what each person would present[[files/Screen Shot 2020-04-26 at 9.08.23 PM.png|thumb|PACE-ICE Flags and Meanings]]
* Also discussed performing hypothesis testing on our data, such as Welch T-test
* We are continuing to conduct literature review and found a paper that was similar to our current work: https://arxiv.org/pdf/1902.06827.pdf
** Utilizes an algorithm layer with CoDeepNeat that is used for evolving DNN architectures and hyperparameters
** CoDeepNeat differs from NEAT because each node in the chromosome represents a layer in a DNN instead of a neuron
** Utilizes Wikipedia and performs toxicity classification on multiple articles
** Also utilizes Chest X-ray image classification
** Showed that an evolutionary approach to optimizing deep neural networks (DNNs) is possible

=== Individual Work ===
* I was tasked with figuring out the meanings of different flags that were utilized within PACE-ICE
* I decided to look up the official documentation for PACE-ICE to see if I could find if the flags were defined in one area
* As shown in the picture on the right, I was able to find a page which had many of the common flags and their meanings listed within the documentation
* -N: Flag used for naming the job being submitted
* -I nodes:ppn: Flag used to tell the scheduler how many nodes and how many processors you want your job to run on
* -I mem: Tells scheduler how much memory the code will use
* -I walltime: Tells scheduler how much wall clock time you expect this job to take to run once
* -q paceib: Tells scheduler you want this job to run in the paceib queue using Infiniband-connected hosts
* -k oe: Tells scheduler to retain standard output and error output of the job and to place this output in home directory of the job

* -m abe: Specifying how to send an email based on the actions the job takes such as beginning execution or terminating 
{| class="wikitable"
!'''Task'''
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Start working on mid-term presentation slides and content
|Ongoing
|February 24, 2020
|March 9, 2020
|
|-
|Continue literature review for Neuroevolution
|Complete
|February 17, 2020
|March 1, 2020
|February 24, 2020
|-
|Find and understand meanings of different flags on PACE-ICE
|Complete
|February 24, 2020
|March 2, 2020
|February 26, 2020
|}

== February 17, 2020 ==

=== General Team Meeting ===
* Peer evals opened up today, need to complete by 4:00 pm February 21st
* Also need to complete notebook by 4:00 pm February 21st

=== Sub-Team Meeting Notes ===
* Sub-team still working on connecting to MySQL 

* Professor Zutty mentioned the possibility of the results of our Sub-team possibly making it into a publication
* I was assigned to start doing literature review on Neuroevolution to situate the work of our subteam with other findings
* Did individual research to learn more about Neuroevolution myself to better conduct literature review/understand what was happening in other papers

* Websites used for individual research: https://towardsdatascience.com/deep-neuroevolution-genetic-algorithms-are-a-competitive-alternative-for-training-deep-neural-822bfe3291f5, https://www.nature.com/articles/s42256-018-0006-z, https://towardsdatascience.com/neat-an-awesome-approach-to-neuroevolution-3eca5cc7930f
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete peer evals
|Complete
|February 17, 2020
|February 21, 2020
|February 21, 2020
|-
|Continue research on/literature review for Neuroevolution
|Incomplete
|February 17, 2020
|March 1, 2020
|
|-
|Get MySQL running on PACE
|Incomplete
|February 17, 2020
|March 1, 2020
|
|}

== February 10, 2020 ==
[[files/Screen Shot 2020-02-21 at 12.42.15 AM.png|thumb|Attempting to connect to MySQL database on my laptop]]

=== Sub-Team Meeting Notes ===
* Had to reclone Emade directory as I had some xml files that were missing for MySQL
* These xml files ended up being those for movie reviews
* Worked with Mohan to reinstall dependencies, also learned more about setting up a conda environment to contain a package with these dependencies
* Also worked with Mohan to try and run script that extracts data from MySQL schema

* Also did some research to learn more information about seeding in relation to primitives
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Learn more about seeding
|Incomplete
|February 10, 2020
|March 1, 2020
|
|-
|Reclone Emade with xml files/set up Conda environment
|Complete
|February 10, 2020
|February 17, 2020
|February 10, 2020
|-
|Make sure to work on filling out notebook
|Complete
|February 10, 2020
|February 21, 2020
|February 21, 2020
|}

== February 3, 2020 ==
[[files/Screen Shot 2020-02-20 at 11.47.04 PM.png|thumb|Able to get PACE-ICE to show current jobs]]

=== Sub-Team Meeting Notes ===
* Continued to attempt to run psb file on PACE
* Followed a new set of directions given by an nlp team member to run PACE that helped to notify my email that a request had been submitted
* Utilized this new script for .pbs file in this link: https://pace.gatech.edu/sites/default/files/presentation_spring2020_0.pdf
* Eventually got the pbs script to run successfully to display the jobs running at that point in time
* Still needed to work on getting MySQL integrated with PACE
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get SQL incorporated with PACE
|Incomplete
|January 13, 2020
|February 21, 2020
|
|-
|Brush up on git knowledge 
|Incomplete
|January 13, 2020
|February 21, 2020
|
|-
|Successfully run PSB script
|Complete
|January 27, 2020
|February 3, 2020
|Complete
|}

== January 27, 2020 ==

=== Sub-Team Meeting Notes ===
[[files/Screen Shot 2020-02-10 at 2.23.29 AM.png|thumb|Setting up the PBS script]]
* Attempted to run psb script on PACE

* Learned how to manipulate psb files on terminal with insert, save commands
* Also learned to vim into psb files to edit it on terminal
* Ran into some issues as I had initially mispelled the file extension name to .psb
* Ran into another error where running the psb file would not notify my email the a request had been submitted
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Figure out why psb script won't notify my email
|Complete
|January 27, 2020
|February 3, 2020
|January 31, 2020
|-
|Get SQL incorporated with PACE
|Incomplete
|January 13, 2020
|February 21, 2020
|
|-
|Brush up on git knowledge
|Incomplete
|January 13, 2020
|February 21, 2020
|
|}

== January 20, 2020 ==
* MLK holiday so non-time-conflict sub-team did not meet

== January 13, 2020 ==
[[files/Screen Shot 2020-02-10 at 1.59.48 AM.png|thumb|Was able to set up PACE]]

=== Sub-Team Meeting Notes ===
* Started working on getting PACE setup through terminal
* Went through directions given by ezcgp team
* Started by setting up SSH keys for PACE on GitHub
* Installed Anaconda on PACE and created ezcgp environment
* Team discussed problems with getting MySQL to run with PACE
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Get SQL incorporated with PACE
|Incomplete
|January 13, 2020
|February 21, 2020
|
|-
|Learn more about PACE and its uses
|Complete
|January 13, 2020
|January 20, 2020
|January 17, 2020
|}
[[files/Screen Shot 2020-02-10 at 2.02.24 AM.png|thumb|Set up SSH keys for PACE on GitHub]]

== January 10, 2020 ==

=== General Team Meeting Notes ===
* Statistics Presentation was given by Dr. Zutty
* Went over how to perform hypothesis testing
* Can use t-statistic fro hypothesis testing with population mean, standard deviation
* Can utilize python libraries for t-tests:
* Use SciPy and Test_ind
* Should utilize smaller tasks and run statistical tests on these tasks

=== Sub-Team Meeting ===
* Our team decided on utilizing PACE
* Started to set up PACE myself with ezcgp directions
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Attempt to set up PACE
|Complete
|January 10, 2020
|January 20, 2020
|January 20, 2020
|-
|Do further research on statistical hypothesis testing
|Complete
|January 10, 2020
|January 13, 2020
|January 13, 2020
|}

== December 2, 2019 ==

=== General Team Meeting Notes ===
* Our sub-team presented our results
* Made sure to fully complete slides on our presentation 

=== Sub-Team Meeting ===
* Link to Final Presentation: https://docs.google.com/presentation/d/1GSIrOEssa06AZDewUNgxEtssS3OQrKnxbpkuIrEwlXY/edit#slide=id.p
* Presented our results
* Discussed future plans for next semester 
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Present final results of sub-team
|Complete
|December 2, 2019
|December 2, 2019
|December 2, 2019
|-
|Discuss future for subteam
|Complete
|December 2, 2019
|December 2, 2019
|December 2, 2019
|}

== November 25, 2019 ==

=== General Team Meeting Notes ===
* Continue finalizing subteam presentations for December 2
* Make sure to complete peer evaluations by December 4
* All teams gave updates on their progress

=== Sub-Teams Notes ===
* Continued to attempt to fix errors with Keras and Titanic Dataset but still couldn't figure out the issues
* Talked with team members about improving accuracy with seed generation
* Discussed more applications of NLP such as with SAT scoring
* Continued to work on preparing slides for final presentation
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Peer Evaluations
|Complete
|November 18, 2019
|December 4, 2019
|December 4, 2019
|-
|Finalize Presentation
|Complete
|November 18, 2019
|December 2, 2019
|December 2, 2019
|}
[[files/Screen Shot 2019-12-04 at 4.20.44 PM.png|center|thumb|Key Value error using Titanic Dataset and Keras that I still need to fix]]

== November 18, 2019 ==

=== General Team Meeting Notes ===
* Hackathon is on Saturday November 23, 2019
* Keep working on Notebooks
* Keep working with sub-teams to prepare for final presentation

=== Sub-Team Meeting Notes - November 22, 2019 ===
* Continue analyzing new datasets, to help categorize articles
* Also started on creating presentation slides for final presentation
* Continued playing around with Keras code, changing activation, loss functions
[[files/Screen Shot 2019-12-04 at 3.08.57 PM.png|center|thumb|Importing Proper modules for Keras]]
[[files/Screen Shot 2019-12-04 at 3.11.13 PM.png|center|thumb|Attempting to utilize Keras for my Titanic dataset]]
* Ran into some problems with Keras as my X_test and X_train variables weren't being properly recognized
* Also needed to figure out how exactly to get accuracy results from my dataset
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Figure out errors with titanic dataset
|Ongoing
|November 18, 2019
|December 2, 2019
|
|-
|Finalize Presentation for subteam
|Complete
|November 18, 2019
|December 2, 2019
|December 2, 2019
|-
|Work on Peer Evaluations
|Complete
|November 18, 2019
|December 4, 2019
|December 4, 2019
|}

== November 11, 2019 ==

=== General Team Meeting Notes ===
* Make sure to keep working with subteam to get prepared for final presentation on December 2

=== Sub-Team Meeting - November 15, 2019 ===
* Looked at image, news datasets
* Worked on trying to generate a seed for EMADE from the news dataset
* Ran into problems with the news dataset
* News article takes location into account as it only takes the first line after the article title
* Can't use location to sort the article
* Also played around with Keras/Tensorflow code sent over the slack
* Thought about ways to utilize Keras for my own titanic Dataset
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look over/play with Keras code sent on slack
|Complete
|November 11, 2019
|November 18, 2019
|November 16, 2019
|-
|Look into news dataset, how to classify articles
|Complete
|November 11, 2019
|November 18, 2019
|November 16, 2019
|}
Article on Keras Documentation: https://keras.io
* Learned about Sequential model of Keras to help organize layers
* Learned about how to feed data to the model itself
* Learned about how to obtain predictions/accuracy results utilizing Keras

== November 4, 2019 ==

=== General Team Meeting Notes ===
* Each sub team group presented the progress they had made 

=== Sub-Team Meeting Notes - November 8, 2019 ===
* Needed to ensure EMADE was properly installed
* Worked on obtaining database connection w/ subteam members
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look up documentation/code for Keras
|Complete
|November 4, 2019
|November 11, 2019
|November 11, 2019
|-
|Learn more about Neural Nets
|Complete
|November 4, 2019
|November 11, 2019
|November 9, 2019
|-
|Ensure that I could connect with database of team members
|Complete
|November 4, 2019
|November 11, 2019
|November 4, 2019
|}
[[files/Screen Shot 2019-12-04 at 1.46.56 PM.png|center|thumb|Article on neural nets to learn more]]
Article Link: https://skymind.ai/wiki/neural-network
* Learned how neural nets worked as layers utilizing clustering for classification
* Help find similarities between data points to group and cluster them
* Layers are made of nodes where the actual computation occurs

== October 28, 2019 ==

=== General Team Meeting Notes ===
* Decided what teams we are going to be on for next semesters
* Decided to be on the NLP sub team dealing with neural networks

=== Sub-Team Meeting Notes ===
* Needed to install a different version of EMADE for subteam
* Also needed to note that sub-teams now also meet on Fridays
{| class="wikitable"
|'''Task'''
|'''Current Status'''
|'''Date Assigned'''
|'''Suspense Date'''
|'''Date Resolved'''
|-
|Install EMADE for NLP Subteam
|Complete
|October 28, 2019
|November 4, 2019
|November 3, 2019
|-
|Learn more about Keras/Tensorflow
|Complete
|October 28, 2019
|November 4, 2019
|November 2, 2019
|}

=== Installing EMADE ===
* Ran into some initial problems in attempting to clone repository as it failed to clone halfway through
* Eventually ended up working as I had to delete the old EMADE repository that I had cloned for previous use in the VIP

== October 21, 2019 ==

=== General Team Meeting Notes ===
* Presented our results from the Titanic Dataset using MySQL and EMADE
* Also compared our results with the the results of the previous two datasets
* Also learned about ADF, ezCGP, NLP

=== Sub-Team Notes ===
* Github Repository: Â https://github.gatech.edu/bsu32/emadeGroup
* Presentation Link: Â https://docs.google.com/presentation/d/1s0Jzx6FY2tZJA3XuXwQ89gayCjDl8HlSzuksCfltXd4/edit#slide=id.g6408953ec7_1_0

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Think about potential sub teams to join for future
|Complete
|October 21, 2019
|October 28, 2019
|October 28, 2019
|-
|Present EMADE results/comparisons
|Complete
|October 21, 2019
|October 21, 2019
|October 21, 2019
|}
[[files/Screen Shot 2019-12-01 at 10.21.31 PM.png|center|thumb|3-D Pareto Front Graph]]
[[files/Screen Shot 2019-12-01 at 10.23.36 PM.png|center|thumb|Final Area Under the Curve Graph]]
[[files/Screen Shot 2019-12-01 at 10.27.05 PM.png|center|thumb|Comparison of the Three Approaches]]

=== Conclusions: ===
* Our final area under the curve graph happened to be smaller than with the other two approcahes
* This decrease in area could be related to the more complex primitives utilized using EMADE, reducing the false accuracy rates of our datatset
* We could have considered running the data using more generations to perhaps obtain more accurate results

== October 16, 2019 ==

=== General Team Meeting Notes ===
* Continue developing solution to Titanic dataset utilizing MySQL and EMADE
* Make sure to get ready for presentation on October 21

=== Sub-Team Notes ===
* Still working on connecting MySQL server with masters/workers
* Predicting that EMADE should give a lower area under the curve in using more complex primitives

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finish setting up masters/workers with MySQL
|Complete
|October 9, 2019
|October 21, 2019
|October 17, 2019
|-
|Utilize EMADE and MySQL on Titanic dataset
|Complete
|October 16, 2019
|October 21, 2019
|October 18, 2019
|-
|Prepare final presentation for October 21, 2019
|Complete
|October 16, 2019
|October 21, 2019
|October 21, 2019
|}
[[files/Screen Shot 2019-12-01 at 9.47.55 PM.png|thumb|[[MySQL]] working with master/workers|center]]

=== MySQL Progress: ===
* Initially ran into some errors with setting up MySQL and connection kept failing as we forgot to change the xml file with the host server
* Eventually resolved the issue by editing the XML file

== October 9, 2019 ==

=== General Team Meeting Notes ===
* Had an introduction to EMADE
* Discussed how to install EMADE and the purpose of it
* Also had an introduction to MySQL
* Learned how to create MySQL connection amongst group members for usage in obtaining/creating data for genetic algorithms
* Also learned about datasets for EMADE:
** data is in zipped csv files
** cross-folded 5 times w/ 5 Monte Carlo trials
** Parameters such as elite pool size, population size, launch size affect the evolutionary process
** Each row is an instance, each column is a feature, final columns contains the truth data

* Main goals:
** weight - minimized (-1.0) or maximized (1.0)
** evaluation function specifies name of a method in 
** <workersPerHost> says how many evaluations to run in parallel
** scr/GPFramework contains the important portion of code

=== Sub-Team Notes ===
* Planned out how to Run EMADE in our group
* Our team leader would set up the SQL server as the master process
* Rest of the team needed to connect as workers
* Needed to create a non-dominated frontier plot run over multiple generations and compare with previous two assignments

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up MySQL and MySQL Workbench
|Complete
|October 9, 2019
|October 16, 2019
|October 11, 2019
|-
|Install EMADE
|Complete
|October 9, 2019
|October 16, 2019
|October 11, 2019
|-
|Set up SQL Server with group
|Complete
|October 9, 2019
|October 21, 2019
|October 17, 2019
|}

== October 2, 2019 ==

=== General Team Meeting Notes ===
* Peer Evaluations due Friday 4pm
* Notebooks due Friday 11:59pm
* Sub-Teams presented their GP Titanic Project Results

=== Sub-Team Meeting ===
* Github Link:  https://github.gatech.edu/bsu32/emadeGroup
* Presentation Link: [https://docs.google.com/presentation/d/e/2PACX-1vSL90uzHtPocLyegASRUGtKyhA5YwrN4W7gR06LEOOYAToNM1FjToTjF0ywVq-2xuU6sJTzKeFalIC3/pub?start=false&loop=false&delayms=3000&slide=id.g6039c91478_0_1 https://docs.google.com/presentation/d/e/2PACX-1vQKLpnDdAeP3WDqky0OND8uzZ0FC1F1v6e4UnxV6tYPKySzyYRBEtKXQuJ6MH6wnWxYWCphIKWgivXk/pub?start=false&loop=false&delayms=3000]
* Group presented our results

=== Sub-Team Notes ===
* Presented our results and took not that Tournament selection is not a good choice for this project
* It was a good idea to switch to NSGA2 for this assignment

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Peer Eval
|Completed
|September 25, 2019
|October 4, 2019
|October 2, 2019
|-
|Make sure to finish notebook
|Completed
|October 2, 2019
|October 4, 2019
|October 4, 2019
|-
|Install EMADE
|Completed
|October 2, 2019
|October 9, 2019
|October 4, 2019
|}
* Should come up with our own evolutionary algorithm next time
* Know when Tournament selection is suitable and not suitable for future assignments
* Utilize more logicals such as greater than, less than

== September 25, 2019 ==

=== General Team Meeting Notes ===
[[files/Screen Shot 2019-10-04 at 10.32.56 PM.png|thumb|My contribution in creating six new features using combinations of sex/PClass]]
* Peer Evals are open and are due by next Friday 4pm
* Utilizing genetic programming/multi-objective optimization to solve Titanic Problem
* Find area under curve of our FPR/FNR graph
* Also want to make sure that we minimize the FPR/FNR rates by minimizing the area under the curve of our graph
* Also should utilize primitives for this project
* Also should utilize an evolutionary loop for the individuals to evolve after each generation[[files/Evolutionalgo.png|thumb|Evolutionary Algorithm Utilized]]
* Utilize Reimann Sums for area under the curve

=== Sub-Team Meeting ===
* Split the work for the GP Titanic Dataset problem by group member
* Myself: Create six new features from our existing Class/Sex feature set to add vectorization to our dataset
* Tushna/Nesha: Work on Reimann Sums for area under the curve[[files/Algorithmresults.png|thumb|Results of our Evolutionary Algorithm by Generation]]
* Tusheet/Brandon: GP/Evolutionary Algorithm coding
* Utilized deep.tools.seINSGA2t
* Also decided to use the primitives of add, subtract, multiply, and, or, and not
* Github Link:  https://github.gatech.edu/bsu32/emadeGroup
* Presentation Link: [https://docs.google.com/presentation/d/e/2PACX-1vQKLpnDdAeP3WDqky0OND8uzZ0FC1F1v6e4UnxV6tYPKySzyYRBEtKXQuJ6MH6wnWxYWCphIKWgivXk/pub?start=false&loop=false&delayms=3000][https://docs.google.com/presentation/d/e/2PACX-1vSL90uzHtPocLyegASRUGtKyhA5YwrN4W7gR06LEOOYAToNM1FjToTjF0ywVq-2xuU6sJTzKeFalIC3/pub?start=false&loop=false&delayms=3000&slide=id.g6039c91478_0_1 https://docs.google.com/presentation/d/e/2PACX-1vQKLpnDdAeP3WDqky0OND8uzZ0FC1F1v6e4UnxV6tYPKySzyYRBEtKXQuJ6MH6wnWxYWCphIKWgivXk/pub?start=false&loop=false&delayms=3000]

=== Sub-Team Notes ===
* Needed to create six new features from Class/Sex
* Decided to use class/sex for each of the six possible combinations (Ex. first-class male, third-class male)
* I would then utilize a 0/1 to see if each individual fell into the feature, 0 if they did not have the feature and 1 if they did have the feature[[files/Pareto2.png|thumb|Final Pareto Front Graph]]
[[files/AUC graph.png|thumb|Area Under the Curve Graph]]

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create six new features from class/sex feature
|Completed
|September 25, 2019
|October 2, 2019
|September 30, 2019
|-
|Finish Peer Evaluation
|Completed
|September 25, 2019
|October 4, 2019
|October 2, 2019
|-
|Help Teammates w/ other parts such as area under curve/genetic programming
|Completed
|September 25, 2019
|October 2, 2019
|October 1, 2019
|}

=== Results ===
* Area under our curve ended up being around 0.130
* Initially had some troubles getting area under the curve as our individual fitness values turned out empty in the code
* Changing to seINSIGA2t eventually helped create our area under the curve
* Utilized 50 generations of evolution for our problem

== September 18, 2019 ==

=== General Team Meeting Notes ===
* Presented our Findings for the Titanic Problem we were assigned the week prior

* Also took note of the other results obtained from the other groups that presented

=== Sub-Team Meeting ===
* Github: https://github.gatech.edu/bsu32/emadeGroup

* Presentation: https://docs.google.com/presentation/d/e/2PACX-1vSL90uzHtPocLyegASRUGtKyhA5YwrN4W7gR06LEOOYAToNM1FjToTjF0ywVq-2xuU6sJTzKeFalIC3/pub?start=false&loop=false&delayms=3000&slide=id.g6039c91478_0_1

=== Sub-Team Notes ===
* The fact that we already knew of statistical information of deaths based on class/sex beforehand was in a form "cheating"
* We were only supposed to utilize the data given in the dataset for our co-dominant solution, or else we would overtrain our data

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Figure out how to not overtrain data for next projects
|Completed
|September 18, 2019
|September 25, 2019
|September 19, 2019
|-
|Learn more about Vectorization
|Completed
|September 18, 2019
|September 25, 2019
|September 20, 2019
|-
|Learn more about ML Algorithms
|Completed
|September 18, 2019
|September 25, 2019
|September 21, 2019
|}
* Our team decided to add vectorization to future projects we were to be assigned 
* Would involve splitting up more of our feature sets into multiple features that would be assigned booleans or 0/1 values
* Also decided to not utilize statistical data of the Titanic Data found online to avoid overtraining data

== September 11, 2019 ==

=== General Meeting Notes ===
* Will be split into groups to work on Kaggle Titanic Data Set[[files/Features1.png|thumb|Features We Chose to Keep]]
* Should learn more about Scikit learn/different ML algorithms
* Subteam: [[Bootcamp Sub-team Fall 2019]] group 2

=== Sub-Team Meeting  ===
* Split up into groups in order to work on Titanic dataset
* Group consisted of myself, Brandon, Tushna, Nesha, and Tusheet
* Objective was to find factors that when combined would contribute the most correlation to survival rates on the Titanic
* Choose five algorithms to be codominant
* Our graph would consist of our calculated false positive/false negative rates for our co-dominant solution[[files/Feature Table1.png|alt=|thumb|Table of our Feature Set]]
*[[files/Pareto(1).png|thumb|Final Graph of Co-Dominant Solution]]Also will be utilizing Scikit learn for the project

=== Sub-Team Notes ===
* Decided to keep the two main features of class/sex
* Our main reasoning behind keeping class was that people of higher classes were often higher up in the boat/easier to reach when evacuating
* Our main reasoning for choosing sex as a feature set was because men often exhibited heroism/evacuated women and children before leading to higher death rates for males
* Github Link: https://github.gatech.edu/bsu32/emadeGroup
* Presentation Link: https://docs.google.com/presentation/d/e/2PACX-1vSL90uzHtPocLyegASRUGtKyhA5YwrN4W7gR06LEOOYAToNM1FjToTjF0ywVq-2xuU6sJTzKeFalIC3/pub?start=false&loop=false&delayms=3000&slide=id.g6039c91478_0_1

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Titanic Kaggle Problem
|Completed
|September 11, 2019
|September 18, 2019
|September 13, 2019
|-
|Learn More about Scikit Learn
|Completed
|September 11, 2019
|September 18, 2019
|September 12, 2019
|-
|Choose Algorithm for Project
|Completed
|September 11, 2019
|September 18, 2019
|September 13, 2019
|}

=== Results ===
* We ended up utilizing the feature set of class, sex, SibSP, Parch, and Age
* We also ended up combining Class/Sex into a feature to give higher numbers to class one vs. 3 and a different number for males and females
* ML Algorithms used: K Neighbors, Neural Networks, Random Forest, Linear Regression, Guassian NB, Decision Tree
* Our highest score ended up being around .84
* Ended up creating a graph of false positive/false negative rates with our co-dominant solution
* I also created most of the presentation for our results as one of my tasks

== September 4, 2019 ==

=== Team Meeting 3 Notes ===
* Optimal Algorithms:
** Space Efficiency
** Minimize Errors (Misclassification)
** Minimize False Positives
** Time Efficiency
** Security
** Precision of Results
** Ease of Usability
** Cost Effectiveness
* Classification Measures
** Build Classifier-build algorithm to predict attribute from a dataset
** Can build other characteristics off of determined attributes from a dataset
*** Ex: Sensitivity, true positive rate
** Maximize true positive/negative[[files/Pareto Front Lab 2.png|thumb|Co-dominant frontier w/ area under curve minimized]]
** Minimize false positive/negative
** Accuracy: (TP + TN / (P + C)) 
** Genotype: DNA (List created), Phenotype (Fitness level, measures of how optimal it was)

=== Sub-Team Meeting Notes ===
* N/A

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete DEAP Lab Part 2
|Completed
|September 4, 2019
|September 11, 2019
|September 9, 2019
|-
|Review Class Notes
|Completed
|September 4, 2019
|September 11, 2019
|September 5, 2019
|}
* Lab 2 Part 2 involved Multi-Objective Genetic Programming
* The main goal was to create a set of co-dominant individuals and minimize the area under the curve of these set of individuals
* This goal would be accomplished by minimizing the mean-squared error and the size of the tree
* Pareto dominance function will return true if the first individual dominates the second
* Population sorted by Pareto-dominance in comparison to the separate individual
* Size of tree-size ends up rising over time, mean squared error quickly drops between 0 and 1

== August 28, 2019 ==

=== Team Meeting 2 Notes ===
* Genome programming: individuals are the functions themselves
* Data is inputted into this individual's genome itself to produce output
* Utilizes individuals in the form of trees
** These trees contain two parts, nodes: contain functions, and leaves: contain parameters acted on by the functions
* Trees are represented by lisp pre-ordered parse tree
** Ex. [+, *, 3, 4, 1] = (3 * 4) + 1
* Crossover: Pick a point in each tree, the tree below is a subtree, exchange the two subtrees between each tree
* Mutations
** Deleting a node
** Inserting a node
** Changing a node
* Symbolic Regression: using genetic programming to evolve a solution to y = sin(x)
* Involves genetic programming, utilizing primitives
* Evaluating a tree: Feed a number of inputs into the tree to obtain an output, can measure sum of squared error between output and actual points in the function
* Primitives to make evolution easier:
** Power()
** Factorial()
** Sin()
** Cos()
** Tan()

=== Sub-Team Meeting Notes ===
* No sub-team meeting

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete DEAP Lab 2 Part 1
|Completed
|August 28, 2019
|September 4, 2019
|September 3, 2019
|-
|Review Day 2 Class Notes 
|Completed
|August 28, 2019
|September 4, 2019
|September 1, 2019
|-
|Do Further Research About Genetic Programming
|Completed
|August 28, 2019
|September 4, 2019
|September 2, 2019
|}
* Lab 2 Part 1 was on the topic of using genetic programming to create the most optimal function (tree with nodes and leaves)
* Utilized in symbolic regression to approximate a function
* The objective was to minimize the squared error between the optimized function created and the actual function[[files/Screen Shot 2019-09-04 at 12.41.10 AM.png|thumb|Creating a New Mutation Method]]
* As generations passed, the squared error between the function being generated and the actual function started to decrease
[[files/Screen Shot 2019-09-04 at 12.36.27 AM.png|thumb|Creating Two New Primitives]]
* This decrease in the error had to do with mutations of different trees and mating with other trees in order to optimize the function

* Also created two new primitives for the trees, and a new mutation method between tress to see how results were affected

== August 21, 2019 ==

=== Team Meeting 1 Notes ===
* Went over general description of 10 week class, content that will be covered about genetic algorithms
* Learned about installing software such as git, anaconda3, Jupiter notebook and DEAP
* Learned about the One Max Problem

* Genetic algorithms - creates new generations w/ mating/mutations in the previous population

* Individual - one candidate
* Population - set of individuals
* Objective - value to characterize individuals that you might maximize or minimize
* Fitness - relative comparison to other individuals within the population[[files/Crossover-single-point-Source-43.png|thumb|Single Point Crossover]]
* Selection - gives preference for fitter individuals, more likely to mate (fitness proportionate)
*[[files/Multi point crossover.png|thumb|Double Point Crossover]]Tournament - winners selected for mating by pairing individuals and selecting by best fitness level
*Types of mutations: Single Point, Double Point
*Algorithm - find solution for best individual

=== Sub-Team Meeting Notes ===
* No sub-team meeting

=== Action Items ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install Git, Anaconda3, Jupiter Notebook
|Completed
|August 21, 2019
|August 28, 2019
|August 27, 2019
|-
|Go Over Class Syllabus/Learning Objectives
|Completed
|August 21, 2019
|August 28, 2019
|August 21, 2019
|-
|Complete Lab 1 Parts 1 and 2
|Completed
|August 21, 2019
|August 28, 2019
|August 27, 2019
|}
* Software, such as Git, Anaconda3, Jupiter Notebook, successfully installed for VIP without error
* Ran through code for Lab 1 to demonstrate One Max Problem, observed how fitness increased from generation to generation
* Learned more about random mating, how these can lead to fitter individuals from generation to generation, and how tournaments also select for more fit individuals from pairings
* Learned about how genetic algorithms can be used within the N-Queens problem
* Fitness for N-queens was determined by interactions between queens
* Therefore, the "objective" was to minimize the number of interactions between queens
* Fitness evaluated w/ number of conflicts and was now going to be minimized in this situation