== Team Member ==
Team Member: Sean Seungil Kim

Email: ksi0120@gatech.edu

Cell Phone: 404-483-4341

Interests: Machine Learning, Soccer, Golf, Car

== January 7th 2019 ==
'''Lecture Notes:'''
* Class meeting room changed: COC room 16
* Went over the first ppt: Generic Algorithm
** Keywords: Individual, Population, Objective, Selection, Fitness, Evaluation, Mate/Crossover, Mutation, Gene, Genome
** Lab due next week. (Refer to the walkthrough for help)
** Install DEAP Library

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Due Date
!Date Completed
!Remarks
|-
|Wiki Notebook
|Completed
|1/7/2019
|N/A
|1/8/2019
|
|-
|Lab 1 Due
|Completed
|1/7/2019
|1/17/2019
|1/13/2019
|
|-
|
|
|
|
|
|
|}
== January 13th 019 ==

'''Lab1 Notes'''
* creator.create = use the fuctions to define the objective, class, and inheritied class.
** creator.created("fitness" => takes tuple, (1.0) to maximize, (-1.0) to minimize for a single objectitve problem. Multi-objective problems take a full tuple, but we haven't touched them yet.
** creator.create(class, in our examples, the classes were "list" objects)
* base.Toolbox = toolbox functions withinthe Deap library. we register the tools needed as necessary.
** toolbox.register = define the individuals and population, and their characteristics.
* Define functions to "evaluate" the individuals.

*N Queens Problem
** Tweaked Mutation Method = Weave Method, changes the spot of the ith queen with the len(individual) - i -1'th queen. => Carried testing a few times, did not seem more effective than the original mutation method.
** Revised function : 

    def mutShuffleIndexesB(individual, indpb):
    tempHolder = 0
    size = len(individual)
    halfSize = round(size/2)
    for i in range(halfSize):
        if random.random() < indpb:
            swap_indx = size -i-1
            individual[i], individual[swap_indx] = \
                individual[swap_indx], individual[i]
    
    return individual,

** Tried to change the parameters ( the probabilities of mutation and mating occurring ). Based on the observation, lower probability of mating and a higher probability of mutation yielded the best result.
*** Refer to the data collection GitHub file for testing methods and results. https://github.com/ksi0120/EMADE/blob/master/Lab%201%20Experiment

==January 14th 2019==
'''Meeting Notes'''

Refer to the PDF to learn more about GP
GP: Genetic Programming

The genome for genetic programming is in tree form.

The output is at the top of the tree 

''Nodes & Leaves'' 

Leaves(Terminal) are the end of the tree = > Inputs

Nodes ( Primitives) connections + - * % 
"Leaves" are the convention used in DEAP

GA => feed the genes to the evaluation function , GP => Genes are the functions

1 + (3*4) = [+1*34] 

Operator first
(0+1)-2  

Arity = # of inputs 

Mating & Crossovers for GP

Mating is limited to the single point mating due to the # of terminals to the nodes

Mutation
Change the operator ( * to % from the above example)

Symbolic Regression
y = sin(x)

primitive set : +,-,*,%

terminals: x x, constants
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Due Date
!Date Completed
!Remarks
|-
|<blockquote>Lab 2</blockquote>
|Completed
|1/14/2019
|1/28/2019
|1/27/2018
|
|}

== January 21st 2019 ==
Lab 2 Notes

creator.create("FitnessMin", base.Fitness, weights=(-1.0,))  => The weight is -1.0,0. Single objective minimization.

creator.create("Individual", '''gp.PrimitiveTree''', fitness=creator.FitnessMin) = > Contrary to the first lab, our individual inherits "gp.Primitive" class.
 pset.addPrimitive(np.power, arity=2)
 pset.addPrimitive(np.reciprocal, arity=1) *added primitives
* Update: above primitives do not work. Reciprocal throws error. I will check later to see if arity is the issue. Otherwise, will replace the reciprocal primitive with a different one

== January 27th 2019 ==
Lab 2 Notes -> Continued

Changed the primitives to absolute and square to prevent the errors coming from having 0 in the individuals.
 pset.addPrimitive(np.absolute, arity=1)
 pset.addPrimitive(np.square, arity=1) *added primitives

 Best individual is negative(subtract(negative(x), absolute(multiply(square(x), add(x, add(x, absolute(x))))))), (0.03982102818504672,)
The best individual from the first lab from lab 2, mutation as given
 -- Generation 19 --
   Min 0.10535653753074872
   Max 858700.7567784111
   Avg 2862.946238663777
   Std 49494.37852241202
When the depth was changed to min = 2, max = 5, above happened. I think the culprit is a combination of deeper depth and series of squares and multiplications..
 .-- Generation 39 --
   Min 1.1608501979530989e-16
   Max 774.4638744159088
   Avg 2.870517027517318
   Std 44.624298013740905
min=5 max =8. Note Higher Std and larger max by the end of the generation.
 -- Generation 39 --
   Min 1.2012964454916326e-16
   Max 4.335339510196316
   Avg 0.31462283276565467
   Std 0.49421317880746185
 -- End of (successful) evolution --
min=7 max=8. Note the SD has gone down significantly as well as the max.

When the gap between the minimum and maximum tree depth is larger, the std deviation and maximum of the population went up.

Added mutation method was
 toolbox.register("nrmutate", gp.mutNodeReplacement, pset=pset)
Refer to https://github.com/ksi0120/EMADE/blob/master/Lab%202%20Experiment.txt for the result. In conclusion, the added mutation performed better. The NR mutation yielded lower max, min, and mean values.

== February 4th 2019 ==
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Due Date
!Date Completed
!Remarks
|-
|Read the lecture ppt
|Completed
|N/A
|N/A
|2/3/2019
|*Review once more 
|-
|Lab 2 Part 2
|Complete
|1/28/2019
|2/4/2019
|2/4/2019
|*Review once more
|}
Notes from the lecture ppt

Gene Pool: Set of genome to be evaluated during the current generation.

<nowiki>*</nowiki>Difference from GA to GP is that GP is based on tree structure whereas GA is a set of values.

Confusion Matrix
{| class="wikitable"
!
!Predicted : Positive
!Predicted: Negative
|-
|Actual Positive
|True Positive
|False Negative
|-
|Actual Negative
|False Positive
|True Negative
|}
For each classfication, there are measurement formulas, 
* TPR -> hit rate = TP/P
* TNR -> True negative rate -> TN/N
* FNR -> False Negative Rate = 1-TPR or FN/P
* FPR -> Fall Out(False Positive Rate) FP/N or 1-TNR
Objective Space: Based on the evaluation method, the objective scores give each individual a point in the space(phenotype).

An individual is Pareto if there is no other individual in the population that outperforms the individual on all objectives.

The set of all Pareto individual is known as the Pareto frontier.

Lab notes
[[files/Current.png|center|thumb]]
 Area Under Curve: 2.3841416372199005
Initial Pareto Plot was above.

The first approach taken was chaning the Crossover and Mutation probabilities to see if they will change the AUC.

Result,
[[files/Mut changed.png|center|thumb|Cx changed to .7 Mut kept same]]
 When the CXPB was changed to .7, and the MPB was kept same, the AUC went up to 4.043603021693458.
When the MPB was changed to .5 and CXPB was kept same at .5, the AUC went even further up to 4.3661585732434585

Changed the approach and tried to lower them.
[[files/.2.2.png|center|thumb|CXPB .2 MPB .2]]
Wheh CXPB was lowered to .2 and MPB was kept the same at .2, The AUC went down to 2.3626560217571373, .9% decrease. 

However, when CXPB was kept at .5 and MPB was lowered to .1,  The AUC increased to 3.8188580584574003. 

Following the pattern, I tried to change the CXPB to .2 and MPB to .3(as the AUC increased when MPB was lowered, I tried to see if increasing the MPB will lower the AUC), but AUC increased to 3.422359389352658.

Lowered CXPB even further to .1 while keeping the MPB same at .2. The AUC went further down to 2.336270966753453.

Seems like when crossover occurs with a lower prob, the AUC is going down. I tried the extreme case to lower the CXPB to .01. As result, I got 2.338791327080944 as AUC. It did go down a bit, but the goal is 1.788106227914925375. 

FIgured that altering with the crossover and mutation probability won't get me far, so I figured that changing the mutation method might help as when the mutation probability was going up, the AUC went up. 

Applied the node mutation I found, and AUC went down significantly.
 def evalSymbReg(individual, points, pset):
     func = gp.compile(expr=individual, pset=pset)
     sqerrors = (func(points)-(np.negative(points) + np.sin(points**2) + np.tan(points**3) - np.cos(points)))**2
 
     return (np.sqrt(np.sum(sqerrors) / len(points)), len(individual))
 
 
 toolbox.register("evaluate", evalSymbReg, points=np.linspace(-1, 1, 1000), pset=pset)
 toolbox.register("select", tools.selTournament, tournsize=3)
 toolbox.register("mate", gp.cxOnePoint)
 toolbox.register("expr_mut", gp.genFull, min_=0, max_=2)
 toolbox.register("mutate", gp.mutNodeReplacement, pset=pset)
 
 toolbox.decorate("mate", gp.staticLimit(key=operator.attrgetter("height"), max_value=17))
 toolbox.decorate("mutate", gp.staticLimit(key=operator.attrgetter("height"), max_value=17))
Above shows the new mutation method applied.
[[files/Newmut.png|center|thumb|
 Area Under Curve: 1.1592588179901462
]]

The new mutation method decreased the AUC to 48.6% of the previous mutation method. CXPB and MUPB were both 0.2.

== February 9th 2019 ==
Titanic Project Notes

https://github.com/ksi0120/EMADE/blob/master/Titanic%20Machine%20Learning.ipynb

Comments are made on the notebook. 

Summary: 

After importing the csv files, I identified the columns with nan rows as those columns cannot be used for prediction. There were 3 columns with null values: Age, Fare, and Cabin.

Before starting to impute the nan rows, Passenger Id was changed as row index due to 2 reasons: Name is hard to identify and passenger ID was the identifier used for the submission file.

Then, I looked into the columns without null values. PClass, Sex, SibSp, and Parch all had somewhat correlation to the Survived data, so I kept them in the data frames.

Afterward, I concatenated the test and train data frames to see what columns had how many null values. out of total 1309 rows, the Cabin column only had 295 non-null rows. As trying to fill in the null rows with such a limited amount of data would be trivial, I dropped the Cabin column.

Once Cabin column was gone, I planned on using FancyImpute library to impute the missing values in the data frame. First, I mapped the strings in Embarked and Sex to integers. Then, the ticket column was dropped as it is hard to analyze a random combination of alphanumeric values. Then using the Fancy Impute "Iterative Imputer" function (MICE), I filled in the null values.

I had all the data ready, so I started to analyze. However, the first classifier, Random Forest, gave too good of a score, .9898, so after trying linear SVC and verifying that linear SVC only gave .7901, I tabulated the confusion matrix values.

Confusion Matrix [[190   1] [  2 102]] 

True Negatives 190 False Positives 1 False Negatives 2True Positives 102 

Above is the value I received. 

And using RandomForest Classifier, I obtained the predicted survived data from test csv file. From Kaggle submission, I received a score of .73.

I believe the imputation might have over-adapted to Train data. If I work on this further to identify a few more methods to improve the score, obtaining a higher Kaggle score might be possible. However, as we have to work on producing the codominance pareto graph, I will stop here for now.
{| class="wikitable"
|-
|Titanic ML
|Complete
|2/4/2019
|2/11/2019
|2/9/2019
|
|}

== February 15th 2019 ==
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Due Date
!Date Completed
!Remarks
|-
|Titanic GP
|Completed
|2/11/2019
|2/18/2019
|2/15/2019
|
|}

Titanic: Genetic Programming

Attempted to solve the GP using weakly typed GP

https://github.com/ksi0120/EMADE/blob/master/TitanicGP.ipynb

Refer to the github file above for the progress.
[[files/Weakly typed gp.jpg|center|thumb]]
The result was not as good as the ML approach result. Talked to James during helpdesk, and I believe it is due to lack of primitives.

The team result was posted using Agrawal's version, strongly typed. The result was about the same, but his minimum false positives and false negatives were lesser than my version.
== Feb 24th 2019 ==
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Due Date
!Date Completed
!Remarks
|-
|Install EMADE
|Completed
|2/18/2019
|Feb 25
|2/24/2019
|Test run not completed yet
|}

Emade installed

Haven't done a test run yet, but all files are downloaded, ready to be run.

== Mar 1st 2019 ==
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Due Date
!Date Completed
!Remarks
|-
|Test Run of Emade
|Completed
|N/A
|N/A
|3/1/2019
|
|-
|Emade Titanic
|In progress
|2/24/2019
|3/4/2019
|N/A
|One member had to travel
|}

Gathered as team,  but ended up going to the help desk.

<u>My EMADE(Windows Environment) was throwing errors.</u>

-> Turned out to be an error coming from EMADE code, import subprocess was missing. James updated the git.

-> Another issue came from not properly installing Anaconda. If anyone is having the same issue, if you came from Miniconda to Anaconda, a fresh reinstall helps. Some of my environmental variables were missing, but the code was running fine. Symptom : The code runs (at least looks like it runs fine) but the error and logs are empty. 

Initial plan was to have Sruthi as the master, but Argrawal and I could not ping her even though her firewall was off and we were on the same network(Eduroam)

Changed the plan and Agrawal is now the master. Both Sruthi and I were able to ping him and get a response.

Everything is not set up.

Our strategy is to use two variates of preprocessed data. One is mine and the other is Agrawal's. 

Mine performed better when we did it as ML in terms of FP/FN , but Kaggle score was low. Obviously, it over - adapted to the train data. On the other hand, Agrawal's FP/FN were not as low as mine, but his Kaggle score was relatively better than mine.  As our goal is to get the lowest FP/FN numbers, we will try both.

Here is step-by-step actions I took to pre-process the data using fancyimpute library
# Drop "'Ticket','Cabin','SibSp'" columns.
# Declare Iterative Imputer, solver = IterativeImputer(n_iter=5, sample_posterior=True, random_state=1)
# Impute using the imputer .TrainX= solver.fit_transform(trainImpute)
# Then use the excerpt from titanic splitter file to fold the dataset 5 times.

== Mar 7th 2019 ==
Tried to connect to Master, but VPN won't work.

Symptoms

You can ping the master using ping ip command on Command Prompt, however you can't neither on SQL nor using Emade.

== Mar 25 2019 ==
Team assigned to Stocks.

First meeting Thursday 7:30 at Culc.

== Mar 29 2019 ==
First meeting:

James joined the meeting to walk us through the Emade.

Current Team Strategy:

3 statsmodel are implemented: Sarimax, Varmax, Var.

Goal:

Implement RNN / Statsmodel to give more primitive options to Emade
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Due Date
!Date Completed
!Remarks
|-
|Understand the Model
|In Progress
|N/A
|N/A
|N/A
|Read over the Emade Code
|-
|Study possible statsmodel / RNN
|In progress
|N/A
|N/A
|N/A
|Yoonwoo provided an article to read
|}

== Apr 1st 2019 ==
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Due Date
!Date Completed
!Remarks
|-
|Understand the Model
|In Progress
|N/A
|N/A
|Can't be done.. Emade is too complicated
|Read over the Emade Code
|-
|Decide on a Statsmodel to implement
|In progress
|Apr 1 2019
|Apr 5 
|Changed
|Yoonwoo provided an article to read
|}
Monday class. 

James again walked us through how the primitives are implemented on the signal_methods file.

The team was separated into two teams: pre-processing and ML primitive implementation.

Will be looking at the article : https://machinelearningmastery.com/findings-comparing-classical-and-machine-learning-methods-for-time-series-forecasting/

to implement additional ML model as a primitive.

== Apr 5th 2019 ==
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Due Date
!Date Completed
!Remarks
|-
|Understand the Model
|Finished
|N/A
|N/A
|N/A
|Can't be done in the next 2 weeks. Too much to learn
|-
|Decide on a Statsmodel to implement
|In progress
|Apr 1 2019
|Apr 5 
|Changed
|Yoonwoo provided an article to read
|-
|Learn LSTM
|In progress
|Apr 5
|Apr 12
|N/A
|Study LSTM
|}
Upon reading a research paper on stock price prediction using ML, I've decided to work on implementing the LSTM model.

Yoonwoo and Jiseok told me that it will be best to use Keras due to its ease of implementation.

https://towardsdatascience.com/predicting-stock-price-with-lstm-13af86a74944

https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/

above link contains more information about the implementation example.

The model will contain a LSTM layer and a dense layer. With help of Jiseok and Yoonwoo, the final model was coded as following with exception of the input shape.

def lstm_keras(data_pair):

model = Sequential()

model.add(LSTM(Samples, (Steps, Features)

model.add(Dense(1))

I've had issues with the Input shape as the LSTM layer required a very specific input shape the following:
* '''Samples'''. One sequence is one sample. A batch is comprised of one or more samples.
* '''Time Steps'''. One time step is one point of observation in the sample.
* '''Features'''. One feature is one observation at a time step
It took me a while to understand the shape of data_pair to give arguments to the model to conform with the requirement.

== Apr 8th 2019 ==
Monday meeting. Agreed to meet Friday / Saturday to work further

== Apr 13th 2019 ==
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Due Date
!Date Completed
!Remarks
|-
|Finish Implementing LSTM
|Completed
|Apr 12th
|Apr 20th
|Apr 20th
|
|}
Spent time with Yoonwoo to work on the input shape further.

Again, from 
* '''Samples'''. One sequence is one sample. A batch is comprised of one or more samples.
* '''Time Steps'''. One time step is one point of observation in the sample.
* '''Features'''. One feature is one observation at a time step.
 Our samples would be 1 sample at a time of 10 time steps and 5 features (Open,High,Low,Close,Volume).
Initial Design was: 

Features would be the number of columns of the train or test data from the data_pair,

Sample would be fixed to 1 and the time steps would be fixed to 10 or number of rows.

Jiseok was busy moving his stats model from signal_methods to methods.py, so we had to stop here.

From Jiseok's explanation,

signal methods are the functions that will convert the stream_data => raw data to the features that will actually be used by the models in methods.py

so in our case, it would make more sense to implement all of our statsmodels and ML models in methods.py as we would like to use those to predict the prices.

== Apr 15th 2019 ==
Monday meeting. Decided to meet again on Apr 19th

== Apr 20th 2019 ==
Finalized LSTM with help of Jiseok.

Finalized : 

def get_keras_regressor(learner, input_shape):

"""Generates a keras regressor

Given a learner object, produce an estimator that can be used

either by itself, or with an ensemble technique

Args:

learner: type of keras regressor to return

Returns:

a keras regressor

"""

activations = ('softmax', 'elu', 'selu', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'exponential', 'linear')

params = dict(learner.learnerParams)

params['units'] = 5 * abs(int(params['units']))

if params['units'] > 100:

params['units'] = 100

params['activation'] = mod_select(activations, params, 'activation')

params['epochs'] = 10 * abs(int(params['epochs']))

if params['epochs'] > 1000:

params['epochs'] = 1000

params['batch_size'] = 2 ** abs(int(params['batch_size']))

if params['batch_size'] > 1024:

params['batch_size'] = 1024

if learner.learnerName == 'mlp_keras':

estimator = KerasRegressor(build_fn=mlp_keras, **params, input_dim=input_shape)

elif learner.learnerName == 'rnn_keras':

estimator = KerasRegressor(build_fn=rnn_keras, **params, input_shape=input_shape)

elif learner.learnerName == 'lstm_keras':

params['recurrent_activation'] = mod_select(activations, params, 'recurrent_activation')

estimator = KerasRegressor(build_fn=lstm_keras, **params, input_shape=input_shape)

else:

estimator = None

return estimator

Since LSTM, RNN, and MLP are implemented almost identical, Jiseok merged them into one function. 

The input shapes are handled based on the matric specifications of the train and test data.

The "Params" are for generating random parameters.

For future students:

Once a method is implemented, the tests can be performed under "Unit_tests."

And also, depending on the type of method, "gp helper" must be also completed

elif selectedLearner == 'mlp_keras':

learnerParams = {'units': 5, 'activation': 6, 'epochs': 1, 'batch_size': 5}

elif selectedLearner == 'rnn_keras':

learnerParams = {'units': 5, 'activation': 6, 'epochs': 1, 'batch_size': 5}

elif selectedLearner == 'lstm_keras':

learnerParams = {'units': 5, 'activation': 6, 'recurrent_activation': 8, 'epochs': 1, 'batch_size': 5}

example of our helper codes are above

elif (learnerName == 'mlp_keras') and (type(newValue) is int):

learner.learnerParams[sorted(learner.learnerParams.keys())[pos % 4]] = newValue

elif (learnerName == 'rnn_keras') and (type(newValue) is int):

learner.learnerParams[sorted(learner.learnerParams.keys())[pos % 4]] = newValue

elif (learnerName == 'lstm_keras') and (type(newValue) is int):

learner.learnerParams[sorted(learner.learnerParams.keys())[pos % 5]] = newValue

and also above.

if regression:

learners = [#'Kmeans', 'GMM', 'Blup', 'OMP', 'SpectralCluster',

<nowiki>#</nowiki>'ArgMax', 'ArgMin', 'DepthEstimate',

'gradient_boosting_regression', 'adaboost_regression', 'random_forest_regression',

'svm_regression', 'decision_tree_regression', 'knn_regression',

'sarima_stats', 'varma_stats', 'hwes_stats', 'mlp_keras', 'rnn_keras', 'lstm_keras']

The methods are also needed to be added to the pool.

For Unit Tests,

def test_lstm_keras(self):

learner = LearnerType("lstm_keras", {'units': 5, 'activation': 6, 'recurrent_activation': 8, 'epochs':1, 'batch_size':5})

stream_test = signal_methods.my_lag_val(self.stock_data, mode=signal_methods.STREAM_TO_FEATURES)

stream_test = methods.single_learner(stream_test, learner)

self.assertIsInstance(stream_test, data.GTMOEPDataPair)

print(stream_test)

Once the unit test is defined as such,

you run the test for methods by running machine_learning_methods_unit_test.py then specifying the test case.

== Comment ==
I hope to get an A from this class. I believe I've put in a fair amount of work into the project.