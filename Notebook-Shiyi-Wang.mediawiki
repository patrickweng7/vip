= Team Member =
Team Member: Shiyi Wang

Email: swang793@gatech.edu

Cell Phone: 470-334-5880

Classes Taking: Machine Learning, Design & Analysis Algorithms, Intro to Cognitive Science, Math Statistics I

Interests: Cooking, Ultimate Frisbee, Phone Photography, Cats 🐈

Subteam: [[Natural Language Processing]] (Current) [[Modularity]] (Past)

Team members: [[Notebook Steven Anthony Leone|Steven Anthony Leone]], [[Notebook Devan Moses|Devan Moses]], [[Notebook Kevin Zheng|Kevin Zheng]], [[Notebook Karthik Subramanian|Karthik Subramanian]], [[Notebook George Ye | George Ye]]

= Fall 2021 =
== Week 16 ==
=== Dec 10, 2021 ===
'''Team Meeting Notes:'''
* Image Processing
** Objectives
*** Improve EMADE on image processing tasks
*** with GP, selection methods, mating, mutation methods, and deep learning.
*** Focus on Multi-class image Classification 
** Dataset Preparation
*** Use the TensorFlow API (ImageDataGenerator) including image resizing, normalization, horizontal flipping
*** Multi-class but not multi-label
** Evaluation Objectives
*** Area Under Curve: Receiving Operator Characteristic (ROC)
*** Number of Parameters in neural network
** Baseline Results & Findings
*** AUC: 0.21895
*** Lowest ROC: 0.169169 (Multiple Convolution layers)
*** Lowest Number of parameter: 30
** Selection Methods
*** NSGA 3 Good for 4-15 objectives (0.377 & 192680000)
*** NSGA 2 performs better
*** Less dimensions -> less axis to provide diversity/spread of reference points.
*** Lexicase (0.348 & 12055200)
*** NSGA 2 performs better
*** Few training cases -> few ways individuals can be selected
** Semantic Operator Results
*** Semantic Crossover: 0.45 & 657455
*** Semantic Mutation
*** Struggle with generated usable individuals
** Geometric Mating & Mutation Methods
*** Partially Matched
*** Ordered Crossover
*** Uniform & Multi Point: 0.365 & 36184500
*** Increase in parameters -> decrease in AUC
** Results
*** Lower Diversity of solutions
*** Worse AUC than Baseline
** Hyper features
*** Manipulate pixels for better visibility
*** Grey level transformation
*** Sharpening Filter
** Future Semesters
*** EMADE.py using the selection method specified in XML instead of hard-coded
*** Add new NN layers
* NLP
** Presented Experiment Methods of runs.
* Stocks
** Objectives
*** Short: Finalize results and conduct analysis with respect to various papers.
*** Long: EMADE for time-series and stock-specific problems. 
** Experimental Focus
*** Beat SOTA and other papers.
*** Run 3 Trials of Objective Sets in EMADE.
** Run Comparison
*** Primitive Analysis: BWMA + ARGMIN: 13.46%.
*** Profit percentage analysis: Loss & average the highest.
*** Not enough Pareto individuals to make the graph.
*** Loss_var is the top profit percentage individual 27.02%.
** Comparison with Paper
*** Takagi-Sugeno fuzzy model with SVR (Support Vector Regression).
*** Best individual use ARGMIN as learner.
*** Individuals is comparing multiple Technical Indicators together and determining its decisions based on which is least.
** Future Work
*** Complete Experimental Trials.
*** Write and Review Paper.
* Neural Architectural Search (NAS)
** Goal
*** Maximizing EMADE Productivity.
*** Growing Deep and Complex Individuals.
** Maximizing EMADE Productivity
*** Inspiration
**** Time being wasted on errored individuals.
**** Need a ways to stop training.
**** Added a TimedStopping Call back.
**** Added a new parameter in input schema.
*** Experiment Setup
**** Average Evaluation Time, Average Accuracy Error, Number of Individuals.
**** Time training does not decrease accuracy errors.
**** No linear relationship between time trained and metrics.
** Growing Deep and Complex Individuals
*** Inspiration
**** Added unique generator methods for NNLearners and Modules within EMADE.
**** Modified NNLearner generator to produce valid nn_learners using a forward.
**** Added Weight Sharing: increase specialization of modules.
**** Generated Mate/Mutation methods for NNLearner and Modules.
*** Experiment Setup
**** Accuracy Bins using Mut/Mat modules, without Mut/Mat, optimal midterm.
**** Elapse Time vs. Accuracy Error.
*** Leveraging Modules
**** Motivation: Pooling Layers, weight sharing.
*** Experiment Setup
**** Accuracy Error, Average Precision Error
** Weight Sharing
*** Inspiration
**** If modules was trained, load old weights.
**** Implement by storing weights in database.
**** Best individual good for 1 epoch (64.42% test accuracy) but weights in database is too slow (<= 3 generations).
** Weight Sharing + Max Pooling
*** Inspiration
**** Implement by weights stored in file.
**** Fewer trivial individuals.
**** Many more generations.
*** Less frequent time stopping.
** Max Pooling after modules
*** Best individuals perform better.
** Residual Connections
*** More consistently better than WS + Max Pooling.
*** More Left Skewed.
*** More trivial individuals.
** Best Individual
*** Train & Test Accuracy: 98.8% & 74%.
*** Takeaways: remove SGD (or only use Adams) & Experiment with training callbacks.
** New Testing/Analysis Tools
*** view_nn_statistics(hash, filepath)
**** Given a hash and an optional filepath, writes the individual’s statistics for each generation into a csv file sorted by age in ascending order.
*** make_parent_graph(hash, parent_information)
**** Creates a visualization of the parent tree of an individual with nodes containing the individual’s hash and tree structure.
** Literature Review
*** Evolving Deep Neural Networks
**** Automated method CoDeepNEAT for optimizing deep learning architecture through evolution
*** Concurrent Neural Tree and Data Preprocessing AutoML for Image Classification
**** Utilize an online learning technique to seed modules with the latest and greatest neural networks.
** Future Works
*** Novelty Detection
*** Novelty Evaluation Methods
*** Potential Directions
* Modularity
** Background
*** Explore ways to abstract parts of individuals.
*** ARL: introduce modularity and reusability in EMADE.
** Code Changes
*** Infrastructure Update: Documentation.
*** Moved information within tuples into classes.
*** CloudCopy Script Changes: No more infinite execution, progress countdown & progress bar.
** Experiments
*** Experiment 1: ARL Complexity Effects on Individuals.
**** Allow the search function to consider larger subtrees depth to increase the potency of ARLs.
**** Improve ARL candidates.
**** No ADF vs. ADF & No ADF vs. New ARL comparisons.
**** False Negatives & False Positives of Individuals Containing ARLs vs. ARL Size.
**** ARL Size and Depth vs. Frequency on the Pareto Front.
**** Takeaways: Size may not inherently imply usefulness.
*** Experiment 2: ARL Effects on Individuals' Objectives.
**** Most ARLs tended to follow general distribution.
**** ARLs which differ from general distribution were rare.
*** Merging CacheV2
** Future Work
*** Merge stocks changes into ARL_Update branch.
*** Continue adding to the research paper: data on extended ARLs and their performance on the titanic dataset.
*** ARL Construction: changes to hyper parameters.
*** ARL selection experiments: frequency and Novelty Reward.

'''Individual Progress'''
* I synced with Rishit on the multi Pareto Front optimization I made with George.
* I reformatted the experiment and result pages based on analysis.
* I presented the experiment section of the [https://docs.google.com/presentation/d/1mnFnhxyJnRowr6T-qh05yUMT50rSYqUQig7FIiPekWI/edit?usp=sharing Final Presentation].

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Sync with teammates on visualization progress made last night
|Completed
|December 9, 2021
|December 10, 2021
|December 10, 2021
|-
|Complete the [https://docs.google.com/presentation/d/1mnFnhxyJnRowr6T-qh05yUMT50rSYqUQig7FIiPekWI/edit?usp=sharing Final Presentation]
|Completed
|December 9, 2021
|December 10, 2021
|December 10, 2021
|-
|Review Notebook before Final Submission
|Completed
|December 10, 2021
|December 13, 2021
|December 11, 2021
|}

=== Dec 9, 2021 ===
'''Subteam Meeting Notes:'''
* Went over a dry run for the [https://docs.google.com/presentation/d/1mnFnhxyJnRowr6T-qh05yUMT50rSYqUQig7FIiPekWI/edit?usp=sharing Final Presentation].
* Clarified the details of specific slides for each member.
* Summarized the findings and shared with all members.

'''Individual Progress'''
* I collaborated with Steven and Rishit to conduct analysis on the overall runs.
* I and Steven [https://drive.google.com/drive/folders/100yIFjUIm2vPUagITxdGSIQHH7KVGppS?usp=sharing collected the valid runs] from all .out files given no error & 16 hr running time.
* I generated the [https://colab.research.google.com/drive/1S5ojJMDKG8L0aNYrzHFjqhDeA19H18SI?usp=sharing Pareto Front diagrams] for all 4 valid runs.
* I manually [https://docs.google.com/presentation/d/193SL8XMhSSKrIV0ihWu3DqZfi1cggBOnBfVBSSkFrX4/edit?usp=sharing colored the region of interest] on iPad given the Pareto fronts.
** Coded version was too complex at the moment given time constraints.
* I and Steven implemented and [https://colab.research.google.com/drive/1S5ojJMDKG8L0aNYrzHFjqhDeA19H18SI?usp=sharing automated the region of interest calculation].
* I and Steven summarized the conclusions according to our results.
** Overall, we are limited in scope to which we can generalize as we had a limited amount of data (4 16 hour runs).
** However, our results are promising and have an indication that we can increase the area under the region of interest, minimizing the area bounded by a seeded individual.
** We need more runs and a more diverse set of primitives.
* I designed the slides for the [https://docs.google.com/presentation/d/1mnFnhxyJnRowr6T-qh05yUMT50rSYqUQig7FIiPekWI/edit?usp=sharing experiment section].
* I and George optimized the scaling of [https://colab.research.google.com/drive/1SwuuPreeXgUUtxsMK3PGkPAftsbPilc0?usp=sharingmulti Pareto front graph]
** To include all the outliers, where there are individuals with extremely low MSE or num_params, I fit them in by changing "xlim" parameter to actual values and "ylim" parameter to relative scale based on their distributions.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete dry run for the [https://docs.google.com/presentation/d/1mnFnhxyJnRowr6T-qh05yUMT50rSYqUQig7FIiPekWI/edit?usp=sharing Final Presentation]
|Completed
|December 9, 2021
|December 9, 2021
|December 9, 2021
|-
|Sort out the valid good runs from all [https://drive.google.com/drive/folders/100yIFjUIm2vPUagITxdGSIQHH7KVGppS?usp=sharing .out files]
|Completed
|December 9, 2021
|December 9, 2021
|December 9, 2021
|-
|Generate the [https://colab.research.google.com/drive/1S5ojJMDKG8L0aNYrzHFjqhDeA19H18SI?usp=sharing Pareto front for each run along with seed]
|Completed
|December 9, 2021
|December 9, 2021
|December 9, 2021
|-
|Automate the [https://colab.research.google.com/drive/1S5ojJMDKG8L0aNYrzHFjqhDeA19H18SI?usp=sharing calculation of region of interest] and the reduction in area
|Completed
|December 9, 2021
|December 9, 2021
|December 9, 2021
|-
|Manually color the region of interests from each graph on [https://docs.google.com/presentation/d/1mnFnhxyJnRowr6T-qh05yUMT50rSYqUQig7FIiPekWI/edit?usp=sharing slide #36]
|Completed
|December 9, 2021
|December 9, 2021
|December 9, 2021
|-
|Write [https://docs.google.com/document/d/1pSSCwE8mI10_l4GPiFldxtVD1Q7LxaJGt5MIaM9RPuA/edit?usp=sharing statistical analysis] for the results
|Completed
|December 9, 2021
|December 9, 2021
|December 9, 2021
|-
|Optimize the scaling with the [https://colab.research.google.com/drive/1SwuuPreeXgUUtxsMK3PGkPAftsbPilc0?usp=sharing multi Pareto front visualization] on [https://docs.google.com/presentation/d/1mnFnhxyJnRowr6T-qh05yUMT50rSYqUQig7FIiPekWI/edit?usp=sharing slide #35]
|Completed
|December 9, 2021
|December 10, 2021
|December 9, 2021
|}

=== Dec 8, 2021 ===
'''Subteam Meeting Notes:'''
* Subdivided presentation responsibilities among all members.
* Decided to finish all EMADE runs by 4 PM EST on Dec 9.
* Decided to dry run for final presentation at 8 PM EST on Dec 9.

'''Individual Progress'''
* I, Rishit, and Steven are scheduled to meet at 4 PM on Dec 9 for analysis.
* I keep EMADE running on PACE-ICE.
* I explored and solved the following technical issue:
** What can we get from the .out files?
*** Open using a text editor (Sublime Text for example)
*** Use command + F to find key word "Pareto"
*** Choose "find previous" (key word)
*** Now you can see: Pareto front datapoints, hypervolume (AUC), and corresponding individuals.
*** The values are there, listed as (MSE, num_params).
** Do we need to convert into .csv files to generate Pareto front?
*** Not necessarily. The Pareto Front of each generation is in the master.out file.
*** Update: Implementation can be found [https://colab.research.google.com/drive/1S5ojJMDKG8L0aNYrzHFjqhDeA19H18SI?usp=sharing HERE].
** We need to calculate Region of Interest (ROI). Is there an easy way to do it or do we need to code it out?
*** We will need to hard code it.
*** Condition: obj1 < default_obj1 and obj2 < default_obj2.
*** Update: Implementation can be found [https://colab.research.google.com/drive/1S5ojJMDKG8L0aNYrzHFjqhDeA19H18SI?usp=sharing HERE].
* I researched and attempted [https://reference.wolfram.com/language/ref/BernoulliDistribution.html visualization of Bernoulli Distribution] with Mathematica.
 DiscretePlot[Table[PDF[BernoulliDistribution[p], x], {p, {0.3, 0.5, 0.7, 0.9}}] Evaluate, {x, 0, 1}, AxesOrigin -> {1/2, 0}, PlotMarkers -> Automatic]

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Keep up EMADE runs on PACE-ICE
|Completed
|December 8, 2021
|December 9, 2021
|December 9, 2021
|-
|Investigate on visualization of Bernoulli Distribution using multiple methods
|Completed
|December 8, 2021
|December 10, 2021
|December 8, 2021
|-
|Explore .out files to find what information we can retrieve from them
|Completed
|December 8, 2021
|December 10, 2021
|December 8, 2021
|-
|Implement Pareto front visualization from EMADE master output files
|Completed
|December 8, 2021
|December 10, 2021
|December 10, 2021
|}

=== Dec 6, 2021 ===
'''Team Meeting Notes:'''
* Every team is asked to define success of the semester.
** Image Processing: Improve EMADE on image processing tasks with GP, selection methods, mating, mutation methods, and deep learning.
** Modularity: ARL complexity effects on individual & ARL effects on individual objectives.
** Natural Language Processing (NLP): Use AutoML to improve Question and Answering Systems.
** Neural Architectural Search (NAS): Maximizing EMADE Productivity & Growing Deep and Complex Individuals.
* Every team will be sharing the experiment to validate its success.
* Dr. Rohling offered slide review on Thursday.

'''Subteam Meeting Notes:'''
* Tasked everyone with final presentation slides.
* Helped debug EMADE/PACE-ICE issues.
* Updated on the "reuse" parameter of the XML file: reduce instances is set to 1.
* Synchronized on the major updates regarding NNLearner2 last week.
** Changed metrics to be MSE and Num params, so that a start index off by one isn't punished as much as one off by 200.
** NNLearner2's output primitive's dense layer pushes all values to 199 or 200 when using Bidirectional Modeling and Attention primitives.
** Some of our runs aren't using the old embedding layers other than new EmbeddingLayer. 
** Individuals with BERTInputLayer fail altogether in standalone tree evaluator.

'''Individual Progress'''
* I and Rishit presented and finalized the experiment setup with Dr. Zutty & Dr. Rohling.
** Generate Pareto front from each run.
** Insert the seeded individual objectives on the same graph.
*** To retrieve the values, we can evaluate the seed by running [https://github.gatech.edu/emade/emade/blob/master/src/GPFramework/standalone_tree_evaluator.py standalone_tree_evaluator.py].
** Define boundaries by the objectives of the seeded individual.
** Record the Region of Interest (ROI) by difference between bounded region and Pareto front AUC.
*** The greater ROI, the greater improvements.
** For analysis, we can present Bernoulli Distribution and distribution of ROI of different runs.
*** It would be valuable if we have more runs (greater statistical significance).
* I was assigned on Experiment section of the [https://docs.google.com/presentation/d/1mnFnhxyJnRowr6T-qh05yUMT50rSYqUQig7FIiPekWI/edit?usp=sharing Final Presentation].
* I fixed my EMADE configuration issues thanks to Manas, Karthik, George, Devan, and Rishit.
** Configuration issue: when seeding before EMADE runs, I encountered the issue: NameError: _mysql is not defined.
** Command applied: python3 src/GPFramework/seeding_from_file.py templates/input_squad.xml seeding_qa
** How to locate the causes?
*** I went over the XML file setup with Karthik and made sure that all parameters are ready to go.
*** Rishit's suggestion: pip install mysql: no such module found. No luck.
*** Manas's suggestion: check to [https://github.gatech.edu/emade/emade README.md file] in repository and manually pip install all the requirements: I did so and all requirements are satisfied already. No luck.
*** Devan's suggestion: run [https://github.gatech.edu/emade/emade/blob/master/src/GPFramework/standalone_tree_evaluator.py standalone_tree_evaluator.py] see what may cause the issue: I realized it is a configuration issue because I meet the same problem when running standalone.py (basically same issue in two completely different scenarios).
** How to solve the problem?
*** Rishit shared his YAML file with me and I was able to create a new conda environment with the correct settings.
*** Now, EMADE is working and generating output without errors.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete Peer Evaluation
|Completed
|December 6, 2021
|December 8, 2021
|December 6, 2021
|-
|Debug EMADE run issues related to NameError: _mysql not defined
|Completed
|December 6, 2021
|December 9, 2021
|December 6, 2021
|-
|Update XML file and update line 1864 in neural_network_methods.py
|Completed
|December 6, 2021
|December 9, 2021
|December 6, 2021
|-
|Keep EMADE running for >= 16 hours
|Completed
|December 6, 2021
|December 9, 2021
|December 9, 2021
|-
|Design [https://docs.google.com/presentation/d/1mnFnhxyJnRowr6T-qh05yUMT50rSYqUQig7FIiPekWI/edit?usp=sharing Final Presentation] slides
|Completed
|December 6, 2021
|December 10, 2021
|December 9, 2021
|}

== Week 15 ==
=== Dec 1, 2021 ===
'''Subteam Meeting Notes:'''
* Code freeze fork of subteam codebase before Final Presentation: https://github.gatech.edu/sleone6/emadebackup
* Everyone reinstalls EMADE with Steven's version and prepare to conduct runs for final presentation.

'''Individual Progress'''
* I debugged and set up EMADE running environment on PACE-ICE.
** I was able to run EMADE unseeded but encountered issue when seeding the individual.
** Update: I was able to resolve the issue on Dec 6 during the Scrum meeting.
* My tips for PACE/EMADE setup
** ssh gt-user@pace-ice.pace.gatech.edu
** Make scratch, db, and MySQL database
** SCP over EMADE folder 
*** Alternatively, I recommend using [https://code.visualstudio.com/blogs/2019/07/25/remote-ssh#:~:text=The%20VS%20Code%20Remote%20%2D%20SSH,Ctrl%20%2B%20Shift%20%2B%20X). VSCode SSH Remote Host Connect]
** Modify all .cnf, .xml, and .pbs files
** Install & Activate conda environment using YAML file [https://github.gatech.edu/cwhaley9/emade/commit/6781e87ba9ffed065a983d12ec38c37191e3082e HERE].
** Create "Amazon" database & Drop all tables
** Start EMADE runs
* I also helped other team members set up in Slack channel.
** Some issues encountered
*** To properly configure, edit input XML file for username, password, reuse parameter, environment name, and objectives (MSE & num params).
*** To run with SQuAD dataset, drop all tables in amazon database and then rerun EMADE.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Reinstall EMADE using [https://github.gatech.edu/sleone6/emadebackup emadebackup fork] shared by Steven
|Completed
|December 1, 2021
|December 6, 2021
|December 1, 2021
|-
|Drop all tables in Amazon database for proper SQuAD runs
|Completed
|December 1, 2021
|December 6, 2021
|December 6, 2021
|-
|Run EMADE with the seeded individual
|Completed
|December 1, 2021
|December 9, 2021
|December 6, 2021
|}

=== Nov 29, 2021 ===
'''Team Meeting Notes:'''
* Image Processing
** Hyperfeature, selection method, and mating/mutations subteams continue to work.
** Code freeze by the end of the week for experiments and presentation.
* Modularity
** Continued doing runs and gathering data for the extended ARL experiments.
** CacheV2 integrations subteam soon begins stocks runs.
* Natural Language Processing (NLP)
** In NNLearner2, we send the test and train data pair in wrong order.
** The length of the test data was not being divided by num_inputs in EMADE.py.
* Neural Architectural Search (NAS)
** Updated code that properly extracting layer frequencies from individuals every generation
** Updated script that is used to generate CIFAR-10 data in proper splits with full color images.
** Implemented weight sharing.
** Working on merging changes and putting the desired EMADE functionality on NN-VIP branch.

'''Subteam Meeting Notes:'''
* Worked on code-freeze codebase and start getting runs in.

'''Individual Progress'''
* I, Steven, and Rishit declared continuous MSE and number of parameters as the [https://github.gatech.edu/sleone6/emadebackup/blob/master/templates/input_squad_noqsub.xml final objectives] we are pursuing.
** continuous MSE: this is a good indicator as F1 score in Q&A systems. It works well with regression questions.
** num_params: this estimates the model complexity. The more parameters, the more complex the model will be.
* I and Rishit exchanged our ideas on experimental design and statistical testings.
** T-test: in statistics, the t-statistic is the ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error.
*** A t-test is a type of inferential statistic used to determine if there is a significant difference between the means of two groups, which may be related in certain features. 
*** It is mostly used when the data sets, like the data set recorded as the outcome from flipping a coin 100 times, would follow a normal distribution and may have unknown variances.
** I and Rishit hope to compare the objectives of the EMADE runs and Steven's NLP homework.
*** However, the problem is we essentially cannot get the same EM and F1 score from EMADE as in the “Steven’s Model”.
*** Rishit suggests we can dive into Steven's code to calculate the objectives manually.
*** I and Steven recommend using the seed individual instead since it is equivalent.
* I looked into library the compute_f1 function located and how the library implements it.
** The goal is to see if we can use exact match and F1 score for EMADE to enable comparisons.
* I worked on deleting all files on PACE-ICE (disk space issue resolved) and port over the latest changes from the features/nnlearner2 branch.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Check EMADE running status using Amazon dataset
|Completed
|November 29, 2021
|December 6, 2021
|November 29, 2021
|-
|Port over the new changes from the features/nnlearner2 branch
|Completed
|November 29, 2021
|December 6, 2021
|November 29, 2021
|-
|Meet with Rishit and Steven on experimental design and objectives
|Completed
|November 29, 2021
|December 6, 2021
|November 30, 2021
|-
|Investigate possible statistical testing strategies such as t-test
|Completed
|November 29, 2021
|December 6, 2021
|November 30, 2021
|-
|Review every objective in the [https://github.gatech.edu/sleone6/emadebackup/blob/master/templates/input_squad_noqsub.xml objective4EvalFunction] to find best pair of objectives
|Completed
|November 29, 2021
|December 6, 2021
|November 30, 2021
|}

== Week 14 ==
=== Nov 24, 2021 ===
'''Subteam Meeting Notes:'''
* Thanksgiving break. No meeting today.

=== Nov 22, 2021 ===
'''Team Meeting Notes:'''
* Image Processing
** Runs in with lexicase, epsilon lexicase, and NSGA-III.
** Run with a new mutation method.
** Fixed a bug in the contrast enhancement code causes .npz formatting of data
* Modularity
** Refactored and documented new implementation of code.
** Added typing and classes to implementation.
** Working on script that allows the team to upload EMADE instances to Google Colab.
** Working on generating visualizations for the extended ARL runs.
* Natural Language Processing (NLP)
** NNLearner2 works on classification but have issues when switching to regression.
** Tested integration of primitives into EMADE.
** Built Keras Models in Google Colab to ensure integration works.
** Fixed batch size issues with Bidirectional Attention.
** Designing experiment for future runs.
* Neural Architectural Search (NAS)
** Completed changes for mating and mutation methods
*** Allows Modules to undergo a selection process, mate with each other, mutate and be scored according to their associated individuals.
** Integrating weight sharing to existing modules in EMADE.
** Integrating novelty detection changes.
** Added a parent column which tracks hash values for NNLearners.
** Model Improvements
*** Removing pooling layers leads to 10x more parameters.
*** Model could overfit on train set (98% train accuracy, 62% test).

'''Individual Progress'''
* I and Rishit presented our tentative experiment plan to Dr. Zutty.
** Generate Pareto fronts for all runs.
** Graph the Normal Distribution for all individuals.
** Insert BiDAF model objectives on the Normal Distribution.
** Use hypothesis testing to see if the BiDAF data point lies in the critical region to reject hypothesis.
*** Hypothesis: AutoML cannot solve Question and Answering system problems.
* Dr. Zutty visualized our experiment plan and pointed out the potential issues.
** I learned that using AUC could be a better alternative for analysis.
** I learned about the concept of Region of Interest (ROI) and realized that we need to set the boundaries for BiDAF model to define our "success".
*** Success: a run produced individuals that beats all of the seeded individual’s objective scores, having a non zero region of interest.
** I learned that instead of using a normal distribution, a more effective way would be Bernoulli distribution which separates reduce in area (success) and increase in area (failure).
* I start to pipeline the experiment procedure and the final design I made with Rishit can be found [https://colab.research.google.com/drive/1S5ojJMDKG8L0aNYrzHFjqhDeA19H18SI?usp=sharing HERE].
* Over the Thanksgiving break, I learned and enhanced my understanding about hypothesis testing and Bernoulli distributions.
** [http://www2.stat.duke.edu/~sayan/Sta613/2018/lec/mult.pdf Lecture 3 Hypothesis testing from Duke University]
*** The classical hypothesis testing formulation is called the Neyman-Pearson paradigm. It is a formal means of distinguishing between probability distributions based upon random variables generated from one of the distributions.
** [https://paulapivat.com/post/dsfs_7/ Data Science from Scratch (ch7) - Hypothesis and Inference]
*** The classic coin-flipping exercise is to test the fairness off a coin. If a coin is fair, it’ll land on heads 50% of the time (and tails 50% of the time). Let’s translate into hypothesis testing language:
**** Null Hypothesis: Probability of landing on Heads = 0.5.
**** Alt Hypothesis: Probability of landing on Heads != 0.5.
*** Each coin flip is a Bernoulli trial, which is an experiment with two outcomes - outcome 1, “success”, (probability p) and outcome 0, “fail” (probability p - 1). The reason it’s a Bernoulli trial is because there are only two outcome with a coin flip (heads or tails).
** [https://www.researchgate.net/publication/321951379_Hypothesis_Tests_for_Bernoulli_Experiments_Ordering_the_Sample_Space_by_Bayes_Factors_and_Using_Adaptive_Significance_Levels_for_Decisions Hypothesis Tests for Bernoulli Experiments: Ordering the Sample Space by Bayes Factors and Using Adaptive Significance Levels for Decisions]
*** The main objective of this paper is to find the relation between the adaptive significance level presented here and the sample size.
*** The main tools used here are the Bayes factor and the extended Neyman–Pearson Lemma.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Pipeline [https://colab.research.google.com/drive/1S5ojJMDKG8L0aNYrzHFjqhDeA19H18SI?usp=sharing experiment procedure] including Pareto front, Region of Interest, and Bernoulli Distribution
|Completed
|November 22, 2021
|November 27, 2021
|December 6, 2021
|-
|Review statistics materials and read related papers regarding hypothesis testing and specifically with Bernoulli Distribution
|Completed
|November 22, 2021
|November 27, 2021
|November 27, 2021
|}

== Week 13 ==
=== Nov 17, 2021 ===
=== Nov 15, 2021 === 

== Week 12 ==
=== Nov 10, 2021 ===
=== Nov 8, 2021 === 

== Week 11 ==
=== Nov 7, 2021 ===
=== Nov 6, 2021 ===
=== Nov 3, 2021 ===
'''Subteam Meeting Notes:'''
* Every returning and new member introduced him/herself in the meeting.
* Debugged BIDAF.py

=== Nov 1, 2021 ===
'''Team Meeting Notes:'''
* Image Processing
** Decided to focus on binary classification task.
** Research workaround geometric and semantic mating and mutation.
** Research new hyperfeature combinations.
* Modularity
** Migrating modularity into Cache V2.
** Extended ARL group continue doing runs and running experiments.
* NLP
** Implemented Modeling layer and output layer.
** Halfway through Bi-directional attention layer.
* NAS
** Ideas we plan to work on in order to improve neural architecture search in EMADE: Triviality Detection, Novelty Detection, Bug fix for swap_layer method, Speeding up EMADE processes, Adding unit tests, Introducing some pause functionality.
** Updating to TensorFlow 2.6 on PACE.

'''Subteam Meeting Notes'''
* Presented NLP and Q&A Systems to bootcamp students.

== Week 10 ==
=== October 27, 2021 ===
'''Subteam Meeting Notes:'''
* Discussed the dimensionality issue between different layers.
* Decided to focus on implementing BiDAF and set aside the Literature Review Primitives.
* Steven shared his homework code for BiDAF layers (with comments for dimensionality for different layers).
* Steven, Rishit, Karthik, Devan working on Attention layer, George on Modeling, me and Kevin on output layer.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Research Output Layer
|In Progress
|October 27, 2021
|October 31, 2021
|
|-
|Implement Output Layer
|In Progress
|October 27, 2021
|October 31, 2021
|
|}

=== October 25, 2021 ===
'''Team Meeting Notes:'''
* NLP
** Presented the work so far in Literature Review
** Answered the question about non-tree structures in Literatures.
* Bootcamp Team #1
** Data Preprocessing
*** Drop Columns, replace null columns, one hot encoding.
** Machine Learning
*** SVM Classifier, Gradient Descent, Neural Network, Random Forest Classifier, Gaussian Process Classifier.
** Genetic Programming
*** Individual (Primitive Tree), Evaluation (FPR, FNR), Crossover (cxOnePointLeafBiased), Mutate (muNodeReplacement), Selection (Selection Tournament).
** EMADE
*** Double Headless Chicken rate and reduce most of the mutations.
* Neural Architecture Search
** Initial Infrastructure
*** Hard to outperform seeds
*** Most individuals are not neural networks.
*** No rules being enforced when connecting layers (dimensionality errors)
** Time Stopping
*** Expectation: Lower training time same as minimizing number of parameters.
*** Implementation: Check train time at the end of each epoch.
** Preprocessing
*** Text Tokenization, One Hot Encoding for multi-class Target Data.
*** Hard to compare for valid individuals.
** Growing Complex Individuals - CoDEEPNeat
*** Separated primitive sets and terminals between ADFs and MAIN primitive.
** Results
*** Decrease in the Average Accuracy Error.
*** Increase in Average elapsed time.
** Testing & Analysis
*** NNLearner SQL Table.
*** Analysis Methods - nn_ind_from_hash, view_nn_statistics, simplify_parents.
*** Tracking layer frequencies across generations.
* Bootcamp Team #2
** Data Preprocessing
*** Drop column, fill Null will statistical values
** Machine Learning
*** Decision Tree, Random Forest, ADA Boost, MLP, SVM
** MOGP
*** SPEA2 for select function.
** EMADE
*** Conda + Python 3.7 was the only working combination (Python 3.8/3.9 does not support multiprocess).
* Image Processing
** Objectives
*** Improve EMADE on image processing tasks
*** Future work: Object detection, image segmentation, image registration
** Literature Review
*** Arrived at CheXnet Paper - Pneumonia Detection on Chest X-Rays.
** Dataset Preparation
*** Used the Tensorflow API (ImageDataGenerator) to conduct the image resizing, normalization, horizontal flipping.
*** Newer augmentations: rotation, shearing, etc.
** Baseline Results on 30 Generations
*** Evaluation Objectives: Precision-Recall AUC, Number parameters.
*** Suspicious AUC: due to objective functions, or dataset issues.
** Selection Methods
*** NSGA III vs. NSGA II: Pre-defines set of reference points to maintain diversity among its solutions.
** Mating & Mutation Methods
*** Added two mating and mutation methods based around semantic: Geometric Semantic Crossover & mutation.
*** Trimming primitive set and simpler seed individuals could yield better results.
*** Added two Geometric Crossover Operators: Simulated Binary & Blended Crossover.
** Hyper-features for Image Processing & GP
*** Grey level transformation
* Bootcamp Team #3
** Data Preprocessing
*** Drop columns, one hot encoding for genders and name titles, add columns for relatives, gender_embarked, replace values with mean.
** Machine Learning
*** SVM, Random Forests, KNN, Logistic Regression, MLP, Gaussion Naive Bayes.
** MOGP
*** Strongly typed primitives to enforce boolean outcome.
*** Selection (tools.selLexicase), Mate (One Point Crossover), Mutate (mutUniform).
** Machine Learning vs. MOGP
*** ML more consistent vetween runs.
** EMADE
*** Modified sel_nsga2 to achieve selTournamentDCD()'s requirement of having an "individual" array divisible by 4.
*** Allow port forwarding on the master'ss home router if the workers will be joining remotely. No VPN was needed to connect.
** Pareto Front Comparison
*** Why ML is better than MOGP? May be not enough generations.
* Stocks
** Primitive Analysis
*** The ML algorithm in the individual uses historical data and technical indicators to predict future trigger points for the stocks.
*** This study is to determine which technical indicators and which ML learners have the best performance.
** Objective functions
*** Profit Percentage, Average Profit Per Transaction, CDF of Profit, Variance of Profit per Transaction.
** Experiment Design
*** Create a pool of objectives and pick out stocks to conduct EMADE runs on.
*** Run EMADE using all possible combination of objectives.
** Comparison to Paper Results
*** Takagi-Sugeno Fuzzy Model with an SVR model to forecast stock prices.
* Bootcamp Team #4
** Data Preprocessing
*** Drop columns, one hot encoding, replace null with median.
** Machine Learning
*** MLP, Decision Tree, SVM, XGBoost
** Primitives for MOGP
*** Strongly typed, logical terminals enable to pass tree inputs to logical operators directly.
** MOGP
*** Mate (cxOnePoint), Mutate (mutUniform), Selection (selTournamentDCD).
** EMADE - 20 Generations
*** AdaBoostLearner (9) > baggedLearner (2) = randShuffle (2) handle data directly.
*** Time expensive and more gradual evolution.
*** Optimal when cost of tuning individuals manually is higher than cost of automating evolution.
* Modularity
** Intro
*** Modularity: Explore ways to abstract parts of individuals.
*** ARL Implementation: Search the population for combinations of parent and children nodes.
*** ARL Complexity: Increase complexity of ARLs via increasing tree depth.
** Goal
*** Allow the search function to consider larger subtrees depth to increase the potency of ARLs.
*** Improve ARL candidate via a new selection functions.
** Bug fixed
*** Contracting overlapping ARLs would either cause multiple deletions.
** Titanic Experimental Setup
*** Default Evolutionary Parameters
** Stock Runs with ARLs
*** Merging stocks code with ARL codebase, different individuals, evaluation methods, etc.
** Results
*** Higher AUC with new ARLs
*** More data is needed to gain insight into effects of ARL Complexity on performance.
*** From preliminary data, size may not inherently imply usefulness.

'''Individual Progress'''
* Completed one more dry run before the midterm presentation.
* Built the tree for Machine Comprehension Using Match-LSTM and Answer Pointer using [https://github.com/cwhaley112/TreeVis TreeVis].
 NNLearner(ARG0, ARG1, answerPointLayer(matchLSTM(LSTM(ARG1, softAttention(LSTM(ARG0))))))

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete one more dry run before the [https://docs.google.com/presentation/d/1GviS4whmKxNpbxn2cMQgcUthRxp1hsmu_NLfDkY00b4/edit#slide=id.gf9ca2fa1ca_3_9 midterm presentation]
|Completed
|October 25, 2021
|October 25, 2021
|October 25, 2021
|-
|Visualize the trees for paper
|Completed
|October 25, 2021
|October 25, 2021
|October 25, 2021
|}

== Week 09 ==
=== October 20, 2021 ===
'''Subteam Meeting Notes'''
* Discussed slides preparations
** Literature review slides (3 papers + visualizations)
** Dataset prep
** Current EMADE status runs on QA
** New eval methods
** Future plans
* Scheduled the dry run on Sunday for the midterm presentation on Monday.

'''Individual Progress'''
* Summarized BIDAF in plain context
** BiDAF is a one of the state-of-art QA systems that aims to pinpoint the location of the Answer within a Context stems from its layered design.
** Each of these layers is like a transformation engine that transforms the vector representation of words.
* Literature Reviewed [https://arxiv.org/pdf/1608.07905.pdf Machine Comprehension Using Match-LSTM and Answer Pointer]
** Investigated on Match-LSTM
*** In textual entailment, two sentences are given where one is a premise and the other is a hypothesis. 
*** To predict whether the premise entails the hypothesis, the match-LSTM model goes through the tokens of the hypothesis sequentially. 
*** At each position of the hypothesis, attention mechanism is used to obtain a weighted vector representation of the premise. 
*** This weighted premise is then to be combined with a vector representation of the current token of the hypothesis and fed into an LSTM, which we call the match-LSTM. 
*** The match-LSTM essentially sequentially aggregates the matching of the attention-weighted premise to each token of the hypothesis and uses the aggregated matching result to make a final prediction.
** Investigated on Answer Pointer Layer
*** Improvement on Pointer Net, a sequence-to-sequence model proposed to constrain the output tokens to be from the input sequences.
* Completed the slides design for [https://docs.google.com/presentation/d/1GviS4whmKxNpbxn2cMQgcUthRxp1hsmu_NLfDkY00b4/edit#slide=id.gf9ca2fa1ca_5_16 paper 3], [https://docs.google.com/presentation/d/1GviS4whmKxNpbxn2cMQgcUthRxp1hsmu_NLfDkY00b4/edit#slide=id.gf9ca2fa1ca_5_26 vis3], and [https://docs.google.com/presentation/d/1GviS4whmKxNpbxn2cMQgcUthRxp1hsmu_NLfDkY00b4/edit#slide=id.gf9ca2fa1ca_3_9 Literature Review Team Goal].

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete the slides assigned
|Completed
|October 20, 2021
|October 25, 2021
|October 24, 2021
|-
|Investigate on Match-LSTM
|Completed
|October 20, 2021
|October 25, 2021
|October 24, 2021
|-
|Investigate on Answer Pointer Layer
|Completed
|October 20, 2021
|October 25, 2021
|October 24, 2021
|-
|Summarize BIDAF in plain context
|Completed
|October 20, 2021
|October 25, 2021
|October 24, 2021
|}

=== October 18, 2021 ===
'''Team Meeting Notes:'''
* Plan on using CPU Hours
** Example: Master requires 1 core. Number coworker = 4. Runtime = 8:00 (constant)
* Image Processing
** Baseline runs showing a limited set of primitives that do not cause trees to error out.
** Implemented most of the hyper features, selection methods and crossover methods.
* Modularity
** Fixed implementation detail regarding ARLs were being created with only a single non-arg node.
** Fixed bug regarding each individual being restricted to a single ARL in order to prevent conflicts.
** Updating arity of ARL upon contracting, with special edge case for treating ARG0 as an arl_arg since
* Natural Language Processing
** Resolved 5,000 + merge conflicts at Hackathon.
** Running into errors with the input - Key error for number of input.
** Literature review team has read several papers and made trees for 6 seeding individuals.
* Neural Architectural Search
** Nested NNLearners
*** Solution: A second data pair was created and the 'MAIN' PrimitiveTypedSet had its output changed to this second datapair to prevent it from being a nestable primitive to itself.
** ADFs mostly aren't valid
*** Solution: ADFs are now structured to only take in a layerlist input and output a layerlist input.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Try implement primitives in Literature
|Completed
|October 18, 2021
|October 25, 2021
|October 18, 2021
|}

== Week 08 ==
=== October 16, 2021 ===
'''Hackathon Meeting Notes:'''
* Resolved 5,000 + merge conflicts at Hackathon.
* Running into errors with the input - Key error for number of input.
* Literature review team has read several papers and made trees for 6 seeding individuals.

'''Individual Progress:'''
* Discovered 3 more papers (now 6 papers in total) with tree-structure infrastructure.
** [https://arxiv.org/pdf/1608.07905.pdf Machine Comprehension Using Match-LSTM and Answer Pointer]
*** This paper proposes an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model to constrain the output tokens to be from the input sequences. 
*** This paper proposes two ways of using Pointer Net for the task. Experiments show that both of the models substantially outperform the best results using logistic regression and manually crafted features.
** [https://arxiv.org/pdf/1610.09996.pdf End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension]
*** This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. 
*** DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. 
*** DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer.
*** Experimental results show that DCR achieves state-of-the-art exact match and F1 scores on the SQuAD dataset.
** [https://arxiv.org/pdf/1908.05147.pdf SG-Net: Syntax-Guided Machine Reading Comprehension]
*** This paper proposes using syntax to guide the text modeling by incorporating explicit syntactic constraints into attention mechanism for better linguistically motivated word representations. 
*** In detail, for self-attention network (SAN) sponsored Transformer-based encoder, this paper introduces syntactic dependency of interest (SDOI) design into the SAN to form an SDOI-SAN with syntax-guided self attention.
*** Syntax-guided network (SG-Net) is then composed of this extra SDOI-SAN and the SAN from the original Transformer encoder through a dual contextual architecture for better linguistics inspired representation. 
*** To verify its effectiveness, the proposed SG-Net is applied to typical pre-trained language model BERT which is right based on a Transformer encoder. 
*** Extensive experiments on popular benchmarks including SQuAD 2.0 and RACE show that the proposed SG-Net design helps achieve substantial performance improvement over strong baselines.
* Tree Generated based on my understanding of the models in the literatures.
 Paper #1: MATCH-LSTM & Answer Pointer
 NNLearner(ARG0, ARG1, answerPointLayer(matchLSTM(LSTM(ARG1, softAttention(LSTM(ARG0))))))

 Paper #2: Dynamic chunk reader
 NNLearner(ARG0, ARG1, rankerLayer(chunkRepLayer(attentionLayer(encoderLayer(ARG0, ARG1)))))

 Paper #3: SG-Net
 NNLearner(ARG0, ARG1, taskLayer(dualContextAggregation(SGSelfAttentionLayer(transformerEncoder(ARG1))), AG0))

=== October 13, 2021 === 
'''Sub Team Meeting Notes:'''
=== October 11, 2021 ===
'''Team Meeting Notes:'''
* Fall Break. No Class on Monday.

== Week 07 ==
=== October 06, 2021 ===
'''Sub Team Meeting Notes:'''
* While waiting for Dr. Zutty to get the code supporting the functionality of loading in multiple EMADE Data Pairs, we assigned subtasks to team members
** Read up on NNLearner, get familiar with what's changed
** When Dr. Zutty uploads the new code, merge it with NN-VIP
** Read up on State of the Art QA models (SQUAD leaderboard, BiDAF, papers, Transformer Models)
** Make Trees w/ Primitives to be Seeded Individuals

'''Individual Progress'''
* I was specifically assigned to come up with some seeded individuals in a tree structure that we could test in standalone_tree_evaluator.py, looking at BiDAF and the SQUAD leaderboard as examples. This would also help us figure out what primitives to write to make QA work.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Further readings on QA models
|Completed
|October 4, 2021
|October 11, 2021
|October 11, 2021
|-
|Work on seeding individuals
|Completed
|October 4, 2021
|October 11, 2021
|October 11, 2021
|-
|Complete Peer Evaluation
|Completed
|October 4, 2021
|October 8, 2021
|October 4, 2021
|}

=== October 04, 2021 ===
'''Team Meeting Notes:'''
* Image Processing
** NSGA-III Implementation - Tweak p parameter, test against working datasets
** Mating/Mutation - New method
** Trouble with seeds
** Hyper feature ideas
* Modularity
** Implemented workaround for add_all_subtrees large individuals bug.
** Fixed bug regarding incorrect arities in contract_arls.
** Switch back to Google Colab as PACE-ICE still causing issues.
* Neural Architectural Search
** Went through the entirety of the main master algorithm loop within EMADE.py. 
** Included a discussion of the different arguments within the master algorithm such as MUTPB, CXPB and reuse. 
** Different chunks of the main loop were explained such as the one which handles creation of new individuals.

== Week 06 ==
=== September 29, 2021 ===
'''Sub Team Meeting Notes:'''
* Went over through some QA Systems basics for the new onboarding members and discussed new sub tasks.
* Steven will make a new fork for us.
* Assigned everyone to start trying to get the SQUAD dataset loaded in.
* Try implement load_textdata_for_QA to handle context/query split
* Try create the XML for SQUAD dataset
=== September 27, 2021 ===
'''Team Meeting Notes:'''
* Image Processing
** Selection Methods using NSGA-III Implementation.
** Search for combinations of Hyper-feature & Primitive Packaging.
** Baseline run unsuccessful, retrying with seed file.
** Update on geometric semantic genetic programming methods.
* Modularity
** Contract ARLs method wasn't properly updating arities of the node(s) surrounding the contracted ARL.
** Problem with add_all_subtrees method: causes problems with decently sized individuals.
** MNIST team working through getting everyone on PACE-ICE to do runs.
* Neural Architectural Search
** A global "supernet" which would operate like a dynamic acyclic graph which stores information about submodel accuracy and weights of most optimal subnet based on a prior generation run (Do not proceed with this idea).
** Weight Sharing to the existing NNLearner evolution process in EMADE.
** Writes to disk which would improve memory usage within EMADE.

'''Individual Progress'''
* Discovered the dictionary type structure in GPFramework that maps data types to functions.
* Because Dr. Zutty was unable to attend, the team met with Anish to explore a method to use the EMADE architecture to produce datapairs organized as "((Context, Query), Answer/Label)".
** We considered developing our own datatype, but that would need us to alter each later in the "neural networks methods.py" script.
** Anish recommended that instead of introducing that risk, we create an overloaded "nn learner" function that takes in "two inputs" and does embedding independently before using BiDaf on the context and query [https://drive.google.com/file/d/16sbKZ2JRheoqeHsNChpx3daltsVClF0K/view?usp=sharing Here] is how the idea is visualized.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Overload NNLearner to pass in query and context separately
|Completed
|September 27, 2021
|October 04, 2021
|September 29, 2021
|-
|Research NNLearners and how to implement QA
|Completed
|October 27, 2021
|October 04, 2021
|September 29, 2021
|}

== Week 05 ==
=== September 22, 2021 ===
'''Sub Team Meeting Notes:'''
* Designed primitives/infrastructure needed to tackle the QA problem with EMADE.
* Encountered a few problem that existing EMADE framework cannot resolve.
** Problem 1: Unlike other datasets, we have two inputs that the model needs to handle separately: the context and the query.
** Solution 1: We create 2 new primitives, the ContextEmbeddingLayer and the QueryEmbeddingLayer. We also create a new type of data pair that we can fetch both the context and query separately in. Therefore, if the passed in data pair is of this new type, we can return the context and query in the ContextEmbeddingLayer and the QueryEmbeddingLayer, respectively.
** Problem 2: The output is determined by calculating the max probabilities of start and end words of the answer (detailed more in Steven's notebook and below). We cannot solely determine the output by calling model.predict(), as the final output should be a list of size 2N with a softmax applied, where N is the number of words in the context.
** Solution 2: With the different type of data pair, we can check in the NNLearner's code for the type via an "if" statement, and determine the output in this way.

'''Individual Progress'''
* Reflection on [https://towardsdatascience.com/the-definitive-guide-to-bi-directional-attention-flow-d0e96e9e666b An Illustrated Guide to Bi-Directional Attention Flow (BiDAF)]
** 1. Embedding Layers
*** BiDAF has 3 embedding layers whose function is to change the representation of words in the Query and the Context from strings into vectors of numbers.
** 2. Attention and Modeling Layers
*** These Query and Context representations then enter the attention and modeling layers. These layers use several matrix operations to fuse the information contained in the Query and the Context. The output of these steps is another representation of the Context that contains information from the Query. This output is referred to in the paper as the “Query-aware Context representation.”
** 3. Output Layer
*** The Query-aware Context representation is then passed into the output layer, which will transform it to a bunch of probability values. These probability values will be used to determine where the Answer starts and ends.
* Reflection on [https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb Word Embedding, Character Embedding and Contextual Embedding in BiDAF — an Illustrated Guide]
** Step 1. Tokenization
*** In BiDAF, the incoming Query and its Context are first tokenized, i.e. these two long strings are broken down into their constituent words.
** Step 2. Word Level Embedding
*** The resulting words are then subjected to the embedding process, where they are converted into vectors of numbers.
** Step 3. Character Level Embedding
*** Character level embedding uses one-dimensional convolutional neural network (1D-CNN) to find numeric representation of words by looking at their character-level compositions.
** Step 4. Highway Network
*** At this point, we have obtained two sets of vector representations for our words — one from the GloVe (word) embedding and the other from 1D-CNN (character) embedding. The next step is to vertically concatenate these representations.
** Step 5. Contextual Embedding
*** The contextual embedding layer consists of Long-Short-Term-Memory (LSTM) sequences. An LSTM is a neural network architecture that can memorize long-term dependencies. When we enter an input sequence (such as a string of text) into a normal forward LSTM layer, the output sequence for each timestep will encode information from that timestep as well as past timesteps. In other words, the output embedding for each word will contain contextual information from words that came before it.
* Reflection on [https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-3-attention-92352bbdcb07 Attention Mechanism in Seq2Seq and BiDAF — an Illustrated Guide]
** 1. Comparison of Sequences and Calculation of Attention Scores
** 2. Conversion of Attention Scores to Attention Distribution
** 3. Multiplying Attention Distribution with Encoder Hidden States to Get Attention Output
* Reflection on [https://arxiv.org/pdf/1611.01603.pdf Bidirectional Attention Flow For Machine Comparison]
** 1. Character Embedding Layer maps each word to a vector space using character-level CNNs.
** 2. Word Embedding Layer maps each word to a vector space using a pre-trained word embedding model.
** 3. Contextual Embedding Layer utilizes contextual cues from surrounding words to refine the embedding of the words. These first three layers are applied to both the query and context.
** 4. Attention Flow Layer couples the query and context vectors and produces a set of query aware feature vectors for each word in the context.
** 5. Modeling Layer employs a Recurrent Neural Network to scan the context.
** 6. Output Layer provides an answer to the query.
=== September 20, 2021 ===
'''Team Meeting Notes:'''
* Market Analysis and Portfolio Optimization
** Prediction-based portfolio optimization model using neural networks
** Support vector machine with adaptive parameters in financial time series forecasting
** Algorithmic Financial Trading with Deep Convolutional Neural Networks: Time Series to Image Conversion Approach
* Image Processing
** Focus on a multi-label image classification problem using the chest x-ray dataset.
** Improve EMADE’s selection methods and mating/mutation process to better handle images with multiple labels.
** NSGA-III and Lexicase as selection methods
** Improve existing features by looking for synergies and packaging them together.
* Modularity
** Extended ARL runs starting off with max depth 10 trees, Everything seems to be working, there exist ARLs with depth > 2.
** Test the significance of the depth of ARLs on the performance of individuals with problems.
* Neural Architecture Search
** Debugging/Reproducing issue with test_swap line 300 EMADE.py
** Modifying the reuse parameter within EMADE.py to allow reuse of all individuals after starting a previously stopped EMADE run
** Adding CIFAR-10 Input Template to EMADE
** Modifying eval_methods.py to check if test_data being input has 90% same classification
** Adding a method for keeping track of NNLearner layer frequency

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Familiarize with the relevant code 
|Completed
|September 20, 2021
|September 27, 2021
|September 22, 2021
|-
|Brainstorm how to effectively implement the query/context data split
|Completed
|September 20, 2021
|September 27, 2021
|September 22, 2021
|}

== Week 04 ==
=== September 15, 2021 ===
'''Sub Team Meeting Notes:'''
* Everyone has been able to get EMADE to run the Amazon dataset on PACE.
* Went over the objectives we needed to figure out to get EMADE to work.
* Decided our objectives would be F1 and number of parameters, to strike a balance between a match/accuracy and complexity. 
** Steven is looking into F1 for QA systems and find out how they worked.
** Karthik created a Google Collab notebook to begin looking at the dataset.
* We would familiarize ourselves with the different layers of a QA system before planning what primitives to make.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Literature review on QA systems
|Completed
|September 13, 2021
|September 20, 2021
|September 15, 2021
|-
|Understand different layers in QA Systems
|Completed
|September 13, 2021
|September 20, 2021
|September 15, 2021
|-
|Complete the [https://drive.google.com/file/d/13bnkVUBPj4NqeRjyic1YlTf10oYrC027/view?usp=sharing Fall 2021 Self Evaluation]
|Completed
|September 13, 2021
|September 20, 2021
|September 13, 2021
|}

=== September 13, 2021 ===
'''Team Meeting Notes:'''
* Market Analysis and Portfolio Optimization
** Stock generalization: Finding models/individuals that perform well on multiple stocks.
** Fundamental Analysis: Using company quarterly balance sheets along with technical analysis to predict stock buy/sells.
** Found empirical study of Genetic Programming generated trading rules in computerized stock trading service system.
* Image Processing
** Add new primitives to EMADE that can help with image processing tasks.
** Explore how can use autoML for either image classification or object detection problems.
** Further paper research in fundamental analysis.
* Modularity
** Explore runs using stock data
** Explore left off work from last semester including New Models, Selection Method, Diversity Measures, etc.
* Neural Architecture Search
** Presented background information about the subteam and topics of neuroevolution.
** Reviewed the top 6 ideas plan to work on in order to improve neural architecture search in EMADE.

'''Individual Progress'''
* Attempted to solve MySQL connection issue on Campus.
** Attempted to reconfigure Port number.
** Attempted to solve through commnon error codes (table credits to Steven Leone).
* Resolved the connection issue after reconnect to Campus VPN.
{| class="wikitable"
!Index
!Error Title
!Cause of Error
!How to Resolve
|-
|1
|Server won't start
|Port is likely in use via submitted job or terminal
|qstat or lsof -i:Port# , then “qdel ID” or “kill Port#” (respectively)
|-
|2
|Access Denied
|It's likely that this is a new database created, or permissions weren't granted correctly.
|Re-grant privileges, specify user address
|-
|3
|Can't Connect to MySQL
|MySQL may be running in the wrong manner (not a terminal, not a submitted job)
|Ensure proper server address (job or from terminal)
|}

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Continue to configure EMADE on PACE ICE
|Completed
|September 8, 2021
|September 13, 2021
|September 13, 2021
|-
|Literature review on QA systems
|Completed
|September 13, 2021
|September 20, 2021
|September 15, 2021
|-
|Understand different layers in QA Systems
|Completed
|September 13, 2021
|September 20, 2021
|September 15, 2021
|}

== Week 03 ==
=== September 8, 2021 ===
''' Sub Team Meeting Notes:'''
* Steven gave a presentation on how QA systems work, producing probabilities for where the answer in a given paragraph starts and stops
* Outlined steps for our problem with Question Answering Systems
** Setup everyone on EMADE/PACE
** Work with dataset, make it work on EMADE
** Implement primitives and infrastructure to make Question Answering problems work with EMADE
** Collect and analyze run results
* Decided on our goal as using EMADE to look for less complex, yet as accurate Neural Architecture for state of the art Question Answering Systems, similar to how BORT was made as a less complex BERT: https://arxiv.org/abs/2010.10499

'''Individual Progress'''
* Familiarized with Natural Language Processing and Question and Answering (QA) Systems.
* Attempt to Setup EMADE on PACE ICE based on [https://github.gatech.edu/emade/emade/wiki/Guide-to-Using-PACE-ICE Guide to Using PACE ICE] and Cameron Whaley's [https://www.youtube.com/watch?v=LashYCCJF3E PACE ICE Setup] video.
** Getting On PACE - Complete
** Transferring EMADE with SCP - Complete
** Setting Up MySQL - Complete
** Setting Up EMADE & Configuring the XML file - Complete
** Launch MySQL - Error
*** Experience mysql working only under campus “VPN” instead of physically being on campus (resolved through campus VPN)
** Launch EMADE on PACE - Error
*** Experience Import error: keras.backend.tensorflow_backend (resolved through re-setup)
*** Experience Disk Space Full issue: 15GB/15GB (resolved by delete all library and re-setup)
* Reflection on [https://arxiv.org/abs/2010.10499 Optimal Subarchitecture Extraction For BERT]
** By applying state-of-the-art algorithmic techniques, they extract the optimal subarchitecture set for the family of BERT-like architectures, as parametrized by their depth, number of attention heads, and sizes of the hidden and intermediate layer.
** They showed that this model is smaller, faster and more efficient to pre-train, and able to outperform nearly every other member of the family across a wide variety of NLU tasks.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Setup EMADE on PACE ICE
|Completed
|September 8, 2021
|September 13, 2021
|September 13, 2021
|-
|Run the Amazon Dataset on EMADE in PACE ICE
|Completed
|September 8, 2021
|September 13, 2021
|September 13, 2021
|-
|Read related literatures on QA Systems
|Completed
|September 8, 2021
|September 13, 2021
|September 10, 2021
|}

=== September 6, 2021 ===
'''Team Meeting Notes:'''
* Labor Day. No meetings.

== Week 02 ==
=== August 30, 2021 ===
'''Team Meeting Notes:'''
* Assigned to the Natural Language Processing Sub Team
* Steven reiterate the brainstorming ideas. Devan suggested adding more primitives for more than embeddings.
* Decided on Wednesday at 2:00 pm EST on Bluejeans for Weekly Sub Team Meetings

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Join the NLP Fall 2021 Slack Channel
|Completed
|August 30, 2021
|August 30, 2021
|August 30, 2021
|-
|Complete Sub Team Rankings on Canvas
|Completed
|August 29, 2021
|August 30, 2021
|August 29, 2021
|}


== Week 01 ==
=== August 26, 2021 ===
''' Sub Team Meeting Notes'''
* Portfolio Optimization and Stocks Team Brainstorming Session
** Research methods of visualizing technical indicators in neural net/CNN
** Extend on last semester’s work and focus on determining most useful technical indicators and EMADE configuration
** Add fundamental analysis methods to EMADE implementation
** Switch gears to Portfolio Optimization
* Natural Language Processing Brainstorming Session
** NLP team worked on researching why individuals produced by EMADE weren’t very complex.
** Add infrastructure to solve more complicated NLP or CV problems (potentially question answering or machine translation) by taking advantage of the NAS team's progress in the same branch.
* Infrastructure Brainstorming Session
** Containerization
** Container Orchestration
** Caching
** Database Improvements
** Message/Work Queues
** AI Optimization

'''Individual Progress'''
* [https://www.researchgate.net/publication/324802031_Algorithmic_Financial_Trading_with_Deep_Convolutional_Neural_Networks_Time_Series_to_Image_Conversion_Approach Algorithmic Financial Trading with Deep Convolutional Neural Networks: Time Series to Image Conversion Approach]
** They propose an algorithmic trading model CNN-TA using a 2-D Convolutional Neural Network based on image processing properties. 
** In order to convert ﬁnancial time series into 2-D images, 15 different technical indicators with different parameter selections are utilized. 
** Each indicator instance gen-erates data for a 15 day period.
** The results indicate that when compared with the Buy & Hold Strategy andother common trading systems over a long out-of-sample period, the trained model providesbetter results for stocks and ETFs.
* [https://ieeexplore.ieee.org/abstract/document/9095246 Searching Better Architectures for Neural Machine Translation]
** Neural architecture search (NAS) has played important roles in the evolution of neural architectures.
** They propose a gradient-based NAS algorithm for NMT, which automatically discovers architectures with better performances. They search the network operations (e.g., LSTM, CNN, self-attention etc) as well as dropout rates to ensure better results.
** They show that with reasonable resources it is possible to discover novel neural network architectures for NMT, which achieve consistently better performances than Transformer, the state-of-the-art NMT model, across different tasks. 
** They discovered architectures could obtain  great improvement over Transformer baselines. 
** They empirically verify that the discovered model on one task can be transferred to other tasks.
* [http://proceedings.mlr.press/v97/so19a.html The Evolved Transformer]
** Apply NAS to search for a better alternative to the Transformer.
** They construct a large search space inspired by the recent advances in feed-forward sequence models and then run evolutionary architecture search with warm starting by seeding initial population with the Transformer
** They develop the Progressive Dynamic Hurdles method, which allows to dynamically allocate more resources to more promising candidate models.
** The Evolved Transformer found in experiments demonstrates consistent improvement over the Transformer on WMT 2014 English-German, WMT 2014 English-French, WMT 2014 English-Czech and LM1B.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Join Brainstorming meetings
|Completed
|August 26, 2021
|August 26, 2021
|August 26, 2021
|-
|Read literatures in brainstorming sessions
|Completed
|August 26, 2021
|August 30, 2021
|August 26, 2021
|}

=== August 23, 2021 ===
'''Team Meeting Notes:'''
* Discussed potential ideas and directions for the new subteams.
** Stock Portfolio Optimization
** Natural Language Processing(NLP)
** Neural Architecture Search(NAS)
** Modularity
** EZCGP
** Interpretability
** Image Processing
** Genetic Fundamentals
** COVID
** Infrastructure
* Voted on the interest of different subteams.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Access VIP Notebook on GitHub Wiki
|Completed
|August 23, 2021
|August 30, 2021
|August 23, 2021
|-
|Join Brainstorming Channels on Slack
|Completed
|August 23, 2021
|August 30, 2021
|August 24, 2021
|}


