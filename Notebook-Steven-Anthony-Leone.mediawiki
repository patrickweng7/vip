== Team Member ==
<b>Team Member:</b> Steven Leone <br>
<b> Major: </b> Computer Science <br>
<b>Email:  </b>sleone6@gatech.edu <br>
<b>Cell Phone:</b> (412)-378-7253 <br>
<b>Interests:</b> Machine Learning, Natural Language Processing, Software Engineering, Algorithms <br>
<b>Sub Team:</b> NLP-NN

= Fall 2021 =

Notebook for this semester can be found at: https://github.gatech.edu/emade/emade/wiki/Steven-Anthony-Leone-Notebook---Part-2

= Spring 2021 =

Notebook for this semester can be found at: https://github.gatech.edu/emade/emade/wiki/Steven-Leone-Spring-2021-Notebook

== MySQL Common Errors Table==
The most common MySQL issues we've run into on PACE are documented below. Use the index to look for a more thorough explanation below.

{| class="wikitable"
!Index
!Error Title
!Cause of Error
!How to Resolve
|-
|1
|Server won't start
|Port is likely in use via submitted job or terminal
|qstat or lsof -i:Port# , then “qdel ID” or “kill Port#” (respectively)
|-
|2
|Malformed Credentials
|Password is stored incorrectly in MySQL due to version discrepancies
|SET PASSWORD FOR 'gt_username'@'%' = PASSWORD('password');
|-
|3
|Access Denied
|It's likely that this is a new database created, or permissions weren't granted correctly.
|Re-grant privileges, specify user address
|-
|4
|Can't Connect to MySQL
|MySQL may be running in the wrong manner (not a terminal, not a submitted job)
|Ensure proper server address (job or from terminal)
|-
|5
|2003 MySQL cannot connect
|PACE has too many users; the host address has changed for MySQL
|After starting mysql, run qstat -n to find the name of the host it’s running on, and swap it in input.xml
|}

===Further Elaboration on Error Descriptions===
* 1) From PACE this semester, I've learned that MySQL can run either as a submitted job to PACE or on the terminal, by opening up another terminal window and running it from the /usr tab. If you're attempting to run MySQL and it won't start, then it's likely that it's already running one one of these methods (the one you're not trying to use). To remove a submitted job, run "qstat" at the terminal, and delete the index of the job that's described as "mysql-job", and you are also listed as the owner with your GTID with that same job. To remove from the terminal, run lost -I:PORT#. If MySQL is running on PORT#, you will see a PID#. run "Kill PID#".
* 2) This error is caused by discrepancies between versions of MySQL, where passwords are stored differently. To update, simply run the command using the root user inside of MySQL.
* 3) Permissions issues often result, especially when specificity is not defined. I recommend granting privileges both to yourself using wildcards, and your specific addresses the error specifies. For example, if it says "Access denied for sleone6@localhost", grant all privileges to "sleone6@localhost", as opposed to just using the wildcard. Theoretically, wildcards should mean you don't have to do this, but often times this is the exact case.
* 4) Similar to #1, but this is often the fault of specifying the wrong server address in the input.xml file. A submitted job normally runs on atl1-1-02-012-5-l, while running from the terminal runs on localhost. Ensure this is properly specified in the <server> tag of the input xml file.
* 5) This often happens when too many users are connected to PACE. Often times, the server address runs on an address like atl1-02-012-5-r instead of atl1-02-012-5-l. To find the right address, run "qstat -n", and the address of your MySQL job will be listed.


= Fall 2020 =

==Week 16: November 30th, 2020==

===Meeting Notes:===
* Stocks
** Had a slower week
** Most work from here on out should be for next semester.
* EZGCP
** Finishing for Presentation
** Getting good visualizations for Wednesday
* NLP/NN
** Getting results in, having some difficulties with PACE
** Running out of disk space on PACE
** Seeing if PACE will consistently obtain runs
** Amazon may be delayed to next semester
* Modularity
** Polishing up Presentation

=== Sub Team Meeting ===
* Lots of Errors with PACE, difficulty with running out of memory
* Continued issues setting up the environment for EMADE-NN-VIP branch for Amazon; gave the developed XML File, data, and Preprocessing script to the rest of the team on the slack to check for accuracy.

=== Final Presentations ===
* Stocks
** Goal: look at how EMADE can help trading AI.
** Base research paper had inconsistencies.
** Wrote technical indicator primitives, including Bollinger bands, CCI, and MFI
** In the future, planning on running new on normalized data, testing more granular data, and finding more literature that is consistent.
* NLP/NN
** We presented
* EZGCP
** OpenCV primitives for preprocessing, involved a lot of testing and debugging
** Fitness or Objective Scores
** Added functionality for genome seeding, saved individuals with best scores
* Modularity
** ARL's are giving more influential and important information for the problem.
** However, there is no statistically significant improvement.

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Resolved
!Due Date
|-
|Rehearse Final Presentation
|Complete
|11/30/2020
|12/02/2020
|12/02/2020
|-
|Clean Up Notebook
|Complete
|11/30/2020
|12/02/2020
|12/03/2020
|-
|Successfully use Amazon Data Set on EMADE
|In Progress
|11/09/2020
|NEXT SEMESTER
|PUSHED TO NEXT SEMESTER
|}

==== Amazon Data Set ====
* Conda Environment Issues have persisted despite numerous attempted fixes.
* I attempted to use the file given by Pulak to create the right environment to run the EMADE-nn-vip branch. However, this encountered the same pitfalls as before.
* I posted all files regarding the Amazon Data Set, including the XML, train.txt, and Preprocess.py files, on the Slack; someone with the environment already set up will be able to see if it runs correctly.
==Week 15: November 23rd, 2020==

===Meeting Notes:===
* Stock Sub Team
** Accomplished a lot
** Went over findings to find function needed to calculate score
** Moved a bit towards their own research
* EZGCP
** Prioritizing baseline runs
** Obtained some individuals (1 test run for 8 hours)
** Evolution was greedy; going from 20 individuals to 400.
* NLP
** Still focusing on getting results in runs
** 2 baseline runs for each dataset: chest and toxicity
* Modularity
** Need to do more analysis why changes are happening
** Especially generations w/ consistent abnormal generations across runs

=== Sub Team Meeting ===
* We are prioritizing baseline runs.
* We divided up people to do some more baseline EMADE runs on datasets.

=== Friday Meeting ===
* A lot of people are having issues running EMADE on PACE.
* Amazon input files are done, but there is trouble setting up the environment.
* Pulak gave me a .yml file to make the Conda environment with; I will be trying this over the weekend.

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Resolved
!Due Date
|-
|Successfully Use Amazon dataset on Emade
|In Progress
|11/09/2020
|
|12/02/2020
|}

==== Amazon Data Set ====
* Attempting to Run Amazon Dataset with Conda and issues encountered.
* First I attempted to run with the same environment as the master branch on EMADE. However, after many module not found errors, I found out that it required a whole different set of libraries and versions.
* First I attempted to create a Conda environment by running "conda env create -f emade-env.yml". However, this had a "ResolvePackageNotFound" error; this means it could not find packages for any of the libraries at the specified versions listed, as shown below.
* First, I tried to solve this using pip; I used pip-install to attempt to install libraries that could not be found with Conda. However, pip was also unable to find some of these libraries.
* I then proceeded to attempt running EMADE without these libraries; as expected, it could not continue due to a dependency on these libraries, such as specific versions of Keras.pickle_wrapper.
* Then I tried to create an environment from other files from the condaEnvironments folder, like py36_win10.yml and Emade_env.txt. However, these files ran into the same issues.
* I proceeded to attempt on my windows machine, meeting similar errors. I posted in the slack, and was given another file someone used to create their environment on icehammer; this was also unsuccessful and I found myself running into the same errors.
* I decided to wait until Friday to debug, so that I could get live feedback on it.

==Week 14: November 16th, 2020==

===Meeting Notes:===
* Focusing on getting runs in
* Able to use Pace now; this means it should be easier to get runs in

=== Sub Team Meeting ===
* Amazon data set is progressing; meeting on Thursday should give more progress

=== Thursday Meeting w/ Chris ===
* I met with Chris on this day to decide how we could use both of our efforts to further progress on the Amazon Dataset.
* After showing him my progress on the XML and preprocessing, we decided that we needed a better way of dealing with non-utf8 characters than simply removing those examples.
* We delegated work up as follows:
** Chris would work on finding a way that would allow us to deal with non-utf8 characters without having to lose those examples.
** I would continue working on the XML file and attempting to run Amazon, while Chris would work towards creatively dealing with non-utf8 characters.
=== Friday Meeting ===
* Amazon data set is progressing
* Data has been saved as 5 folds in test and train files
* XML file is mostly written; wikidetox dataset was looked at as a guide
* Unsure where functions for toxicityFitness, geneToxicity, and parentToxicity are; will need to find these.
* Answer: these are just the names that outputs are labelled as; they can be customized without writing any new functions.
* Trying to get Pace setup with runs
* If we have access to ice hammer, we should be running on it

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Resolved
!Due Date
|-
|Write Amazon XML File
|Complete
|11/09/2020
|11/20/2020
|12/02/2020
|-
|Successfully Use Amazon dataset on Emade
|In Progress
|11/09/2020
|
|12/02/2020
|}

==== Amazon Data Set ====
* The final component required to run Amazon on Emade was the XML file, which specified specifics about the run and the formatting of the data.
* I, again, was able to look upon previous work for help in creating these files. However, the XML file was different for this branch than the master branch, as was used with the titanic. For example, the outdated acronym GTMOEP was replaced with EMADE throughout the code and files; this required evaluation functions to be renamed. 
* Rather than directly copying fields from the toxicity XML, I tried to look into each one to find out what they meant. Most of the changes I made in creating the Amazon XML were in changing the probabilities of mutating and mating functions to give them at least close to uniform probabilities.
==Week 13: November 9th, 2020==

===Meeting Notes:===
* Code Base is in a Work in Progress Stage
* Targeting finishing development by next week
* CV Sub-team got data from 8 out of 15 classes; not generalizing well.
* For now, it was put on hold for this reason.
* Sentiment Analysis on Amazon Reviews is ongoing (my task)

=== Sub Team Meeting ===
* EMADE mostly successful on PACE.
* Will have to begin prioritizing runs next week.

=== Friday Meeting ===
* Development of new features is mostly wrapping up.
* CV Sub-team still has issues with bounded box.
* Amazon data has been mostly pre-processed.

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Resolved
!Due Date
|-
|Write folds of clean Amazon data
|Complete
|11/09/2020
|11/13/2020
|12/02/2020
|-
|Write Amazon XML File
|In Progress
|11/09/2020
|
|12/02/2020
|-
|Successfully run EMADE w/ Amazon
|In Progress
|11/09/2020
|
|12/02/2020
|}

==== Amazon Data Set ====
* I looked at the toxicity and titanic datasets to determine how the data files should be formatted.
* I decided to use the K=5 fold used with the titanic dataset to test performance.
* This required using scikit's folding.
* I added onto the Preprocess.py script, and looked up the methods used by the preprocessing toxicity file for reference. Most of them could be reused for Amazon.
* I ran the script, originally using np.savetxt(). However, the format of the outputted txt files was different. I then realized that the variable assignments in the toxicity preprocessing actually wrote the final files, and adjusted my script to write in this format.
* Running the Preprocessing script required 15-20 minutes due to the sheer volume (approximately 3 million) of the dataset.

==Week 12: November 2nd, 2020==

===Meeting Notes:===
* Monday Meeting: was spent acquainting first Semester Students
* Friday: was an extension of that
* Returning Students: set a code deadline of 2 weeks from now.
* Last week reserved for experimenting and results.

=== Sub Team Meeting ===
* My task: Run EMADE with the Amazon Sentiment Analysis Dataset
* https://www.kaggle.com/bittlingmayer/amazonreviews
* Progression on other features is running smoothly.

=== Friday Meeting ===
* Informed team of progress on Amazon Reviews Dataset
* Final format to run in EMADE involves ".csv.gz" files, with the number of sentiment in a cell next to the textual data.
* Text should remain text; word embedding algorithms like GLOVE (and eventually BERT) will be used by EMADE.
* Toxicity dataset should be similar enough to use as a baseline, though there are some differences (more even, roughly50/50 y-value distribution in this dataset).

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Resolved
!Due Date
|-
|Find Dataset
|Completed
|10/30/2020
|11/01/2020
|11/02/2020
|-
|Discover what using EMADE with Amazon Dataset Requires
|In Progress
|11/02/2020
|11/06/2020
|11/06/2020
|}

==== Amazon Data Set ====
* I began attempting to make the Sentiment Classification on Amazon Reviews Dataset run on EMADE: https://www.kaggle.com/bittlingmayer/amazonreviews
* Downloads didn't seem to open properly on Windows, so I downloaded on Mac, saved as a text file, and emailed it.
* Some characters in the dataset aren't UTF-8; I've put in a try-catch til I fix that. Thus, it currently removes these examples from the final dataset.
* Successfully converted data to a working format, with pairs of format sentence first, then sentiment number after it.
* Ex: ["Amazing Book!", "1"]
* Next steps will be putting this in a format that EMADE will work on. Currently unsure if I have to obtain word embeddings before EMADE starts running, or if that happens during; will have to check on this.

==Week 11: October 26th, 2020==

===Meeting Notes:===
* Bootcamp has ended; will be meeting on Mondays now instead.
* My Sub Team Assignment: NN (formerly NLP)
* Last Week, Stocks Team had a dataset that yielded a loss.
* EZGCP has no huge developments; implementation team updated their branch recently
* NLP (slash NN) decided what tasks to assign first semester students, wants to run on Chess dataset on EMADE this week.
* Modularity: thought of first semester student tasks

=== Sub Team Meeting ===
* Creating NNLearner, which constructs neural architecture using LSTM layers, linear layers, etc.
* Currently Comparing to paper, have a deadline of end of semester.
* First Semester Task: reading literature, test individuals, find datasets, or Get EMADE working on PACE
* Meeting Friday, 4:30-5:30

=== Friday Meeting ===
* Meeting is focused on assigning first years to tasks
* Options
** Attempt to get EMADE to run on PACE (tried previously by other team members, not successfully though)
** Look for new Datasets to run EMADE with
** Join returning students in their tasks
* Maxim will be working on PACE.
* John will be working with CV returning students.
* I will be working on finding a new dataset.
* Chris will also work on finding a new dataset.

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Resolved
!Due Date
|-
|Read Paper
|Completed
|10/26/2020
|10/29/2020
|10/30/2020
|}

==Week 10: October 21st, 2020==

===Lecture Notes:===
* Presentation Day: All groups presented on what they worked on
* Stocks
** Market Analysis and Optimization
** Overview: First semester of a subgroup attacking the stocks problem. The goal was to optimize stock trading methods.
** Used Genetic Programming, and researched current Machine Learning Techniques to combine to run the problem.
** Regression isn't something that had been done much before.
** To Implement, they ran EMADE with Regression and Collab.
** Implemented Regression Primitives in EMADE
** Results: Had values that gained money, but were short of the paper's results
** In the future, would like Longer EMADE runs with 5+ workers.
** Even longer term, LSTMs and RNN's being implemented as EMADE primitives would be a goal, in addition to looking into other time windows and exploring other preprocessing.
** First Semester students who join will be researching Technical Indicators from Papers.
* Bootcamp Group 3
** Will be comparing Titanic results from EMADE, GP, and ML.
** Preprocessing: Dropped Name and Ticket Columns
** Machine Learning: used MLP NN, Logistic Regression, Random Forest, SVM, Decision Tree, and KNN. Neural Networks performed the best on the Pareto Front.
** Applied Genetic Programming, using Strongly typed GP, aiming to minimize FNR and FPR.
** Used Google Cloud MySQL instead of personal routers.
** Emade was very clustered toward optimizing both of the results rather than exploring the entire Pareto Front.
* Modularity
** Exploring ways to abstract parts of individuals.
** This allows for creating "building blocks" that can help with genetic functions.
** Has explored Koza Automatically Defined Functions, Angeline AR, and Rosca ARL (currently working on).
** ARL -> Adaptive Representation Through Learning. A function that is dynamically evolved and can be called by other functions.
** An ARL might improve a population's overall fitness, allows a population to converge faster, and has other benefits.
** Also used the Titanic Dataset: 40 generations, ten trials.
** Current work: Refactoring ARL Arguments (didn't allow data manipulating primitives), Experimented limiting what ARL's could be created (made a check that ARL's must directly impact data so that only useful ARLs were made), and Experimenting with new selection methods (fixing the arguments, directly increased probability of getting ARLS).
** Also tried a dataset with more complexity: Handwritten Digits. This would likely make more ARL's. 
** First Semester students who join will be running and designing experiments with their ideas.
* Bootcamp Group 1
** Will be also using Titanic problem to compare results from EMADE, GP, and ML.
** Used Random Forest, Naive Bayes, Logistic Regression, and a Multilayer Perceptron Classifier.
** GP AUC: 0.277
** Ran 17 Generations, 311 final valid individuals; ended with 235 individuals on the Pareto Front.
** Emade used 5-fold cross validation on the titanic training data.
** Emade had the best accuracy, but balanced less overall.
* Natural Language Processing
** Work done mostly falls under NN as well.
** Motivation and Goal: Apply evolutionary approach to neural architecture search using EMADE.
** Compares to paper "Evolutionary Neural AutoML for Deep Learning"
** Uses Toxicity Dataset and Chest X-ray Dataset
** NNLearner: different layers are primitives within EMADE's tree structure.
** Example learner after 182 generations: NNLearner(Arg0...), had embedding layers and output layers.
** Adaptive Mutating Functions.
** Potential Next Steps; Adding more Primitives.
** Wikidetox Dataset: Uneven dataset of toxic vs non-toxic dataset.
** In the future, they want to incorporate prertrained language models into EMADE (BERT, ELMo).
** Meet 4:30 on Fridays.
** Join if you have GPU/AWS Access.
* Bootcamp Group 2
** Tested 8-9 ML Models with AUC = 0.21245.
** The Decision Tree performed the best.
** Had two EMADE Runs: Headless chicken "Zany" run and one with the initial Hyperparameters.
* EZGCP
** Is a graph based structure instead of tree based.
** Uses a DAG of Primitives.
* Bootcamp Group 4 -> we presented our findings comparing results on the Titanic Dataset from EMADE, GP, and ML.

=== Action Items: ===
None for this week; next week we'll receive sub team assignments.

==Week 9: October 14th, 2020==

===Lecture Notes:===
* Worked on progress on EMADE

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Resolved
!Due Date
|-
|Emade Assignment
|Complete
|Sep 30th, 2020
|October 20th, 2020
|Oct 21st, 2020
|}

=== EMADE Assignment: ===
* Friday Meeting: Having figured out how to run EMADE successfully, we started our final run. We set a deadline of Sunday for the run.
* Sunday Meeting: We met, created our presentation, and worked on understanding our EMADE results.

==Week 8: October 7th, 2020==

===Lecture Notes:===
* Worked on progress on EMADE
* DEAP may need it's version changed.
* Assignment: 
** 1 person should have the sql server set up and act as the master process, the rest should connect their workers.
** Run for a substantial number of generations, just as in the last assignment.
** Make a plot of your non dominated frontier at the end of the run, compare with ML and MOGP assignments.
** Make any other plots and figures to show your analysis of EMADE running on the Titanic problem, see if you can find some successful trees
** This assignment will have a presentation on Monday the 21st, compare all 3 results.

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Resolved
!Due Date
|-
|Emade Assignment
|In Progress
|Sep 30th, 2020
|
|Oct 21st, 2020
|}

=== EMADE Assignment: ===
* Environment has been setup successfully, in an environment named "emade1"
* Tried several times to set up and install MySQL and MariaDB; each ended in failure.
* Resulting Error was a "PID" could not connect.
* As this is a reoccuring issue on my macbook, I switched to a slower windows machine, and resetup the "emade1" environment.
* Setting up MariaDB worked on this windows laptop. However, running EMADE required several hours.
* The next step was meeting with team members to compare results and form the presentation.
* Via texting chats, we decided that Thursday, October 15th 2020 was a good time to meet to start doing this.



==Week 7: September 30th, 2020==

===Lecture Notes:===
* EMADE Lecture
** EMADE = "Evolutionary Multi-objective Algorithm Design Engine".
** It combines a multi-objective evolutionary search with high-level primitives to automate the process of designing machine learning algorithms.
** Emade Steps:
*** Configure a mysql server on your machine (5.6 or 5.7, no MySQL 8; else MariahDB)
*** download and install git-lfs
*** Run the setup module to install the package
** Running Emade
*** Start at top level directory python3 src/GPFramework/launchGTMOEP.py templates/input_titanic.xml
*** Input file: xml file that configures EMADE:
**** First one configures python (will be python 3 on my machine)
**** Second and Third, Grid and Slurm, do not matter when running locally.
*** dbConfig
**** Username and password will be for MySQL; user should have full permissions on the database specified
*** Datasets: specifies the train and test files; Emade folds it 5 times.
*** Objectives:
**** Specifies minimization (-1) or maximization (1)
**** evaluationFunction specifies which method from GPFramework/evalFunctions.py should be used
**** Achievable and goal are used for steering the optimization, lower and upper are used for bounding.
*** workersPerHost: Keep this small on a laptop (specifies how many evaluations to run in parallel).
*** Evolution Parameters:
**** All of these control the various hyperparameters that affect the evolutionary process.
** Emade Output
*** Best place to observe outputs: MySQL databases
*** You can connect from the command line using Mysql -h hostname -u username -p
*** You can select a database with "use database_name"
*** Basic SQL to interact with it, using "*" as the wildcard
*** Good Progression on the Pareto Front takes hours (successful evaluations should be within first 20 minutes)
* Assignment: 
** 1 person should have the sql server set up and act as the master process, the rest should connect their workers.
** Run for a substantial number of generations, just as in the last assignment.
** Make a plot of your non dominated frontier at the end of the run, compare with ML and MOGP assignments.
** Make any other plots and figures to show your analysis of EMADE running on the Titanic problem, see if you can find some successful trees
** This assignment will have a presentation on Monday the 21st, compare all 3 results.

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Resolved
!Due Date
|-
|Peer Evaluations
|Completed
|Sep 30th, 2020
|Oct 1st, 2020
|Oct 2nd, 2020
|-
|Emade Assignment
|In Progress
|Sep 30th, 2020
|
|Oct 21st, 2020
|}

=== EMADE Assignment: ===
* Note: OpenCV must still be installed from last week, as there are compatibility issues with Python 3.8.
* MySQL 5.7 was downloaded and installed. It had to be done by right clicking and opening, as Apple could not verify the software had no malware (due to it being an older version of MySQL).
* The Installation was completed successfully.
* Upon further investigation of the OpenCV issue, I found that a solution was to create a different environment with Anaconda and set its version to Python 3.6 .
* The above solution worked; the environment was named "py3" 
* Next, libraries were installed with pip buy running the command "pip install xgboost lmfit multiprocess hmmlearn deap opencv-python". 
* Next, the command "bash reinstall.sh" was run in order to build all the required files. 

==Week 6: September 23rd, 2020==

=== '''Lecture Notes:''' ===
* Presented Titanic Powerpoint
* One Hot Vectors are used to encode categorical data. They help avoid implying that there is a numeric relationship between the data.
** For example, encoding "Woman" as 1 and "Man" as 0 would imply that the category "Man" is somehow one less than "Woman".

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Resolved
!Due Date
|-
|Download EMADE
|Complete 
|Sep 23rd, 2020
|Sep 23rd, 2020
|Sep 30th, 2020
|}

=== Download EMADE Notes: ===
* Instructions were followed from https://github.gatech.edu/emade/emade
* Git and Git LFS were installed.
* The command "git config --global credential.helper cache" was run to reduce login prompts.
* EMADE was then cloned using "git clone https://github.gatech.edu/emade/emade"
* Anaconda was installed for Python3
* Opening a new directory, EMADE was CD'ed into. Conda was used to setup the environment.
* There was an error found; Python 3.8.3 was too high and incompatible.
* To resolve this, I first attempted to find a more recent version of Anaconda to install.
* With further investigation, I realized this was an issue with Python 3.8 and OpenCV being incompatible. 

== Week 5: September 16th, 2020 ==

=== '''Lecture Notes:''' ===
* Group Project: Titanic classifier with Genetic Program
* Submit one csv file this time, as a team
* Add powerpoint to team page
* Present Power Point on next Wednesday

=== Team Meeting: ===
* Planned on Monday Meeting to create classifier

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Resolved
!Due Date
|-
|Titanic GP Group Project
|Complete 
|Sep 16th, 2020
|Sep 21st, 2020
|Sep 23rd, 2020
|-
|Prepare to Present Titanic Project
|Complete
|Sep 21st, 2020
|Sep 22nd, 2020
|Sep 23rd, 2020
|}

=== Monday Meeting: ===
* Titanic GP Group Project
* Data was parsed and transformed in the same manner as in the previous dataset
* Our primitive set was created, and had most primitive operations from the Numpy library added, including addition, subtraction, multiplication, division, and comparison logic and trigonometry functions.
* We defined success as minimizing our False Positive and False Negative Rates, and wrote an evaluation function to provide a fitness based on this criteria.
* Using 300 individuals per generation, we eventually obtained a pareto dominant set. This was then compared to the Machine Learning Models.
* In short, the GP Titanic Model worked much better, having lower false positive and false negative rates and a smaller area under the curve for the pareto front.

== Week 4: September 9th, 2020 ==

=== '''Lecture Notes:''' ===
* TODO: Notebook Self assessment
** Rubric is on Notebooks page
* First Group Assignment
** Use Titanic Data Set on Kaggle
** Group 4
** Use Scikit to obtain models
** Documentation: scikit-learn.org
** Task: Define a common set of feature data
** Each member will come up with a model and score it
** Ensure all members of team have co-dominant solutions
** The example model is very naive with great accuracy: this is because of the binary status of the classification, and the 3 quarters survival rate of the Titanic

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Resolved
!Due Date
|-
|Group Project
|Complete 
|Sep 9th, 2020
|Sep 16th, 2020
|Sep 16th, 2020
|-
|Notebook Self Grade Rubric
|Complete
|Sep 9th, 2020
|Sep 13th, 2020
|Sep 16th, 2020
|}

=== Notebook Assessment: ===
* I reviewed my Notebook this week, made minor cleans to it, and scored myself based on the provided rubric. It is displayed below.
* The primary areas to improve are adding teammate names (these were recently assigned), adding some justifications and reflections to my work, providing references, and usability as a reference.

=== Group Activity: Titanic Data Set ===
* Part 1: Individual
** First, the data was downloaded off of Kaggle:
*** gender_submission.csv
*** test.csv
*** train.csv
** Then, the necessary libraries (numpy, scikit, pandas) were all imported.
** The train and test files were parsed using pandas
** Cleaning up the data was then influenced by teammates. Using peer recommendations, some columns were dropped, including Cabin, Age, Embarked, and Ticket.
** Finally, the data was split up with a 70-30 split for validation and training data.
** Using this data, a Support Vector Machine Classifier was trained. Using default hyperparameters, the validation data was tested with 66.79% accuracy.
** Using scikit's metrics, the following confusion matrix was generated:
** Next, a grid search was run with the SVM in order to obtain the best possible hyperparameters.
***  Best Hyperparameters: {'C': 2, 'coef0': 0, 'decision_function_shape': 'ovo', 'gamma': 'auto', 'kernel': 'rbf'}
***  Final Accuracy: 0.65
** Unfortunately, the results were not good for such a binary classification problem, receiving only a 65% accuracy on the final run with the test data. Note that this is barely better than a function that always returns the same exact classification, as it would achieve about 50%. Thus, a random forest was used and received the same procedure, grid searching with a stratified 3 fold.
*** RESULTS: {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'min_impurity_decrease': 0, 'n_estimators': 25}
*** SCORE: 0.8426
** These results were much better and yielded 82.46% accuracy. Therefore, they were used in part 2.

*Part 2: Group Collaboration
** As a team, we then came together to compare our results. Our team used random forests, neural networks, and knn.
** We plotted our points on a pareto frontier, and discovered that only one point was off the frontier.
** The model that was off of the Pareto frontier was revised until it appeared back on. Our results are shown below.


== Week 3: September 2nd, 2020 ==

=== '''Lecture Notes:''' ===
*  Topic "Multiple Objectives": The MO in MOGA and MOGP
*  Objective: Recognize the power of multiple objective optimization, understand Pareto dominance and using it to affect mating probability, understand classification terms
*  Analogy: What we look for in a mate is different for each individual person, and has multiple criteria
*  How do we do this for algorithms, using a vector of scores for multiple objectives?
*  In Binary Classification, everything is a 1 or 0
**  True Positive -TP: How often are we identifying the desired object?
**  False Positive - FP: How often are we identifying something else as the desired object?
**  False Negative - FN: We failed to classify a desired object as the desired object
**  True Negative - TN: We successfully did not classify something else as the desired object
*  Classification Measures
**  We have Positive Samples and Negative Samples (all true states)
**  We put this data into an algorithm called the classifier
**  We obtain results and can put them in a confusion matrix: https://en.wikipedia.org/wiki/Confusion_matrix
**  TPR = Hit Rate or Recall = TP/(TP+FN) = TP/P
**  TNR = Specificity = TN/(TN+FP) = TN/N
**  False Negative Rate = FN/(TP+FN) (Measure of Error) = 1 - TPR
**  False Positive Rate (Fallout) = FPR = TN/(FP+TN) (Measure of Error) = 1 - TNR
**  Precision or Positive Predictive Value = TP/(FP + FP)
**  False Discovery Rate = FP/(TP + FP)
*  Objective Space
**  Objective Scores give each individual a point in objective space (the scatter plot)
**  This may be referred to as the phenotype of the individual
**  Pareto Optimality: an Individual is Pareto if there is no other individual in the population that outperforms the individual on all objectives
**  The set of all Pareto individuals is known as the Pareto Frontier
**  We still want to ensure non Pareto points still have some probability of mating
*  Algorithms that use Pareto Optimality
**  NSGA II: Nondominated Sorting Genetic Algorithm II
***  Population is separated into non domination ranks (Pareto Frontier is Rank 0, rank behind that is 1, etc.)
***  We then use a tournament to select. Ties are broken by crowding distance (Higher wins). You reward individuals with neighbors far away with this, and this is desirable as this part of the Pareto Frontier is covered less.
**  Strength Pareto Evolutionary Algorithm (SPEA2)
***  An individual's S (strength) is how many others in the population it dominates
***  An individual's R (rank) is the sum of the strengths of the individuals that dominate it (Pareto Frontier has rank 0)
***  A distance to the kth nearest neighbor is calculated and a fitness of R+1/(kth distance + 2) is obtained
=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Resolved
!Due Date
|-
|Lab 2 Part 2
|Completed
|Sep 2nd, 2020
|Sep 7th,2020
|Sep 9th, 2020
|-
|Email Python and ML skills
|Completed
|Sep 2nd, 2020
|Sep 2nd, 2020
|Sep 2nd, 2020
|}
=== Lab 2 (Part 2): ===
* Multi Objective Programming
* Created new individuals and a new fitness with the creator
** Used multiple weights (-1, -1) for the fitness this time, as there are multiple objectives
* Initialized primitives (addition, subtraction, sin, cos, etc.) 
* Reinitialized toolbox functions, and declared a new evaluation function that handles multiple objectives
* Defined a function to compare two individuals, and see if one is dominant over the other, and visualize the objective space
* Initialized 300 random individuals 
* Compared Individuals to a single chosen Individual, and mapped them in the Objective Space:
* The blue point was compared to all the other points; it dominated green points and was dominated by the red.
* This visualization makes sense, as the goal is to minimize tree size and have the lowest MSE. The most dominant individuals are towards the bottom.
* After this, the main evolutionary algorithm was defined, and run. The best individual was negative(cos(multiply(add(cos(sin(cos(multiply(add(cos(cos(x)), cos(add(multiply(x, x), sin(x)))), tan(x))))), cos(x)), tan(x)))). It had a fitness of (0.27530582924947056, 25.0), which is still fairly minimal for tree size and MSE. The graph below shows the average and minimums for each of the generations
* Red and orange are tree size, blue and green are MSE. While MSE clearly drops very quickly, the tree size seems to steadily increase.
* As this graph only shows progress with the objectives and not the objective space, another graph was made that shows the Pareto Frontier, and how individuals fall among it. 
* Area Under the Curve of the Pareto Frontier is used to measure the performance of the pareto frontier. The current area under the curve is approximately 2.46.
* In order to lower the area curve by at least 25% to at least 1.84, a new genetic program was implemented. Initially, number of generations was increased, but this seemed to only increase area under the curve (up to twice as much). Other hyperparameters were then tuned; lowering MUTPB to be below 0.01 was quite effective and produced an area under the curve of approximately 0.702.

== Week 2: August 26th, 2020 ==

=== '''Lecture Notes:''' ===
*  For Future Reference: Write about what I learned in Labs in the Notebook, and what I actually did
*  Tree Representation
**  Nodes are primitives and represent functions
**  Leaves are called terminals and represent parameter
*  Lisp preordered parse tree.
**  You have your operator followed by operands
*  Crossover: exchanging subtrees
**  Randomly pick a point in the tree, and exchange these subtrees
*  Mutation: includes inserting, deleting, or changing a node or subtree
*  We can evaluate a Tree with Error as sum of actual - expected squared
=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Resolved
!Due Date
|-
|Lab 2 Part 1
|Completed
|Aug 26, 2020
|Aug 27, 2020
|Sep 2nd, 2020
|}
===Lab 2 (Part 1):===
* Genetic Programming
* Created Individuals
** Inherit from DEAP's PrimitiveTree
** Set operators as addition, subtraction, multiplication, and division for 2 operands
** Added numpy's exp2 and tan functions, with arity 1.
* Created toolbox with population made up of these defined individuals
* Defined evaluation function as mean square error between the function and goal function
* Added several mutations, including "mutShrink", which replaces a branch with one of its arguments
* Set constraints on tree heights to 17 in mutating and mating functions
* Ran 40 Generations
** Best individual was "multiply(exp2(multiply(x, x)), add(multiply(x, x), x)), (0.04644464850190968,)"

* By the 40th Generation, the function was quite well replicated, as shown by the graph, with almost zero error.
* Most actually reached this point around the 10th Generation.

== Week 1: August 19th, 2020 ==

=== '''Lecture Notes:''' ===
* Note books are important
* Genetic Algorithms: Create populations that reproduce, selecting the best individuals from each batch.

=== Action Items: ===
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Date Resolved
!Due Date
|-
|Join VIP Slack group
|Completed
|Aug 19, 2020
|Aug 19, 2020
|Aug 26, 2020
|-
|Install Jupiter Notebook
|Completed
|Aug 19, 2020
|Aug 19, 2020
|Aug 26, 2020
|-
|Review class lecture slides
|Completed
|Aug 19, 2020
|Aug 20, 2020
|Aug 26, 2020
|-
|Lab 1
|Completed
|Aug 19, 2020
|Aug 19, 2020
|Aug 26,  2020
|}
===Lab 1:===

Learned how to use Jupiter Notebook, create Genetic Algorithms to solve the all 1's problem and n queen's
* For the N Queen's Problem, the Best Individual was [10, 15, 19, 9, 0, 8, 18, 11, 1, 6, 17, 3, 2, 13, 5, 16, 14, 12, 7, 4] after 100 generations. The Progression over generations is shown below. Note that the minimum was actually reached well before the 100 individuals, and the objective was to minimize the fitness. Therefore, 100 generations was likely not entirely necessary, as you can tell by the near completely horizontal slope of all lines after around generation 40.
* The all 1's problem didn't quite converge to the final best possible result, and had a best individual of  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1].
* Increasing the generations would have likely solved this problem, as it would inevitably fill in the extra 1 eventually.

