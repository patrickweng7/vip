== Team Member ==
Team Member: Temi Ogunsanya

Email: togunsanya3@gatech.edu
Cell Phone: 646-301-2180

Interests: Machine Learning, Gaming, Chess, Anime

== September 9th, 2021 ==
* Had first team meeting for the image classification team
* decided that we would all research and return with a set of papers to discuss.

* https://arxiv.org/pdf/1808.03818.pdf
** This paper describes doing neural architecture search to create CNNs on the CIFAR10 and CIFAR100 data set
** Dataset: https://www.cs.toronto.edu/~kriz/cifar.html
** Compared state of the art manually tuned networks against automatically generated models
* https://ieeexplore.ieee.org/abstract/document/4798001
** Medical image segmentation using GA study 
** A couple of approaches can be taken, including optimizing a statistical formula or optimizing parameters for an RNN
** The paper discusses 5 different approaches that can be taken with links to relevant implementations

Image segmentation would likely be more difficult than a pure classification problem. Ideally the training set would be something more involved than the MNIST data set. 



== August 30th, 2021 ==
* Posted in Image Classification interest about an interesting classification paper
https://dl.acm.org/doi/abs/10.1016/j.jvcir.2016.11.017

* This paper describes using GA to optimize a radial basis function for image classification problems. This paper was scoped directly to flood detection, which resulted in a flooded, non-flooded assessment. Could likely be expanded to a similar problem set such as tumor detection

A radial basis function is a simple NN that uses a radial basis function as an activation function. This is a simple 3 layer network with input nodes, inner activation functions and an output node layer. 

genetic algorithms can be used to optimize center vector placement, which is typically manually chosen after analyzing the data. 

* Attended the second group meeting
* placed my team ranking:

1. Image Classification
2. Stocks
3. NLP

* Asssigned to the Image Classification team. Put in meeting date rankings. 

== April 30th, 2021 ==
'''Team Meeting Notes:'''
* Final presentation for the semester
* Lots of interesting results from all the teams
* Seems like many of the sub-teams made good progress.
* Stock team added some additional technical indicators to their model


'''Personal Notes:'''
* Presented slides during the presentation.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Present final presentation
|Complete
|April 30th, 2021
|
|April 30th, 2021
|}

== April 27th, 2021 ==
'''Team Meeting Notes:'''
* Met with team to provide feedback

'''Personal Notes:'''
* Worked on updates to include hyper-parameter optimization
* Did not have much luck in generating models with this method
* Primarily failures during the PACE runs
* Discussed with the team and plan to continue the work next semester
* Worked on some visualizations and provided PACE runs with Cameron Whaleys commit

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete peer evaluation
|Complete
|April 19th, 2021
|
|April 24th, 2021
|-
|Work on slides for presentation
|Complete
|April 19th, 2021
|
|April 26th, 2021
|-
|Present to team
|Complete
|April 24th, 2021
|
|April 27th, 2021
|}


== April 19th, 2021 ==
'''Team Meeting Notes:'''
* Met with team to give update on progress

'''Personal Notes:'''
* Do a deep dive into the code base. Look into increases the hyper-parameter primitives used to generate individuals
* Could we improve results by taking more active control of the hyper-parameters. 
** For example, the LSTM learner uses a lot of set values, could we improve results of the models by increasing the primitive size and making the individuals more tune-able. 
** Do some investigation and read other papers that focused on hyper-parameter optimization using GA 


'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Read other relevant literature
|Complete
|April 19th, 2021
|
|April 24th, 2021
|-
|Deep dive code base
|Complete
|April 19th, 2021
|
|April 23th, 2021
|}

== April 12th, 2021 ==
'''Team Meeting Notes:'''
* Met with the team and provided updates
* discussed the setup of my PACE environment and the run I did

'''Personal Notes:'''
* After PACE environment was set up did a couple of runs in the environment with the base setup.
* None of the individuals I set up were able to perform better than the seed set. 
* Read **Genetic algorithm for neural network architecture optimization** by Janati Idrissi et. al
** The paper discussed attempting to use neural architecture search on the classic data-iris data set
** The individuals were generated with primitives based around the number of hidden layers and the number of nodes within that layer for an ANN.
** the GA was a multi-objective optimization where the both architecture size and MSE were minimized. 
** 20 tests were run with 20 distinct random weight values, with a learning rate of 0.1 and a momentum of 0.6
** the models that were generated performed with an accuracy of between 97.78% and 96.66%

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Do PACE Run
|Complete
|April 12th, 2021
|
|April 15th, 2021
|-
|Finish PACE Setup
|Complete
|April 12th, 2021
|
|April 14th, 2021
|-
|Study relevant literature
|Complete
|April 13th, 2021
|
|April 15th, 2021
|-
|}


== April 7th, 2021 ==
'''Team Meeting Notes:'''
* Presentation on NLP and team. Discussed the main classes that were used to generate individuals and the logic used for creating the genetic algorithm


'''Personal Notes:'''
* Watched a video by one of the other team members on how to setup PACE
* Walked though some of the process. Ran into some issues on establishing a connection

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Start PACE setup
|Complete
|April 4th, 2021
|
|April 7th, 2021
|}

== March 29th, 2021 ==
'''Team Meeting Notes:'''
* Met with NLP team for the first time. 
* Provided introductions and explained everyone's past experience with ML and neural architecture optimization


'''Personal Meeting Notes:'''
* Pulled the local branch of EMADE
* attempt to familiarize myself with the code-base, and try and get an understanding of team objective and goals.
* From reading through meetings notes, determined the objective was primarily on utilizing EMADE to generate novel neural architectures that performed better than the networks
individuals were seeded with.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up runtime environment
|Complete
|May 29th, 2021
|
|April 2nd, 2021
|}

== March 22nd, 2021 ==
'''Team Meeting Notes:'''
* Team meeting presentation

* Stocks
** Discussed what technical indicators they used and why they chose those technical indicators
** Discussed the results of some of their individuals, discussed how some models performed better on some tickers which is to be expected.
** discussed future work in adding more indicators

* ezCGP
** Discussed the system and explained graph based architecture

* NLP
** Discussed some difficulty in getting significant individuals during their runs
** explained the use of the Amazon data set and how they were attempting to generate models for sentiment analysis 

'''Personal Meeting Notes:'''
* Presented slides that I worked on 
* Placed team selection
** decided to pick the NLP team and stock team. Was placed

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Present results to team
|Complete
|May 22nd, 2021
|
|May 22nd, 2021
|}

== March 17th, 2021 ==
'''Team Meeting Notes:'''
* reviewed emade runs, reviewed individuals. 
* Found the results were interesting and in some cases better than the MOGP individuals.
* Discussed presentation. 

'''Personal Meeting Notes:'''
* Refactored MOGP output to represent final values in terms of false positive/False negative rate instead of absolute values
* Worked on slides for the presentation in the up-coming week. 

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with team to create presentation
|Complete
|March 13th, 2021
|
|March 17th, 2021
|}

== March 5th, 2021 ==
'''Team Meeting Notes:'''
* Met with sub-team to set-up mySQL server
* Was able to connect as a worker node to Justin. 

'''Personal Meeting Notes:'''
* Set-up a node worker and ran a EMADE run on the test set.
* Ran into a lot of build issues setting up EMADE. Mainly due to the necessary packages not being setup in my environment. 

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up MYSQL
|Complete
|March 1st, 2021
|
|March 5th, 2021
|-
|Do first EMADE run
|Complete
|March 2nd, 2021
|
|March 5th, 2021
|}

== February 24th, 2021 ==
'''Team Meeting Notes:'''
* Sub team presentation date.
* interesting to note how other team approached the problem
* for the most part a lot of other teams used NSAG2

'''Personal Meeting Notes:'''
* Pulled EMADE package locally.
* Downloaded MySQL server and attempted to set up connection

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Download EMADE, and MYSQL Server
|Complete
|Feb 20th, 2021
|
|Feb 25th, 2021
|}

== February 21th, 2021 ==
'''Team Meeting Notes:'''
* hour long meeting to discuss presentation
* Split up the slides between the team members

'''Personal Notes:'''
* Worked on the slides describing evolution algorithms for the presentation. 
* Filled out information based on the work that was done and discussed the training algorithm. 
* The final algo received a percentage accuracy of  83.1%.

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with team to plan presentation
|Complete
|Feb 21th, 2021
|
|Feb 21th, 2021
|-
|Complete presentation slides
|Complete
|Feb 21th, 2021
|
|Feb 22th, 2021
|}

== February 19th, 2021 ==
'''Team Meeting Notes:'''
* 3 hour team meeting to discuss MOGP
* Tested various selection algos

'''Personal Notes:'''
* After team meeting I made some modifications to the MOGP algorithm to retain original population as well as mutated population
** This was based after some posts I read online and it allowed us to reduce the AOC of our final generation
** it was interesting to note that the final population essentially ended up being the pareto front, as we were always carrying strong candidates and avoiding mutation them away.
** Potential that this may have limited genetic diversity, but overall it seemed to improve our results
* Used a combination of NSGA2 and Tournament-Selection as the selection algorithm. Tournament selection initially, which was mainly used as a sorting algorithm, this helped that stronger individuals
were crossed over with other strong individuals. NSGA2 was the final selection algorithm used to prune for the next generation. 

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with team to work MOGP
|Complete
|Feb 17th, 2021
|
|Feb 19th, 2021
|-
|Read up on MOGP and best selection algorithm to use
|Complete
|Feb 17th, 2021
|
|Feb 20th, 2021
|}

Image of the output pareto front after the combination of my additions with the rest of the teams.

[[files/Temi_Pareto_front_2.png]]

== February 17th, 2021 ==
'''Team Meeting Notes:'''
* Main goal for this week was to meet to implement a GA on the titanic data set
* Used gp.mutUniform for the mutation algorithm
* Used onePoint for the mating algorithm. 
* The selection algorithm we chose was selNSGA2. https://pymoo.org/algorithms/nsga2.html We chose this algorithm because it was better optimized for multi-attribute selection than tournament selection
* The best algoirthm overall was interesting:
 safe_division(Fare, cos(negative(safe_division(add(add(add(tan(power(Parch, SibSp)), Parch), multiply(add(subtract(multiply(Embarked, cos(sin(Fare))), Sex), safe_division(Embarked, Embarked)), cos(sin(safe_division(add(multiply(add(subtract(Pclass, sin(tan(Fare))), safe_division(SibSp, Parch)), add(subtract(Pclass, Sex), safe_division(SibSp, cos(power(power(SibSp, Pclass), sin(tan(SibSp))))))), SibSp), multiply(cos(sin(add(subtract(Pclass, Sex), safe_division(Sex, Sex)))), negative(SibSp))))))), SibSp), add(Embarked, Embarked))))))
** Which was found after 200 generations. It focused on optimizing one  false negatives, and the downside of getting (0, 309) most of the positive cases wrong.
*The best observed accuracy was 82%. Slightly better than the accuracy achieved by my random forest implementation
*Contrary to my initial assumption. Combining all the results and taking the median of the pareto front did not achieve a higher accuracy, as the result was only 75%.
*There is a likelihood that taking an aggregate of the pareto front would result in more consistent results across multiple large data sets.
*As we were minimizing un-normalized FN and FP, the area under the curve was 9964.[[files/Pareto front temi.png|left|thumb]]

'''Personal Notes:'''
* Most of the work was done with the team this week
* Did some optimizations of the data set personally. This included normalizing some data including fares. Updating the genetic algorithm to select the best individuals from the combination of the offspring set and the parents
* Since the selection size was always equal to the population size initially, we would never prune bad individuals. Rather the selection process would just end up sorting the population.
* By making some tweaks I was able to see a steady improvement over time, as initially the pareto front was stumbled upon. and the final generation was randomly spattered.
* With the tweaks, the hall of fame was the final generation and for the most part the strong individuals who got carried on over time. 

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with team to work on GA titanic
|Complete
|Feb 17th, 2021
|
|Feb 20th, 2021
|-
|Work on slides for presentation
|Complete
|Feb 20th, 2021
|
|Feb 22th, 2021
|}

== February 10, 2021 ==
'''Team Meeting Notes:'''
* Met with team to discuss titantic data set and how we would pre-process it.
* Decided to keep data set pre-processing mainly the same. Opted to one-hot encode cabin and gender because we believed it would provide better results
* Would work on models individually then sync before the wednesday meeting.
* 
* On the meeting on Monday before the Wednesday meeting we graphed out the averages and the FP and FN rates of our models. 
* After a bit of tuning we were able to determine we had a pareto optimal set
* 
'''Personal Notes:'''
* Worked on setting my model for the titanic data set
* After a bit of research, found that boosting would probably provide good results.
* Used the adaboost library from sciKit-learn. Experimented with a number of different input classifiers and get the best results with boosted RandomForests. 
 clf = AdaBoostClassifier(base_estimator=RandomForestClassifier(), algorithm="SAMME", n_estimators=100)
[[files/Confusion matrix temi.png|left|frameless]]
[[files/Confusion matrix titanic temi.pdf|frameless]]

* The model above achieved an accuracy of around 81%. The confusion matrix can be seen above. 

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Meet with team to discuss titanic project
|Complete
|Feb 10th, 2021
|
|Feb 12th, 2021
|-
|Pick Model type for data set
|Complete
|Feb 10th, 2021
|
|Feb 13th, 2021
|-
|Tune data set to ensure Pareto optimal
|Complete
|Feb 13th, 2021
|
|Feb 15th, 2021
|}

== February 3, 2021 ==
'''Team Meeting Notes:'''
* Third lecture was about multiple objective optimization/
* Gene pool much like the name implies is that set of features that are evaluated in a given generated.
* Individuals have specific genotypic descriptions, and the search space is the set of all possible genomes
* Each individual is evaluted using an objective function, MSE, Cost, TPR, FPR
* An individual is Pareto if no other individual outperforms it
* It follows logically that we derive evolution based on individuals who are pareto. This is known as the Pareto frontier. 
* 
'''Personal Notes:'''
* Completed the second portion of the GP lab
* The lab defines a pareto dominance function in order to rank individuals
* 
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Third Lab Assignment
|Complete
|Feb 3rd, 2021
|
|Feb 6th, 2021
|}

== January 27, 2021 ==
'''Team Meeting Notes:'''
* Second team meeting pertaining to genetic programming
* Focus on the individual being the function itself
* Program can be represented as a tree structure.
* Nodes:
** Primitives and represent functions 
*Leaves:
**Terminals and represent parameters
*Use pre-order tree-traversal in order to determine the parse tree
*Cross over is the process of exchanging subtrees
**These are exchanged to produce children
*Mutation involves inserting a node or subtree or deleting/changing a node

'''Personal Notes:'''
* Completed Lab 2 in emade
* Added np.divide and np.power as primitives.
* Added the nodeReplacement mutation.
* Best individual is <code>add(multiply(add(multiply(multiply(x, x), x), multiply(x, x)), x), add(x, multiply(x, x)))</code>
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete second lab
|Complete
|Jan 27, 2021
|
|Jan 30th, 2021
|}
== January 20, 2021 ==
'''Team Meeting Notes:'''
* First group lecture for new teammates.
AutoML based on biologically inspired genetic algorithms 
* Each new individual is created through mating/mutation based on the previous population. 

* Individual: Specific Candidate in the population
* Population: Group of individuals whose properties will be altered
* Objective: a value used to characterize individuals that you are trying to maximize or minimize
* Fitness: Relative place in comparison to the rest of the population.
* Evaluation: How the fitness is computed.  

Selection:
* Represents survival of the fittest. Gives preference to better individuals: 
    - Fitness Proportionate: Generates offsprings based on the probability of the fitness level
    - Tournament: Several tournaments among individuals. Winners are selected for mating. 
Mating
* Crossover: Take random n parts from parents and mix.
* Mutation: Change random parts of the child to maintain diversity.

'''Personal Notes:'''
* Completed lab1 for an intro to genetic algorithm
* Explored how GA can be used to solve the n queens problem 
* Read about different types of mutation strategies, such as '''Non-Uniform, Gaussian and Flip Bit''' https://en.wikipedia.org/wiki/Mutation_(genetic_algorithm)
** Flip bit: takes a random genome and flips a bit
** Shrink adds a random number taken from the distribution
** uniform takes the chosen gene with a uniform random value
* Looked into how GA can be applied to different problems. For example: time-series application can be achieved through use of Autoregressive  Integrated  Moving  Average (ARIMA)(https://corporatefinanceinstitute.com/resources/knowledge/other/autoregressive-integrated-moving-average-arima/). The GA can be used to optimize the 3 inputs in the ARIMA model:  '''(''p, d, q'')'''  which are used to estimate future value based on past value. 
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete intro lab
|Complete
|Jan 20, 2021
|
|Jan 24th, 2021
|}