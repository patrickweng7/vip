== About Me ==
'''<big>Team Member: Tushna Eduljee</big>'''

'''<big>Major: Computer Science</big>'''

'''<big>Year: Junior</big>'''

'''<big>Email: teduljee3@gatech.edu</big>'''

'''<big>Cell Phone: 704-450-1947</big>'''



='''<big>Third Semester</big>'''=

Midterm Notebook Self Evaluation:
[[files/Self Eval.png|none|thumb|658x658px|Completed Self Evaluation Form]]


== ''' December 2nd, 2020 [Week 47]''' ==

'''General Meeting Notes:'''
* We completed the presentations!
* NLP NN presentation https://docs.google.com/presentation/d/1Z8PjzaP_LjhyycpGSeXX7rIHdVYkxHzqg8vaKVEYhdU/edit#slide=id.gaec8ee0d3b_1_3
* Finish up notebooks
** Write specific to-dos that are actually actionable tasks
** For each of these tasks you did, include pictures, code snippets, etc., to show you did the work
** Emphasize not only team work but also your individual contributions
** Someone in a later semester should be able to replicate what you did, using your notebook

* Stocks Market Analysis and Portfolio Optimization:
** Used a reserach paper in order to measure EMADE successfulness on the same (or simiilar) task
** Created new Technical Indicator Primitives in EMADE
*** Paper's technical indicators:
**** Stochastic Oscillator
**** Relative Strength Indicator
**** William's R%
**** Simple Moving Average
**** Exponential Moving Average
**** Moving Average Convergence and Divergence
**** Ease of movement
***** Distance Moved
***** Box Ratio
*** Team's primitives:
**** On-Balance Volume
***** Price is up intra-day -> add volume to OBV
***** Price is down intra-day -> subtract volume to OBV
***** Volumes over 3-14 days (cumulative)
***** The more days its tracks the better it becomes
**** Bollinger Bands
***** A movement KPI
**** Commodity Channel Indicator
***** Indicate trend prices vs average prices over time
***** Used to pinpoint areas where stocks are bought in surplus vs sold in surplus
**** Money Flow Index
***** Volume is key
*****
** Output of paper vs EMADE runs done by team
*** They believe some fluke gave them more accurate predictions
*** Paper Results (Predicted: 24.287%, Truth: 8.647%)
*** EMADE Results (Predicted: 5.053%, Truth: 12.314%)[[files/A1.png|center|thumb]][[files/A2.png|center|thumb]]
*** Next semester
**** Normalize the data and try again
**** Test larger test ranges + more granular data
**** Increase genetic labeling
* EzCGP team
** Incorporating CV primitives into runs for preprocessing
*** Filtering
*** Thresholding
*** Blurring
*** Normalize
** Instead of using F1 score + accuracy to guage performance, now using precision
** In terms of PACE issues:
** When deciding between Conda install vs Pip installing for '''<u>tensorflow</u>''', they thought pip install was better based off of their personal experience
** EzCGP team results up against the CIFAR10 dataset
*** Data split used: 60:20:20; training, validation, testing
*** Ran for 9 generations (at a rate of roughly 6 hours per generation)
**** Best individual:
***** F1 Score - 0.97
***** Precision - 0.98
***** Recall - 0.97
**** Gen 1 AUC: 0.921
**** Gen 9 AUC: 0.982
**** [[files/A3.png|center|thumb]]
** Further efforts into research within EZCGP include:
*** Blocking experiments (MNIST dataset)
**** Aiming to affect convergence of a population to competitive fitnesses
*** Aging Evolution
**** Pruning old individuals to allow them to "die"
***** Increased diversity in individuals
***** Hopeful on if it will increase results
*** Construct reference curves from previously produced curves
** Future Goals
*** Researching new mating methods
*** More blocking experiments
*** More experiments on MNIST dataset
*** "Marrying" EZCGP and EMADE
**** I'm unsure what this means. Maybe ask?
**** Dr. Zutty touched on this in his questions and clarified!
*** Continue neural architecture search experiment
* NLP NN
** Slides that I created + presented:
** [[files/C12.png|center|thumb]] [[files/C13.png|center|thumb]] [[files/C14.png|center|thumb]]
** CV team overall's slides
** [[files/D1.png|center|thumb|464x464px]]
* [[files/D2.png|center|thumb|470x470px]]
[[files/D3.png|center|thumb|482x482px]]
[[files/D4.png|center|thumb|523x523px]]
* Modularity
** Creating something called "ARLs"
** These are abstract primitives that group other primitives together as "building blocks" such that they can't be segmented apart for further generations
** They are essentially combinations of primitives into a single node as a new individual
** ARLs are selected based on frequency and distribution
** Individual broken down into possible candidates and then abstract those components into a single node which is now known as an ARL
*** Selected based on cumulative distribution from frequency and fitness
*** I had  question regarding if this is similar to "pruning" in decision trees, and asked it! Dr. Zutty and the team both clarified some concepts. Rather than pruning (which happens after the fact), this is preemptive. but similar!
** ARLs are selected based on frequency and distribution
** Titanic dataset results
*** Conducted runs on the titanic dataset
*** Planning on incorporating MNIST dataset here
*** Aimed to minimize FP & FN
*** Began seeded runs with 5 individuals, capped at 40 generations
** Team conducted numerous experiments
*** Experiment 1
**** Difference as the fitness of itself and most fit parent (good child = positive value)
**** For some reason, there is some noticable statistical significance around generations 16 to 19
**** As generations continued, this converged once again
*** Experiment 2
**** Linearly scaled selections for tournament with ARLs
**** Statistical significance around generations 11 to 19
**** As generations continued, this converged once again
*** Experiment 3
**** Worked with ARL creation using datapair fix
**** If they don't take in a data pair split (split of individuals), then it's not a valid ARL
**** Statistical significance not found
*** Experiment 4 (combination of 2 and 3)
**** Worked on to reducing bloat with added data pair restrictions
**** As an individual genearlly has only up to a couple ARLs, it doensn't experience bloat of its own
**** Bad results that could have been due to small sample size
*** MNIST dataset experiments (to see how ARLs perform on a non-trivial dataset, unlike Titanic)
**** In comparison to Titanic, it was much slower
**** Ran for 49 generations
**** Really good results acheived
**** These shoudl be taken with a grain of salt as it could be a result of overfitting
** Planning on:
*** Combining efforts with ARLs (Adaptive Representation through Learning) and ADFs (Automatically Defined Functions)
*** Aim to allow ARLs to evolve as well, by adding more complexity to the creation process
'''Final Subteam Meeting Notes:'''
* We discussed our future lines of work!
* This is my third and final semester of VIP and thus I had to leave behind my future correspondence and let everyone who is continuing know what work needed to be done to further the efforts I made with new primitives this semester
* In regards to all three of the new primitives created, unit testing and corroborations of usefulness must be done before the primitives are put into full use! The later is also super important because there are inputs that need to be tweaked and tried by EMADE and this is time consuming so these primitives must only be put into production if there is some use found for them and they are actually improving the CV task.
''''''My Action Items + Progress:''''''

Ideas for future semester:

Testing the 3 new CV primitives I added this semester

New primitive ideas to pursue
* Adaptive thresholding [in EMADE now]
* Altering colorspaces [not in EMADE]
* Image processing: Cropping [unsure]
* Maximum thresholding [unsure]
* Hysteresis [unsure]
* Hough circle or line transforms [not in EMADE]
* Edge detection [exists]

Pushed my code:

https://github.gatech.edu/emade/emade/commit/640b591ef16fe4caeb7208bc2ac5909e61123f47

Tusheet pushed Otsu's Binarization that he coded:

https://github.gatech.edu/tgoli3/emade/commit/03164e5bbd21d477f5fc95d2eaa1c6e456b1a025#diff-021b05f8d8765cb9660464e3de776995
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete notebook for the week
|Completed
|December 2, 2020
|December 3, 2020
|December 3, 2020
|-
|Complete the presentation
|Completed
|December 2, 2020
|December 3, 2020
|December 3, 2020
|-
|Rehearse the presentation
|Completed 
|December 2, 2020
|December 3, 2020
|December 3, 2020
|-
|Give the presentation
|Completed
|December 2, 2020
|December 2, 2020
|December 2, 2020
|-
|Push code to EMADE nlp-vip branch with my 3 new primitives
|Completed
|December 2, 2020
|December 2, 2020
|December 2, 2020
|-
|Testing the 3 new CV primitives I coded this semester
|Pending
| -
| -
| -
|-
|Adding more primitives that could enhance our particular task within CV (ie. chest xray classification)
|Pending
| -
| -
| -
|}

== ''' November 30th, 2020 [Week 46]''' ==

'''General Meeting Notes:'''
* Complete final peer evaluations
* Final notebook submissions due soon
* Final presentaitons on December 2 (6-9pm)
'''Monday Subteam Meeting Notes:'''
* Final meeting before end of semester presentations
* Asked any last important questions
* Stocks
** Showed final results from EMADE runs
** Working on analyzing these for final presentaiont
* EzCGP
** In the process of obtaining final runs
** Completing up their presentation + adding these baseline runs^
* NLP
** Finishing touches to the slides being placed: https://docs.google.com/presentation/d/1Z8PjzaP_LjhyycpGSeXX7rIHdVYkxHzqg8vaKVEYhdU/edit#slide=id.p
** CV team working on completing documentation, adding final touches to the new primitives and some baseline testing (not unit testing, which will be reserved for next semester)
** I will be presenting the infromation regarding all of the new primitives work that I've headed this semester. Along with the primitives that I've researched and coded, Tusheet will be talking about Otsu's binarization, which he coded, and Jon will be talking about the documentations efforts that he made this semester
* Modularity
** Have multiple experiements for which they are finalizing their results and analysis
** Presented their presentation's dry run in the general meeting
**
'''Friday Subteam Meeting Notes:'''
* All of our runs are completed and results gathered
* Simply focusing on the delivery of the information now, and showing to the class our progress this semester
* We did a dry run through the presentation and made sure everyone was comfortable with the slides that they were doing
* I'm finishing up the coding of the primitives and now am just working on delivery of the information regarding my work this semester

''''''My Action Items + Progress:''''''

After sending the proposal document to Dr. Zutty, I received some clarification on the three questions on the proposal document

(1) These 3 primitives are new to EMADE, correct? Any thoughts on them?

Yes! Except Otsu's binarization which is in some capacity in spatial methods.

Action taken: we will implement 2d Otsu's, which applies the same method as the 1D implementation but at a local level instead of global. Basically the combination of Otsu and adaptive 

thresholding.

(2) This primitive has some extra input params that need to be set. How would this work with emade? For example, “block size”, which decides the size of the neighbourhood area. Do we just set some default value or does EMADE have the capability to test various neighborhood values.

We can always expose parameters to your helper functions when you register the primitive. EMADE will then be able to mutate or crossover the values.

(3) These go in spatial methods, right?

Yes

I began coding the primitives from here. Tusheet was able to take on coding Otsu's Binarization, and I coded Adaptive Mean Thresholding and Adaptive Gaussian Thresholding.

I pushed the code to the nlp-vip branch in spatial methods.

Jon also added all 3 primitives to the documentation!

[[files/C11.png|center|thumb]]

Finally, I worked on completing my presentation slides regarding all of our progress this semester and the new CV primitives that I headed making.

https://docs.google.com/presentation/d/1Z8PjzaP_LjhyycpGSeXX7rIHdVYkxHzqg8vaKVEYhdU/edit#slide=id.gaecbff40f4_1_1

[[files/C12.png|center|thumb]]
[[files/C13.png|center|thumb]]
[[files/C14.png|center|thumb]]

I also wrote and rehearsed a script for what I would say for the final rpesentation:

[[files/C15.png|center|thumb|741x741px]]

Finally, the last couple steps before wrapping up this semester would be:

(1) Confirming whether I have exposing parameters to EMADE correctly when making my primitivies (emailed Zutty)

(2) Completed notebook

(3) Make sure I passed on all information needed to Jon and rest of the team, so that my efforts could continue next semester (beginning with unit testing)

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete notebook for the week
|Completed
|November 30, 2020
|December 2, 2020
|November 30, 2020
|-
|Brainstorming my to-do list for how to go forward with this new primitives task
|Completed
|October 26, 2020
|November 2, 2020
|October 26, 2020
|-
|Finding out more about CV primitives in general (how are they even being stacked up in EMADE)
|Completed
|October 26, 2020
|November 2, 2020
|October 26, 2020
|-
|Ask professor Zutty about some primitive ideas that I have
|Completed
|November 16, 2020
|November 23, 2020
|November 16, 2020
|-
|Learning about the CV primitives in EMADE
|Completed
|November 16, 2020
|November 23, 2020
|November 16, 2020
|-
|Looking for gaps in the preexisting EMADE CV primitives
|Completed
|November 16, 2020
|December 2, 2020
|November 23, 2020
|-
|Monitor the new documentation efforts with Jon + answer questions as they come up
|Completed
|November 16, 2020
|November 23, 2020
|November 23, 2020
|-
|Observing the Chest XRay runs to see how they could be improved
|Completed
|November 16, 2020
|December 2, 2020
|November 23, 2020
|-
|Getting some ideas for primitives that would be good additions
|Completed
|November 23, 2020
|December 2, 2020
|November 23, 2020
|-
|Confirming that I'm not missing anything with Dr. Zutty (getting approval)
|Completed
|November 23, 2020
|December 2, 2020
|November 23, 2020
|-
|Ask Professor Zutty about how EMADE varies input parameters
|Completed
|November 23, 2020
|December 2, 2020
|November 23, 2020
|-
|Coding the primitives
|Completed
|November 23, 2020
|December 2, 2020
|November 30, 2020
|-
|Work on finishing the presentation
|Completed
|November 30, 2020
|December 2, 2020
|November 32, 2020
|-
|Prctice presentaiont slides
|Completed
|November 30, 2020
|December 2, 2020
|November 35, 2020
|-
|Testing (likely next semester, considering how much I have to teach myself to accomplish this)
| -
| -
|December 2, 2020
| -
|}

== ''' November 23rd, 2020 [Week 45]''' ==

'''General Meeting Notes:'''
* Make sure to check my midterm grades so I can use that feedback to improve for next time
* Final notebook submissons are coming soon
* December 2 will probably be our final presentation
'''Monday Subteam Meeting Notes:'''
* Stocks
** Working on finishing up their new primitives that they've made (technical indicator primitives)
** Will be performing runs with these primitives on EMADE
** Will have to evaluate using "trend score" as this is what is used by the paper they are comparing results with
** Must figure out how to calcualte this
* EzCGP
** Must complete baseline runs before the final presentation, so most people are tryingto get this done ASAP
** Heavy resource usage to get this done: RAM (128 GB), 2 GPUs
** Plan before final presentations:
*** Presentation shell by November 19th
*** Fill slides over the weekend (everyone does their own)
*** Finish slides rough draft by November 23rd
*** Finalize slides and have rough runthrough done by November 30th
* NLP NN
** Obtain the results for both datasets ASAP so analysis and conclusions can be done before final presentation
*** Using both ice hammer + pace, complete following runs:
**** 2 baseline runs for toxicity
**** 2 baseline runs for chest xray datasets
**** 2 runs with neural networks and all primitives
**** 2 runs with all mating and mutation functions
**** 2 runs with larger population size
** Analysis of chest xray runs must be done (area under curve and comparing to LEAF paper)
** Trial runs with Amazon product review dataset, hopefully can also be done
** Jon has done more work on documentation efforts for the CV team: https://www.notion.so/Computer-Vision-Primitives-6f160347b15c4f3e8c0ccac10b9bc749
** We are chugging along on new primitives work
** Our final results repo: https://github.gatech.edu/pagarwal80/EMADEResults
* Modularity
** Titanic runs
*** Promising results. Expecting significance once more samples are collected
*** ARLs working well
** MNIST runs (must see how well all of these results^ generalize to a more complex dataset)
*** prepared in EMADE to be run but having some issues
*** planning on doing a mock presentation dry run with Dr. Zutty before Dec. 2 (maybe we also should do that?)
'''Friday Subteam Meeting Notes:'''
* Other team members are finalizing their runs and analysing their runs (told them to let me know if any help is needed from my end with the runs
* My work is the new primitives task (https://docs.google.com/document/d/1mOr7D0yCq0v51za8k7MIaVC_EC-6tPOsOEAZgEw9YgM/edit?usp=sharing)
* Adaptive Mean Thresholding:  https://www.tutorialspoint.com/opencv/opencv_adaptive_threshold.htm
**Working on analysing the code[[files/B1.png|center|thumb|600x600px]]
* Adaptive Gaussian Thresholding:  https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html
*[[files/B2.png|center|thumb|598x598px]]
* Otsu's Binarization:  https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html
** Gave Tusheet a couple research papers I skimmed through regarding this topic:
** https://ieeexplore.ieee.org/document/5344078
** https://www.researchgate.net/publication/309543000_A_Robust_2D_Otsu%27s_Thresholding_Method_in_Image_Segmentation
* Jon is working on documentation
** https://www.notion.so/Computer-Vision-Primitives-6f160347b15c4f3e8c0ccac10b9bc749
* Other team members are that they have performed some baseline runs already
* All results and data collection is now on our GitHub repo
* Baseline runs being conducted for Chest X-Ray dataset
* The rest of the team is planning on getting around to these baseline runs Amazon dataset

''''''My Action Items + Progress:''''''

Progress on the documentation is progressing! Jon has been working dilligently on taking my notes as well as his and merging everything into the notion documentation page.

Progress on '''signal methods''' documentation
[[files/Cv6.png|center|thumb|578x578px]]

Progress on '''spatial methods''' documentation
[[files/Cv7.png|center|thumb|575x575px]]

I have also narrowed down the list created last week for new potential primitive ideas:
** '''Adaptive thresholding [not in EMADE]'''
** Altering colorspaces [not in EMADE]
** Image processing: Cropping [unsure]
** Maximum thresholding [unsure]
** Hysteresis [unsure]
** Hough circle or line transforms [not in EMADE]
** Edge detection [exists]

I'll be starting my new primitive efforts with some adaptive thresholding primitives. These will most likely be added to the spatial methods file.

Adaptive thresholding is really good for scanned documents and other non-uniform lighting conditions, and therefore could come in handy for various image datasets where we’re looking to apply EMADE. For example, this is shown in the images below. On the left is the newspaper scan before adaptive thresholding, and the one on the right is after adapative thresholding is applied.
[[files/Cv8.png|center|thumb]]

'''Primitive #1: Adaptive Mean Thresholding'''

cv2.ADAPTIVE_THRESH_MEAN_C : threshold value is the mean of neighbourhood area.

All the thresholding methods currently in spatial_methods.py apply thresholds as global values across the entire input image. This is not as effective in conditions where the image has different lighting conditions in different areas. In this case, adaptive thresholding yields really good results. The algorithm in an adaptive thresholding primitive calculates the threshold for small regions of the image, separately, and gives better results for images of varying illumination.

[[files/Cv10.png|center|thumb]]
[[files/C1.png|center|thumb]]
'''Primitive #2: Adaptive Gaussian Thresholding'''

cv2.ADAPTIVE_THRESH_GAUSSIAN_C : threshold value is the weighted sum of neighbourhood values where weights are a gaussian window.

As mentioned above, adaptive thresholding is really good for scanned documents and other non-uniform lighting conditions, and therefore could come in handy for various image datasets where we’re looking to apply EMADE.
[[files/Cv10.png|center|thumb]]
[[files/C5.png|center|thumb]]

'''Primitive #3: 2D Otsu’s Binarization'''

cv2.THRESH_OTSU: algorithm finds the optimal threshold value

This form of thresholding is much more effective than (it seems) the thresholding methods that are currently in spatial_methods.py and this is because it appears that those thresholding methods are running with a default threshold value. I’m curious if EMADE does any tweaking to that value at all in order to optimize it when it’s applied to images. But if not, I believe Otsu’s binarization thresholding could come very handy for our purposes in the EMADE framework.

In global thresholding, we used an arbitrary value for threshold value and use trial and error to understand if what we selected was a good threshold value or not. But consider a bimodal image (where intensity histogram has two peaks → similar to our scan dataset of chest xrays). For that image, we can approximately take a value in the middle of those peaks as threshold value, which is what Otsu binarization calculates on its own (that ideal threshold). This works really well for images that are bimodal, and thus should give great results over all other thresholding methods for our chest xray dataset which is greyscale, as it can help emphasize the affected ‘white’ areas that are key for classification of certain disease labels.

1D Otsu's appears to already be in some form in spatial_methods.py

The primitive that I am planning on adding is Otsu's 2D implementation. This applies the same method as the 1D implementation but at a local level instead of global. Basically the combination of Otsu and adaptive thresholding.

'''<u>Proposal document</u>'''

Now that I have narrowed down my new primitive ideas, I composed the proposal - along with the questions that I needed to make sure were addressed before beginning implementation - into a document. I need to address the following questions:

(1) These 3 primitives are new to EMADE, correct? Any thoughts on them?

(2) This primitive has some extra input params that need to be set. How would this work with emade? For example, “block size”, which decides the size of the neighbourhood area. Do we just set some default value or does EMADE have the capability to test various neighborhood values.

(3) These go in spatial methods, right?

[[files/C9.png|center|thumb|722x722px]]

The final step before implementation is to now run everything by Dr. Zutty!

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete notebook for the week
|Completed
|November 23, 2020
|November 30, 2020
|November 23, 2020
|-
|Brainstorming my to-do list for how to go forward with this new primitives task
|Completed
|October 26, 2020
|November 2, 2020
|October 26, 2020
|-
|Finding out more about CV primitives in general (how are they even being stacked up in EMADE)
|Completed
|October 26, 2020
|November 2, 2020
|October 26, 2020
|-
|Ask professor Zutty about some primitive ideas that I have
|Completed
|November 16, 2020
|November 23, 2020
|November 16, 2020
|-
|Learning about the CV primitives in EMADE
|Completed
|November 16, 2020
|November 23, 2020
|November 16, 2020
|-
|Looking for gaps in the preexisting EMADE CV primitives
|Completed
|November 16, 2020
|December 2, 2020
|November 23, 2020
|-
|Monitor the new documentation efforts with Jon + answer questions as they come up
|Working on this!
|November 16, 2020
|November 23, 2020
| -
|-
|Observing the Chest XRay runs to see how they could be improved
|Working on this!
|November 16, 2020
|December 2, 2020
| -
|-
|Getting some ideas for primitives that would be good additions
|Working on this!
|November 23, 2020
|December 2, 2020
| -
|-
|Confirming that I'm not missing anything with Dr. Zutty (getting approval)
|Working on this!
|November 23, 2020
|December 2, 2020
| -
|-
|Ask Professor Zutty about how EMADE varies input parameters
|Working on this!
|November 23, 2020
|December 2, 2020
| -
|-
|Help Tusheet get onboarded for his coding task of Otsu's Binarization
|Working on this!
|November 23, 2020
|December 2, 2020
| -
|-
|Coding the primitives
| -
| -
|December 2, 2020
| -
|-
|Testing (likely next semester, considering how much I have to teach myself to accomplish this)
| -
| -
|December 2, 2020
| -
|}

== ''' November 16th, 2020 [Week 44]''' ==

'''General Meeting Notes:'''
* Cannot view midterm grades
* These will be put into canvas SOON!
'''Monday Subteam Meeting Notes:'''
* Stocks
** Working on making sure their primitives are working well and aren'y anomalous
** Working on trend signal calculation as they can't compare to the paper without doing this (and the runs on EMADE will be basically usless because there's nothign to really compare them to)
* EzCGP
** Research team
*** Working on baseline runs
** Practicals team
*** Working on setting up environment to complete runs (ram AND GPU/CPU requirments)
* NLP NN
** Rest of team created documentation for PACE ICE
*** https://www.notion.so/maximgeller/Configuring-EMADE-on-PACE-60aedf065abc445096617c3cec875a11
** Preparing for runs as a good amount are necessary
*** Splitting up jobs between people and between Google Colab, Pace + icehammer
** Where discussing incorporating an additional dataset (would also be a good excercise for those who are new)
*** Amazon product review dataset
*** <nowiki>https://www.kaggle.com/bittlingmayer/amazonreviews</nowiki>
* Modularity
** Doing runs on datapair experiemnt
** testing on the MNIST dataset they're encorporating (outside of titanic dataset which is too simple)
'''Friday Subteam Meeting Notes:'''
* My update is that I'm working on the new primitive taks for the chest x-ray dataset
** My google doc (which anyone is welcome to add their ideas to:  https://docs.google.com/document/d/1mOr7D0yCq0v51za8k7MIaVC_EC-6tPOsOEAZgEw9YgM/edit?usp=sharing
* Documentation efforts continuing
** https://www.notion.so/Computer-Vision-Primitives-1d6225122e0c4d6b9abe57ab005e5cf1
* Team is working on splitting up the runs to be performed between Pulak, Anish, Alex, Anuraag, Tusheet 
* Baeline runs needed for:
** Toxicity
** Chest X-Ray

''''''My Action Items + Progress:''''''

By now, I had completed all of the background research that I needed in order to get caught up to speed with signal processing, image processing, masks, filters, and image math. I also had a decent grasp over the primitives for edge detection and thresholding, etc, that EMADE already incorporated.

The next step was to observe any gaps that may exist in EMADE and see how I can fill those gaps with any new primitive ideas.

This would have to be done in the following steps:

(1) Look into various OpenCV methods that could be applied in spatial methods with an EMADE shell

(2) Look into primitives that are useful for the scanned images and varying lighting conditions that are present in the chest xray dataset

(3) Look into vital CV concepts and then backtrack and see if they are implemented in existing EMADE CV primitives
* Sources:
** https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_table_of_contents_imgproc/py_table_of_contents_imgproc.html
** https://stackabuse.com/introduction-to-image-processing-in-python-with-opencv/
** https://www.youtube.com/playlist?list=PLAp0ZhYvW6XbEveYeefGSuLhaPlFML9gP
* New primitive ideas to pursue
** Adaptive thresholding [not in EMADE]
** Altering colorspaces [not in EMADE]
** Image processing: Cropping [unsure]
** Maximum thresholding [unsure]
** Hysteresis [unsure]
** Hough circle or line transforms [not in EMADE]
** Edge detection [exists]

I decided to start a brainstorming document where I put all of these ideas and developed the best ones that I thought would be most impactful for me to go ahead and implement, especially in terms of the chest xray dataset.
[[files/Cv1.png|center|thumb]]

I also worked with Jon to begin the documentation efforts. I sent over all of my documentation thus far (from the past week) where I recorded the different aspects of the signal and spatial files in EMADE, as well as some notes about resources for learning about them. As Jon is coming in with not as much knowledge about the area, he felt more comfortable taking the documentation task and running with that, as he gets familiar with the CV space. He is beginnign a notion documentation page that should detail a lot more (for any more beginners to come) what all is contained inside of these 3 files for CV primitives in EMADE.
[[files/Cv5.png|center|thumb|352x352px]]

Jon will be taking point on this task of documentation!

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete notebook for the week
|Completed
|November 16, 2020
|November 23, 2020
|November 16, 2020
|-
|Brainstorming my to-do list for how to go forward with this new primitives task
|Completed
|October 26, 2020
|November 2, 2020
|October 26, 2020
|-
|Finding out more about CV primitives in general (how are they even being stacked up in EMADE)
|Completed
|October 26, 2020
|November 2, 2020
|October 26, 2020
|-
|Ask professor Zutty about some primitive ideas that I have
|Completed
|November 16, 2020
|November 23, 2020
|November 16, 2020
|-
|Learning about the CV primitives in EMADE
|Completed
|November 16, 2020
|November 23, 2020
|November 16, 2020
|-
|Monitor the new documentation efforts with Jon + answer questions as they come up
|Working on this!
|November 16, 2020
|November 23, 2020
| -
|-
|Observing the Chest XRay runs to see how they could be improved
|Working on this!
|November 16, 2020
|December 2, 2020
| -
|-
|Looking for gaps in the preexisting EMADE CV primitives
|Working on this!
|November 16, 2020
|December 2, 2020
| -
|-
|Getting some ideas for primitives that would be good additions
| -
| -
|December 2, 2020
| -
|-
|Confirming that I'm not missing anything with Zutty (getting approval)
| -
| -
|December 2, 2020
| -
|-
|Coding the primitives
| -
| -
|December 2, 2020
| -
|-
|Testing (likely next semester, considering how much I have to teach myself to accomplish this)
| -
| -
|December 2, 2020
| -
|}

== '''November 9th, 2020 [Week 43]''' ==

'''General Meeting Notes:'''
* N/A
'''Monday Subteam Meeting Notes:'''
* Stocks
** Working on making sure their primitives are working well and aren't anomalous
** Technical indicator primitives also being revised and some testing being done to work on anomalies
* EzCGP
** Having some PACE issues, as a team
** Research team
*** Working on baseline runs
** Practicals team
*** Working on baseline runs
* NLP NN
** CV team ran on the chest-xray dataset using YOLO this week
*** <u>Conclusions</u>
*** Bounding boxes aren't working well on chest x-rays dataset
*** Out of 15 total classes that we want to correctly identify and label in the chest xray dataset, only 8 of them are being detected with bounding boxes at all
*** In addition, those 8 classes didn't even have good results within themselves and were highly inaccurate
*** Looking into VGGNet instead
** Found Kaggle dataset on Amazon product review (sentiment analysis) that could be a good dataset to try EMADE with
*** Dataset:  https://www.kaggle.com/bittlingmayer/amazonreviews
*** This is also a good task for newbies on the team to get familiar with all the moving parts of EMADE and performing runs
* Modularity
** Doing runs on datapair experiemnt
** testing on the MNIST dataset they're encorporating (outside of titanic dataset which is too simple)
'''Friday Subteam Meeting Notes:'''
* CV team ran on the chest-xray dataset using YOLO this week
** Bounding boxes aren't working well on chest x-rays dataset
** Out of 15 total classes that we want to correctly identify and label in the chest xray dataset, only 8 of them are being detected with bounding boxes at all
** In addition, those 8 classes didn't even have good results within themselves and were highly inaccurate
** Looking into VGGNet instead
* Found Kaggle dataset on Amazon product review (sentiment analysis) that could be a good dataset to try EMADE with
** Dataset:  https://www.kaggle.com/bittlingmayer/amazonreviews
** This is also a good task for newbies on the team to get familiar with all the moving parts of EMADE and performing runs

* New PACE documentation efforts
** https://www.notion.so/maximgeller/Configuring-EMADE-on-PACE-60aedf065abc445096617c3cec875a11
* I've been doing research into the CV methods in general as well as how they are stored and incorporated into EMADE
** Been doing a lot of background research and fundamentals work, so that when I begin the next phase (brainstorming and narrowing down and coding the primitives, I don't have to backtrack to this step again)
** It has been a relatively slow week on physical progress for that reason
** Been watching a lot of online explanations for concpets that are hard to grasp in writing
** Been reading a lot of online documentation to try and understand the reasons behind various CV techniques (so that when it comes to our particular CV task of classification, I can better choose the most optimal primitives for our task that would give best ROI to implement asap
* Paper for today's meeting:
** https://arxiv.org/abs/2010.08512
** I didn't read it before the meeting this time, as I had a lot of heavy reading and catching up to do on my own this week, for my individual task
** After discussions it seemed to outline a similar yet definitely different thought process when compared to the concept of dropout regularization''''''My Action Items + Progress:''''''

I took a lot of time this week to learn up about the methods in these files. This is the foundation for the new primitives effort, because I do not know a single thing about any of the primitives in the files already (how they work, what they're used for, why they are used, etc) and therefore can't even start brainstorming new primitives until this groundwork is completed.

'''<u>Signal Methods</u>'''

https://github.gatech.edu/emade/emade/blob/nlp-nn/src/GPFramework/signal_methods.py

<u>Window Functions:</u>

A window function performs a calculation across a set of table rows that are somehow related to the current row.

Signal processing typically uses a Fourier Transform to convert data captured in the time domain to the frequency domain. If a signal is captured in a periodic manner, the resulting signal has no leakage and requires no windowing (left). However, if the signal is captured in an aperiodic manner, there will be leakage and a window function is necessary (right).

Wikipedia: In signal processing and statistics, a window function (also known as an apodization function or tapering function) is a mathematical function that is zero-valued outside of some chosen interval, normally symmetric around the middle of the interval, usually near a maximum in the middle, and usually tapering away from center.

<u>Window Functions in EMADE:</u>

https://www.youtube.com/watch?v=YsqGQzJ_2V0

https://www.youtube.com/watch?v=iwLnVKNGxAo

<nowiki>**</nowiki>Hann(ing) Window (raised cosine)**

<nowiki>**</nowiki>Hamming Window**

https://www.youtube.com/watch?v=HlpV-ow6kLs

<nowiki>**</nowiki>Cosine Window**

https://www.youtube.com/watch?v=aEGgyz4Cguw

<nowiki>**</nowiki>Tukey Window (tapered cosine)**

http://matlab.izmiran.ru/help/toolbox/signal/tukeywin.html

<nowiki>**</nowiki>Lanczos Window (scaled central sinc lobe)**

https://old.cescg.org/CESCG99/TTheussl/node11.html

<nowiki>**</nowiki>Bartlett (Triangular) Window**

<nowiki>**</nowiki>Gaussian Window**

<nowiki>**</nowiki>Blackman-Harris Window**

<nowiki>**</nowiki>Nuttall window**

<nowiki>**</nowiki>Kaiser Window**

<nowiki>**</nowiki>Planck-taper window**

<u>Filter Functions:</u>

Center of Mass

Edge Detection Canny Helper

https://www.youtube.com/watch?v=sRFM5IEqR2w

Corner Detection Harris Helper

https://www.youtube.com/watch?v=veieEffgd5A

Corner Detection Min Eigen Val Helper

https://www.mathworks.com/help/vision/ref/detectmineigenfeatures.html

Highpass Fourier Ellipsoid Helper

Highpass Irst

Highpass Fourier Shift Helper

Highpass Fourier Gaussian Helper

Highpass Fourier Uniform Helper

Highpass Unsharp Mask Setup

Highpass Laplacian

https://www.youtube.com/watch?v=pFWmqGbkoWg

Highpass Sobel Derivative Setup

https://www.youtube.com/watch?v=uihBwtPIBxM

Median Filter Helper

https://www.youtube.com/watch?v=TYNbTdzm-5w

Lowpass Fourier Shift Helper

Lowpass Filter Median Helper

Lowpass Filter Average Helper

Lowpass Filter Gaussian

https://www.youtube.com/watch?v=ZSvJHHZ-HDI

Lowpass Filter Bilateral Helper

Lowpass Fourier Ellipsoid Helper

Lowpass Fourier Gaussian Helper

Lowpass Fourier Uniform Helper

Spatial Methods

https://github.gatech.edu/emade/emade/blob/nlp-nn/src/GPFramework/spatial_methods.py

This file seems to contain a few different classes of functions (as I could make out from the comments in the code):
* Morphological Functions
** Morphological functions perform transformations on binary images using their shape. A kernel (or structuring element) is used to determine what operation will be applied. Below is an example image and two common kernels applied (erosion and dilation).
** Erosion
***'''Erosion''' removes pixels on object boundaries[[files/Sc12.png|center|thumb]]
** Dilation
***'''Dilation''' adds pixels to the boundaries of objects in an '''image'''[[files/Sc13.png|center|thumb]]
** Opening (combination of erosion and dilation)
*** the dilation of the erosion
***[[files/Sc15.png|center|thumb|202x202px]]
** Closing (combination of erosion and dilation)
*** the erosion of the dilation
***[[files/Sc14.png|center|thumb|238x238px]]
* Bitwise Logic Operators
** AND, OR, NOT, etc
** https://www.youtube.com/watch?v=mc846qb0ngk
** https://docs.opencv.org/master/d0/d86/tutorial_py_image_arithmetics.html
* Image Math
**https://www.youtube.com/watch?v=4FrSVdDA1Ec[[files/Cv16.png|center|thumb]]
* Scalar Math

Feature Extraction Methods

https://github.gatech.edu/emade/emade/blob/nlp-nn/src/GPFramework/feature_extraction_methods.py

Professor Zutty confirmed that this file will not be of much use for my purposes.

'''I completed learning about the CV primitives in EMADE this week! I believe I understand enough about the various primitives in emade as well as how computer vision tasks work on pixels in image data. With this knowledge, I think I can start looking at the Chest XRay dataset novelties and how I may come up with ideas for improving performance on it. This is what I will be doing from next week onward.'''

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete notebook for the week
|Completed
|November 9, 2020
|November 16, 2020
|November 9, 2020
|-
|Finding out more about CV primitives in general (how are they even being stacked up in EMADE)
|Completed
|October 26, 2020
|November 2, 2020
|October 26, 2020
|-
|Learning about the CV primitives in EMADE
|Completed
|October 26, 2020
|November 2, 2020
|November 2, 2020
|-
|Observing the Chest XRay runs to see how they could be improved
|Working on this!
|November 9, 2020
|December 2, 2020
| -
|-
|Looking for gaps in the preexisting EMADE CV primitives
| -
| -
|December 2, 2020
|
|-
|Getting some ideas for primitives that would be good additions
| -
| -
|December 2, 2020
|
|-
|Confirming that I'm not missing anything with Zutty (getting approval)
| -
| -
|December 2, 2020
|
|-
|Coding the primitives
| -
| -
|December 2, 2020
|
|-
|Testing (likely next semester, considering how much I have to teach myself to accomplish this)
| -
| -
|December 2, 2020
|
|}

== ''' November 2nd, 2020 [Week 42]''' ==

'''General Meeting Notes:'''
* Due to time conflict I wasn't able to make it to the meeting today, unfortunately.
* I'll be sure to communicate with my teammates regarding what happened, and I made sure to tell them any updates from my end (regarding my new primitives effort)
'''Monday Subteam Meeting Notes:'''
* Due to time conflict I wasn't able to make it to the meeting today, unfortunately.
* I'll be sure to communicate with my teammates regarding what happened, and I made sure to tell them any updates from my end (regarding my new primitives effort)
'''Friday Subteam Meeting Notes:'''
* CV Team:
** Working with Chest X-Ray dataset
** Wrote script for images to feed the bounded box problem being encountered (not exactly sure what this means, will have to followup with Tusheet regarding what actual issues they were facing because I've had some experience in this area and am curious)
** Split up the dataset to prepare for training
** Created text files from all images, containing metadata regarding label, normalized width, height, etc
** Performed 150 epochs of training using some another GitHub repo
** Conclusion: bounding boxes are not working well + a new plan has to be come up with

''''''My Action Items + Progress:''''''

I took a lot of time this week to learn up about the methods in these files. This is the foundation for the new primitives effort, because I do not know a single thing about any of the primitives in the files already (how they work, what they're used for, why they are used, etc) and therefore can't even start brainstorming new primitives until this groundwork is completed.

'''<u>Signal Methods</u>'''

https://github.gatech.edu/emade/emade/blob/nlp-nn/src/GPFramework/signal_methods.py

<u>Window Functions:</u>

A window function performs a calculation across a set of table rows that are somehow related to the current row.

Signal processing typically uses a Fourier Transform to convert data captured in the time domain to the frequency domain. If a signal is captured in a periodic manner, the resulting signal has no leakage and requires no windowing (left). However, if the signal is captured in an aperiodic manner, there will be leakage and a window function is necessary (right).

Wikipedia: In signal processing and statistics, a window function (also known as an apodization function or tapering function) is a mathematical function that is zero-valued outside of some chosen interval, normally symmetric around the middle of the interval, usually near a maximum in the middle, and usually tapering away from center.

<u>Window Functions in EMADE:</u>

https://www.youtube.com/watch?v=YsqGQzJ_2V0

https://www.youtube.com/watch?v=iwLnVKNGxAo

<nowiki>**</nowiki>Hann(ing) Window (raised cosine)**

<nowiki>**</nowiki>Hamming Window**

https://www.youtube.com/watch?v=HlpV-ow6kLs

<nowiki>**</nowiki>Cosine Window**

https://www.youtube.com/watch?v=aEGgyz4Cguw

<nowiki>**</nowiki>Tukey Window (tapered cosine)**

http://matlab.izmiran.ru/help/toolbox/signal/tukeywin.html

<nowiki>**</nowiki>Lanczos Window (scaled central sinc lobe)**

https://old.cescg.org/CESCG99/TTheussl/node11.html

<nowiki>**</nowiki>Bartlett (Triangular) Window**

<nowiki>**</nowiki>Gaussian Window**

<nowiki>**</nowiki>Blackman-Harris Window**

<nowiki>**</nowiki>Nuttall window**

<nowiki>**</nowiki>Kaiser Window**

<nowiki>**</nowiki>Planck-taper window**

<u>Filter Functions:</u>

Center of Mass

Edge Detection Canny Helper

https://www.youtube.com/watch?v=sRFM5IEqR2w

Corner Detection Harris Helper

https://www.youtube.com/watch?v=veieEffgd5A

Corner Detection Min Eigen Val Helper

https://www.mathworks.com/help/vision/ref/detectmineigenfeatures.html

Highpass Fourier Ellipsoid Helper

Highpass Irst

Highpass Fourier Shift Helper

Highpass Fourier Gaussian Helper

Highpass Fourier Uniform Helper

Highpass Unsharp Mask Setup

Highpass Laplacian

https://www.youtube.com/watch?v=pFWmqGbkoWg

Highpass Sobel Derivative Setup

https://www.youtube.com/watch?v=uihBwtPIBxM

Median Filter Helper

https://www.youtube.com/watch?v=TYNbTdzm-5w

Lowpass Fourier Shift Helper

Lowpass Filter Median Helper

Lowpass Filter Average Helper

Lowpass Filter Gaussian

https://www.youtube.com/watch?v=ZSvJHHZ-HDI

Lowpass Filter Bilateral Helper

Lowpass Fourier Ellipsoid Helper

Lowpass Fourier Gaussian Helper

Lowpass Fourier Uniform Helper

Spatial Methods

https://github.gatech.edu/emade/emade/blob/nlp-nn/src/GPFramework/spatial_methods.py

This file seems to contain a few different classes of functions (as I could make out from the comments in the code):
* Morphological Functions
* Bitwise Logic Operators
* Image Math
* Scalar Math

<nowiki>*******</nowiki> Didn't get to this, this week *******

<nowiki>*******</nowiki> Will pick back up here next week *******

Feature Extraction Methods

https://github.gatech.edu/emade/emade/blob/nlp-nn/src/GPFramework/feature_extraction_methods.py

Anish mentioned that this file would probably not be of much use to my task of adding in novel CV primitives. I'll confirm this with Zutty and enquire further about the purpose of this file.

{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete notebook for the week
|Completed
|November 2, 2020
|November 9, 2020
|November 2, 2020
|-
|Ask about Feature Extraction Methods in next week's monday meeting
|Completed
|November 2, 2020
|November 9, 2020
|November 9, 2020
|-
|Finding out more about CV primitives in general (how are they even being stacked up in EMADE)
|Completed
|October 26, 2020
|November 2, 2020
|October 26, 2020
|-
|Learning about the CV primitives in EMADE
|Working on this!
|October 26, 2020
|November 2, 2020
|October 26, 2020
|-
|Observing the Chest XRay runs to see how they could be improved
| -
| -
|December 2, 2020
| -
|-
|Looking for gaps in the preexisting EMADE CV primitives
| -
| -
|December 2, 2020
|
|-
|Getting some ideas for primitives that would be good additions
| -
| -
|December 2, 2020
|
|-
|Confirming that I'm not missing anything with Zutty (getting approval)
| -
| -
|December 2, 2020
|
|-
|Coding the primitives
| -
| -
|December 2, 2020
|
|-
|Testing (likely next semester, considering how much I have to teach myself to accomplish this)
| -
| -
|December 2, 2020
|
|}

== ''' October 26th, 2020 [Week 41]''' ==

'''General Meeting Notes:'''
* Received new members in our subteam
* Updated everyone on plans with moving forward for the second half of the semester (goals, work to be done, expectations, etc)
'''Monday Subteam Meeting Notes:'''
* Stocks
** Finished new technical indicator functions (the ones outside of the paper that they came up with)
** Working with first semesters on new work
* EzCGP
** Working with first semesters on new work
** Research Team
*** Work on implementing research paper
** Implementation Team
*** Working on bugs with baseline runs + new branch work
* NLP
** Going to perform runs with YOLO on chest xray dataset (cv team update)
** Update: Getting better results than the CoDEEPNEAT paper does on the same datasets
* Modularity
** Working with first semesters on new work (probably running some experiments and tests for the team)
'''Friday Subteam Meeting Notes:'''
* We got feedback from our presentation about incorporating stacked learners
* Others within the CV team are working on researchign YOLO (You Only Look Once) model for progress on our Chest X-Ray dataset
* Team provided some links to read up on (I also took some peeks because I was curious):
** real time: https://arxiv.org/abs/1506.02640
** intro to yolo: https://pjreddie.com/darknet/yolo/
*** I enjoyed this one!
** more on bounding boxes for CV tasks: https://arxiv.org/abs/2004.06816
* As for me, I decided to just delve head first into the three files that Zutty confirmed I'd be focusing on

''''''My Action Items + Progress:''''''
* I have created an outline for how my work will be laid out for the remainder of the semester
* If anyone decides to join me, we can work through the list together by divvying up tasks!
* The steps for me are looking like they will be the following:
** (1) Finding out more about CV primitives in general (how are they even being stacked up in EMADE)
** (2) Learning about hte CV primitives in EMADE (I don't have that much experience in the area and so I'll have to do a lot of research and reading!)
** (3) Observing the Chest XRay runs to see how they could be improved
** (4) Looking for gaps in the preexisting EMADE CV primitives
** (5) Getting some ideas for primitives that would be good additions
** (6) Confirming that I'm not missing anything with Zutty (getting approval)
** (7) Coding the primitives
** (8) Testing
*** This will most likely have to be pushed till next semester, because I have quite a bit of work cut out for me as it is, especially with the amount of learning I'll have to do in order to onboard myself into CV primitives and how they work, and what is important to note about them
I began this week by scoping out the three files that were mentioend to me and just looking into how EMADE combined primitives for CV tasks. I asked some questions regarding this in our weekly update meetings with professor Zutty and he also explained how this works! The primitives for CV are also stacked and therefore take in an input shape, apply some transformations to the input, and then extrude an output shape that is the same sa what went in so that all primitives can be stacked together in any order desired.

Signal Methods

https://github.gatech.edu/emade/emade/blob/nlp-nn/src/GPFramework/signal_methods.py

[[files/Sc8.png|center|thumb]]

This file seems to conain something called "Window Functions". I have no idea what this means, so I'll have to do some reading up on it.

Spatial Methods

https://github.gatech.edu/emade/emade/blob/nlp-nn/src/GPFramework/spatial_methods.py
[[files/Sc9.png|center|thumb]]

This file seems to contain a few different classes of functions (as I could make out from the comments in the code):
* Morphological Functions
* Bitwise Logic Operators
* Image Math
* Scalar Math

Feature Extraction Methods

https://github.gatech.edu/emade/emade/blob/nlp-nn/src/GPFramework/feature_extraction_methods.py
[[files/Sc10.png|center|thumb]]

I am very unsure about this file! I'll have to speak with Anish regarding the signifiance of these, to see how to move forward. We set up a time to meet and discuss what he knew about these files so that I could gain a better understanding about them!

[[files/Sc11.png|center|thumb]]
* 
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete notebook for the week
|Completed
|October 26, 2020
|November 2, 2020
|October 26, 2020
|-
|Brainstorming my to-do list for how to go forward with this new primitives task
|Completed
|October 26, 2020
|November 2, 2020
|October 26, 2020
|-
|Finding out more about CV primitives in general (how are they even being stacked up in EMADE)
|Working on this!
|October 26, 2020
|November 2, 2020
|October 26, 2020
|-
|Learning about the CV primitives in EMADE
| -
|October 26, 2020
|November 2, 2020
|October 26, 2020
|-
|Observing the Chest XRay runs to see how they could be improved
| -
| -
|December 2, 2020
| -
|-
|Looking for gaps in the preexisting EMADE CV primitives
| -
| -
|December 2, 2020
|
|-
|Getting some ideas for primitives that would be good additions
| -
| -
|December 2, 2020
|
|-
|Confirming that I'm not missing anything with Zutty (getting approval)
| -
| -
|December 2, 2020
|
|-
|Coding the primitives
| -
| -
|December 2, 2020
|
|-
|Testing (likely next semester, considering how much I have to teach myself to accomplish this)
| -
| -
|December 2, 2020
|
|}

== ''' October 19th, 2020 [Week 40]''' ==

'''General Meeting Notes:'''
* Midterm presentation were completed by all the sub-teams and the first years
'''Monday Subteam Meeting Notes:'''
* My team's presentation (NLP-NN + CV subteam): https://docs.google.com/presentation/d/1Z8PjzaP_LjhyycpGSeXX7rIHdVYkxHzqg8vaKVEYhdU/edit#slide=id.ga3dcb7a2bf_6_2
* Feedback for CV:
** Got new ideas to model the chest x-ray dataset in YOLO as a bounded box problem
** Good idea to go ahead with the new primitives work
** I will be taking point on those efforts, and it'll be my responsibility to drive that work forward from here on out
** Zutty reinforced that I should look into the following files and confirmed I had been doing work in the right place: spatial_methods.py, signal_methods.py, and feature_extraction_methods.py
'''Friday Subteam Meeting Notes:'''
* Distributed tasks for the CV team on how to implement these new ideas

''''''My Action Items + Progress:''''''

I was in charge of adding my ideas of future work to the slides:

(1) Completing the runs on the chest xray dataset and evaluating performance

(2) Incorporating another dataset (really good new member job!)

(3) Delving deep into the CV primitives that are in EMADE and exploring how they may be improved

(4) Utilizing EMADE chest xray performance to see how certain examples may be misclassified, and observing if there are any novelties in these examples for which we can introduce new primitives that may be better for those tasks

I decided to branch off from the rest of the CV team who are doing the chest xray runs and go forward with leading the efforts to develop new primitives in EMADE for the CV task!

I was responsible for conveying the information regarding this new effort that was going to be taking place on the CV subteam!

This was my idea and I therefore completed all of the background research necessary before launching into creating my pitch in the midterm presentation slides regarding future work and opportunity for new students.

This is also an area that I have been super interested in, so it will really help me grow my personal knowledge in the area while contributing to EMADE and our CV task (specifically, chest xray dataset).

I did extensive research into the three files that I had been pointed towards spatial_methods.py, signal_methods.py, and feature_extraction_methods.py

And made these slides in the midterm presentation on my own, to go along with my pitch and progress thus far:
[[files/Sc1.png|center|thumb|489x489px|sc1]]
[[files/Sc2.png|center|thumb|489x489px|sc2]]

In addition to this, I wrote my speech out that I would be saying during the midterm presentation and practiced it extensively to make sure I was going to be delivering the information in an effective manner that coupled the graphics on the screen.
[[files/Sc3.png|center|thumb|508x508px|sc3]]
[[files/Sc4.png|center|thumb|582x582px|sc4]]

I took on these slides to speak on which is work that I have and plan to continue working on for the rest of the semester. I pitched it to others on the CV team as well as new arrivals, for anyone who wanted to join my efforts!
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Weekly notebook update
|Completed
|October 19, 2020
|October 23, 2020
|October 23, 2020
|-
|Completed midterm presentation
|Completed
|October 15, 2020
|October 19, 2020
|October 14, 2020
|-
|Practiced midterm presentation
|Completed
|October 12, 2020
|October 19, 2020
|October 14, 2020
|-
|Delivered midterm presentation
|Completed
|October 19, 2020
|October 19, 2020
|October 19, 2020
|-
|Prepped for zutty's feedback and next steps with my new task of CV primitive work
|In progress
|October 19, 2020
|October 23, 2020
| -
|}

== ''' October 12th, 2020 [Week 39]''' ==

'''General Meeting Notes:'''
* Midterm Presentation is on Monday 19th October 2020 @5pm
'''Monday Subteam Meeting Notes:'''
* Stocks
** Did some baseline runs (I believe) and calculated standard error of their models
** Todo: explore paper's results and how they were calculating different metrics
* EzCGP
** Research Team
*** Presentation
*** Research paper work and summaries on those
** Implementation Team
*** Baseline runs and analyzing the results
* NLP
** Working on adaptive mutation, attention, shuffle layers
** Could be space within NLP NN to write a paper? Discussed with Dr. Zutty
** For this we require an industry novelty, or really good results on something preexisting - requires further review! Would be fun!
* Modularity
** My internet cut out and I couldn't catch most of this team's update
'''Friday Subteam Meeting Notes:'''
* Resolved the MySQL database error + anaconda errors we were facing, as a team
* Fixed up my colab environment and it is now ready to perform runs (https://colab.research.google.com/drive/1bhRodLmoobRtAJFtrKLkGtR__wlHbB1Q)
** EMADE
** Dependencies
** Anaconda environment
** Port forwarding
* This week, I did a lot of work with Anish to discuss how to move forward in terms of CV primitives, and what had already been done in EMADE in that space
** He mentioned three different files that I could look into: spatial_methods.py, signal_methods.py, and feature_extraction_methods.py
** I will be prioritizing looking into these further, next week
* This week, I am prioritizing ideating and preparing how to pitch out project in the midterm presentation
''''''My Action Items + Progress:''''''

I created a task plan for divvying up our future work within the CV team. We have quite a few members on this (sub) subteam and I was just doing some brainstorming this week on how we could move forward most effectively. This will also come in really handy for when we get new members as well, so we can pitch out project as best as possible in the midterm presentations.

(1) Completing the runs on the chest xray dataset and evaluating performance

(2) Incorporating another dataset (really good new member job!)

(3) Delving deep into the CV primitives that are in EMADE and exploring how they may be improved

(4) Utilizing EMADE chest xray performance to see how certain examples may be misclassified, and observing if there are any novelties in these examples for which we can introduce new primitives that may be better for those tasks

I added this information to our midterm presentation slides later on!!

I resolved all of my Google Colab issues and became ready to perform runs this week! I also was working with the team on some common issues with the port forwarding.
* Link --> <nowiki>https://colab.research.google.com/drive/1bhRodLmoobRtAJFtrKLkGtR__wlHbB1Q</nowiki>
Team performed EMADE runs on Google Colab
* Results [Stats Notebook] --> https://colab.research.google.com/drive/17vC-WvvFMBhG3y8yROE8qSIwhjMEWrdM?usp=sharing

I am part of the CV subteam and assisted in creating the following slides (IN ADDITION TO MY OWN, DETAILED IN NEXT WEEK'S ENTRY):
[[files/Sc5.png|center|thumb]]
[[files/Sc6.png|center|thumb]]
[[files/Sc7.png|center|thumb]]
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete notebook for the week
|Completed
|October 12, 2020
|October 19, 2020
|October 19, 2020
|-
|MySQL bug fix
|Completed
|October 12, 2020
|October 19, 2020
|October 14, 2020
|-
|Conda bug fix
|Completed
|October 12, 2020
|October 19, 2020
|October 14, 2020
|-
|Generate the model for chest X-Ray dataset
|Completed
|October 12, 2020
|October 19, 2020
|October 15, 2020
|-
|Began working on the midterm presentation outline for what I was going to present
|Completed
|October 12, 2020
|October 19, 2020
|October 14, 2020
|-
|Start looking into areas of future work within CV subteam for incoming members after midterm presentations
|Completed
|October 12, 2020
|October 19, 2020
|October 14, 2020
|-
|Document ideas for areas of new work + expansion (done above)
|Completed
|October 12, 2020
|October 19, 2020
|October 15, 2020
|-
|Decide what aspect I want to explore for the rest of the semester (what has highest ROI for us, right now?)
|Completed
|October 12, 2020
|October 19, 2020
|October 15, 2020
|}

== ''' October 5th, 2020 [Week 38]''' ==

'''General Meeting Notes:'''
* Notebooks due for midterm evals on October 5th (TODAY)
'''Monday Subteam Meeting Notes:'''
* Stocks
** Working on technical indicators (overarching goal) but got done adding some SciKit regression functions
** NN work with Keras being done
* EzCGP
** New documentations on augmentor pipeline architecture (I have no idea what this means, but would love to learn more!)
** Research Team:
*** Working thru conda issues on PACE - "Disk Quote exceeded"
** Implementation Team
*** Adding new primitives
*** Loading CIFAR-10 dataset
* NLP NN
** Pulak and his side of NLP NN working on two approaches for invalidating individuals (aborting them before evaluation and genomic healing (changing representation of fatal instances instead of removing)
** CV team working thru PACE issues (exploring Colab as an alternative)
** I suggested more word embeddings that are better - is a line of work that they are exploring on Pulak's side of NLP NN (not CV subteam)
** Also doing some work with shufflign mutation function layers
* Modularity
** Looking into MNIST Dataset
** New ARLs evaluation seems that they are more useful btu this is on Titanic, so looking into more complex dataset perhaps?
'''Friday Subteam Meeting Notes:'''
* CV team finished Google Colab setup --> https://colab.research.google.com/drive/1CTgRr52mTcykxolHUGWYODvbC8gGCZ6N#scrollTo=nKFsz9GLJP-K
** EMADE
** Dependencies
** Anaconda environment
** Port forwarding
* I was not experiencing the same MySQL issues on Colab as Tusheet seemed to be having, at first (linking colab and database)
** We discussed what may have been causing this (as we all experienced something similar last semester)
* I was however running into some minor conda problems with imports and the colab file itself
** Communicated these to the team and worked with Anuraag and Tusheet to solve the issues
* Ppaer discussed in meeting this week: Adaptive Mutation in Genetic Algorithms by Libelli & Alba
** https://link.springer.com/article/10.1007/s005000000042
* Anish found these papers that we are planning on looking into, for next week
** Paper 1: https://arxiv.org/abs/1901.11117
*** Interesting paper!! excited to talk about this with the group
** Paper 2: https://lilianweng.github.io/lil-log/2020/08/06/neural-architecture-search.html
*** I don't think this paper is really applicable to us (as far as my knowledge goes with EMADE... I could totally be wrong, so curious on what others will say)
** Paper 3: https://arxiv.org/abs/2003.12056
*** Their setup looks much different than ours, not sure how this will transfer over
''''''My Action Items + Progress:''''''

Notebook completed thus far, in time for midterm evals.

I finished setup of my Google Collab environment this week. Running into some minor conda problems but I'm sure I can work with Anuraag and Tusheet to get these situated, because they aren't running into the same issues as I am. Plan to sort this out with them soon

https://colab.research.google.com/drive/1CTgRr52mTcykxolHUGWYODvbC8gGCZ6N#scrollTo=nKFsz9GLJP-K
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete notebook for the week
|Completed
|October 5, 2020
|October 12, 2020
|October 12, 2020
|-
|Read paper 1: https://arxiv.org/abs/1901.11117
|Completed
|October 9, 2020
|October 12, 2020
|October 10, 2020
|-
|Read paper 2: https://lilianweng.github.io/lil-log/2020/08/06/neural-architecture-search.html
|Completed
|October 9, 2020
|October 12, 2020
|October 10, 2020
|-
|Read paper 3: https://arxiv.org/abs/2003.12056
|Completed
|October 9, 2020
|October 12, 2020
|October 11, 2020
|-
|Google Colab Setup
|Completed
|October 10, 2020
|October 12, 2020
|October 6, 2020
|-
|Port forwarding [hookup complete, with help from teammates]
|Completed
|October 5, 2020
|October 12, 2020
|October 7, 2020
|-
|Conda environment having some more issues again! No one can figure out why. Need bug fix and soon!!
|In Progress
|October 5, 2020
|October 12, 2020
| -
|-
|Model for chest X-Ray dataset
|In Progress
|September 21, 2020
|October 19, 2020
| -
|}

== ''' September 28th, 2020 [Week 37]''' ==

'''General Meeting Notes:'''
* Midterm peer evals are upcoming!!
'''Monday Subteam Meeting Notes:'''
* Stocks
** Colab working for them I believe (if so, maybe could work with them thought I think it may be more complex for our uses)
** Found some interesting research papers nad began some implementation todos
* EZCGP
** Had some internet issues and wasn't able to catch this unfortunately
* NLP NN
** Completed some runs and shared details on those
** Discussed future plans and goals for the NLP team
*** Looking into adaptive mutation
*** I suggested BERT and ELMo word embeddings - they are looking into this on Pulak's side of NLP NN
*** Adverserial regularization being looked into
*** I have also been thinking about new primitive work
* Modularity
** Had some internet issues and wasn't able to catch this unfortunately
'''Friday Subteam Meeting Notes:'''
* N/A, Couldnt make the meeting this week unfortunately

''''''My Action Items + Progress:''''''

I finished facilitating the transition to Google Colab for all of our runs. Everything was set up and ready to go, so we could now pivot from using PACE and hopefully get a move on with our preliminary runs https://colab.research.google.com/drive/1CTgRr52mTcykxolHUGWYODvbC8gGCZ6N#scrollTo=nKFsz9GLJP-K

Now we are beginning our efforts with port forwarding.

I finished up reading the new paper we were discussing this week: [[files/MarsiliLibelli-Alba2000 Article AdaptiveMutationInGeneticAlgor.pdf]]

I'm wasn't sure where I stood on it, so i don't have many personal notes, but I was interested in others' take and opinions in the meeting

Midterm peer evaluations due soon + I completed mine.
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Midterm Peer Eval
|Completed
|September 28, 2020
|October 5, 2020
|September 30, 2020
|-
|Found alternative to PACE that works
|Completed
|September 21, 2020
|October 5, 2020
|September 28, 2020
|-
|Finished hooking up Google Collab for our purposes
|Completed
|September 28, 2020
|October 5, 2020
|October 4, 2020
|-
|MYSQL on Collab
|Completed
|September 28, 2020
|October 5, 2020
|September 29, 2020
|-
|Dependencies on Collab resolved
|Completed
|September 28, 2020
|October 5, 2020
|September 28, 2020
|-
|Conda environment setup on Collab
|Completed
|September 28, 2020
|October 5, 2020
|September 30, 2020
|-
|Preliminary tests on Collab with chest x-ray dataset
|In Progress
|September 21, 2020
|October 4, 2020
| -
|-
|Model for chest X-Ray dataset
|In Progress
|September 21, 2020
|October 19, 2020
| -
|-
|Complete notebook for the week
|Completed 
|September 28, 2020
|October 5, 2020
|October 5, 2020
|}

== ''' September 21st, 2020 [Week 36]''' ==

'''General Meeting Notes:'''
* N/A
'''Monday Subteam Meeting Notes:'''
* Stocks
** Planning on modeling their own study after the research papers they read (chose one out of many researched)
** Use ADD to see if the EMADE architecture can be applied to this problem such that it received novel results and outperforms the paper
** Planning on using AWS with Google Collab
*** Ask why they aren't just using GCP? Curious.
* Modularity
** They were running into quite a few hurdles
** "Cannot connect to database during query" MySQL error --> similar to one NLP was experiencing (including me)
** We also couldn't figure it out
** The team plans on working with us (NN) to try and fix up common inssues we're having in a more efficient manner
** Also having colab issues when hooking with MySQL database (going to try connecting to local IP address)
* EzCGP
** Currently looking for more research papers and discussing new ideas
** Working on unit testing new EzCGP features that have been worked on thus far
** Will be loading cifar-10 dataset into EMADE for testing purposes
** Discussed more ideas from additional research papers discovered
* NLP
** Updates from CV subteam:
*** Setting up PACE for EMADE runs basically finished (hopefully)
** Updates on paper reviewed last week [https://link.springer.com/article/10.1007/s005000000042<nowiki>] are that it doesn't really align with our goals, and won't be of much use for us</nowiki>
** Working on (1) Word embeddings and (2) Adaptive mutations
** Results with activation functions and pretrained embeddings mutation functions:
*** Best individual after 100 generations: 0.03555 score, individual: NNLearner(ARG0, OutputLayer(ARG0, GRULayer(55, sigmoidActivation, 10, falseBool, trueBool, EmbeddingLayer(94, ARG0, gloveTwitterWeights, InputLayer(ARG0)))), 100)
*** Best individual after 70 generations: 0.03558 score, individual:NNLearner(ARG0, OutputLayer(ARG0, GRULayer(93, defaultActivation, 150, falseBool, falseBool, EmbeddingLayer(1, ARG0, gloveTwitterWeights, InputLayer(ARG0)))), 95)
*** The validation accuracy remained the same: 0.04027.
*** Focusing on enabling crossover across various layers
'''Friday Subteam Meeting Notes:'''
* CV team working on completing PACE hookup as new issues have arrised with the conda environment
** ERROR: exceeded disk quota
** SOLN: Deleting extra datasets, .git files (large portion), and uneeded input datasets from EMADE PACE folder
** ERROR: conda errors with linking with PACE
** SOLN: re-loaded anaconda and re-created conda environment
* Discussed moving completely to Google Colab instead of PACE in order to complete our runs
* This may allow us to have less issues like the ones we've been facing since last semester with PACE
* Tusheet and I decided to start working on getting Collab set up, based on some initial work by Anish
** https://colab.research.google.com/drive/1CTgRr52mTcykxolHUGWYODvbC8gGCZ6N#scrollTo=nKFsz9GLJP-K
* Entire team: reading https://www.researchgate.net/publication/220117106_A_Genetic_Algorithm_with_Adaptive_Mutations_and_Family_Competition_for_Training_Neural_Networks
** My notes to present next meeting to the team (after reading):
*** Paper's adaptive mutations not effective (really slow) due to the immense amount of calculations in each iteration of the method

'''My Action Items + Progress:'''

I worked on the setup with Google Collab on my machine, to test out if this would be a better alternative to PACE (while most of the rest of the group continued on PACE, in the meantime, in case it turned out that would be our best option).

Worked with Tusheet to learn about transfer learning a bit more. Consulted the rest of the team on what their thoughts were as well, and how we would take this forward.

Repo resource: https://github.com/vinaya8/Transfer-Learning-on-Chest-X-Ray-Images

Tusheet may continue on with the transfer learning task while I continue work on the Collab issues we are having, to get a solid foundation for runs.

I've also been working on the Google Collab setup process with the team: https://colab.research.google.com/drive/1CTgRr52mTcykxolHUGWYODvbC8gGCZ6N#scrollTo=nKFsz9GLJP-K
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Completing a preliminary run on chest x-ray dataset
|In Progress
|September 21, 2020
|October 4, 2020
| -
|-
|EMADE Seeded (Pre-Trained) Model created
|Completed
|September 21, 2020
|September 28, 2020
|September 26, 2020
|-
|Complete notebook for the week
|Completed 
|September 21, 2020
|September 28, 2020
|September 28, 2020
|}

== ''' September 14th, 2020 [Week 35]''' ==

'''General Meeting Notes:'''
* Two deliverables for next week's general meeting:
** Updated notebook
** Completed self-evaluation form (can be found above)
* Stocks:
** Subgroup 1: ML in trading
** Subgroup 2: EMADE integration with Pre-Developed indicators
** Subgroup 3 (low-pri): Resaerch based group (developing original technical indicators in EMADE)
** Will be finding lots of papers with research in the area (team is just starting out)
* EzCGP:
** Subgroup 1: Research + new features
*** Each member researched numerous papers
*** Planning to short list to 3-5 papers total
** Subgroup 2: Existing codebase improvements
*** Created common guidelines for development
*** Ran tests on multi-gaussian problem
* NLP:
** Subgroup 1: NN
*** Accumulating studies and ideas to improve EMADE performance on task
*** Progress made on Colab + EMADE setup
** Subgroup 2: CV
*** Accumulating studies and ideas to improve EMADE performance on task
*** Working on new PACE issues, but making good progress
* Modularity:
** Working on improving selection for individuals
** Improving tournament select method that exists (will take them them the next couple weeks)
'''Monday Subteam Meeting Notes:'''
* This week I plan to locate a github repository for seeding purposes
* Finish runs + seeded tests using chest x-ray dataset on PACE
'''Friday Subteam Meeting Notes:'''
* I did some research and found a really good github repo that has a pretrained model on chest xray data that we can use for seeding:
** https://github.com/paloukari/NIH-Chest-X-rays-Classification
* Discussed the goal of having Friday meetings be about sharing interesting papers related to improving our performance
** Neuroevolutionary learning papers 
** More details on the vip goals document:
*** https://docs.google.com/document/d/1jrezh0mv2DKAzgtlhbHza7O9h7FKutCCPlP5enfTmP4/edit?usp=sharing
* Discussed the goal of having initial runs on pace completed by the end of the week

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Conduct a mini lit review for Friday's subteam meeting
|Completed
|September 14, 2020
|September 18, 2020
|September 16, 2020
|-
|Skim the following paper: https://link.springer.com/article/10.1007/s005000000042
|Completed 
|September 14, 2020
|September 18, 2020
|September 16, 2020
|-
|Finish updating notebook entries for the semester thus far
|Completed
|September 14, 2020
|September 21, 2020
|September 20, 2020
|-
|Complete notebook self evaluation
|Completed
|September 14, 2020
|September 21, 2020
|September 20, 2020
|-
|Completing a preliminary run on chest x-ray dataset
|In Progress
|September 7, 2020
|September 21, 2020
| -
|-
|Finding pretrained models for seeding
|Completed
|September 7, 2020
|September 21, 2020
|September 20, 2020
|-
|Complete notebook for the week
|Completed
|September 14, 2020
|September 21, 2020
|September 21, 2020
|}


== ''' September 7th, 2020 [Week 34]''' ==

'''General Meeting Notes:'''
* Labor day weekend - did not have a general meeting this week
'''Monday Subteam Meeting Notes:'''
* Labor day weekend - did not have a monday subteam meeting this week
'''Friday Subteam Meeting Notes:'''
* NLP NN branch should be set up with PACE for our task

* Tusheet updated us on his MySQL errors that he was having
* As I was a tad behind the group in getting everything set up for the run, I hadn't tried it out yet and when I did I realized that I was having similar issues
* MySQL seemed to be having issues due to the mycnf changes that I had made in regards to the path variables changing
* After this MySQL issue is ironed out once and for all, I plan on beginning the preliminary test on the Chest X-Ray dataset
* I also managed to look into the transfer learning concept again, and will be on the lookout for github repositories that contain pretrained models with which we can seed our model
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|NLP NN branch setup on PACE
|Completed
|September 7, 2020
|September 14, 2020
|September 8, 2020
|-
|Resolve new MySQL errors
|Completed
|September 7, 2020
|September 14, 2020
|September 12, 2020
|-
|Completing the preliminary run (hinged upon task above)
|On Hold
|September 7, 2020
|September 21, 2020
| -
|-
|Finding pretrained models for seeding
|On Hold
|September 7, 2020
|September 21, 2020
| -
|}

== ''' August 31st, 2020 [Week 33]''' ==

'''General Meeting Notes:'''
* Discussed team updates
* Our structure for providing updates is that what we work on in the CV group will be relayed by Anuraag to Pulak, who will then bring up pain points and progress updates in Monday meetings
* Stocks subteam was created and divided into three areas:
** ML in trading
** EMADE integration with Pre-Developed indicators
** Developing original technical indicators in EMADE (put on hold until group is more grounded)
* EzCGP divided into two subgroups:
** Group that works on maintaining the current repository and improving the work that has been done so far
** Group for researching and creating new features to incorporate into EzCGP
* NLP:
** Google Colab with EMADE setup for short runs and tests
** Running EMADE on toxicity datasets to observe all features that come out in the pareto front and those that don't
** Adding in more hyperparameters for the evolution to manipulate
** Looking into new functions that can be added into the evolutionary process
** Potential for adding new word embeddings (ELMO, BERT, GPT2/3)
** [CV] Exploring chest x-ray dataset and establishing a baseline run for this semester in order to be able to compare performance later in the semester
** [CV] Define the functions that are already a part of EMADE for CV tasks
** [CV] Definte the functions that must be added for CV tasks
'''Monday Subteam Meeting Notes:'''
* The instructions for setting up PACE to run with EMADE are not exactly working the way that they are currently
* These issues were faced by everyone on the team
* Edits must be made to the mycnf file in order to resolve issues in the path
'''Friday Subteam Meeting Notes:'''
* I worked with the team in order to complete the changes that needed to be made to mycnf file
* Updated the team on the fact that I was successful in launching PACE and being able to run some preliminary tests
* We discussed that our Friday meetings would generally be to update our subgroups (ie. updating the CV group) on our progress, as well as holding sessions with the entire NLP subteam in order to share important ML papers and github repos that we researched and found that may be able to inspire us into adding certain functionalities and features into EMADE in order to improve the performance of EMADE for our task
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Edit mycnf file appropriately after discussion with team
|Completed
|August 31, 2020
|September 7, 2020
|September 5, 2020
|-
|Get PACE to work
|Completed
|August 31, 2020
|September 7, 2020
|September 5, 2020
|-
|Identifying the new dataset we would be using
|Completed
|August 31, 2020
|September 7, 2020
|September 3, 2020
|-
|Finding some papers/github repos that may come in handy in improving EMADE performance for our task
|Completed 
|August 31, 2020
|September 7, 2020
|September 4, 2020
|}

== ''' August 24th, 2020 [Week 32]''' ==

'''General Meeting Notes:'''
* We confirmed our subteams
* I confirmed my choice to be a part of the NLP subteam again this year, specifically working on the CV division of the team
* I will be working on a smaller team within NLP along with Tusheet, Anuraag, Vietfu, and Sanket
* Other teams:
** Stocks
** EzCGP
** Bloat
** Modularity (AFD)
'''Monday Subteam Meeting Notes:'''
* We hopped on a call directly after the general meeting + updates in order to discuss our meeting times and to get on the same page for what to work on this week
* Documented meeting minutes:
** https://docs.google.com/document/d/1jrezh0mv2DKAzgtlhbHza7O9h7FKutCCPlP5enfTmP4/edit?usp=sharing
** Additions/suggestions that I made, so we can improve upon work done over the summer: [[files/Screen Shot 2020-09-21 at 2.42.11 PM.png|thumb|Pictured in blue: additions that I made to the document and discussed with the team|none|543x543px]]
*Anish shared with us a Google Colab file so that we could help in the process of connecting EMADE up with Google Colab
**files/ https://colab.research.google.com/drive/1Hn-nMMNj3R922A_HFelQJwShatdfeONl?usp=sharing
**This Google Colab link would be used in order to try out short runs
**Notes:
***Google Colab is a Jupyter Notebook that is hosted on a Google VM instance
***This allows us to work on a VM with pretty good GPU/TPU, CPU, and RAM specs for free
***It doesn't last however (recycles after ~90 minutes with a closed tab and ~ 18 hours with an open tab)
***Good for testing things and small runs (especially if GPU is needed)
*Each subteam within NLP gave their updates on what their pain points were as well as progress that had been made from the previous meeting
*We officially made the CV subteam after the go ahead from Dr. Zutty
*Anuraag, Tusheet and I from the CV team shared our common issue of having log in issues with PACE
**We all worked together with Pulak in order to resolve these by the end of the meeting
'''Friday Subteam Meeting Notes:'''
* We decided that Anuraag would be in the best position to head the CV team and relay information between team members and Pulak (+ the rest of the NLP team), so that Pulak would be able to add the CV team updates in with the rest of the NLP team updates when updating Dr. Zutty and other teams
* The CV team discussed our to-dos:
** We decided that we would be using PACE for training and EMADE runs (rather than Colab because Colab recycles often and therefore we would have to repeatedly run our trials
** We identified that Anshul had done looking into the CV applications on EMADE, and made note that we would ask him questions as they arose
** Logging into PACE and ensuring environment setup
** Training EMADE on chest x-ray dataset
** Noted that we could potentially make use of Transfer Learning
** All of these updates were to be shared with Dr. Zutty at our next meeting (Anuragg would relay them to Pulak prior to the Monday meeting)
*The experienced members worked together to introduce the document created last semester for PACE setup and installations, so that newer members could be caught up to speed with all of the installation and prep work needed in order to run EMADE on PACE
**https://docs.google.com/document/d/1VIv4EM1KhaUToZpEhKNx3lQs-Hu7QSOk4VEqZcx2tIs/edit?usp=sharing
*Re-review the paper (to really get a soid grip on it)
*Go through both Cifar 10 dataset as well as Chest X-Ray dataset
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Work through the EMADE on Colab setup instructions
|Completed
|August 24, 2020
|August 31, 2020
|August 27, 2020
|-
|Make any edits to the EMADE on Colab setup instructions that are necessary (collaborate on NLP team)
|Completed 
|August 24, 2020
|August 31, 2020
|August 29, 2020
|-
|Study transfer learning as well as other ways to improve EMADE performance for our task + discuss with team
|Completed
|August 24, 2020
|August 31, 2020
|August 30, 2020
|-
|Research the various datasets discussed (Chest X-Ray + Cifar 10)
|Completed
|August 24, 2020
|August 31, 2020
|August 30, 2020
|}

== ''' August 17th, 2020 [Week 31]''' ==

'''General Meeting Notes:'''
* This was our first meeting of the semester!
* We discussed course details and goals
* There was a general rundown of the various subteams that we could choose to be a part of
** Three that stood out to me:
*** CV subteam
*** Stocks subteam
*** NLP subteam (I have been on this team for 2 semesters)
'''Monday Subteam Meeting Notes:'''
* This meeting took place directly after the general class meeting
* We discussed subteam formation, particularly the details of potentially having a CV team within NLP
* I requested that the progress that occured within NLP over the summer be reviewed with the entire team, so that we could be caught up to speed
* While Anish was giving the overview, I made some suggestions for areas where we can improve the NLP codebase and do some work this semester
* Documented meeting minutes:
** https://docs.google.com/document/d/1jrezh0mv2DKAzgtlhbHza7O9h7FKutCCPlP5enfTmP4/edit?usp=sharing
** Additions/suggestions that I made, so we can improve upon work done over the summer: [[files/Screen Shot 2020-09-21 at 2.42.11 PM.png|thumb|Pictured in blue: additions that I made to the document and discussed with the team|none|543x543px]]
*By the end of this meeting, we decided that CV could essentially be a part of the NLP team
* Subteam formation/selection process:
** I decided that I would be a part of the CV team
** I will be working most closely this semester with the rest of the CV team within NLP: Tusheet, Anuraag, Vietfu, Sanket
'''Friday Subteam Meeting Notes:'''
* Once this CV subteam is officially a subteam within NLP, after discussion with Dr. Zutty, we should see who will head the NLP team in general, and then also if it is necessary to assign a head of the CV team so that Pulak (coordinating NLP tasks) doesn't have to keep track of general NLP as well as CV
* Decided tasks that NLP (+ CV) will be working on this semester:
** Get Collab working for NN branch
** NN team:
*** Elmo
*** Bert
*** GPT2 and/or GPT3
**Evolution team:
***Add more functions for mutation
***Add more functions for mating
***Concatenate layers
**CV team:
***Two datasets we could use to figure out a good benchmark dataset (The paper used Chest X Ray dataset):
****Paper: https://stanfordmlgroup.github.io/competitions/chexpert/
****NIH chest xrays
****Cifar 10 / Cifar 100
***Goals: settle on a good benchmark dataset, and get that dataset integrated into EMADE
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Run ideas by Dr. Zutty at general meeting
|Completed
|August 17, 2020
|August 24, 2020
|August 24, 2020
|-
|Log into PACE successfully
|In progress
|August 17, 2020
|August 24, 2020
| -
|-
|Make any version updates to PACE that are needed
|In progress
|August 17, 2020
|August 24, 2020
| -
|-
|Work on adding details to the VIP NLP Goals document
|Completed
|August 17, 2020
|August 24, 2020
|August 18, 2020
|-
|Read Stanford paper on the CV Chest dataset
|Completed
|August 17, 2020
|August 24, 2020
|August 20, 2020
|-
|Do some research into the CV Xray dataset
|Completed
|August 17, 2020
|August 24, 2020
|August 20, 2020
|}

='''<big>Second Semester</big>'''=

== ''' April 20th, 2020 [Week 30]''' ==

'''Lecture Meeting Notes:'''
* Automatically Defined Functions (ADF's) [observing how ADFs can be used to improve upon the evolutionary process]
** Primitive Analysis
*** Try to find the most important and useful aspects of any particular individual
** ADF Selection
*** Tournament method created which selects individuals with more ADFs
** Differential fitness
*** How an individual has evolved since the time of its parents
* Research Fundamentals
** With the use of heuristics, neat-GP aims to control bloat intelligently
** Because it is an intelligent mechanism, and not explicit, it provides an advantage over other bloat-control techniques
** Crossover
*** Commonality between two individuals
** Speciation
*** Assigned based on shared tree structures
** Distance metric may have to be revised as tree structure is not as conductive to this sort of measurement

* NLP
** Neural Network Subteam
*** Use primitives as layers
**** ReluLayer
**** ELU
**** SeLU
**** GloVe Embedding
***** word-to-vector mapping layer
*** Image Warping
*** Toxicity Dataset
**** Classifying upon dataset of toxic comments
*** Text Summarization
**** Stemming and lemmatization performed for words and sentences
**** This team worked with Google Collab to enable future teams to have an easier time working with EMADE
* EZCGP
** Took 41 hours to run 39 generations
** Validation accuracy and score for F1 are really similar
** 16-layer Convolutional Neural Network [VCG-16]
*** 66.32% test accuracy (taking 10 epochs)
** LeNet-5
*** 62.72% test accuracy (also taking 10 epochs)
** Etc.
'''My Slides Made:'''
[[files/Final1.png|center|thumb|480x480px]]
[[files/Final2pic.png|center|thumb|474x474px]]
[[files/Final3.png|center|thumb|476x476px]]
[[files/Final4.png|center|thumb|473x473px]]
[[files/Final5.png|center|thumb|471x471px]]

'''Statistical Analysis:'''

Once the statistical analysis functions were ran on the data we obtained from the EMADE runs, we were able to see how my primitive and Tusheets primitive held upin comparison to different combinations of primitivies applied

We are able to analyze these results and see if the inclusion of any particular primitive actually increases the accuracy of the text summarization, or not

Below is an image which shows the calcualtions of false positive, false negative, true positive, and true negative values for each of the primitives
[[files/Stats1.png|center|thumb|593x593px]]
[[files/Stats2.png|center|thumb|599x599px]]
[[files/Stats3.png|center|thumb|605x605px]]

As seen in these graphs above, NumNamedEntities was less volatile and more precise (if you look at the graph incrementation), and had a slight decrease in performance over time. However, TFISF had less generations (10 compared to 24), but overall had both a better and a relatively steadily improving accuracy.

'''Future Steps:'''
* The TFISF took a longer time, even with optimization, when compared to the NumNamedEntities
* For this reason, in the future semester(s) we are looking to definitely optimize this primitive
* Add more detail to documentation to list all the changes needed when adding a new primitive
* Use an external MySQL implementation to reduce database issues
* Have more time to do analysis/run tests on how our individuals performed
'''Personal Completed Tasks This Semester:'''
* Completed the run of my primitive, which resulted in master.out, worker.out and pareto data
* Ideated various statistical analysis that could potentially be done for the final presentation
* runEMADE.pbs script created in order to queue up the job of creating individuals using the primitive
* MySQL working perfectly
** root and password setup completed
** setup unique port number
* Update input_summaries file with server port number and password information
* Changes to EMADE.py & GP_Framework_Helper.py made in order to properly provide initialization information to EMADE

* PBS script updated for MySQL
* `.my.cnf` written for MySQL
* Documentation for PACE created [in process of updating, with new changes to PACE that were made]
* Unit test for tfisf primitive written
* PBS script written in order to run unit test primitive on PACE
* Primitive ran on PACE (showing that the PACE installations thus far (w/ the exception of MYSQL) were successful)
* Changes made to primitive as needed, based on run [n/a]
** The run was successful and therefore, I concluded that no edits were needed to be made with the tfisf primitive itself
* Completed presentation slides
** Presentation Link - https://docs.google.com/presentation/d/1pHF3fIhblYOS1TLtuORmHh5DDBoqpQEuYQdHitaXcfo/edit#slide=id.p
* Conda environment setup completed
* Downloaded necessary additional packages for EMADE to complete the runs
* Obtained EMADE on PACE ICE using ssh key on gatech github
* MYSQL connected up
* Installed additional packages on PACE that are needed for the runs that I am going to start writing PBS scripts for [from the EMADE README file]
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Statistical tests and analysis of results
|Complete
|March 9th, 2020
|April 20th, 2020
|April 19th, 2020
|-
|Final presentation
|Complete
|April 10th, 2020
|April 20th, 2020
|April 19th, 2020
|-
|Discuss presentation order and rehearse with team
|Complete
|April 10th, 2020
|April 20th, 2020
|April 19th, 2020
|-
|Final peer evaluations
|Complete
|April 10th, 2020
|April 22nd, 2020
|April 21st, 2020
|-
|Final notebook edits
|Complete
|April 20th, 2020
|April 27th, 2020
|April 25th, 2020
|}




== ''' April 17th, 2020 [Week 30]''' ==

'''Lecture Meeting Notes:'''
* Prepared for final presentation
** BlueJeans meeting with Dr. Rohling set for Sunday to review final presentation [5-6pm]
** Discussed our plans for the statistical analysis, as well as our personal progress and how we aim to push through until the presentation with obtaining results
* Discussed putting the guide to EMADE on PACE on the VIP Team Wiki
** [[Guide to Using PACE|https://vip.gatech.edu/wiki/index.php/Guide_to_Using_PACE]]
* Discussed the google doc that Alex started with all of the templates and instructions created regarding EMADE setup on PACE
* The document includes general setup information, from the point of even ssh-ing into PACE
* It contains information regarding configuring and starting the MySQL servers and ssh'ing into a node
* It also includes templates for two different PBS scripts that we found extremely useful: one to queue up a job onto PACE and the other to launch an instance of the MySQL server.
*Template made for `.my.cnf`:[[files/Cnfpic.png|center|thumb|366x366px]]

* Instructions for MySQL port setup:[[files/Portnum.png|center|thumb|472x472px]]

'''Discussed Goal:'''
* Currently working towards accomplishing the '''<u>''third''</u>''' goal on this list:
** (1) Submit a job on PACE without it crashing immediately
** (2) Submit the script to run the MySQL server
** (3) Submit the script to run the unit test on PACE
** (4) Run for individuals with my primitive on PACE to get data for statistical tests
'''Process log for PACE issues:'''
* Up until this point, all of the necessary edits should have been made such that I could easily just add my job onto the queue and run on my port of the MySQL server
* However, even with all of these changes, I was still encountering this error from earlier:
[[files/Errorfile1.png|center|thumb|682x682px]]
* We decided to have another team meeting, so Alex, Anuraag, Tusheet, and I got together via bluejeans to discuss our problems once again and work through this final stretch of errors.
* Upon a little bit of back and forth with them, it seemed that everyone was extremely puzzled with this error and had never experienced anything like it.
* After a couple hours of trying a wide range of things, I decided that perhaps I could just email Jason and he could pinpoint what none of us had been able to see.[[files/Emailpic2.png|center|thumb|784x784px]]

* After a little bit of back and forth and some explanation, he was able to pinpoint my error which was an incorrect path in the PBS script.
* After making this edit, the job submitted perfectly and even after waiting half an hour to view any errors, everything seemed to be running fine!
* From here, all I had to do was continue waiting for the job to complete, and then send the resultant data from my primitive TFISF on to Zack, who would be focusing on the statistical analysis portion, with some input from us
'''Personal Completed Tasks Thus Far:'''

<u>''[NEW]''</u>
* MySQL working perfectly
** root and password setup completed
** setup unique port number
* Update input_summaries file with server port number and password information
* Changes to EMADE.py & GP_Framework_Helper.py made in order to properly provide initialization information to EMADE
* runEMADE.pbs script created in order to queue up the job of creating individuals using the primitive
* Completed the run of my primitive, which resulted in master.out, worker.out and pareto data

'''<u>Worker Output</u>'''
[[files/WorkerFile.png|center|thumb|757x757px]]

'''<u>Master Output</u>'''
[[files/Masteroutput.png|center|thumb|681x681px]]

<u>''[OLD]''</u>
* PBS script updated for MySQL
* `.my.cnf` written for MySQL
* Documentation for PACE created [in process of updating, with new changes to PACE that were made]
* Unit test for tfisf primitive written
* PBS script written in order to run unit test primitive on PACE
* Primitive ran on PACE (showing that the PACE installations thus far (w/ the exception of MYSQL) were successful)
* Changes made to primitive as needed, based on run [n/a]
** The run was successful and therefore, I concluded that no edits were needed to be made with the tfisf primitive itself
* Completed presentation slides
** Presentation Link - https://docs.google.com/presentation/d/1pHF3fIhblYOS1TLtuORmHh5DDBoqpQEuYQdHitaXcfo/edit#slide=id.p
* Conda environment setup completed
* Downloaded necessary additional packages for EMADE to complete the runs
* Obtained EMADE on PACE ICE using ssh key on gatech github
* MYSQL connected up
* Installed additional packages on PACE that are needed for the runs that I am going to start writing PBS scripts for [from the EMADE README file]
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Port setup for MySQL
|Complete
|April 10th, 2020
|April 17th, 2020
|April 10th, 2020
|-
|Successfully complete a run on the server
|Complete
|April 10th, 2020
|April 17th, 2020
|April 17th, 2020
|-
|Obtain all necessary files from the run and pass them along for statistical analysis
|Complete
|April 17th, 2020
|April 20th, 2020
|April 18th, 2020
|-
|Final presentation
|In progress
|April 10th, 2020
|April 20th, 2020
|April 19th, 2020
|-
|Statistical tests need to be done [after ^ are accomplished]
|In progress
|March 9th, 2020
|April 20th, 2020
| -
|}





== ''' April 10th, 2020 [Week 29]''' ==

'''Lecture Meeting Notes:'''
* Updated Zutty and rest of the team (who weren't able to make it to the separate work meeting) on my progress with our errors
* Planned out what calculations and statistical analysis would need to be done in time for the final presentation
* The goal of this analysis is to show whether or not our primitives have or have not aided in getting more accurate summary results
'''Discussed Goal:'''
* Currently working towards accomplishing the '''<u>''second''</u>''' goal on this list, after which the rest should come quite easily:
** (1) Submit a job on PACE without it crashing immediately
** (2) Submit the script to run the MySQL server
** (3) Submit the script to run the unit test on PACE
** (4) Run for individuals with my primitive on PACE to get data for statistical tests
'''Process log for PACE issues:'''
*By this point, although I had not finished solving this error, Alex made some headway on other issues that were further down the line which we should also edit in our PACE environments, so then these changes were what I decided to work on for this past week (along with solving my error)
[[files/SLACK4.png|center|thumb|598x598px]]
* <nowiki>I began by editing `EMADE.py` at the line ~~~~ (_inst.datasetDict[dataset]['type'] == 'summarydata') ~~~</nowiki>
** This line in the file lets us define what's necessary for the individual to even be attempted, in this case the existence of both LearnerMod and TFISF
** I changed this line with information specific to my primitive:[[files/SLACK5.png|center|thumb|767x767px]]

* I also needed to edit the `gp_framework_helper.py` file and add my primitive so that it can actually be used
** This is something we should always do when creating primitives but depending on our setup may not have been done
** <nowiki>A line should be added ~~~~pset.addPrimitive(___)~~~~ with our primitive and information</nowiki>
* Then before continuing, I reinstalled before running again, and cleared my database in between each run
*Running on the same SQL port at the same time
**We then realized that another source of our problems with everything crashing repeatedly was that multiple people cannot run SQL at the same time, while on the same port
**To resolve this, a line had to be added to the `.my.cnf` file with each team member's uniquely chosen port number
**Then the template xml file had to be updated accordingly (in our case, input_summaries) with the new port information [my port number is 3315] [[files/SLACK7.png|center|thumb]]
* Finally, I had to edit some MySQL root configurations because I kept getting permission denied errors when trying to enter the MySQL server
[[files/SLACK6.png|center|thumb|716x716px]]
* The problem was that the user 'root' on my computer did not have a password
* In order to fix this, I visited the PACE SQL documentation and followed the following steps:
** ran FLUSH PRIVILEGES;
** ran UPDATE USER SET password=password('_____<u>my_pass</u>______') where USER='root';
** ran FLUSH PRIVILEGES;
* These steps enabled me to get mysql running with a particular password, set by me

* Furthermore, we decided it would be good to clear the database between EMADE runs
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Make edits to all the files specified by Alex in the Slack
|Complete
|April 3rd, 2020
|April 10th, 2020
|April 6th, 2020
|-
|Set up a unique MySQL port
|Complete
|April 10th, 2020
|ASAP
|April 10th, 2020
|-
|Successfully complete a run on the server
|In progress
|April 10th, 2020
|April 17th, 2020
| -
|-
|Statistical tests need to be done [after ^ are accomplished]
|On hold
|March 9th, 2020
| -
| -
|}




== ''' April 3rd, 2020 [Week 28]''' ==

'''Lecture Meeting Notes:'''
* Updated Zutty and rest of the team (who weren't able to make it to the separate work meeting) on my progress with our errors
'''Discussed Goal:'''
* Currently working towards accomplishing the '''<u>''first''</u>''' goal on this list, after which the rest should come quite easily:
** (1) Submit a job on PACE without it crashing immediately
** (2) Submit the script to run the MySQL server
** (3) Submit the script to run the unit test on PACE
** (4) Run for individuals with my primitive on PACE to get data for statistical tests
'''Process log for PACE issues:'''
* Finally now that the MySQL versioning seems to have been sorted out, I once again tried to submit the PBS script to the queue in order to launch the MySQL instance, and finally it did not error out immediately!
* Now that I was able to completely and effectively debug the reason for the MySQL instance not running properly on the queue, and it no longer crashed immediately after the job was submitted, I had that SQL instance running and decided to try and run something on it.
* In order to do this, I had to replace the host and password information in the template file with the address of the node running the database [for our purposes, this was input_summaries]
* After doing this, all of the configuration should have been ready to simple add the PBS script for EMADE onto the queue and running for data, however, I once again experienced the errors with my job immediately erring out. The following is an image of the error:
[[files/Scnshot1.png|center|thumb|780x780px]]
* The team seemed to think that there was a type and that it should be `xml` not `xsd`, so I went and edited this file
* However, it immediately became clear that this was not a typo whatsoever, and I was back to square one with this error.
* Then the team proposed that perhaps I didn't have all the python packages installed and Pulak suggested that maybe I could just try running `./reinstall.sh` in the EMADE directory
* He thought that perhaps it wasn't a problem with the file, but just something configuration-related that went wrong, so ideally reinstalling everything and trying to run should fix it
* The file itself seemed to have been corrupted because the shell script simply would not run properly, so I decided that since there were only three commands in the script, I would simply run them manually.[[files/SLACK3.png|center|thumb|600x600px]]
* However, when I ran them, I experienced the following error:[[files/Scnshot2.png|center|thumb|573x573px]]

* By this point, although I had not finished solving this error, Alex made some headway on other issues that were further down the line which we should also edit in our PACE environments, so then these changes were what I decided to work on for the upcoming week (along with solving my error)'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Sustain an instance of the MySQL database, so that I can work on the next step
|Complete
|February 22nd, 2020
|April 3rd, 2020
|April 1st, 2020
|-
|Get PACE working again [a job is able to be run ^ pbsmysql.pbs]
|Complete
|February 28th, 2020
|April 3rd, 2020
|April 1st, 2020
|-
|Make edits to all the files specified by Alex in the Slack
|In progress
|April 3rd, 2020
|April 10th, 2020
| -
|-
|Figure out why the error above is occuring
|In progress
|April 3rd, 2020
|April 10th, 2020
| -
|-
|Statistical tests need to be done [after ^ are accomplished]
|On hold
|March 9th, 2020
| -
| -
|}





== ''' March 27th, 2020 [Week 27]''' ==

'''Lecture Meeting Notes:'''
* The beginning of distance learning
* BlueJeans is the platform we will be using for all virtual team meetings
* Had a discussion with the team regarding where we all last left off, and how to progress from there with our PACE problems so that we can accomplish our goal of having statistical tests by the end of the semester, as the very least
'''Discussed Goal:'''
* Upon joining this meeting, I had downloaded but not yet tried out the VPN needed to access the PACE server, so this would be the first step in beginning work in distance format
* Moving forward within the next couple days, I plan on meeting on bluejeans with Alex and Anuraag and whoever else can make the meeting, and figuring our how we can get around the PACE issues we are having, so that we can:
** (1) Submit a job on PACE without it crashing immediately
** (2) Submit the script to run the MySQL server
** (3) Submit the script to run the unit test on PACE
** (4) Run for individuals with my primitive on PACE to get data for statistical tests
'''Process log for PACE issues:'''
* Working towards the first of the four goals listed above: (1) Submit a job on PACE without it crashing immediately
* The job that I am working towards running on PACE is the MySQL server instance through PBS script
* I am no longer getting the same errors, ever since I (1) updated the `environment.yml`, (2) updated the `pbsmysql.pbs` file, (3) wrote the `.my.cnf` file
* Now whenever I submit a job, the following error shows up in the qstat listing, and the job immediately errors out:
[[files/Pic1.png|center|thumb|535x535px]]
* On call with the rest of the team, we discussed different errors that we were all having and perhaps how to fix them. Once I got past the error in the outfiles from the run above, I began experiencing a new error[[files/Slack3.png|center|thumb|774x774px]]

* To resolve this error, my team suggested that I try simply deleting all the database-related folders, and reconfiguring the MySQL server structure from scratch, with proper versioning. To do this I once again followed the document Alex had written up, but this time in tandem with online PACE MySQL setup resources, and this time the error seemed to have solved itself
** Emade Pace Setup Document:
*** https://docs.google.com/document/d/1VIv4EM1KhaUToZpEhKNx3lQs-Hu7QSOk4VEqZcx2tIs/edit?usp=sharing
'''Personal Completed Tasks Thus Far:'''
* PBS script updated for MySQL
* `.my.cnf` written for MySQL
* Documentation for PACE created [in process of updating, with new changes to PACE that were made]
* Unit test for tfisf primitive written
* PBS script written in order to run unit test primitive on PACE
* Primitive ran on PACE (showing that the PACE installations thus far (w/ the exception of MYSQL) were successful)
* Changes made to primitive as needed, based on run [n/a]
** The run was successful and therefore, I concluded that no edits were needed to be made with the tfisf primitive itself
* Completed presentation slides
** Presentation Link - https://docs.google.com/presentation/d/1pHF3fIhblYOS1TLtuORmHh5DDBoqpQEuYQdHitaXcfo/edit#slide=id.p
* Conda environment setup completed
* Downloaded necessary additional packages for EMADE to complete the runs
* Obtained EMADE on PACE ICE using ssh key on gatech github
* MYSQL connected up
* Installed additional packages on PACE that are needed for the runs that I am going to start writing PBS scripts for [from the EMADE README file]
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Sustain an instance of the MySQL database, so that I can work on the next step
|In progress
|February 22nd, 2020
|March 9th, 2020
| -
|-
|Get PACE working again
|In progress
|February 28th, 2020
|March 6th, 2020
| -
|-
|Statistical tests need to be done [after ^ are accomplished]
|On hold
|March 9th, 2020
| -
| -
|}





== ''' March 9th, 2020 [Week 26]''' ==

'''Lecture Meeting Notes:'''
* Midterm presentation [https://docs.google.com/presentation/d/1pHF3fIhblYOS1TLtuORmHh5DDBoqpQEuYQdHitaXcfo/edit#slide=id.p<nowiki>]</nowiki>
'''Personal Progress Notes:'''
* ''Although Alex, Anugraag and I met for 6-7 hours on the Sunday before the presentation, in addition to our team meeting on Friday, we were still not able to figure out all of the bugs with PACE such that we could get our runs to work with MySQL jobs submitting error-free''
* ''For this reason, I am making the decision to simply run EMADE locally and then show the results for this run (along with my Unit Test's results)''
* ''I updated my presentation slides to say that the results I would show would be from the local EMADE runs''
'''Presentation [slides I helped with]:'''
[[files/P1.png|center|thumb|440x440px]]
[[files/P2.png|center|thumb|439x439px]]
[[files/P3.png|center|thumb|434x434px]]
[[files/P4.png|center|thumb|433x433px]]
'''Presentation [my slides]:'''
[[files/Myslide1.png|center|thumb|437x437px]]
[[files/Myslide2.png|center|thumb|438x438px]]
[[files/Myslide3.png|center|thumb|435x435px]]
'''Completed Tasks Thus Far:'''
* PBS script updated for MySQL
* `my.cnf` written for MySQL
* Documentation for PACE created [in process of updating, with new changes to PACE that were made]
* Unit test for tfisf primitive written
* PBS script written in order to run unit test primitive on PACE
* Primitive ran on PACE (showing that the PACE installations thus far (w/ the exception of MYSQL) were successful)
* Changes made to primitive as needed, based on run [n/a]
** The run was successful and therefore, I concluded that no edits were needed to be made with the tfisf primitive itself
* Completed presentation slides
** Presentation Link - https://docs.google.com/presentation/d/1pHF3fIhblYOS1TLtuORmHh5DDBoqpQEuYQdHitaXcfo/edit#slide=id.p
* Conda environment setup completed
* Downloaded necessary additional packages for EMADE to complete the runs
* Obtained EMADE on PACE ICE using ssh key on gatech github
* MYSQL connected up
* Installed additional packages on PACE that are needed for the runs that I am going to start writing PBS scripts for [from the EMADE README file]
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Midterm presentation
|Completed
|February 22nd, 2020
|March 9th, 2020
|March 9th, 2020
|-
|Local EMADE run
|Completed
|March 6th, 2020
|March 9th, 2020
|March 8th, 2020
|-
|Sustain an instance of the MySQL database, so that I can work on the next step
|In progress
|February 22nd, 2020
|March 9th, 2020
| -
|-
|Get PACE working again
|In progress
|February 28th, 2020
|March 6th, 2020
| -
|-
|Statistical tests need to be done [after ^ are accomplished]
|On hold
|March 9th, 2020
| -
| -
|}




== ''' March 6th, 2020 [Week 25]''' ==

'''Lecture Meeting Notes:'''
* Finish up presentations soon
* Discuss time conflicts with the presentation, and schedule our time appropriately
* Finalize role of any newcomers on the team
'''Discussed Goal:'''
* To rectify problems brought on by PACE's scheduled maintenance
** Create a `.my.cnf`file to physically reconfigure MySQL
*** Follow steps from document we created, so we know if any edits need to be made
** Update the PBS script used to start MySQL [some changes need to be made so it is conducive with PACE once more]
* Have proper results from runs by the time of the presentation
** Meet again with Alex and Anuraag to attempt and debug why PACE no longer works on my box (and theirs)
'''Personal Completed Tasks Thus Far:'''
* PBS script updated for MySQL
[[files/Pbs1.png|center|thumb|414x414px]]
* `my.cnf` written for MySQL
[[files/Cnf.png|center|thumb|413x413px]]
* Documentation for PACE created [in process of updating, with new changes to PACE that were made]
* Unit test for tfisf primitive written
* PBS script written in order to run unit test primitive on PACE
* Primitive ran on PACE (showing that the PACE installations thus far (w/ the exception of MYSQL) were successful)
* Changes made to primitive as needed, based on run [n/a]
** The run was successful and therefore, I concluded that no edits were needed to be made with the tfisf primitive itself
* Completed presentation slides
** Presentation Link - https://docs.google.com/presentation/d/1pHF3fIhblYOS1TLtuORmHh5DDBoqpQEuYQdHitaXcfo/edit#slide=id.p
* Conda environment setup completed
* Downloaded necessary additional packages for EMADE to complete the runs
* Obtained EMADE on PACE ICE using ssh key on gatech github
* MYSQL connected up
* Installed additional packages on PACE that are needed for the runs that I am going to start writing PBS scripts for [from the EMADE README file]
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Midterm presentation slides
|Completed
|February 22nd, 2020
|ASAP
|February 28th, 2020
|-
|Create `my.cnf` file
|Completed
|March 6th, 2020
|ASAP
|March 6th, 2020
|-
|Replace old `pbsmysql.pbs` file
|Completed
|March 6th, 2020
|ASAP
|March 6th, 2020
|-
|Meet Alex & Anuraag on weekend to continue working through bugs
|Completed
|March 6th, 2020
|March 9th, 2020
|March 8th, 2020
|-
|Sustain an instance of the MySQL database, so that I can work on the next step
|In progress
|February 22nd, 2020
|March 9th, 2020
| -
|-
|Get PACE working again
|In progress
|February 28th, 2020
|March 6th, 2020
| -
|}



== ''' February 28th, 2020 [Week 24]''' ==

'''Lecture Meeting Notes:'''
* PACE has been down for the past while and our progress has been paused
* Updated the team regarding the errors I was facing with the MySQL runs
* Helped test out the documentation for PACE that my team created, and told them where I was having issues with the instructions so that it could be fixed in iterative development, and would be foolproof documentation for the next group working with PACE
'''Discussed Goal:'''
* Working towards having proper results to present in the midterm presentation, because although I was able to run my unit test, due to problems with PACE this week, we have been unable to run jobs without a host of errors that we have never seen before.
* In order to work towards trying to uncover what changed with PACE, we have to change a few files to ensure certain environment variables have not changed (in case that is causing the issue with PACE - any version changes or environmental variables having been reset)
* Make note of the dos2unix converter
** Coverts to unix files for PBS scripts
* Then a proper setup with the PACE documentation had to be conducted once again, to ensure proper reset of the PACE environment, due to the maintenance period that changed our runs and took place the previous week
''''''Personal'''''' '''Completed Tasks Thus Far:'''
* PBS script written for MySQL
* Documentation for PACE created [in process of updating, with new changes to PACE that were made]
* Unit test for tfisf primitive written
* PBS script written in order to run unit test primitive on PACE
* Primitive ran on PACE (showing that the PACE installations thus far (w/ the exception of MYSQL) were successful)
* Changes made to primitive as needed, based on run [n/a]
** The run was successful and therefore, I concluded that no edits were needed to be made with the tfisf primitive itself
* Completed presentation slides
** Presentation Link - https://docs.google.com/presentation/d/1pHF3fIhblYOS1TLtuORmHh5DDBoqpQEuYQdHitaXcfo/edit#slide=id.p
* Conda environment setup completed
* Downloaded necessary additional packages for EMADE to complete the runs
* Obtained EMADE on PACE ICE using ssh key on gatech github
* MYSQL connected up
* Installed additional packages on PACE that are needed for the runs that I am going to start writing PBS scripts for [from the EMADE README file]
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Write PBS script to see if the MySQL download worked properly 
|Complete
|February 22nd, 2020
|February 28th, 2020
|February 26th, 2020
|-
|Sustain an instance of the MySQL database, so that I can work on the next step
|In progress
|February 22nd, 2020
|February 28th, 2020
| -
|-
|Replace old environment.yaml file
|Complete
|February 28th, 2020
|March 6th, 2020
|February 28th, 2020
|-
|Replace old PACE configuration with new environment variables which enable the runs to work again
|In progress
|February 28th, 2020
|March 6th, 2020
| -
|-
|Meet Alex & Anuraag outside of meeting time to continue working through bugs
|Scheduled
|February 28th, 2020
|March 6th, 2020
|February 28th, 2020
|}




== ''' February 22nd, 2020 [Week 23]''' ==

'''General Meeting Notes:'''
* Midterm hackathon meeting
* Personal goals to accomplish this meeting:
** Get final clarifications about running the PBS script from Alex, as the run itself will be done on my own time [once PACE is back up]
* Group goals to accomplish this meeting:
** Complete at least half of the team presentation
** Discuss what all is needed to be put on the team presentation
** Discuss next milestones, which would be completed by any first semester students on our team
'''Personal Completed Tasks Thus Far [this meeting]:'''
*Broke down the meaning of different aspects of a PBS script
**Notes:
***You need to be on campus, or connect via VPN
***Login Nodes: For logging in and submitting jobs
***Compute Nodes: For running codes/simulations
****No direct access by users
****Allocated per-job by the scheduler [when a job is "submitted"]
***Submitting batch jobs:
****Everything needs to be scripted (eg. a PBS script)
***PBS Script
****A ‘PBS script’ that includes resource requirements, environmental settings, and tasks
****Submitting a job: `qsub  file_name.pbs`
****Check on your submitted job: `qstat`
****Cancel a job: `canceljob <u>_PID_</u>`
***The output and error logs are printed into files [naming specified per script], as they would appear on the screen, and saved in the same directory as the corresponding script.
***Example PBS script w/ annotated meanings:

<nowiki>#</nowiki>PBS -N hello                                                            // Name of the run

<nowiki>#</nowiki>PBS -l nodes=2:ppn=4                                            // 3 nodes, 4 cores each [PACE specifications for this run]

<nowiki>#</nowiki>PBS -l pmem=2gb                                                   // 16GB memory usage total for the run

<nowiki>#</nowiki>PBS -l walltime=15:00:00                                        // Job is killed after 15 hours

<nowiki>#</nowiki>PBS -q coc-ice                                                        // Name of the queue

<nowiki>#</nowiki>PBS -j oe                                                                 // Puts [o]utput & [e]rror files in below format

<nowiki>#</nowiki>PBS -o myjob.out                                                    // Output files for the run

<nowiki>#</nowiki>PBS -m abe                                                             // Notify via email on start and end of job

<nowiki>#</nowiki>PBS -M youremail@gatech.edu                              // Email to send updates

* Writing my own PBS script to run the unit test primitive
** Now that I have broken down how to write the PBS file, I was able to write my own
** I used some of the following resources, as well:
*** http://latisresearch.umn.edu/creating-a-PBS-script
*** https://help.github.com/en/enterprise/2.19/user/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent
** The output that I received when running my unit test and primitive are below
[[files/PACE run.png|alt=PACE run|center|thumb|458x458px]]
[[files/PACE run2.png|center|thumb|455x455px]]
* Completed my personal slides, explaining my primitive to first semester students, in order to show them what type of work they may potentially be doing for our team
[[files/Pres1.png|center|thumb|456x456px]]
[[files/Pres2.png|center|thumb|454x454px]]
[[files/Pres3.png|center|thumb|452x452px]]
* Completed some slides for the team
[[files/Pres4.png|center|thumb|444x444px]]
[[files/Pres5.png|center|thumb|443x443px]]

'''Conclusions:'''
* Unit test for tfisf primitive written
* PBS script written in order to run unit test primitive on PACE
* Primitive ran on PACE (showing that the PACE installations thus far (w/ the exception of MYSQL) were successful)
* Changes made to primitive as needed, based on run [n/a]
** The run was successful and therefore, I concluded that no edits were needed to be made with the tfisf primitive itself
* Completed presentation slides
** Presentation Link - https://docs.google.com/presentation/d/1pHF3fIhblYOS1TLtuORmHh5DDBoqpQEuYQdHitaXcfo/edit#slide=id.p
* Conda environment setup completed
* Downloaded necessary additional packages for EMADE to complete the runs
* Obtained EMADE on PACE ICE using ssh key on gatech github
* MYSQL connected up
* Installed additional packages on PACE that are needed for the runs that I am going to start writing PBS scripts for [from the EMADE README file]
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Writing PBS script for PACE ICE
|Completed
|February 21st, 2020
|February 28th, 2020
|February 22nd, 2020
|-
|Run unit test on tfisf primitive [for this PBS script is required and ^ must be completed first]
|Completed
|February 21st, 2020
|February 28th, 2020
|February 22nd, 2020
|-
|Make changes to tsisf primitive as needed, based on unit test results
|Completed
|February 21st, 2020
|February 28th, 2020
|N/A
|-
|Completed a large portion of the presentation [including my personal slides]
|Completed
|February 21st, 2020
|February 28th, 2020
|February 22nd, 2020
|-
|Write PBS script to see if the MySQL download worked properly 
|In progress
|February 21st, 2020
|February 28th, 2020
| -
|-
|Sustain an instance of the MySQL database, so that I can work on the next step
|In progress
|February 21st, 2020
|February 28th, 2020
| -
|}





== ''' February 21st, 2020 [Week 23]''' ==

'''Lecture Meeting Notes:'''
* Peer evaluations are open for completion
* Make sure to refine notebooks
'''Discussed Goal:'''
* Commit all progress to github on EMADE nlp-app branch
'''Personal Completed Tasks Thus Far:'''
* Now that I was able to activate my account on PACE ICE and I obtained EMADE on PACE ICE using the ssh key on gatech github, the next step is to be able to actually test the primitive which I wrote, on this platform using my unit test.
* To do so, I completed the conda environment setup on PACE this past week and have started work early, with communicated with Alex, on how to write the PBS scripts which allow us to run things on PACE
* Alex has given me a little bit of an overview on how to write a PBS script, where it belongs in the directories, etc.
* I have also found the following resource which I plan on using when I begin writing my own:
** https://pace.gatech.edu/sites/default/files/pace-ice_orientation_1.pdf
'''Conclusions:'''
* Unit test for tfisf primitive written
* Conda environment setup completed
* Downloaded necessary additional packages for EMADE to complete the runs
* Obtained EMADE on PACE ICE using ssh key on gatech github
* MYSQL connected up
* Installed additional packages on PACE that are needed for the runs that I am going to start writing PBS scripts for [from the EMADE README file]
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finish unit test for tfisf primitive
|Completed
|February 14th, 2020
|February 21st, 2020
|February 20th, 2020
|-
|Commit finished unit test to github
|Completed
|February 14th, 2020
|February 21st, 2020
|February 21st, 2020
|-
|Finish Conda environment setup onto PACE
|Completed
|February 14th, 2020
|February 21st, 2020
|February 19th, 2020
|-
|Writing PBS script for PACE ICE
|In progress
|February 21st, 2020
|February 28th, 2020
| -
|-
|Run unit test on tfisf primitive [for this PBS script is required and ^ must be completed first]
|In progress
|February 21st, 2020
|February 28th, 2020
| -
|}

== ''' February 14th, 2020 [Week 22]''' ==

'''Lecture Meeting Notes:'''
* N/A : missed meeting due to being sick
* Get updates from remaining team members
'''Discussed Goal:'''
* Commit all progress to github on EMADE nlp-app branch
'''Personal Completed Tasks Thus Far:'''
* Activate PACE ICE account
* Obtain EMADE on PACE ICE using ssh key on gatech github
* MYSQL connected up
** https://docs.pace.gatech.edu/software/mysql/#mysql-on-pace
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Commit finished unit test to github
|In progress
|January 31, 2020
|February 21st, 2020
| -
|-
|Conda environment setup on PACE
|In progress
|February 14th, 2020
|February 21st, 2020
| -
|-
|Complete VIP notebook updates
|Completed 
|February 14th, 2020
|ASAP
|February 17th, 2020
|-
|Communicate with team about missed meeting
|Completed
|February 14th, 2020
|February 14th, 2020
|February 14th, 2020
|-
|Communicate with professor about missed meeting
|Completed
|February 14th, 2020
|February 14th, 2020
|February 14th, 2020
|}

== ''' February 7th, 2020 [Week 21]''' ==

'''Lecture Meeting Notes:'''
* Notebook evaluation upcoming
* Reviewed with professor whether we were on track with the goals that he had in mind for us at the beginning of the semester. 
'''Discussed Goal:'''
* Make sure to complete individual primitives
* Get MYSQL integrated with EMADE
* Statistical analysis for professor
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finishing touches to individual primitive
|Completed
|February 7th, 2020
|February 14th, 2020
|February 13th, 2020
|-
|Integrate MYSQL with EMADE
|Completed 
|February 7th, 2020
|February 14th, 2020
|February 14th, 2020
|-
|Statistical analysis
|In progress
|February 7th, 2020
|February 14th, 2020
|August 23, 2019
|}




== ''' January 31st, 2020 [Week 20]''' ==

'''Lecture Meeting Notes:'''
* Discussed problems that people were having with PACE
** https://pace.gatech.edu/pace-ice-instructional-cluster-environment-education
* Workshop after meeting to make sure anyone who had questions could ask them to those who had success with it
'''Discussed Goal:'''
* Reconfirmed progress on individual goals
** https://docs.google.com/spreadsheets/d/16LnB6_qO10a7kytynsUZIDvcSJ2k5sMugYYCchePBy4/edit?usp=sharing
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Need help running EMADE, in general [on PACE and without]
|In progress
|January 31, 2020
|February 7th, 2020
| -
|-
|Write unit test(s) for tfisf priitive
|In progress
|January 31, 2020
|February 14th, 2020
| -
|-
|Run the unit tests on PACE ICE in order to confirm credibility of primitive
|In progress
|January 31, 2020
|February 14th, 2020
| -
|-
|Edit primitive if needed
|In progress
|January 31, 2020
|February 7th, 2020
| -
|-
|Contribute to group goal of creating list of tests
|Completed
|January 31, 2020
|February 7th, 2020
|January 31, 2020
|}
== ''' January 24th, 2020 [Week 19]''' ==

'''Lecture Meeting Notes:'''
* Reminder that we can use PACE ICE to run anything we need in regards to testing
* Roles to assign:
** Confirm we can all run PACE ICE
*** Created a key in PACE and linked it to the GaTech Github so we can SSH into EMADE forked on our git
** Get EMADE onto PACE ICE
*** Will have to delete lots of unused data sets, etc, in order to get everything onto EMADE so it can run
** Make sure our individual primitives work by writing unit tests
** Edit primitives if needed
** Create list of statistical tests to run
** Modify dataset (perhaps shrink size)
'''Discussed Goal:'''
* Reconfirmed progress on individual goals
** https://docs.google.com/spreadsheets/d/16LnB6_qO10a7kytynsUZIDvcSJ2k5sMugYYCchePBy4/edit?usp=sharing
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Create google doc spreadsheeet to coordinate individual goals
|Completed
|January 24, 2020
|January 31, 2020
|January 24, 2020
|-
|Make sure git works on PACE ICE
|Completed
|January 24, 2020
|January 31, 2020
|January 24, 2020
|-
|Get EMADE onto PACE ICE
|Completed 
|January 24, 2020
|January 31, 2020
|January 31, 2020
|-
|Write unit test(s) for tfisf priitive
|In progress
|January 24, 2020
| -
| -
|-
|Run the unit tests on PACE ICE in order to confirm credibility of primitive
|In progress
|January 24, 2020
| -
| -
|-
|Edit primitive if needed
|Completed
|January 24, 2020
|January 31, 2020
|January 31, 2020
|-
|Contribute to group goal of creating list of tests
|In progress
|January 24, 2020
|January 31, 2020
| -
|}





== ''' January 17th, 2020 [Week 18]''' ==

'''Lecture Meeting Notes:'''
* Time conflict: 4pm on Fridays in Klaus, old meeting room.
* Received PACE ICE account
* Agreed to use t-statistic as a testing measure
* Develop more primitives (to increase the population and make testing more accurate)
'''Discussed Goal:'''
* Discussed the goals of genetic algorithms
* Discussed the goals of the VIP group
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Assist text summarization team subteam to brainstorm methods of hypothesis testing
|Completed
|January 17, 2020
|January 24, 2020
|January 24, 2020
|-
|Fill the second when is good form to decide on meeting day that works for everyone
|Completed 
|January 17, 2020
|January 24, 2020
|January 17, 2020
|}








== ''' January 10th, 2020 [Week 17]''' ==

'''Lecture Meeting Notes:'''
* Beginning of semester meeting for time conflict members
* Same team was chosen for VIP [nlp team, but time conflict]
* Time conflict: 4pm on Fridays at Van Leer 483B [later changed to Klaus in old meeting room].
'''Discussed Goal:'''
* The next few meetings of this semester has the following goals:
** Assure we have used  made significant improvements to NLP using EMADE
** How can we back up the usefulness of our work?
** Evaluate the usefulness of our progress within EMADE.
** Test our work using:
*** T-Statistic
*** Ttest_ind
*** Python SciPy library
*** Welch t-test
**** Computing t-statistic without a population mean/standard deviation
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Subteam selection
|Completed
|January 10, 2020
|January 17, 2020
|January 10, 2020
|-
|Review cumulative subteam goals
|Completed 
|January 10, 2020
|January 17, 2020
|January 10, 2020
|-
|Make hypothesis-testing parameters for subteam project
|In progress
|January 10, 2020
|January 17, 2020
|January 17, 2020
|-
|Join new time conflict slack
|Completed
|January 10, 2020
|January 17, 2020
|January 10, 2020
|}

='''<big>First Semester</big>'''=


== ''' December 2nd, 2019 [Week 16]''' ==

'''Lecture Meeting Notes:'''
* Subgroup presentation link: h[https://docs.google.com/presentation/d/1KPcNsmbxPipkDncRDKeScHZCRR71gXoDNyumCGimTVc/edit?usp=sharing ttps://docs.google.com/presentation/d/1KPcNsmbxPipkDncRDKeScHZCRR71gXoDNyumCGimTVc/edit?usp=sharing]
* GitHub Repository Link:  https://github.gatech.edu/emade/emade
* Presented subteam progress since the beginning of the semester, as well as my individual contribution (the tf-isf primitive for text processing in EMADE)
* Text Classification
** Made headway in stemming and lemmatization
* Text Summarization
** Still in the process of fully integrating everything into EMADE
** Created a 'padding' of 255 sentences [if this is not filled out, blank sentences are used to pad]
** 3 new primitives were made (by first semester students)
*** TFISF [my contribution to the NLP subteam this semester]
*** Text rank
*** Num named entities
** Did not finish integration into EMADE as the process did not finish running
* Personal contribution
** Created my own primitive with a partner and added it to text_processing_methods.py in the EMADE framework
** This was the tf-isf primitive
** Learned a lot of new information about how exactly EMADE data-pairs worked
** Learned about NLP concepts that I did not know before
*** Read this research paper: https://www.sciencedirect.com/science/article/pii/S1877050915006869
**** Details different automatic text summarization methods
** Presented my primitive to the class & detailed goals for the upcoming semester [[files/TF-ISF 1.png|none|thumb|415x415px|TF-ISF 1]][[files/TF-ISF 2.png|none|thumb|415x415px|TF-ISF 2]]

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Finish up VIP notebook
|Complete
|November 25, 2019
|December 4, 2019
|December 3, 2019
|}

== ''' November 25th, 2019 [Week 15]''' ==

'''Lecture Meeting Notes:'''
* Do the peer evaluation
* Complete the primitive, add it to the text_processing_methods.py within the EMADE framework
* Push our changes to the nlp-app branch
* Add our primitive & talking points to the slides
* Final presentations: Monday, 2nd December 2019

'''Subteam Meeting Notes:'''
* Nesha made final edits to the primitive
* I completed the testing of the primitive with a csv file as the expected data input from EMADE
* I accordingly made a few edits, and then proceeded to push the final tf-isf primitive to the github
[[files/Final primitive.png|none|thumb|427x427px|Final primitive]]
[[files/Final primitive continued.png|none|thumb|429x429px|Final primitive continued]]

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Complete the primitive
|Completed
|November 4, 2019
|November 26, 2019
|November 24, 2019
|-
|Push to github
|Completed
|November 25, 2019
|November 25, 2019
|November 25, 2019
|-
|Complete presentation slides
|Completed
|November 25, 2019
|December 2, 2019
|December 1, 2019
|-
|Do the final year-end peer evaluation
|Completed
|November 14, 2019
|December 4, 2019
|December 1, 2019
|-
|Complete VIP notebook
|In progress
|November 25, 2019
|December 4, 2019
|December 3, 2019
|}

== ''' November 18th, 2019 [Week 14]''' ==

'''Lecture Meeting Notes:'''
* Class-wide hackathon planned for the weekend
* Get ready for final presentations with my subteam group
* Complete our contribution to the presentation (our primitive)
* Not sure how to put the new values for each sentence back into a data pair object

'''Subteam Meeting Notes:'''
* Continued work on our primitive
* Solved our problem of not knowing how to iterate through our input data [used np.nditer()]
* Made use of the pandas DataFrame to visualize the vectorizer step of our tf-isf method, so we could identify any problem areas and inconsistencies
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Tf-isf primitive
|In progress
|November 4, 2019
|November 26, 2019
|November 24, 2019
|-
|Check if I can make it to the hackathon
|Completed
|November 18, 2019
|November 22, 2019
|November 19, 2019
|}

== ''' November 11th, 2019 [Week 13]''' ==

'''Lecture Meeting Notes:'''
* Bloat
** Collecting more data and benchmarking results
* ezCGP
** First semester students working on visualization of individuals and mating
'''Subteam Meeting Notes:'''
* Worked on tf-isf primitive
* Had difficulty iterating through the incoming data pair object from EMADE
* Were confused regarding how the tfidfvectorizer assigned values to data (what did the object return)
** https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XefamzJKifW
*** This website helped me understand how a vectorizer worked, and the input/output of a vectorizer object
*** It also introduced the concept of a DataFrame object, which is used for visualization and traansparency of the data you are working with
*** We were able to utilize this later, when trying to understand the output of our tfisf function
** I found the following website that gave me more information about this:
*** https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html

* We decided that in order to truly observe how our primitive was working, at different stages in the tf-isf process, we needed sample data
[[files/Sample Train & Test Dataset.png|none|thumb|456x456px|Sample Train & Test Dataset]]

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Tf-isf primitive
|In progress
|November 4, 2019
|November 26, 2019
|November 24, 2019
|-
|Updating notebook
|Completed
|November 11, 2019
|November 18, 2019
|November 15, 2019
|}

== ''' November 4th, 2019 [Week 12]''' ==

'''Lecture Meeting Notes:'''
* Updates from other subteams
** Preprocessing
*** Working on image processing
** ezCGP
*** Working on increasing the speed of their code
*** Fixed parallelization
** ADF
*** Fix seeding problems
** Bloat control
*** Aims for NeatGP to work along with EMADE
*** Working on benchmark testing

'''Subteam Meeting Notes:'''
* NLP
** Text summarization
*** Adding primitives [using first semester students]
*** Finding tf-isf
** Text classification
*** Sentiment analyzation
* Tf-isf
** "Term frequency inter sentence frequency"
** Assigns every sentence a particular tf-idf score
** Must ensure that sentence length doesn't affect
* Data comes into the primitive as rows and columns
** Each row is a paragraph 
** Each column is a unique sentence 
* Follow the count vectorizer function to write tf-isf primitive 
* Consider the fact that each sentence is a different length
** Certain sentences shouldn't have a greater score simply because it has more words 
** Solve for this problem 
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Branch off of nlp-app
|Completed
|November 4, 2019
|November 11, 2019
|November 4, 2019
|-
|Begin writing primitive, following after the count vectorizer function
|Completed
|November 4, 2019
|November 11, 2019
|November 4, 2019
|-
|Update text_processing_methods.py with our primitive
|In progress
|November 4, 2019
|November 26, 2019
|November 24, 2019
|}

== ''' October 28th, 2019 [Week 11]''' ==

'''Lecture Meeting Notes:'''
* Selected the NLP team to join
* Analyzed of whether to join NLP Text Classification or NLP Text Summarization
* Joined NLP Text Summarization
* Received updates from the other sub teams as well
** ezCGP
*** Working on writing more effective code
** ADF
*** Testing
** NLP
*** Beginning to assign first years the task of building primitives
*** Working on integration with EMADE

'''Subteam Meeting Notes:'''
* Asked general questions regarding how primitives work in EMADE
* Asked about how EMADE works with text processing
* Asked how our work (creating primitives) would play into the overall course of the NLP text summarization project [big picture view]
* Fellow team member Nesha and I were assigned the primitive tf-isf to create so that EMADE could use it to process incoming text data
* Provided with research paper as a resource to read about tf-isf, as the code we have to look at is only for tf-idf
** https://www.sciencedirect.com/science/article/pii/S1877050915006869
* Tf-idf
** "Term frequency inter document frequency"
** Assigns every term/word a particular tf-idf score
** The score would be lesser if the term appears in a document fewer times
** A lesser score indicates that the word is more important, as it is more rare
* Our task is to look at the tf-idf to understand how we should implement tf-isf
* Tf-isf
** "Term frequency inter sentence frequency"
'''Completing Assigned Task:'''
* I found the following resources, when doing background research about coding a TF-IDF function in Python
** https://towardsdatascience.com/tfidf-for-piece-of-text-in-python-43feccaa74f8
** https://www.geeksforgeeks.org/tf-idf-model-for-page-ranking/
* Then I walked through the code provided on these sites, so that I could understand what may be the best way for me to write my function
* When I thought I had a good place to start, I began to code in Jupyter notebook
[[files/Calculated TF Term Frequency.png|thumb|Calculated TF Term Frequency|401x401px|none]]

[[files/Calculated IDF InterDocument Frequency.png|thumb|401x401px|Calculated IDF InterDocument Frequency|none]]

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Join NLP slack
|Completed
|October 28, 2019
|November 4, 2019
|October 28, 2019
|-
|Understand the steps of tf-idf, conceptually
|Completed
|October 28, 2019
|November 4, 2019
|November 1, 2019
|-
|Implement a basic tf-idf function
|Completed
|October 28, 2019
|November 4, 2019
|November 1, 2019
|}

== ''' October 21st, 2019 [Week 10]''' ==

'''Lecture Meeting Notes:'''
* GitHub Repository Link: https://github.gatech.edu/bsu32/emadeGroup
* Learned about the comparison between single objective, multi-objective, and EMADE results from various groups
* Teams of first semester students presented their EMADE analysis of the Titanic dataset
* Teams of experienced students presented their subteam specializations, and how they were using EMADE to tackle various research problems.
* Subteam options
** NLP - Natural Language Processing
*** Text summarization
**** Building primitives for text data
*** Text classification
** ezCGP - Cartesian Genetic Programming
** EMADE Data Preprocessing
** Bloat Control
** ADF

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Look further into each team & ask questions to interesting groups after presentations
|Completed
|October 21, 2019
|October 28, 2019
|October 21, 2019
|-
|Select a team for the remaining of the semester
|Completed
|October 21, 2019
|October 28, 2019
|October 24, 2019
|}


== ''' October 16th, 2019 [Week 9]''' ==

'''Lecture Meeting Notes:'''
* Subteam workshop to work on EMADE version of Titanic presentation
* Must use EMADE and MySQL

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Develop a solution for the Titanic problem using EMADE and MySQL
|Completed
|October 16, 2019
|October 21, 2019
|October 19, 2019
|-
|Set up MySQL server with common master and remaining workers
|Completed
|October 16, 2019
|October 21, 2019
|October 19, 2019
|-
|Modify input_titanic.xml
|Completed
|October 16, 2019
|October 21, 2019
|October 18, 2019
|-
|Complete presentation
|Completed
|October 16, 2019
|October 21, 2019
|October 20, 2019
|}
[[files/Diagram Pareto Front 3D.png|center|thumb|Diagram Pareto Front 3D]]
[[files/AUC for EMADE.png|center|thumb|AUC for EMADE]]
[[files/Screen Shot 2019-12-04 at 9.33.55 AM.png|center|thumb|Presentation]]
[[files/Titanic Presentation.png|center|thumb|Titanic Presentation]]

== ''' October 9th, 2019 [Week 8]''' ==

'''Lecture Meeting Notes:'''
* EMADE crash course
* Input file
** input_titanic.xml
** Configures all of the various inputs needed for EMADE to run properly
** input data comes in the form of zipped csv files
** data goes through 5 Monte Carlo trials
** a datapair object in EMADE consists of a test file and a train file
** a row in the EMADE data set is an individual
** a column in the EMADE data set is a feature
* Qualities of EMADE
** Weights specify if a particular feature should be maximized (1.0) or minimized (-1.0)

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Install MySQL
|Completed
|October 9, 2019
|October 16, 2019
|October 11, 2019
|-
|Configure MySQL Server
|Completed
|October 9, 2019
|October 16, 2019
|October 11, 2019
|}



== ''' October 2nd, 2019 [Week 7]''' ==

'''Lecture Meeting Notes:'''
* Subgroup presentation link: https://docs.google.com/presentation/d/1RwcyFbL1C-e6YEfBiACScUvhBgpbBREkZtY7o0-oYl4/edit?usp=sharing
* GitHub Repository Link: https://github.gatech.edu/bsu32/emadeGroup
* Presented findings on Kaggle Titanic Dataset

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Set up emade
|Completed
|September 25, 2019
|October 9, 2019
|
|-
|Finish notebook
|Completed
|September 25, 2019
|October 4, 2019
|October 4, 2019
|}

== ''' September 25th, 2019 [Week 6]''' ==

'''Lecture Meeting Notes:'''
* Subgroup presentation link: https://docs.google.com/presentation/d/1rMDDg4WH29CR59xe8M1DLh-Vgbc1CmPQwzuPh_Sp90o/edit?usp=sharing
* GitHub Repository Link: https://github.gatech.edu/bsu32/emadeGroup
* Presented findings on Kaggle Titanic Dataset
* Single objective vs multiple objective genetic programming algorithms

'''Personal Notes:'''
* Simple primitives: - and, or, not, +, -, /, %
'''Initial Team Responsibilities (September 30, 2019):'''
* Brandon & Tusheet:
** Single Objective Genetic Programming and Evolutionary Algorithm (SelBest) [this was changed in later iterations]
* Tushna & Nesha:
** Area under the curve per generation with reimanns sum
** Graphical representation of AUC
* Sanket:
** Feature set
** Presentation
'''Final Team Contribution (October 2, 2019):'''
* Brandon & Tusheet:
** Single Objective Genetic Programming and Evolutionary Algorithm (SelBest)
* Tushna & Nesha:
** Multiple Objective Genetic Programming and Evolutionary Algorithm (selNSGA2)
[[files/Screen Shot 2019-10-02 at 4.20.42 PM.png|thumb|317x317px|Pareto frontier code|none]]

[[files/Screen Shot 2019-10-02 at 4.33.45 PM.png|thumb|341x341px|selNSGA2|none]]
* Area under the curve per generation with reimanns sum
[[files/Screen Shot 2019-10-02 at 4.14.51 PM.png|thumb|346x346px|Area under curve per generation|none]]

*Graphical representation of AUC
**[[files/Screen Shot 2019-10-02 at 4.06.01 PM.png|none|thumb|330x330px]]
** Presentation
* Sanket:
** Feature set
** Presentation

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Research multiple objective genetic programming algorithms
|Completed
|September 25, 2019
|October 2, 2019
|September 30, 2019
|-
|Book crosland room for team meeting
|Completed
|September 25, 2019
|September 26, 2019
|September 25, 2019
|-
|Peer evaluations (due next week)
|Deffered
|September 25, 2019
|October 4, 2019
|October 4, 2019
|-
|Apply multiple objective genetic programming algorithm on Titanic data set
|Completed
|September 25, 2019
|October 2, 2019
|October 2, 2019
|-
|Create non-dominating graph of false negatives vs false positives per generation
|Completed
|September 25, 2019
|October 2, 2019
|October 2, 2019
|-
|Calculate Area Under the Curve for each generation with reimanns sum
|Completed
|September 25, 2019
|October 2, 2019
|October 2, 2019
|-
|Graphically represent AUC (y-axis) for each generation (x-axis)
|Completed
|September 25, 2019
|October 2, 2019
|October 2, 2019
|-
|Create presentation
|Completed
|September 25, 2019
|October 2, 2019
|October 2, 2019
|}

== ''' September 18th, 2019 [Week 5]''' ==

'''Lecture Meeting Notes:'''
* Subgroup presentation link: https://docs.google.com/presentation/d/1rMDDg4WH29CR59xe8M1DLh-Vgbc1CmPQwzuPh_Sp90o/edit?usp=sharing
* Presented findings on Kaggle Titanic Dataset

'''Personal Notes:'''
* Improving feature sets & results:
** Whitening
*** Some interesting information regarding whitening: http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/
** Normalizing all data so it's on the same scale
** Affects models that take into consideration distance between numerical data clusters
** Vectorization
*** Some interesting information regarding vectorization: http://enhancedatascience.com/2018/05/07/machine-learning-explained-vectorization-matrix-operations/
** Weighing different features differently
** Multiple learners

'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Research above concepts
|Completed
|September 18, 2019
|September 25, 2019
|September 20, 2019
|}

== ''' September 11th, 2019 [Week 4]''' ==

'''Lecture Meeting Notes:'''
* Cleaning data is the first step to creating a machine learning algorithm
* Tools:
**Pandas takes in CSVs and data files and represents them as tabular frames in python
*** Pandas functions allow you to replace rows and manipulate the data tables.
** Numpy
*** Allows for mathematical operations to be done on row/column data in the table
** Scikit
*** Enables the data training, testing and splitting process
*** Splits the data randomly between test and training data
* Model architectures
** Random forest
** Decision trees
* Codominance

'''Personal Notes:'''
* Regressions vs classifiers
** Regressions are for continuous data output (ie. predicting housing prices)
** Classifiers are for discrete data where there is only one answer (ie. who survived vs died, who won the election, what animal is in this image)
*** Survived vs died is a binary classifier
* Linear/Quadratic regression
** Rather than binary classification, regression predicts largely variable data output
** Videos: Andrew Ng [ML specialist at Stanford]
* Preprocessing
** Reducing dimensionality
** Dimensions are features
*** Titanic: gender, cabin level
** The model will get confused with too many features, and may overfit the model
* Testing
** Keep your testing and training data separate, so you can avoid overfitting your model to the data

* Important features identified:
** Keep in mind:
*** If data is incomplete (can fill in data that is incomplete, on your own)
** Definitely important:
*** Passengers class
*** Passengers sex
*** Passengers age
** Maybe important:
*** Passengers parch
*** Passengers fare
'''Team Meeting Notes:'''
*Recorded in jupyter notebook
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Research concepts discussed in lecture, on my own
- Look into different models for training (random forest, decision tree, linear regression, quadratic regression)

- Get familiar with SciKit-Learn, k nearest neighbor, etc.
|Completed
|September 11, 2019
|September 18, 2019
|September 13, 2019
|-
|Pip install SciKit-Learn
|Completed
|September 11, 2019
|September 18, 2019
|September 13, 2019
|-
|Meet with sub-team
|Completed
|September 11, 2019
|September 18, 2019
|September 15, 2019
|-
|Select features
|Completed
|September 11, 2019
|September 18, 2019
|September 15, 2019
|-
|Create 5 algorithms
|Completed
|September 11, 2019
|September 18, 2019
|September 17, 2019
|-
|Create presentation
|Completed
|September 11, 2019
|September 18, 2019
|September 16, 2019
|}

== ''' September 4th, 2019 [Week 3]''' ==

'''Lecture Meeting Notes:'''
* Important measures for an efficient algorithm:
** Minimized errors of misclassification
** Memory usage
** Space complexity
** Maximizing true positives
** Minimizing false positives
** Time efficient
** Secure
** Precision
** Usability
** Cost effective
* Confusion matrix:
** Sensitivity = (# of true positives) / (# of true positives + # of false negatives)
** Specificity = (# of true negatives) / (# of true negatives + # of false positives)
* Each algorithm has a score that represents 
'''Discussed Goal:'''
* Discussed the application of cancerous cell detection with sub-team, and determined that we would like to move forward with the deep learning team
** Sensitivity = (# of true positives) / (# of true positives + # of false negatives)
** Specificity = (# of true negatives) / (# of true negatives + # of false positives)
* Discussed how our current algorithms could work with the deep learning team.  Specifically how could our image processing and feature extraction be utilized by the deep learning team
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Attend lecture & take notes
|Completed
|September 4, 2019
|September 11, 2019
|September 4, 2019
|-
|Update notebook
|Completed 
|September 4, 2019
|September 11, 2019
|September 9, 2019
|-
|Complete Lab 2 pt. 2
|Completed
|September 4, 2019
|September 11, 2019
|September 9, 2019
|}

== ''' August 28th, 2019 [Week 2]''' ==

'''Lecture Meeting Notes:'''
* In true genetic programming, the individual is the function itself.
* In this way, when we are talking about mutations and crossovers in the 'individual', we are talking about mutations and crossovers in the algorithm.
* Trees are used to represent various aspects of a genetic program. In this way, it is easier to identify pieces which will later be mutated or merged with other trees as crossover.
* Leaf nodes = terminals
* Other nodes = primitives or operations
* Data storage for a program is done using lisp preordered parse trees.
* Mutations in genetic programming:
** Removing/adding/changing a node
** Removing/adding a subtree
* Crossovers in genetic programming:
** Genetic programming employs single point crossovers [subtrees are swapped from a single point on the tree]
* Weakly-typed genetic program: allows for the use of broader set of primitive values
* Strongly-typed genetic program: requires a certain type
* Evaluating a tree:
** Place an input into the tree, and compare tChe output with the truth.
** This allows you to measure the error.
* Complex primitives
** Can move past using basic operations as primitives (+, -, *, /) and can also use factorials and powers
** This allows for less complex trees, that are more efficient and easier to understand
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Attend lecture & take notes
|Complete
|August 28, 2019
|September 4, 2019
|August 28, 2019
|-
|Update notebook
|Complete
|August 28, 2019
|September 4, 2019
|September 1, 2019
|-
|Complete Lab 2 pt. 1
|Complete
|August 28, 2019
|September 4, 2019
|September 3, 2019
|}

== ''' August 21st, 2019 [Week 1]''' ==

'''Lecture Meeting Notes:'''
* Genetic Algorithms:
** Created using processes observed in nature, such as mating, mutation, and crossover.
** These processes are applied to individuals in the previous population, and then their fitness is evaluated.
** Theoretically, with numerous repetitions of the process, the best individual (one with idea fitness) would result.
* Objective:
** A value that is being either maximized or minimized, and is used as a benchmark to evaluate the fitness of an individual
* Fitness:
** Measure of how an individual compares to others, based on the objective.
** The higher the fitness value, the higher the probability of being chosen for mating.
* Crossover
** Represents mating between individuals.
* Mutation
** Random modifications, to maintain diversity, just as found in nature.
* Applying Algorithm to Population
** Initialize a population, randomly, and determine its fitness
*** Repeat:
**** Select parents from population
**** Perform crossover (creating new population)
**** Perform mutations on that population
**** Determine population fitness
** Example:
*** One Max Problem [Lab 1]
**** Begins with individuals that each have a list randomly instantiated with 1s or 0s
**** Goal: eventually produce an individual with a list of all 1s
'''Discussed Goal:'''
* Discussed the goals of genetic algorithms
* Discussed the goals of the VIP group
'''Action Items:'''
{| class="wikitable"
!Task
!Current Status
!Date Assigned
!Suspense Date
!Date Resolved
|-
|Attend weekly lecture
|Completed
|August 21, 2019
|August 28, 2019
|August 21, 2019
|-
|Download Anaconda
|Completed 
|August 21, 2019
|August 28, 2019
|August 23, 2019
|-
|Set up environment & import required libraries
|Completed
|August 21, 2019
|August 28, 2019
|August 23, 2019
|-
|Complete Lab 1 [located in VIP GitHub repository]
|Completed
|August 21, 2019
|August 28, 2019
|August 25, 2019
|-
|Review lecture notes
|Completed
|August 21, 2019
|August 28, 2019
|August 25, 2019
|-
|Create VIP notebook
|Completed
|August 21, 2019
|August 28, 2019
|August 25, 2019
|}