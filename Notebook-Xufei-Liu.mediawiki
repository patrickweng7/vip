== Team Member ==
[[files/Picture of me.jpg|thumb|Picture of Xufei Liu]]
Name: Xufei Liu

Email: xliu725@gatech.edu

Interests: Operations research, machine learning, combinatorics, stochastic processes

= Fall 2021 =
Subteam: Modularity 

Members: [https://github.gatech.edu/emade/emade/wiki/Notebook-Vincent-Huang Vincent Huang], [https://github.gatech.edu/emade/emade/wiki/Notebook_Angela_Young Angela Young], [https://github.gatech.edu/emade/emade/wiki/Notebook_Bernadette_Gabrielle_Santiago_Bal Bernadette Bai]

== October 1, 2021 ==

====== Lecture Notes ======
* Peer evaluations due next week and open on Monday
* Modularity
** Depth is increased but Vincent still has to debug
** Having trouble logging into the sql node, but could only sometimes connect
*** Potential check: SQL might not let us connect based on what node we're connecting from, need to change sql settings
*** Check the accounts that we made with wildcard host, we want user@% host.
*** mysql -h nodename -u username -p
*** Want username to be xliu725, but if we go to the node that we see that logging in locally (localhost), we get two users from two nodes
** 'xliu725'@'%'
** https://docs.pace.gatech.edu/interactiveJobs/interactive_cmd/ (may want an interactive job if we can't ssh)
* Stocks
** Team split up
** EMADE full individuals are a length of list 4
** Various crossovers allow you to cross over multiple individuals
*** Can also crossbreed

====== Action Items ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Peer evaluations due
|Incomplete
|10/1/2021
|10/8/2021
|
|-
|Clean up notebook
|Incomplete
|10/1/2021
|10/8/2021
|
|-
|Uploade new emade-cloud folder to google collab
|Incomplete
|10/1/2021
|10/8/2021
|
|-
|}

== September 24, 2021 ==

====== Subteam Meeting 9/25 ======
* Continued to work to set up PACE
** Found out walltime is 8 hours - worried that we can't get long-enough runs
** May transition back to google collab for now
* Bernadette and I managed to connect to our database for a minute before getting kicked out again. Will continue to work on this issue
** Currently working on fixing the MNIST dataset as well
* Vincent is looking into an error in the adf.py codebase that I may try to help debug as well.
* Bernadette managed to merge the stocks team dataset and evaluation functions with our codebase on Vincent's github branch found here: https://github.gatech.edu/vhuang31/emade

====== Personal Exploration ======
* Currently working on setting up the rest of PACE-ICE
** Managed to start an instance of the database but can't verify that it works
** Keep getting error when I run mysql -h atl1-1-02-012-5-l
* Connected for a moment once more but couldn't get it to function continuously
* Working on getting emade into database as well
** Recloned emade from Vincent's branch using git clone
** Working on using SCP to get emade onto pace.
** Code is: scp -r emade xliu725@pace-ice.pace.gatech.edu:~

====== Action Items ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Figure out how to connect to the database and verify connection
|Complete
|9/24/2021
|10/1/2021
|9/30/2021
|-
|}


== September 17, 2021 ==

====== Lecture Notes ======
* Things we need to do for the next meeting dealing with images
** Check out our branch
** Make a subfolder for our images
** Change it locally 
** Push changes
* PACE-ICE work
** /storage/home/hpaceice1/shared-classes/materials/vip/AAD
** Numhosts is how many workers are needed to run
** Workers per host is number of subprocesses
* Stocks
** Working on recreating their paper 
* Modularity 
** Currently working on runs with mnist dataset
** Issue with MacBooks trying to unpack files and messing up Google collab
** Look at runs with stocks dataset in the future to see if that works
** Potentially look at Jasonâ€™s algorithm when looking at mnist data
** Not enough generations last semester which may have caused our messed up pareto fronts

====== Subteam Meeting 9/23 ======
* Working together to set up PACE-ICE for emade.
** Can't connect to our cluster when we're trying to check the database
** Figured out how to run the database and connect to PACE-ICE

====== Personal Exploration ======
* Working on installing PACE-ICE and getting it set up. Below are the commands that are run and debugging done (link with instructions is at https://github.gatech.edu/emade/emade/wiki/Guide-to-Using-PACE-ICE):
** First I ssh'd into pace from terminal
** Created the scratch folder and db folder within scratch
** Had issues with the .my.cnf file and transferring it to pace using SCP
*** Also needed to change the "USERNAME" to "xliu725".
*** Instead, connected to PACE first and used vim to create the .my.cnf folder in the ~ directory by copying and pasting.
** Ran mysql_install_db --datadir=$HOME/scratch/db
** entered the user folder and started local MySQL instance
*** mysqld_safe --datadir='/storage/home/hpaceice1/xliu725/scratch/db'
* However, got stuck on the step where we're trying to check whether the database is working or not. Unsure why the command of mysql -h atl1-1-02-012-5-l won't work.

====== Action Items: ======

{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Look into PACE-ICE issues with .my.cnf
|Complete
|9/18/2021
|9/23/2021
|9/24/2021
|-
|Find the MNIST error with datasets
|Complete
|9/18/2021
|9/23/2021
|9/24/2021
|-
|}

== September 10, 2021 ==

====== Lecture Notes ======
* Gabe is joining us today!
* Class updates
** In the wiki page, you can link to the files stored otherwise in the notebook.
** Git add/commit/push any images to files under your name and then link to your notebook
** Self-graded notebook rubric is due next week
* Stocks update
** Met to discuss tasking and what the semester will look like, looking at how to write the paper in the future
*** Results from last semester weren't publishable quality yet
*** Uncertain about whether they should build an outline or work on replicating results
*** Jason: Regardless of final results, work is still worth writing up in an article
** Currently thinking about meeting other non-time conflict students to incorporate the stocks team into the paper
** PACE-ICE is up and running once again!
* Modularity
** Contacted Gabe for help (Thanks Gabe :D)
** Need to fork off the code base
** Jason has also streamlined MNIST data if we want to use that
*** Take a look at gen_mnist data set
*** New saved pickle format, with corresponding template file
*** Jason's file uses precision and recall - but we're really far from the original emade branch
** May also need to assess choices we've made when looking for adfs/arls
** These algorithmic decisions could also be changed and experimenced with
*** May want to change hyperparameters or how to use ranking, and how we select arl's could be diversified
** See which parts are arbitrary and can edit in the future
*** Try to make five arl's each generation which can definitely change
** Could also start an exercise of trying to write a paper/potentially have an outline
** Full paper submission date is January 20th for GECCO

====== Subteam Meeting 9/16 ======
* Met up on Thursday to talk about having more runs
** Getting everyone set up on Google Collab again
** Connecting to new database that Bernadette set up for the semester
* Issue with the superindividual last semester
** May need to change our seeding file and find new individuals for it

====== Personal Exploration ======

Notebook Maintenance: 25/25
Meeting Notes: 15/15
Personal Work and Accomplishments: 30/35
Useful Resource: 20/25
Total: 90/100

Comments: Could add more links and write more about personal exploration. Currently haven't contributed as much since I'm working on getting PACE-ICE set up for modularity.

====== Action Items: ======

{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Self-graded notebook rubric
|Complete
|9/10/2021
|9/17/2021
|9/16/2021
|-
|Attempt runs for MNIST
|Complete
|9/10/2021
|9/17/2021
|9/16/2021
|-
|Contact Vincent on arl-depth
|Complete
|9/10/2021
|9/17/2021
|9/15/2021
|-
|}

== September 3, 2021 ==

====== Lecture Notes ======
* Discussing the possibility of working together
** Modularity could take some of the primitives from stocks and change the evaluation functions
** Analyzing the new dataset from stocks
* Directions we could pursue:
** Using the stocks data to test on
** Implementing new primitives, evaluation functions, and thus far
** Looking at how to store primitives in the database
** Have new baseline runs for both mnist and stocks before implementing new primitives

====== Subteam Meeting 9/8/2021 ======
*Potential ideas for things to pursue this semester
**Potential ways to look at data visualizations
**Add/integrate in the stocks subteam
*Currently compiling a list of things we know and things we need to learn for the future
*Potentially have all modularity folks meet on Friday from 3-4
*Created draft of action items and potential paths to pursue here: https://docs.google.com/document/d/1nrIWrMjVsJGYhjUZHZEiTStSvHmNjziNbVNA94ekhBM/edit?usp=sharing
* Keep doing mnist runs for now
** Work on merging the changes with the stocks team

====== Action Items: ======

{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Create new modularity channel
|Complete
|9/3/2021
|9/7/2021
|9/4/2021
|-
|Reach out to stocks to ask about their work
|Incomplete
|9/3/2021
|9/10/2021
|Removed
|-
|Email Zutty about moving the meeting times
|Complete
|9/8/2021
|9/11/2021
|9/9/2021
|-
|}

= Spring 2021 =
Subteam: Modularity 

Members: [https://github.gatech.edu/emade/emade/wiki/Notebook-Vincent-Huang Vincent Huang], [https://vip.gatech.edu/wiki/index.php/Notebook_Angela_Young Angela Young], [https://vip.gatech.edu/wiki/index.php/Notebook_Bernadette_Gabrielle_Santiago_Bal Bernadette Bai]

== April 26, 2021 ==

====== Lecture Notes ======
*Stocks
**Working on makring sure to finish baseline runs
**Seeing how well their individuals are doing through their graphs
**Need to still merge their changes to master now that they have new technical indicators.
**Looking at new decision tree
*ezCGP
**Looking at how new crossovers are doing in terms of visualizing
**So far individuals are doing well
**Using one-point crossover mating methods
*Modularity
**Our subteam has new objectives with three experiments
***See subteam meeting 4/25/2021 for more information
**Looking at precision, recall, and Cohan score
**May need to flatten objective scores to just 2d things, such as precision and recall. 
***May get rid of Cohan Kappa
*NLP
**Currently have 6 folds with 15 generations after a multi-hour run
**Going to review code on Wednesday and run over presentation soon
**Wrapping up the semester

====== Subteam Meeting ======

*Returning students so far are doing well but have two interrupted master process
**I will take over as the new master instead of Vincent to see if it'll work
*New students
**Going to have Gabriel be the master process and have them work
*Kevin is currently looking at code database, changes, etc.

====== Personal Exploration ======

*Added on to the final PowerPoint: https://docs.google.com/presentation/d/1SLLHwjsy-ZHV4OqAXDBclBeTzqrSbyCNCJNRZs0a8Kc/edit#slide=id.g812dce5abf_2_375
**Modified the future work slides
**Took a look at the takeaways slide
***How can we fix super individuals in the future?
***We might want to add/use other objectives that aren't as correlated
***There's the chance that the current F1 and Cohen kappa scores are correlated
**Looked at Pareto front graphs on the PowerPoint for analysis purposes
***Unable to draw conclusions because of the odd shape of the graphs due to super individuals
*Worked on new MNIST runs with old ARL structures
**When I tried to be master, I kept running into a tensorflow error. As a result, Vincent was the master and I was the worker process
**You can see generated Pareto fronts from our baseline runs here: https://drive.google.com/drive/folders/1FqnNg19p_QNYFmbbQAD0vWT0kguwuVoT 
**Currently, on the emade database, we see that databases 6-9 were valid and could be analyzed
***Database schemas earlier were interrupted runs that didn't perform as well
*Analysis for mnist_old_arl_6 database
**The csv file is found here: https://drive.google.com/file/d/15S4SG2DYcbt8HnNNDoL9DhVx36dHh7Sx/view?usp=sharing
**Currently working on code to generate a pareto front for the csv that can be found here: https://drive.google.com/file/d/1uAoayfMn8RRcugqyLm_wvmKT9mX6zhIJ/view?usp=sharing
*From the final pareto fronts generated, you can see that there is a super-individual which dominates the graph
**Unfortunately, this holds true for many individuals
**As a result, we need to balance out individuals in the future to have easier to analyze results
**The new individual below was generated by Cameron Whaley's code
**Analysis of new super-individual vs the one's Angela and I generated before
***First, notice that the new individual is much shorter with only 4 real layers
***The individual generated by Angela and I has depth 7
***Interestingly enough, similarities show that the KNN learner type with K:3 and weight 0 is included. However, ours is a "Grid" while theirs is "Single"
***Pass tri-state is also seen
*The input file used for the new runs are seen here: https://drive.google.com/file/d/1SR9Eq17N5AJhxR3yxcRuelWzFa9rG5GC/view?usp=sharing
**Notice that the new objectives have changed and are based on F1 score and Cohen Kappa
**In addition, the follow code snippets are also changed:
***<maxAdfSize>3</maxAdfSize>
***<shouldUseADFs>true</shouldUseADFs>

[[files/NewInvid.png|A graph of one of the new MNIST super individuals generated through Cameron Whaley's code.]]

====== Final Presentations 4/30/2021 ======

*Stocks
**Objectives
***TA-Lib indicators in emade
***Evolvability of EMADE individuals
***Use larger dataset and take a loot at all sorts of stocks
****Incorporate data analysis and statistical evaluations of individuals 
***Technical indicators and Piecewise Linear Representation 
****Threshold value determines number of piecewise functions 
**Exponential Smoothing
**Removed TriState and Axis parameters to Technical indicator primitives
**Added new Datasets to optimize (Larger Date Range)
**TA-Lib Technical Indicators were redone and Indicators without TA-Lib were calculated manually
**Current results from EMADE
***300+ generations with 4 objectives.
****New objective: Normal CDF on Distribution, correlated with Profit Percentage. AUC of pareto front decreased over the generations
***300+ generations with 3 objectives. 
****Removed Profit Percentage since it was correlated with another objective. 
****Saw an overall decrease in AUC of pareto front, but had a spike from restarting the run in the middle.
**EMADE Analysis
***Monte Carlo Simulations. 
***Calculate profit on stock with buy and sell trades to compare to performance of Individuals. 
****Individuals performed better than randomness.
**Primitives Analysis
***The algorithm uses historical data and technical indicators to predict stock, so bad TIs can mess up the results of this analysis.
**Future work: Comparing levels of generalization of optimal models, statistical analysis of seeding on AUC
***Create bounded objective functions
***Applying fundamental analysis in addition to Technical Analysis, Look at the effectiveness of emade on different intervals of trading, look at another paper to base more research off of.
*ezCGP
**Cartesian block structure very specific to ezCGP
***Some nodes are active while others are inactive to see which works the best
***Stages of blocks, such a preprocessing, data classification, etc are grouped into certain blocks of code that needs to be passed through
**Since the midterm, theyâ€™ve removed augmentation and preprocessing
***This resulted in high training accuracy but lower validation accuracy. However, it is much faster
***Unfortunately there were overfitting issues and a lack of connected layers
**Want to replicate with CIFAR-10 without transfer learning
***Also worked on visualizing genomes and researching/testing new mating methods
**However, diversity was not doing well and individuals had few layers so they manually analyzed individuals
***Initial population individuals matched targeted population
***Larger individuals actually did worse, maybe because there were more room for mistakes
***Potentially look at incrementally larger individuals
**Finished inclusion and testing of pooling and drop out for experiments to compare to midterm benchmarks
***With new improvements, they had a 68.5% accuracy which is much more than 56.3% and pooling can improve the evolved architecture
***However, dropout layers did not do well
**Looking at dense layers
***Will adding fully connected layers improve performance?
***However, there was a lower validation and training accuracy. It had low diversity as well and didnâ€™t do as well compared to SOTA or transfer learning.
**WIth visualizations, they improved their tech
***Inactive nodes, layer arguments, and node numbers are added
***Can visualize multiple individuals as needed
***Easy to use command-line interface makes it easy to visualize individuals
**Want to work on better seeding as well
***Looked at online code from TensorFlowâ€™s github
**CGP Paper overview and crossover
***Looked at symbolic regression problems
***Each generation contains offsprings and best individuals from previous generations
**Meta parameter search
***Can get expensive for computational efforts, but fitness is defined as the mean result
**New mating method is through using one-point crossover
***Can get expensive for computational efforts, but fitness is defined as the mean result
***Mating converges faster than runs without mating
**Point mutation - look at percent of parentâ€™s genes to create new child genotype
**Next semester - new mating methods, existing CNN architectures
*NLP
**Take an evolutionary approach to NLP via neural architecture using emade
**Different layer primitives
**In the past:
***Focused on Neural Architecture Search (NAS) and created new primatives for them
***Looking into computer vision applications 
***Too many trivial solutions
**This semester:
***Streamlining how the team ran EMADE and pick better datasets in the future to have better runs.
***Ran emade and examined the shortcomings of their implementation of NAS
**Pretrained embedding layers
***NN layer which input is vector representation of words learned by a predefined vocabulary
**Documented over 50 primatives
**Amazon Product Reviews Dataset
***A binary classification and balanced dataset. 
***Only used half the train data since it was large
**Baseline Models Used
***FastText Model used for benchmarking and achieved 91.73% accuracy
****Seeds are doing really well comparitively
****AUC of results marginally decreased when looking at seeds
** Non-trivial solutions
*** Best individual has a 92.8% accuracy after 22 generations, and this experiment has replicated similar results
**Improvements were very strong at the start but later becomes more marginal after a sharp drop-off
***With the first run, we see there were 17 Pareto optimal individuals, though 7 were seeded
***The elapsed time of individuals was a metric for complexity with an AUC of 0.027
***The second run was 21 generation but only had 4 individuals on pareto front that werenâ€™t seeded, and had an AUC of 0.04.
***Outperformance of seeds became more obvious past generation 20, with best accuracy being 0.9313.
**Takeaways: No discernable pattern in misclassified reviews
***Longer reviews werenâ€™t as good, and dataset could be labeled better
**Future work
***Increase network complexity and decrease failure rate through examining the structure
***Also might want to return to CV and avoiding multilabel datasets
***Improve EMADEâ€™s outlook when seeded poorly
***Look at NNLearners as subtrees
*Modularity
**Questions
**Small selection from MNIST population so that we can get more generations in a small amount of time, but as a result, there's a smaller generation
***This is also because we have a larger seeding file
**We have many individuals on slide 8
***Scalar multiply can help accentuate complexities of the pictures
**We also check for validity of individuals first before looking for ARLs

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Finish final PowerPoint
|Complete
|4/26/2021
|4/30/2021
|4/29/2021
|-
|Analyze baseline runs with old architecture
|Complete
|4/26/2021
|4/30/2021
|4/28/2021
|-
|}

== April 19, 2021 ==

====== Lecture Notes ======

*Nearing the end of the semester, with final presentation happening on April 30th from 6-8:50.
*Stocks
**Clarified the main objective of the team
***Working to make software that can predict for any stock after training with specific data.
**Looking into running something not as volatile, such as data with longer time frame
**Currently working on creating new fitness function
*ezCGP
**For visualization, they're adding parameters.
**Currently, best individual is only at 55% accurate
**Difficulty building the individual right now.
*Modularity
**Finally have something running without errors on the depth problem from new database and primative
**Want to see if there are any other issues as well
**Not getting consistently valid individuals due to poor objectives and seeds
**Two objectives getting the same values from when Angela and I ran it
*NLP
**Currently, multiple long hour runs led to over 0.9 accuracy which is great.
**Main categories are debugging, improving evolution, using learners as subtrees, pace-ice things

====== Subteam Meeting ======

*We need to wrap up for the end of the semester.
*Setting first semester students up with figuring out edge cases and running emade
**Also need to code new objective functions for new seeded runs.

====== Subteam Meeting 4/25/2021 ======

*Semester is wrapping up and we need new exploration/analysis
**Returning semester students will be using mnist runs with adf structure
**First-semester students wrote new objective functions and revamped the seeding file for new runs!
**First semesters will be doing runs without adfs
**Kevin and Gabriel will be working on fixing new architecture before trying runs as well
**We'll try to finish around 4 or 5 runs to analyze for our final presentation

====== Personal Exploration ======

*Used git checkout mnist to see changes to our mnist branch
*Our templates file for input_mnist.xml was updated, and I added in our database username, etc as needed
*Changed the following
**<maxAdfSize>3</maxAdfSize>
**<shouldUseADFs>true</shouldUseADFs>
*Ran a worker process for two runs with Vincent as the master process
*First run uses database mnist_old_arl_0
**Had 55 valid individuals but got stuck on generation 12
**As a result, we needed to rerun with new architecture
*Second run uses database mnist_old_arl_1
**Currently only has run for 3 hours with 29 valid individuals
**To find valid individuals I run the SQL query 
***select * from individuals where `FullDataSet F1 Score` is not null;
*I will soon analyze the Pareto front with the new objectives
**Potentially look at just 2 at a time and see which individuals satisfy all of them
**Could also flatten to 2 objectives instead of looking at hypervolume
*From one quick glance at the results, it seems as if one individual is doing much better than some of the other individuals
*Thinking about steps to take for the databases issue for next semester since we won't have time this semester
**Step one: Retrieve individual from database by using pymysql and the primary key for the ARL
**Step two: Place the ARL on the node where it was called/connect it to the individual
**Step three: Expand the ARL by undoing what the other subteam within Modularity has done
**Step four: Close the MySQL connection
*Currently looking within the various files to see where the code might be inserted -> for now, waiting to save writing actual code for this till next semester.

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Try new runs on mnist with adfs
|Complete
|4/19/2021
|4/26/2021
|4/25/2021
|-
|Start final presentation to work on
|Complete
|4/19/2021
|4/26/2021
|4/25/2021
|-
|}

== April 12, 2021 ==

====== Lecture Notes ======

*Stocks
**Preparing runs for Thursday
**Complete methods
**Working on seeding and did a previous run
***Focusing on fixing things wrong with their recent run
**What happens when you swap in different spots?
*ezCGP
**Trying out block coding and new changes to see if they're helpful
**Analyzed why some individuals only had 4 or 5 nodes
**Lots of individuals generated but not always chosen, so they changed objective scores
**Visualized individuals to see why certain routes were picked
***Don't have anything that skips
**Currently only have easy primitives added
*Modularity
**Need new objective functions since this is same and not multi-objective
**Having issues generating valid individuals - but Angela and I found several good generations, we need to verify
**Our data also isn't one-hot encoded
***Need better multi-class metrics
**May switch to recall or accuracy

====== Subteam Meeting ======

*Talked with Jason and Gabriel about the mnist run that Angela and I did
**Goal was to minimize F1 accuracy scores
**Difficult to generate a pareto front since the run was more single-objective than multi-objective
**We took a look at some of the valid individuals that did well

====== Subteam Meeting 4/17/2021======

*This subteam meeting is specifically for the databases issue (and is an optional meeting)
**Certain primitives have some issues when converting to database storage
**Currently, three edge cases get lost
**Learners take in learner type, and learner type takes in ARL, but this doesn't get stored
***We want to look into how to better store individuals
*Currently database stores id, input, output, arl expression
**We want to use a more modular design where we can pull arls from the database depending on what the node references
**Currently, the pickle column stores pickle file in the form of "blobs" that we can unpickle using python to get the information back out
***Every individual has its own pickle, which stores individual in its tree form
***code: for node in pickle: print(node.name)
*Goal
**After finding ARLs, store in the database
**While evaluating individuals, unpack ARLs from the pickle
**Example
***Generation 10 finishes evaluating
***Selects individuals
***We create ARLs
***Generation 11 starts
***Now we have new individuals with ARLs in them for evaluation
**Most emade functionality is in EMADE.py through the evaluation() function
***Previous, we just did Learner(ARG, ARL(2)) and run since the ARL was a lambda
***Now, we need to see that if we see an ARL in an individual, then we read from the database and substitute nodes as needed. 
*EMADE originally used ADFs
**We may want Learner(ADF, args) and have it unpack what the ADFs were
**Our population is a list of individuals which is a list of nodes and primitives
**Emade is actually a list of lists where the first index is the individual which might contain ADFs
***Indices after that would be an ADF
**We see that [0] is Learner(ADF, args), [1] is Window filter, [2] is ...
**These ADFs however are tied specifically to the individuals rather than being able to be accessed
*We'll either rewrite evaluate() function or evaluate_individual function
**eg. If ADF/ARL in individual: extract and substitute with a post evaluation function where we re-compress stuff
*We might also want to look at wrapper_methods.py to see how we create primatives, and pre-post primative functions
*Current possible tasking
**ARL/contract Storage in database
**ARL evaluation/expansion where we read from the database
***I have been assigned to this topic with Angela, Rishit, and Gabriel
*Working off of tree_database branch which is old architecture
**Looking at EMADE.py, evaluate() and evaluate_individual() function, maybe wrapper_methods.py (though not preferable)
**Need to communicate with first team to know how to read and write from the database. Maybe look at sql_connection.py

====== Personal Exploration ======

*Angela and I worked on creating another run and set up a work session together
**Unfortunately, we kept getting errors after setting use_adfs to true
**I also looked at cloud copy script but couldn't figure out how to get rid of recursion after several attempts
**At best, we still had to delete the duplicated emade-cloud folder.
**Angela created visualization of an individual
**I graphed the F1 accuracy score against the generations, but couldn't find a trend from our current run
*We tried to seed a run and use new objective functions like false positive and false negative
**Unfortunately, we only got 8 valid individuals
**This is because those only work for binary classification problems
*Gabriel suggested using precision and recall which we might have to code in
*Looked into the emade.py code source repository
**We see that for evaluate_individual on line 1474, currently the arguments are an individual and a dataset.
**Current to-do: If 'adf' is discovered while evaluating, run the expand_adfs() method to expand out the tree again, then recompile the individual as expected
**Idea for writing expand_adfs() -> we need to make sure that we can reference the adfs through the database and insert in the individual as needed. (Line 1547 of the code)
**Look into pymysql and sql alchemy
**Look at adf_update
**We see that to use the database, we need to go to sql_connection_orm_master.py and look from line 94 to line 114.
***You can also see our best individual from the current runs before we moved to new objectives as seen by the visualization we worked on below:
[[files/OldInvid.png|A visualization of our current best individual.]]

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Look into CloudCopy.sh
|Complete
|4/12/2021
|4/18/2021
|4/13/2021
|-
|Look up other statistics
|Complete
|4/12/2021
|4/18/2021
|4/17/2021
|-
|Create visualizations of individuals
|Complete
|4/12/2021
|4/17/2021
|4/16/2021
|-
|}

== April 5, 2021 ==

====== Lecture Notes ======

*Stocks
**Look at what our code does and what the purpose of the subteams are
***New first semester students are looking into technical indicators
**Try to create new objective functions
*ezCGP
**Added maxpooling and drop layers
**Ran 50 generations with 8 individuals
**Have an accuracy of 68.4%, but very little diversity
***Mostly one individual with many mutations
**Dropout layer individuals were likey to be dropped at the start.
*Modularity
**Runninig benchmark runs of MNIST and using F1 scores and accuracy
**Trying to work out some bugs
*NLP
**Getting new members set up in PACE to start working
**Got 87% with only 1000 instances so far, can work with a reduced training set.
*Statistics Lecture
**If we get ten times to get results from generations of AUC, we have the following statistics
***Mean
***Variance = E[x^2]-E[x]^2
***We can correct by bias by finding s^2 = n*sigma&2/(n-1)
**Hypothesis testing
***Computing a probability of observing a sample at least as extreme as ours given our assumption of the underlying truth.
***The lower the alpha level, the higher the confidence interval and the less likely youâ€™d have a type 1 error
****The opposite is a type 2 error
***We use a t-table to find the test statistic as needed from the results
**One possibility is we can use Welchâ€™s t-test, or an unequal variance t-test
***We want to see the power we have to reject the null hypothesis from our t statistic
***You can compute how many trials you need to get a rejection of the null hypothesis once you quantify your experiments.

====== Subteam Meeting 4/4/2021 ======

*Updates
**Gabriel has been helping debug
**Kevin finished contract_arls but it doesnâ€™t work.
**Need to be able to access primitives within the individual to analyze it
**Potentially rewrite script to work for mac? So that infinite recursion doesnâ€™t happen
**I can try it but Iâ€™m not sure
**Need to get emade-cloud working
*How to do analysis
**Traditionally emade is trained on feature data and make predictions from that
**However, stream data (used in images) is different because images are more complex
**This means we donâ€™t train on images directly unless we use convolutional neural networks - this way models donâ€™t get trained on pixels
**Machine learning is trained on feature extraction where images are converted to feature data first before they get trained on the model
**With MNIST, what happens is there is an additional step needed to make individuals valid.
*Our current data set
**Arg 0 is the data set that we feed in. It might use other primitives such as FFT and so forth.
**This primitive converts stream data into feature data which makes it into a valid individual.
**This extra step may result in less valid individuals

====== Personal Exploration ======

*Helped Krithik Acharya with seeding
**He forgot to update the mnist.xml files
*Finally reuploaded emade-cloud
**Made sure to delete recursive folders within to shorten upload time
**Only need to upload around 201 files
*Ran emade with Angela using google collab, with Angela as the master and me as the worker.
**Completed 1 run with Angela that lasted about 4 hours
**933 individuals in 48 generations
**75 valid individuals, with two individuals that have F1 accuracy scores of 1
**Have multiple individuals with 0.89 or 0.88 scores
*Plan to visualize the individuals soon and analyze them
*Problem: The objective scores are off right now, and we see both objective scores are equal. We may want to modify this.
*Questions:
**How many images are tested on?
**How Why does MyProd and ThresholdBinary doing so well as overall predictors?
**Below is a graph of the overall F1 scores that did the best in a certain generation over multiple generations
***Unfortunately, we can't see much of a pattern here.
**We need to analyze the individuals more on their own and rerun with multi-objectives, since this set of results only has one real objective.
[[files/GenerationF1Score.png|A graph of F1 scores over generations.]]

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Look up more possible statistics tests to use for analyzing
|Moved
|4/5/2021
|4/11/2021
|Move to Next Week
|-
|Review statistics notes
|Complete
|4/5/2021
|4/11/2021
|4/7/2021
|-
|Run emade and review valid individuals
|Complete
|4/5/2021
|4/11/2021
|4/10/2021
|-
|}

== March 29, 2021 ==

====== Lecture Notes ======

* First semester students assigned to teams as needed
* 7 students assigned to each team
* Stocks
** Discussed using fitness functions to optimize invidiuals
** How are stronger individuals made? More diverse individuals?
** May use Monte Carlo Method
* ezCGP
** Looking at dense layers
** Continued analyzing runs from the semester
* Modularity
** Looking at how we can proceed in the future
** Looking into using the MNIST dataset for more runs
* NLP
** Wants to get more nontrivial results as needed

====== Subteam Meeting ======

* Sent out form to returning and new subteam members
* Discussed whether to focus on MNIST baseline runs or work on increasing diversity/mutation functions instead
* Went over a powerpoint introducing new members to modularity

====== Subteam Meeting 4/4/2021 ======

*Not much work done, Ivanna is working on the implementation
*Vincent is working on documentation for add_all_subtrees
**Found a bug that needs to be resolved
*Angela and Xufei have been working on getting baseline runs
**Need to rename file
*Some people have made it to 37 generations
*Weâ€™re at a good spot to just do some analysis to the database and individuals
**Renamed database to mnist_hoes

====== Personal Exploration ======

*First struggled to read the files as needed
**The solution was to change the directly to emade-cloud rather than emade-datapair
*Could not upload the full emade-cloud folder
**Paused upload and manually uploaded the templates folder as needed
**Ran the code and uploaded other things as needed
**Added in code to update the version of scipy and numpy used
*For some reason canâ€™t get mnist database to run with accuracy 
**Not getting valid individuals
**Even the four seeded individuals are not returning the right things
**Reached out to Gabriel to ask how we can get valid individuals and why our function for F1 score and accuracy aren't working.
*I'm running google collab as master process, Angela Young is the worker
** We created over 800 individuals but we can't get scores for any of them
** We're using the code linked here: https://colab.research.google.com/drive/1i_niAH2dxqdsdA-SMU3tYXCOW0DMRUXK?usp=sharing

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Work on getting runs seeded
|Complete
|3/29/2021
|4/4/2021
|4/3/2021
|-
|Run Google Collab with Angela Young as Worker
|Complete
|3/29/2021
|4/4/2021
|4/3/2021
|}

== March 22, 2021 ==

====== Class Presentations: ======
* Stocks team
** Look at how EMADE introduces Stock market trend analysis
** Research step and implementation
*** Picked a new paper that is more consistent and wants to create a model that's better than the one in the paper
*** Writing Technical Indicator functions for data preprocessing
*** Write TIs that actually account for volumetric data
** Scope definition/Literature review
*** Paper uses Piecewise Linear Representation (PLR) that recognizes trends in stock price movement
** Implementation
*** Technical indicators used include simple moving average, bias, relative strength index, moving average, stochastic oscillator, Williams, transaction volume, and single day change for everything
*** Wants to expand technical indicators to match the paper
*** PLR Algorithim: Make trends visible in stock prices
*** Trends convert to trading signals and discuss threshold value using Genetic algorithm
*** Exponential smoothing - uses NN trained with TIs to predict trading signal
**** 15% tolerance to smooth out the predictions of whether to buy or sell
** Current issues
*** Figuring out smaller implementation details
*** How to generate proper thresholds per stock
*** Generating valid individuals in EMADE after seeding individuals
** Results
*** Predicted trading signals using a seeded run with 100 individuals, running it for 77 generations
*** About a 20% profit with tree size of 148 (for the best individual)
*** Does not use TI primitives at all
** Future
*** Look into getting a lot more technical indicators implemented into the run
*** Want to look at different time granularities
*** Look at various stocks like penny stocks, crypto, etc
*** Check out neural network evolution
*** Isolate evolution for technical indicators
* Bootcamp 1
** Data preprocessing - feature selection methods with univariate selection, feature importance, correlationship heatmap
*** Looking at variables that are the most correlated
** Used SGDClassifier, NuSVC, Random Forest, Quadratic Discriminant, AdaBoostClassifier, etc
** MOGP results minimized the FN and FP squared to encourage both of them decreasing
** The MOGP performed much better than ML models except 2
** Methodology: Minimal data cleaning with no parameters removed (they dropped Name, Embarked, and PClass for ML and MOGP)
** Ran for 21 generations and found 36 pareto individuals in final generation
*** Runs faster for cluster run due to shared computer power, and less redundancy
*** In addition, emade allows for more diversity.
*** EMADE has higher accuracy
* ezCGP
** Introduction
*** Uses graph-based structure instead of tree based structure 
*** This encourages custom primitives and data types
*** Uses a block strucure as well to avoid mixing
** Current progress
*** Focusing on Neural Architure search - This way they can recreate results with no transfer learning
*** Checking current assumptions
*** Transformers help with vision and language tasks
*** Began experimenting with genetic algorithms since there wasn't much existing research
*** Untrained model performed with over 10 hours of training
*** Ran experiments on training parameter benchmarking that converges after 20 epochs
**** Currently testing if ezcgp framework would create architecture complex enough without using transfer learning blocks
***** First ran a few generations after removing transfer learning blocks
**** Potentially overfitting
* Bootcamp 2
** Extrapolate title from each passenger's name
*** This way, you can expolate missing ages from title
** Imported fare from PClass
** Did Hot Encoding for Sex to make sure one is not prioritized
** Dropped Cabin, embarked from parameters
** Minimized FNR and FPR by squishing
*** Ran with a population of 300 individuals for 40 generations
*** GP surpassed ML
** Had 25 generations of EMADE
*** EMADE did slightly worse compared to MOGP and ML
*** Did they not use the same feature data preprocessing?
* Modularity
** Our presentation. Questions/Comments are below.
** Are there space complexities since we're running/storing each of the nodes? Should we use some worker process features to help with resolving time/space complexities?
*** Definitely takes up more space compared to before
*** Time complexity could also be an issue since we have to add more processes to EMADE processes but not too big of a concern
* Bootcamp 5
** Preprocessing
*** Made a chart to see which features were most correlated
*** Dropped columns as the chart said.
** GP
*** Fixed/normalized features before passing them in
*** Used NSGA II after trying out multiple selections
** ML
*** Used 1500 generations may have overfitted individuals, so they chose to go with first run of 50 individuals
*** Need to balance between number of generations and runs
** EMADE
*** Changed the dataset in the exact same ways
*** Didn't see significant improvements for EMADE which wasn't significantly better than GP or ML
*** Paired with the longesr running time, this may not be the best
* NLP
** Tried Nerual architecture search to work on EMADE
*** Focused on Kaggle's Amazon dataset.
** Found many trivial solutions and want to figure out why
** PACE-ICE standardizes runs 
** Started documenting NLP Primitives
*** Fully explains 7 implemented primities and a work in progress one
** Checked how to get a dataset up and running in EMADE
** Decided to analyze the  Amazon Product Reviews Dataset
*** Binary Classification and Balanced Dataset
** Worked on Kaggle baseline model but was hard to replicate in EMADE
** May use FastTEXT as primative
** Explored non-seeded runs, and got same results as a seeded run 
*** Need to analyze why individuals are failing
** In the Future
*** Want to expand into PyTorch functionality
*** Work on high compatibility with SOTA attention-based models
*** Look at deeper learning compability
**** Obstacle is refactoring code from Keras to PyTorch, resource prohibitive
** Goals
*** Try exploration more with amazon dataset
*** Look at trivial solutions
*** Identify weaker primatives
*** Look more into PyTorch 
* Bootcamp 3
** Data preprocessing
*** Dropped Name, ticket, and cabin since they either missed information or weren't revelant
*** Filled in missing values such as Age or Fare
*** One-Hot encoded Sex/Embarked, noticed some class imbalances
** MOGP and ML run
*** For MOGP they added 9 primitives to help evole more complex individuals
*** Hyperparameters include 200 generations, 200 selection size
*** Used mutUniform and cxOnePointLeafBiased
*** Used tournament selection with initial population, with NSGA-II to select best offsprings
*** MOGP pareto front is 0.1102.
** EMADE
*** Had 26 generations for about 3 hours, however difficult to get estimate of FPR and FNR
*** Through every generation the pareto front significantly increased
*** Almost all optimal individual has an AdaBoostLearner as the outer primitive (only other different one was myPlanckTaper)
*** Has high phenotypic diversity, but low genotypic diversity
** Comparison
*** The machine learning models are fastest time wise
*** EMADE has less individuals, but will have better AUC over time
*** We see that MOGP is best at individual tuning and # of pareto front individuals. 
* Bootcamp 4
** Data PreProcessing
*** PClass had a low correlation, Names had no value in the string format, both don't really help so were dropped
*** Filled in null values of age with the average
** ML and MOGP models
*** Ran XGBoost, AdaBoost, etc. Also looked at CatBoost.
*** Also looked at K-Nearest Neighbors
*** For Neural network they got 82.7% accuracy
** GP
*** For titanic MOGP, they used mutUniform, mutShrink, and csOnePoint wiht SPEA2 and selBest
*** Machine learning ended up doing better
** EMADE
*** Ran for 20 generations with 12 individuals on the pareto front without changing anything
** In the very end, it seems that genetic programming did the best with ML doing the worst. However, if EMADE ran for longer it may have done better.

====== Subteam Meeting 2/28/2021 ======
* Work session since we're not too sure
** Most people just run emade and don't have too much else
** We need to normally compare against the baseline but we might not have too many results
** Around generation 20, we begin getting smaller standard deviation
*** As a result, all our experiments look like they're significant
** However, we may need to fix that/find a way to make it better
** We could all make baseline runs
* Could potentially look into diversity papers and see if we can make a stand-alone function for improving diversity
** This way other subteams may be able to use them as well.

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Try Baseline Runs
|Complete
|3/28/2021
|4/3/2021
|4/2/2021
|-
|Research more topics
|Complete
|3/28/2021
|4/3/2021
|4/1/2021
|}
== March 15, 2021 ==

====== Lecture Notes: ======
* Stocks
** Ran emade using new primatives, got interesting results
*** Not that good - potentially because they didn't run for long enough
*** Lower population back to 60 individuals so we can have crossover mutations
** Need to consider crossover, mutation function differences even with 700 individuals
* ezCGP
** Got benchmark results last week, now working on improving results
** Currently experimenting with individuals
* Modularity
** Now working on pushing code for working to the depth problem
** Want to set up a practice presentation sometime this week
** Still working through errors
* NLP
** Also getting a MySQL error when trying to run it
** Individuals are having issues with generating
** MySQL Operational error
* Note that presentations are next week

====== Subteam Meeting ======
* Working on updating ARL_Update method
* Potentially practice presenting over the weekend
* Vincent is still working on adding all subtrees
** Vincent is working on mapping leaf nodes
* Angela is still struggling to get emade to work.

====== Subteam Meeting 3/21/2021 ======
* Finishing up the PowerPoint slides for the presentation
** Working on adding our own slides and future slides for the powerpoint
* Going to present to the advisors today at 11:00 and get feedback.
* Personal: Added on slides for search_individual, modifying the future work part

====== Personal Exploration ======
* Added slides on to our powerpoint here: [https://docs.google.com/presentation/d/1iaC5dHYX7G-NWuCAFjjVVzKHMm7NUCk_kHP72tQmZc4/edit#slide=id.gc98f9b8f3e_1_16 https://docs.google.com/presentation/d/1iaC5dHYX7G-NWuCAFjjVVzKHMm7NUCk_kHP72tQmZc4]
* Future work: Look at implementing complexity count and adjust for that in the Find_ARLs updated method

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Add to PowerPoint
|Complete
|3/15/2021
|3/17/2021
|3/18/2021
|-
|PowerPoint Rehersal
|Complete
|3/15/2021
|3/22/2021
|3/21/2021
|}
== March 8, 2021 ==

====== Lecture Notes: ======
* Stocks
** Working on getting a full run of emade
** Working on primitives still
** Start preparing data sets from template files
** Still figuring out how to find optimal threshold - make sure they're using truth data
* ezCGP
** Not sure if preprocessing is helping or not because it's counterintuitive
** Surprised not adding dense layers
** Working on creating stable evolutionary structure
** Developing new seeds through current population
** Make sure validation set isn't too similar to your training set - high validation score
* Modularity
** People working on depth stuff - have the individual components are done and we just need to combine the code.
** This week - we can begin experiments (hopefully)
* NLP
** Hitting snags with baseline models
*** Posting questions on stackoverflow
** EMADE is now running on the amazon dataset, currently doing another run on generation 24.
* Note: Midterm presentations are coming up! We get new students assigned the next week.

====== Subteam Meeting ======
* Working on pushing code to git
* Make sure I'm using ARLUpdate branch
** Potentially try downloading campus VPN

====== Subteam Meeting 3/14/2021 ======
* Add to the presentation and include all of our updates
* Try to run a practice presentation with the professors to get feedback and fix it
* Finish pushing code.

====== Personal Exploration ======
* Commited my code to the repository
** Looked at intro to github here: https://www.earthdatascience.org/workshops/intro-version-control-git/basic-git-commands/
** Commited my code to the ARL_Update branch after testing it.
** Had merge conflicts when I tried to push it.
** Ran git merge to resolve
***You can see the completed adf.py file here at my point of committing: https://drive.google.com/file/d/1wy-x7xql8ZR-TQE8zjvgvh8orCamEi8Q/view?usp=sharing

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Find ARLUpdate branch
|Complete
|3/8/2021
|3/15/2021
|3/8/2021
|-
|Work on pushing new and improved code
|Complete
|3/8/2021
|3/15/2021
|3/9/2021
|}
== March 1, 2021 ==

====== Lecture Notes: ======
* Stocks
** Working on developing volume based indicators
** Currently trying to implement it into emade
** The paper seems to be different this time and they fully understand it
*** This paper is more consistent
** New research paper doesn't contradict itself
** Trying to create volume primitives
* ezCGP
** Fixed past pipeline bugs
** Currently visualizing graphs of the individuals
** Need to look at confusion matrices and their differences
* Modularity
** Currently working on our code base and modifying the search methods
** Just has one more commit left to the documentation
* NLP
** Currently only NSGA II is working and other things are getting weird errors
** Collaborating with stocks team to try and resolve error
** Jon is wrapping up the documentation
** Expect a couple of runs and have set up one instance
** Potentially using wrong deap version

====== Subteam Meeting ======
* New find method by Kevin is described by two different components
* We want to test each individual method before running it in the overall method
* Currently working on implementing changes to the code base
** I've already finished my search_individual method but I still need to test it before submitting it to our github code
** Gabriel is working with Angela and Regina on the database while the rest of us are changing methods within the code
** This session was mostly a work session

====== Subteam Meeting 3/7/2021 ======
* Bernadette made the generate_adf methods
** Still needing to test it
** Same as me!
* This subteam meeting is still a work session.

====== Personal Exploration ======
* Began coding the search_individual method
** Called with Kevin Lu to work on navigating through git so that I can upload my changes onto there
** Still need to test my current code with print statements to make sure it won't break our codebase
** The code I wrote is attached [https://codeshare.io/24ZZv8 here].
* Difficulty in installing the necessary packages for emade
* Found a solution here: https://stackoverflow.com/questions/64963370/error-cannot-install-in-homebrew-on-arm-processor-in-intel-default-prefix-usr
** The reason is because my macbook has a M1 chip
** Code to use is: arch -x86_64 /usr/local/homebrew/bin/brew install <packagename>
* Also needed to create an environment for emady using conda create --env myenv
* Each time from now on, must do conda activate myenv, and then did conda install lightgbm
* Code to run emade
** Just once: conda activate myenv 
** bash reinstall.sh 
** python src/GPFramework/seeding_from_file.py templates/input_titanicADF1.xml seeding_titanic_benchmark 
** python src/GPFramework/launchEMADE.py templates/input_titanicADF1.xml 
* Got new error when trying to run emade
** "Not a gzipped file"
** May be a macOS issue
** Worked when I reinstalled emade with git lfs install.

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Test search_individual method
|Complete
|3/1/2021
|3/8/2021
|3/7/2021
|-
|Push method to git branch 
|Complete
|3/1/2021
|3/8/2021
|3/8/2021
|-
|Peer Evaluations
|Complete
|3/2/2021
|3/8/2021
|3/3/2021
|}
== February 22, 2021 ==

====== Lecture Notes: ======
* Stocks
** Working on generating PLR
** Looking at different folds using tech company stocks
*** Apple, Boeing, Verizon
* ezCGP
** Try to get a full run of ezcgp
** Try to initialize individuals
** Continue working on PACE-ICE
* Modularity
** Done with documentation side of things 
** Focusing on depth problem/location ARLs alongside smaller ARLs
* NLP
** Getting infrastructure set up
** Trying to start a run on the primitives 

====== Subteam Meeting ======
* Plans for finding the ARL
** Looking for partial ARLs
** All ARLs are same size before but now they're different sizes. Consider number of nodes in an ARL
*** Are bigger ARLs really better?
** Point of ARL is to minimize chance of destructive properties
** Note that ARLs will begin to have things in common
* Tasks to help find ADF
** Make sure update time works
** Also look at generate ADF

====== Subteam Meeting 2/28/2021 ======
* I will be working on the search_individual method
** Vincent is working on add_all_subtrees and Bernadette is working on generate_adf.
** Kevin assigned the tasks and is working on the find_adf tree.
* This is a work session.
** Finished coding the search individuals method

====== Personal Exploration ======
* Focus on getting Sphinx set up
** Need to download a C complier in order to get everything working. Downloaded X Code from the mac app store.
** Did some research into the way that nodes are stored
** It looks like the current individuals are a list of nodes in order from left to right
*** Notice that search_individual needs to traverse the individual from left to right, so we can just iterate through the codebase
** Reviewed the current find_adfs method listed in the class.

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Review the codebase
|Complete
|2/21/2021
|3/1/2021
|2/28/2021
|-
|Finish getting Sphinx set up
|Complete
|2/21/2021
|3/1/2021
|2/28/2021
|}
== February 15, 2021 ==

====== Lecture Notes: ======
* Stocks
** Looked at google scholar to find more research papers that uses genetic programming
** Uses special labeling methodology for piecewise linear algorithms to mark special points and times.
** Currently focusing on plr stuff.
** Focusing on creating a new fitness function.
*** Issue: Focuses on 2008 stock data which happens during the recession.
* ezCGP
** Currently working on using PACE-ICE without it crashing
** Precision/recall is a little difficult
** Working on their branch to see if they can finish their current trials
** Potentially look at CFAR10 since memory might be an issue
* Modularity
** Created new modularity page to update things
** Working on changing architecture after looking into sphinx
** Looking to explore it more to see if we can get a more solid idea
** Need to rewrite how we find our ARLs since the depth issue is from our method of finding
** Trying to store more information in the database
* NLP
** Next week hopefully have more runs in pace
** Still working on getting everything set up

====== Subteam Meeting ======
* Past subteam meeting is recorded and uploaded
* Went over weaknesses of current architecture, had brief introduction to emade
* Add own thoughts to wiki page and potential future goal
* Set up survey to set up a new time to potentially work on sphinx
** Kevin asked if we should try to edit functions
** Mess around with codebase or just with dev?

====== Subteam Meeting 2/21/2021 ======
* Documentation has been assigned to Bernadette and Vincent
* Read documentation
* Tasking for the week
** Generalized depth vs partial ARL
** Potentially work on different methods
** I will work with Kevin on modifying the code base for bettering ARLs.

====== Personal Exploration ======
* Notebook self evaluation
** Name and contact: 5  
** Teammate names: 5 (linked above)  
** Neat, legible: 5  
** Organization: 5  
** Updated weekly: 5  
** Group topics: 5  
** Other individuals: 4  
** To do items: 5  
** To do consistency: 5  
** To do cancellation: 4  
** Level of details: 5  
** References: 4  
** Useful resource: 4

====== Action Items ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Read over articles of other subteam members
|Complete
|2/15/2021
|2/22/2021
|2/21/2021
|-
|Watch recorded modularity meeting
|Complete
|2/15/2021
|2/21/2021
|2/21/2021
|-
|Notebook Self Evaluation
|Complete
|2/15/2021
|3/1/2021
|2/21/2021
|}

== February 8, 2021 ==

====== Lecture Notes: ======
* Market Analysis Team
** Discussed goals of the semester to align with research paper
*** Trying to find another research paper like the one last semester but more consistent
*** See how to use EMADE to implement results from the paper
*** How to improve past primatives 
*** Look at fundamentals such as APIs to use for more data
*** Discuss which stocks to target if it is not specified in the paper
*** Test on multiple stocks
** Future goals
*** Finish selecting a new research paper
*** Figure out actionable changes to make from the research paper
*** Common theme among papers: Look for papers with volume of data to use
* ezCGP Team
** Looked at past research paperes to look at transformeres and hyperparameters with genetic evolution
** Updated their problem file
*** Started working on PACE-ICE
*** Plan to test updated problem file and see the changes
** Looking into minGPT
* Modularity (Our subteam)
** Did literature review to look at papers and digest them
** Currently decided to read each other's papers
** Focus on expanding complexity for ARLs
* NLP
** Focusing solely on NLP to troubleshoot
*** Preprocessing vs unbalanced dataset? Dividing into multiple subproblems
*** Focus on controls while working on Amazon dataset.

====== Subteam Meeting ======
* Focus more on literature review to figure things out
* Try to figure out Sphinx
** Set up documentation for our project
** Automatically pulls from our code base
** Auto module pulls out individuals from the source code directly
** Can also pull out docstrings
* Current [[Modularity|Modularity page]]

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Read over articles of other subteam members
|Incomplete
|2/7/2021
|2/15/2021
|Moved to next week
|-
|Summarize current paper on Wiki Page
|Complete
|2/7/2021
|2/15/2021
|2/8/2021
|}

== February 1, 2021 ==

====== Lecture Notes: ======
* Going over subteam progress goals
** Stocks
*** Play with selection methods
*** Check parameters
*** Try to incorporate more emade in the project
*** See how different sorts of data impact the dataset
** Ezcgp
*** Two team members use two separate projects
*** Look at preprocessing and altering data potentially
*** Use transformers to work on image coding
** NLP
*** No new updates for this semester
** Modularity
*** Interest in making deeper ARLs that are more than just one depth
*** Literature review for now for this week and next week, see what we can expand upon
*** Are ARLs always useful?
**** Last semester built upon flawed assumption

====== Subteam Meeting: ======
* Trying to make source code easier to read. Gabriel is going to write source code to make it easier to read.
* Let's work on a literature review this week
** Goal: Bring in one paper to talk about
** Presentation on intro to ARLs, possible resources, etc

====== Subteam Meeting 2/7/2021: ======
* Going over literature review to see what we can improve
** Gabriel - https://link.springer.com/article/10.1007/s10462-019-09706-7
*** A review of modularization techniques in artificial neural networks 
*** See how we can add larger trees to our projects
*** Comparing graphs to trees
** Kevin Lu
*** [https://link.springer.com/chapter/10.1007/978-3-540-88906-9_54 Mutation only genetic algorithms]
*** Brings an idea of each individual mutation probability based on fitness
*** A lot like my paper
** Vincent Huang
*** https://emade-vip.slack.com/archives/CMGTQ5JMS/p1612725361012400
*** Diversity based genetic programming
*** Measures diversity using distance and tree depth on the graph to get the similarity 
*** Also looks at subtree differences, which is how many changes you make from one tree to get to another tree.
*** You can also track generations to see the common parent that led to lots of high fitness individuals
**** Multiple ways to see how "similar" trees are.
** Bernadette
*** Looking at introducing diversity into our individuals as well
*** '''[https://dl.acm.org/doi/pdf/10.1145/3321707.3321718 Convergence and Diversity Analysis of Indicator-based Multi-Objective Evolutionary Algorithms]'''
*** [https://dl.acm.org/doi/pdf/10.1145/3376916 h'''ttps://dl.acm.org/doi/pdf/10.1145/3376916''']
*** '''[https://www.researchgate.net/profile/Michael_Emmerich/publication/235412985_On_Quality_Indicators_for_Black-Box_Level_Set_Approximation/links/0c960514cad7fc4ead000000.pdf On Quality Indicators for Black-Box Level Set Approximation]'''
** Angela
*** Discussing communities in regards to modularity
*** Establishing bounds within networks without having it become too rigid

====== Personal Exploration ======
* Worked on reviewing current literature and research paper
* '''[https://ieeexplore.ieee.org/abstract/document/7748328 A Survey of Modularity in Genetic Programming] by George Gerules, Cezary Janikow'''
** Previous limitations: For the Human Genome Project, there was no intrinsic value of a subroutine, since subroutine's fitness didn't differentiate from other building blocks of code.
** Sometimes, created routines aren't removed which leads to unfit routines
** Rosca and Ballard used subroutines or building blocks with their own fitness functions to identify useful ones and add them to a function set of an evolved genetic programs.
** Ways to keep track of usefulness
*** Structural complexity - number of nodes in a tree for subroutine
*** Evaluation complexity - number of nodes in a tree and number of times a call is made to the routine
*** Evaluation complexity - keeps track of "call hierarchies"
*** Description complexity - uses minimum description length MDL
**** This happens when a problem is coded in a minimal way to describe the overall representation.
**** Still confused, might ask about it at subteam meeting.
** As the GP run happens, we track newly created building blocks that are added to the function set and remove them if they are "unfit".
** Subroutines are subtrees with a depth between 2 and 4
** Methods and heuristics to test existing algorithms currently existing
*** Random method - selects blocks in program randomly
*** Randomfit method - selects blocks using either tournament or fitness proportional selection
*** Wholeprogfitness - selects block according to same fitness function
*** FITNESSBLOCK - selects blocks according to same fitness used for entire program.
*** BLOCKACTIVATION - does selection after crossover. We evaluate the child and determine whether it will replace the parents
*** Frequency method - selects block based on highest number of times it happens in population
*** Frequency Program - selects block based on highest number of times it occurs in single program
*** Schema - selects block according to average fitness of block in relation to average fitness of all programs
*** Correlation - selects blocks according to tournament or fitness proportional. Then a statistical correlation happens in the tree against subprograms in the tree.
*** Saliency - modifies building blocks to look for large scale differences in fitness. 
**** If modification causes large negative change, then subroutine has high semantic relevance to overall fitness.
** Randomness is also introduced
*** Low fitness subroutines are replaced with mutations - etc randomly generated routines
** Ended with mixed results. One reason it might not do better than vanilla GP may be because of the non-commutative nature of functions chosen for the evolutionary target.
* '''[https://bluejeans.com/playback/s/dKmkzdE9KG8o2IX0KjqqpX57A9CTAdntXRw4hdSgEBAd6bsLu6xwiAO1yc68i5zN Intro to Modularity Video]'''
** Made by Gabriel to introduce Angela and me to the topic.
** Presentation is found [https://docs.google.com/presentation/d/1yrkD411TYEVQ8OMiqODLsoXdDiVo43PZBZpW9-1TNlo/edit#slide=id.p here]
** ADFs (Automatically Defined Functions)
*** Created by Kozas. These are subtrees that are tied to an individual and only the individual. They can also grow and evolve.
** Adaptive Representation through Learning (ARLs)
*** Created by Rosca, and once we find a useful subtree, we lock it in.
*** Keeps a global pool of subtrees for any individual to call
*** ARLs can grow through nesting, so new ARL uses old ARL
** The two are not mutually exclusive!!
** Our current fork only has ARLs but we occasionally refer to them as ADFs.
** Our current work
*** Search population for parent and children nodes with 1 depth
*** We choose a combination of nodes from how often they're used
*** Abstract combinations into a single node
*** They can wrap around each other
*** Globally accessible through a primitive set
*** Current process
**** Selection to update_representation to mating to mutation
*** We have find_adfs and generate_adfs (should be arl)
** Future work
*** Diversity so we don't limit search space
*** Selection method to encourage the spread of ARLs
*** Mutation methods to spread ARLs through the population and have a mutation function that replaces nodes with ARLs
*** We can create new individuals with ARLs in every generation

====== Action Items ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Work on literature review this week
|Complete
|2/1/2021
|2/7/2021
|2/6/2021
|-
|Find one paper to talk about by Sunday
|Complete
|2/1/2021
|2/7/2021
|2/6/2021
|-
|Watch intro to modularity video
|Complete
|2/1/2021
|2/7/2021
|2/7/2021
|}

== January 25, 2021 ==

====== '''Lecture Notes:''' ======
* Daniel Martin takes over as leader for ezCGP subteam
* Stocks subteam will be led by the same two members
* Modularity subteam: Still led by Gabriel Wang
* Neural Network/Architecture

====== Subteam Meeting 1/31/2021 ======
* Looking at research topics for the semester
* Talked about increasing diversity in our work
* Review literature, find papers about them
* Can discuss more ideas with Gabriel or Dr. Zutty
* Gabriel will be recording a video recording the work done so far so that we can look at possible things to explore
* I need to do some light research and reading to see what other topics there are.
* Potentially add more depth to our ARLs and get larger subtrees
* Potentially look at GECCO conference and literature

====== Action Items ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Completed Date
|-
|Create wiki page and set it up for the next semeseter.
|Completed
|1/25/2021
|2/1/2021
|1/26/2021
|-
|Meet with new subteam to go over modularity
|Completed
|1/25/2021
|1/31/2021
|1/31/2021
|}

= Fall 2020 =
Subgroup 2 (Bootcamp): [[Notebook Aryaan Anuj Mehra|Aryaan Mehara]], [[Notebook Bernadette Gabrielle Santiago Bal|Bernadette Bal]], [[Notebook Jon Greene|Jon Greene]], [[Notebook Han Mai Nguyen|Hannah Nguyen]]

Subgroup 2 Github Page: https://github.gatech.edu/amehra37/Titanic_ML_Group2

== August 19, 2020 ==

====== '''Lecture Notes:''' ======
* Genetic algorithms depend on previous generations/populations and will eventually create the best individual who's fitness is optimized.
** Initialize the population, then evaluate and select the best ones. Continue until you find the best one.

* Key words:
** Individual - one solution to the problem
** Population - set of individuals who we are altering properties of
** Objective - the problem that you are trying to solve through minimizing/maximizing
** Fitness - relative; how well is this individual compared to others?
** Evaluation - function that "computes" objective of individual
** Selection - survival of the fittest, prefers better individuals
*** Fitness proportionate - greatest fitness = higher chance of selection
*** Tournament - randomly pair individuals and the winners will reproduce

* Single Mate Crossover vs Double Point crossover to exchange mating between individuals
* Mutate: Random modifications to individuals to maintain diversity
* '''Actions:''' Randomly initialize, determine fitness, select parents, perform crossover, perform mutation, determine fitness, etc...
* '''One Max Problem Example''' - Either 0 or 1 with 100 values. We want an individual of all one's.
** Fitness = sum of all numbers to get objective value, we want to get to 100 

====== '''Lab 1 Notes:''' ======
* '''One Max Problem'''
** 1) Create FitnessMax using base.Fitness class and Individual. Base fitness can either be (1.0,) for maximizing or (-1.0,) for minimizing.
** 2) We create the random generator for the numbers in the population and make our individuals use that random generator 100 times. For the population, we create it to call the individual function multiple times.
** 3) We create the evaluation function. In this case it's just a sum.
** 4) We have 4 functions defined in the toolbox for evaluating, mating, mutating, and tournament selection.
** 5) For the genetic algorithm, we start by creating a population and use a loop for every generation
** 6) We use a selection process to choose the best individuals for mating
** 7) We mate the individuals and create children, then mutate the children slightly if necessary to keep diversity.
** 8) Now we take over the original population and replace it with all of the offspring.[[files/N Queens Graph.png|thumb|286x286px|A graph of the N queens problem from lab 1.]]
'''N Queens Problem vs One Max Problem'''
* For the N queens problem, we want to minimize conflicts between queens, so for base.Fitness we use (-1.0,).
* Our evaluate function counts queens on each diagonal
* We define our own cross-over function to swap pairs of queens.
* The idea is the same as the One Max Problem, but we tweek the functions for mating, selection, evaluation, etc.
* For mutation function I swapped the queens in a different way in the list.

====== Action Items ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Create Wiki
|Complete
|8/19/2020
|8/26/2020
|8/19/2020
|-
|Join Slack
|Complete
|8/19/2020
|8/26/2020
|8/19/2020
|-
|Complete Lab 1
|Complete
|8/19/2020
|8/26/2020
|8/24/2020
|}

== August 26, 2020 ==

====== Lecture Notes ======
'''Genetic Programming'''
* Instead of putting individual through an evaluator to get a score, the individual consumes data and returns the output
** The output will be what is evaluated
** In example, the function is Y=X^2, and we evaluate the final output.
* Tree representation
** We use tree structure with nodes as primitives (functions) and leaves as terminals (parameters)
** The input is a terminal and the output is the final function value?
** Lisp Pre-ordered Parse Tree
*** Operator followed by inputs (such as [+,*,3,4,1] is 3*4+1 = 13)
*** We start with the top and work our way out
*** Crossovers in tree-based GP is exchanging subtrees
** Crossovers work by exchanging subtrees
** Mutation - insert notes/subtree, delete, or change node/subtree
* Example: Symbolic Regression
** Evolve solution to y=sin(x) using primitives +,*,-,/
** Terminals include integers and x (a variable)
** Evaluating the tree:
*** Feed input points into function to get outputs and then run f(X)
*** Measure the error through MSE or SSE to see how close it is
** Which primitives would make it easier?
*** Power, Factorial, Sin, etc.
'''Lab 2 Part 1'''
* Added primitive np.maximum and np.square
* Tried out three mutate functions (mutShrink, mutNodeReplacement, mutEphemeral)
[[files/MutUniform.png|none|thumb|Original mutUniform function]]
[[files/MutShrink.png|none|thumb|Tested results with mutShrink function.]]
[[files/MutNodeReplacement1.png|none|thumb|Visual for mutNodeReplacement graph.]]
[[files/MutEphemeral.png|none|thumb|Graph for mutEphemeral.]]

====== Action Items ======
{| class="wikitable"
!Task
!Statuss
!Assigned Date
!Due Date
!Date Completed
|-
|First Half Lab 2
|Complete
|8/26/2020
|9/2/2020
|8/31/202
|}

== September 2, 2020 ==

====== Lecture Notes ======
'''Multiple Objectives/MO in MOGA and MOGP'''
* Supplying a population of solutions and not just single objectives!
* Translation of vector of scores from evaluation to a fitness value
* Gene pool
** Set of genome that are evaluated
* Evaluation/Scores
** True positive: How often we identify the desired objective
** False positive: How often we mistakenly identify something else as the desired objective.
** Objective: Set of measurements to score the genome/individual
*** Objective space: Set of objectives
* [https://en.wikipedia.org/wiki/Confusion_matrix Classification Measures] (Wiki Page linked)
** Data set has positive and negative samples that go through a classifier
** Through that you get Confusion Matrix with actual positives and negatives, with type 1 and type 2 errors. 
** Type 2: False negative, False positive: Type 1 error
** Visit the wiki [https://en.wikipedia.org/wiki/Confusion_matrix confusion matrix] page to refresh notes.
** '''Sensitivity/True Positive Rate/Hit Rate/Recall''': Number of TP/P = TP/(TP+FN), bigger is better
** '''Specificity (SPC) or True Negative Rate (TNR)''': TN/N = TN/(TN+FP), bigger is better
** '''False Negative Rate (FNR)''': FN/P = FN/(TP+FN), smaller is better
** '''Fallout/False Positive Rates''': FP/N = TN/(FP+TN) = 1-TNR = 1 - SPC, smaller is better
** '''Other measures'''
*** Precision/Positive Predictive Value: TP/(TP+FP), bigger is better
*** False Discovery Rate: FDR = FP/(TP+FP) = 1-PPV, smaller is better
*** Negative predictive value (NPV): TN/(TN+FN), bigger is better
*** Accuracy: ACC = (TP+TN)/(P+N), bigger is better
* '''Fitness Computation'''
** In the objective space with two objectives, we can have a graph. You can use objective functions like MSE, cost, complexity, TPR, etc.
** We give each individual a point in the objective space, called the '''phenotype''' (expression of genes) vs genotype
** '''Pareto Optimality:''' Individual is Pareto if no other individual outperforms the individual on all objectives. Set of all Pareto individuals is the Pareto frontier, and all represent unique contributions. We want to favor these individuals
*** We want to discover the true Pareto frontier.
'''NSGA II''': Nondominated sorting Genetic Algorithm II
* We separate population into nondomination ranks
* Individuals chosen using binary tournament, with lower rank beating higher rank. If same rank, we break tie with crowding distance (sum of distances of other points to this one) with higher distances winning, since it's further away.
* Strength S: How many in the population it dominates
* Rank R: Sum of S's of individuals that dominate it, nondominated if R = 0.
* We calculated the distance to find o^k and R+1/(o^k + 2) for fitness.
'''Lab 2'''
* Changed the mutation function to MutShrink and increased generation number.[[files/Original Pareto Front 1.png|none|thumb|Original pareto front with mutUniform.]][[files/Graph with mutShrink.png|none|thumb|This is the graph with the mutShrink function before we create the pareto front.]][[files/Imagejfdk.png|none|thumb|New pareto front when changed to mutShrink for mutation function.]]

====== Action Items ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Second half of Lab 2
|Complete
|9/2/2020
|9/9/2020
|8/30/2020
|-
|Email Jason with ratings
|Complete
|9/2/2020
|9/9/2020
|9/2/2020
|}

== September 9, 2020 ==

====== Class Notes ======
* Talked about titanic example, trying to create a better machine learning algorithms
** Need same pre-processed feature data, multiple objectives have to be co-dominant results
* Don't want data that has missing data, so we need to clean the dataset.

====== Group Meeting: ======
Meeting on 9/12/2020
* Aryaan Mehra helped with two versions of feature engineering to try and clean the dataset.
* We agreed to explore on our own to see if we can create a better dataset for ML
Meeting on 9/14/2020
* Decided to use the dataset that I cleaned for us to use machine learning on.
* I dropped all columns except for survived, age, sex, and Pclass for the titanic data.
* We decided to try different ML models on the dataset that I cleaned.

====== Personal Exploration ======
'''Contributions to the group:'''
* Cleaned dataset by dropping fare, embarked, sibsp, parch, cabin, name, and ticket.
* With the training and smaller test data, I got 0.844 in the test using the decision tree classifier.
* Jon decided to use an MLP model while Bernadette used Gradient boost model.
* Jon helped create a graph with our points as a makeshift Pareto front.
[[files/Confusion matrix 1.png|thumb|Confusion matrix from a Gaussian NB model using my cleaned data.]]
'''Modeling and test runs:'''
* Unfortunately, the models aren't as accurate when submitted to kaggle, with only about a 0.74 test rate.
* Link to personal exploration file: https://github.gatech.edu/amehra37/Titanic_ML_Group2/blob/master/Personal%20Exploration%20Xufei.ipynb
* Tried GaussianNB, decision tree classifier, random forest classifier, svm.SVC, NN (neural network), KNeighbors classifier and decided to stick with decision tree classifier.
*[[files/Titanic Pareto v2 jg.png|thumb|Pareto front for our group with models from cleaned data.]]Decided to use Gaussian NB model to get about 0.75 on Kaggle.
'''Self Graded Rubric:'''
* Name and contact: 5
* Teammate names: 5 (linked above)
* Neat, legible: 5
* Organization: 5
* Updated weekly: 5
* Group topics: 5
* Other individuals: 4
* To do items: 5
* To do consistency: 5
* To do cancellation: 4
* Level of details: 5
* References: 2
* Useful resource: 4

====== '''Action Items''' ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Self Grading Rubric
|Complete
|9/9/2020
|9/16/2020
|9/13/2020
|-
|Email and meet team
|Complete
|9/9/2020
|9/16/2020
|9/12/2020
|-
|Kaggle machine learning titanic
|Complete
|9/9/2020
|9/16/2020
|9/15/2020
|}

== September 16, 2020 ==

====== Class notes: ======
* Presentation Notes: Have a title slide
* Graphs:
** Have a title, labels, clearly readable
** Pareto front lines go to appropriate direction for min vs max problems
** Make sure you include page numbers during the presentation.
* You can have text on slides since presentations must be "stand-alone".
* However, don't present too long. It'll also go on our wiki page.

* On the bootcamp subteam page, you can look at the wiki and see past results from other members in past teams.
* No restrictions on primitives and operators! Can set it up as primitive tree, regression problem, etc.
'''Subteam Meeting 9/16/2020:'''
* Decide to create outline of presentation before the meeting
* Next meeting to be held on Saturday to determine how to progress
* Third meeting on Monday to finalize PowerPoint and presentation
* For now, we'll individually look at potential primitives for our project until the next meeting.
'''Subteam Meeting 9/20/2020'''
* Divided up slides to work on for presentation: I will work on ML slides and confusion matrices
* Aryaan had created template for GP modeling
** I will be exploring/adding and taking away primitives and mutation functions
* Aryaan has created the pareto front for the graph which I will be editing and changing
'''Subteam Meeting 9/23/2020'''
* Went over powerpoint which is linked [https://docs.google.com/presentation/d/1PjjLMWpuzzlQbO89IK7YysEGPhbUQ1qgWy0L_YwcQcU/edit?usp=sharing here]
* We still need to create a csv file with our data points to upload, however, the pareto front for GP is complete.

====== '''Personal Exploration:''' ======
* Looked at past presentations for ideas on how to modify titanic data
** For our GP model, here are possible evaluation functions:
*** Summing up the FP/FN and trying to minimize that
*** Do we want to prefer one accuracy over the other? Just look at FP or FN?
*** Our mutation function: mutNodeReplacement or mutUniform perhaps?
*[[files/XufeiParetoFront.png|thumb|1x1px]]Machine learning models
** Added slides with machine learning confusion matrices and names of matrices.
** ML Models used with resulting accuracy score[[files/ParetoFront5.png|thumb|Pareto front that I designed in jupyter notebook using the 8 ML models.]]
*** Decision Tree: 0.8508474576271187
*** K Neighbors Classifier: 0.7796610169491526
*** Support Vector Machines: 0.7932203389830509
*** Neural Network: 0.8338983050847457
*** Random Forest Classifier: 0.8440677966101695
*** Gaussian NB: 0.7694915254237288
*** Stochastic Gradient Descent: 0.8338983050847457
*** Passive Aggressive Classifier: 0.7288135593220338
** Tweaked and adjusted Pareto front to make it look nicer. Result is what I've created on the side.
GP Models:
* Aryaan created a template for GP modeling which Jon and I have been working on tweaking by adding more primitives. Unfortunately, it seems that the more we add, the lower the AUC gets. We have linked to it here: https://github.gatech.edu/amehra37/Titanic_ML_Group2/blob/master/Titanic_GP.ipynb

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Meet with team to work on titanic model
|Complete
|9/16/2020
|9/23/2020
|9/20/2020
|-
|View past presentations for bootcamp
|Complete
|9/16/2020
|9/23/2020
|9/18/2020
|-
|Explore new primitives and ideas for model
|Complete
|9/16/2020
|9/19/2020
|9/21/2020
|-
|Work on new evolutionary model for titanic
|Complete
|9/16/2020
|9/23/2020
|9/20/2020
|-
|Create presentation for subteam presentations
|Complete
|9/16/2020
|9/23/2020
|9/22/2020
|}

== September 23, 2020 ==

====== Class Presentations/Notes: ======
* '''Subgroup 2: (Us)''' 
** Presented first
** Should adjust for tournament selection since it is mostly for one objective instead of multi-objective.
* '''Subgroup 1:'''
** Used genHalfAndHalf with mutUniform for their loosely typed tree that returns a float. One point crossover mating.
** ML algorithms was not as good at GP accuracy, but ML was better at balancing false positives and negatives.
* '''Subgroup 3:'''
** Strongly types primitives with override for 0's with NSGA II selection with single point crossover and mutUniform. The AUC under the MOGP Pareto front is lower and more varied when compared with the ML AUC.
* '''Subgroup 4:'''
** Used NSGA2 and one point crossover, along with mutUniform. Had 200 generations.
** Genetic programming had lower AUC as well.

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Clone emade repository
|Complete
|9/23/2020
|9/30/2020
|9/28/2020
|}

== September 30, 2020 ==

====== Class ======
* '''EMADE: Evolutionary Multi-objective Algorithm Design Engine'''
** Combines multi-objective evolutionary search with high level primitives to automate process of designing ML algorithms
** To run emade...
*** Start at top level directory to run
*** Input file: xml document to configure moving parts in EMADE
*** Configure a MySQL connection, running locally server can be localhost.
*** Objectives: Names used as columns in database with the weight specifying if it should be minimized or maximized
**** Evaluation function specifies the  name of method in src/GPFramework/evalFunctions.py
**** Achievable/goal is used to steer the optimization, lower and upper bounds it.
*** We can make the memory limit low (2-3) per worker as EMADE is resource intensive.
*** Evolution parameters 
**** Controls "magic constants" or hyperparameters
***** Combines the elite pool with the offspring
**** Adding -w makes you a worker instead of starting a master process,
*** We need about 20 minutes to an hour, etc to get successful completion and see evolution.
* Run EMADE as a group with 1 person setting up the sql server as the master process
** Run for many generations
** Play with SQL, play with database
** Plot non-dominated frontier at end of run to compare with ML and MOGP assignments
** Make other plots and figures to analyze EMADE running to find successful trees
** Presentation on Monday the 21st

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Clean Notebook/Revise
|Complete
|9/30/2020
|10/7/2020
|10/3/2020
|-
|Configure mysql serve (5,6,7)
|Complete
|9/30/2020
|10/7/2020
|10/5/2020
|-
|Download/install git-lfs
|Complete
|9/30/2020
|10/7/2020
|10/5/2020
|}

== October 7, 2020 ==
'''Class'''

Went over basic download instructions for getting emade and mysql up and running on computer. Class was just to pop in and pop out for any questions that any of us might end up having.

'''Subteam Meeting 10/12/2020'''
* Tried to connect to Aryaan's mysql server to work
* Examined EMADE file
* Reviewed last recording of bluejeans to see which items to change
* Aryaan said he'd talk to Jason to fix the issue

'''Subteam Meeting 10/13/2020'''
* Issue was that we were not connected to campus vpn
* We all managed to connect to the server and ran EMADE for 30 generations
* Bernadette created the final powerpoint: https://docs.google.com/presentation/d/1tLoWIUuHeTLkjpWgDoTORUlnknGVHr7sNlDJSx1zsXI/edit?usp=sharing
'''Action Items:'''
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Meet with Subteam to Discuss emade preparations
|Complete
|10/7/2020
|10/14/2020
|10/12/2020
|-
|Connect to Aryaan's sql server
|Complete
|10/12/2020
|10/14/2020
|10/13/2020
|}

== October 14, 2020 ==

====== '''Class Meeting''' ======
* Office hours to go over some issues we had
* We needed to install a different version of mysql and finish up the powerpoint
* Might change some of the parameters of the emade folder.
[[files/Image177.png|thumb|443x443px|Headless chicken run with improvements and changing the code for the probabilities.]]

====== '''Subteam Meeting 10/15/2020''' ======
* Analyzed data from the first EMADE run
* Ended up with a messed up Pareto front but we fixed it later
Decided to change some parameters for another run
* Increased probabilities of headless chicken crossovers and decreased probabilities of some mutations

====== '''Personal Contribution:''' ======
* Added to the powerpoint to create the conclusion and comparison slides
* Final presentation: https://docs.google.com/presentation/d/1tLoWIUuHeTLkjpWgDoTORUlnknGVHr7sNlDJSx1zsXI/edit?usp=sharing
* Practiced presentation by myself and timed myself to see how long the presentation lasts.

====== '''Action Items:''' ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Modify Emade File
|Complete
|10/14/2020
|10/19/2020
|10/17/2020
|-
|Finish Final Powerpoint
|Complete
|10/14/2020
|10/19/2020
|10/18/2020
|-
|Rehearse Presentation
|Complete
|10/18/2020
|10/19/2020
|10/19/2020
|}

== October 19, 2020 ==

====== Class Presentations ======
* Stocks subteam
** Created CEFLANN Architecture and looked at implementing Keras NN in EMADE
** Looking at building portfolios using GP and EMADE
** Used technical indicators such as traders heuristics, price, volume, open interest, etc
** Did research on multiple potential indicators
** CEFLANN works quicker than other ones
*** Preprocessed data same way the paper did, with dat afrom Yahoo Finance for S&P 500 data
*** Ran regression on EMADE from technical indicators as features and let it go for 85 generations
*** Potential other EMADE primitives with ML models and look at other time windows
*** First semester members:
**** Create own technical indicator primitives for EMADE for the SPY ETF stocks dataset
**** Get idea from papers and implement them to EMADE adapting
** Bootcamp subteam 3
*** Dropped name and ticket for data, and created the "Deck category"
*** Applying machine learning models
*** AUC for ML models: 0.2541
*** Strongly typed GP with logical, relational, algebraic primitives with csOnePoint function for mating
**** mutUniform function for mutating
**** Minimized FNR and FPR
*** ML EMADE model
**** Created 3D pareto front graph
*** Challenges:
**** Used NSGA II
**** One-hot encoding
**** Bias towards dominant individuals
** Modularity
*** Try to abstract ML models even more
**** Automatically defined functions
**** ARL (Adaptive representataion through learning) introduces modularity into EMADE
***** Once they find good secion of tree, they make it a new primitive that can be used across the population
*** Future work
**** Want to see how ARLs perform on non-trivial dataset with more signal and spatial primitives 
**** New heuristics to add new ARLs with Differential Fitness, more intelligent ARLs, etc.
**** Implement more intelligent ARLs that can search through population for individuals
** Bootcamp Subteam 1
*** Dropped Name and Ticket and accounted for missing values
**** Split data into training and testing sets wiht 70% and 30% split
**** For ML they printed out all the confusion matrices
**** The MOGP had adding, subtraction, multiplication, negation, max, and min with 1 point crossover and mutUniform
**** For EMADE, they ran 17 generations with 311 final valid individuals, with 235 individuals on the pareto front
***** Used 5-fold cross validation on titanic training data
***** Pareto front is really, really small...?
** NLP NN
*** "Evolutionary Neural AutoML for Deep Learning"
*** They use tree based networks instead of graph based structures
*** Add in terminals and terminal mutations which will take a longer runtime
*** Allow one layer to branch into multiple layers in a tree structure
**** Example learner ran for 182 generations
*** No difference between using single point crossover vs two point crossover
*** Adaptive mutation function: Reduce mutations of good individuals!
*** Worked on adding primitives for the Chest X-Ray stuff
*** Worked on multiple different datasets to test their NN models
*** Potential BERT embeddings in the future
** Bootcamp subteam 2
*** Us! 
*** Fix EMADE data set to be like our other tested data
** ezCGP
*** Uses a block structure
*** They use Gaussian Noise, various types of pooling layers, and convolutions with refactored transfer learning pipeline
*** New structure using CIFAR10 to train all individuals in full (which takes a really long time)
*** Working to reduce computation time
**** Super-convergence runtime reduction
**** Future: better mating methods and better network architecture seeding
** Bootcamp subteam 4
*** Used NSGA II with mutUniform and cxOnePoint
*** 200 generations with 50 individuals elected each time, and AUC of 0.16.
*** 28 generations with 0.0277 area under curve which is really, really low!
*** EMADE < GP < ML for AUC

====== '''Action Items:''' ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Make Rank Lists of Subteams
|Complete
|10/19/2020
|10/26/2020
|10/23/2020
|}

== October 26, 2020 ==

====== Class Meeting ======
* Part of ezCGP subteam
* Joined their call after to figure out what we need to do
** Plan to meet once on Saturday and once on Monday to get crash course into ezCGP information
** Need to clone git repository and do initial set up with git accounts before Saturday meeting
[[files/EzCGP nodes.png|thumb|Basic overview of an example tree for ezCGP cartesian tree.]]
====== Subteam Meeting 10/31/2020 ======
* From 10:00-11:00
* Intro to CGP (Cartesian genetic programming)
** A tree on it's side, DAG (Directed acyclic graph) with not too many inputs, only left to right
** If just DAG structure and not DAG cartesian structure, you can have multiple inputs and rows linking to each other, but only in one direction, sometimes can be squished into one row instead of multiple rows.
*** We only use one row
** Features: reusable nodes, fixed lengths, inactive notes
*** To figure out inactive nodes, work backwards from the output since it only goes in one direction. 
*** Gets rid of bloat. 
** Implementation (Accepted ones)
*** 1 row with main columns for nodes 
*** "1+4" evolutionary strategy
**** No mating (too destructive), but get for mutant offspring from 1 parent, only mutate UNTIL we get to an active node 
* Intro to ezCGP
** Custom DAG, custom primitives/data types
*** Have not gotten to custom features yet 
** [https://ieeexplore.ieee.org/abstract/document/6815728 Original Paper] 
** We have a main output node and we count it by indices 
** Rather than random numbers floating around, we have a table of random values for the blocks to choose, grabs hyperparameters from list of arguments
*** Now we only have to pass in data, since primitives already in this bank
**** True mutate to false, 2 to another integer, etc with each having own way to mutate 
**** Arguments might not be used, might be used multiple times 
** Main node is a dictionary for strongly typed primitives  
** Blocks
*** We have blocks of code where data preprocessing happens in first chunk and data classification in second chunk 
*** Thus there are sub genome structures with own rules, parameters, etc. for more customizability 
*** We define the order but the computer decides most of the custom components 

====== Personal Exploration: ======
* Set up SSH Keys for windows which was the hardest part
** Had to look up windows guide here: https://phoenixnap.com/kb/generate-ssh-key-windows-10
** Downloaded git bash in order to install SSH
** Changed python version to an earlier version that works with open ssh

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Set up public github account
|Complete
|10/26/2020
|10/31/2020
|10/28/2020
|-
|Set up dependencies/download packages
|Complete
|10/26/2020
|10/31/2020
|10/28/2020
|-
|Learn about intro to ezCGP
|Complete
|10/26/2020
|10/31/2020
|10/31/2020
|}

== November 2, 2020 ==

====== Class Meeting: ======
* Stocks team
** Caught up first semester students
** Added primitives to emade
** Working on editing data to find sources of error
* ezCGP (our subteam)
** Code review and and questions section after main meeting for new students
** Past meeting last week for new students on info
* NLP
** Working on coding and caught up new students
* Modularity
** Got statistical significance showing improvement for first ten generations for changes to tournament selection
** Looking into more literature to modify their ARL vs alternative selection
** First semester students got a lecture on modularity

====== Subteam Meeting: ======
* Need to scope out what the subteam has done in the past.
* First semester students notes:
** We have observations as rows and columns as various features, and the final y is the classification
** X => data with observations and features, eg 500 x 10
** Y => classification matrix, eg 500 x 1
*** We want to solve for XA + B = Y so that a0x0 + a1x1 _ + ...b = y + error where A is size 10 x 1 and is a linear transformation
** Neural network: A set of linear transformations and nonlinear functions
*** We have layers where each layer is a linear transformation and nonlinear functions
*** Data -> NN with 3 layers
*** X_input*A_0 + B_0 = X_0
*** X_0*A_1 + B_1 = X_1
*** X_1*A_2+B_2 = X_2
*** Each transformation can whittle down the matrix to lower dimensions
*** Non linear transformations with some linear components! Maps n dimensions to 1 dimension
** Images
*** The matrix are pixels corresponding to picture dimension and each value is between 0 and 255 
*** Next we do a computation with a kernel and image chunk to get a number
**** In turn, this replaces to get a "future space", but mapping image to a same size or smaller size
*** We apply more kernels to get another future map from this one, etc
*** Eventually, we hope the final classification is enough to get us a classification
* Logistics
** Make sure tensor flow is 2.x version
*** import tensorflow as tf
*** tf.keras.....
** Built model/graph first (DAG)
*** input = tf.keras.Input_Layer(image-size(500,600,5)) #defines shape
*** method = tf.keras.Conv2d(kernel=(3,3))
*** x = method(input) OR x = tf.keras.Conv2d(kernel=(3,3))(x)
*** This can be followed by multiple other laters
*** kernel is activation which is nonlinear function
*** For the last layer we have y = tf.keras.Conv2d(...)(x)
*** tf.keras.Model(input,y) to show where to start and where to stop

====== Subteam Meeting 11/5/2020 ======
* Returning students - set up a time to meet outside of class
* New students (me) - three experiements, try implementing the titanic data for ezcgp and for old students to use as benchmark.
** Compare to titanic emade results vs ezcgp
* Steps for getting started
** Save dataset into dataset folder and use datatools
** Next week we meet with Jason to figure stuff out

====== Personal Exploration: ======
* Read through old entries of ezCGP's meeting to see what they've accomplished.
** Link to subteam is [[Fall 2020 Sub-team Weekly Reports#EZCGP|here]]
** Originally in 2 small groups, one for research to look for papers to read for ideas and implementation. Another is for code maintenance to work on current code.
** Began runs on multi-gaussian test problems to make sure primitives work
** Migrated old primitives to new code development branch and created [https://github.com/ezCGP/ezCGP/wiki/Github-Flow:-Committing-Code guidelines]
** Learned new framework with Tensorflow2 and tried to seed high performing individuals into new framework
** Set up PACE-ICE to start trying to run new issues

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Review previous week's work
|Complete
|11/2/2020
|11/9/2020
|11/8/2020
|-
|Look into implementing titanic data into ezCGP
| --
|11/5/2020
|11/9/2020
|Moved to next week
|}

== November 9, 2020 ==

====== Class Meeting ======
* Stocks subteam
** Worked on sanity checks to compare results to paper
*** Paper explained functions used to explain trend score, but the table provided was not consistent
*** Focused on genetic algorithms with the truth data that they already had
*** Results are not currently lining up paper results
* EZCGP (Our subteam)
** Set up base and connection issues on Saturday for returning sub-members
** Try to have baseline scores done by next meeting
** Fixed some problematic methods since last time
*** No more tensor flow errors
* NN/NLP
** Adding to the code base with new targeting done on data sets so they'll have a few weeks for tests
** The bounding boxes researchers found that 8 out of 15 classes worked on xray dataset so it may not generalize well.
** Have been looking at Amazon product reviews on Kaggle.
* Modularity 
** First semesters are going to be assigned runs and experiments so they can begin analysis
** Continue doing runs from GTRI inspiration
** Selection method uses binary tournament
** Should also do a sanity check and need to compare to the benchmark

====== Subteam Meeting: ======
* Make sure PACE documentation sheet is updated
* Very lowkey, once PACE is working we'll be able to help as well
* Grant username access as well
* In future, make sure you branch off of 2020 repo to show your own work

====== Personal Exploration ======
* Working on installing SSH for windows
** Used gitbash and solved the issue
** Also updated Windows 10 so that I could install the SSH client
** Got PACE-ICE working through the documentation that was given out
* Downloaded a special software called WinSCP to make using PACE-ICE easier
** Link and information found here: https://docs.pace.gatech.edu/storage/winscp_guide/
** Notes for this:
*** You have to be on Tech's vpn on or campus to connect
*** For host name leave it as pace-ice.pace.gatech.edu
*** Leave the password section blank - a gui will pop up and ask you to input the password after it connects
** WinSCP works as a GUI method of navigating through PACE-ICE server so you won't have to do it on command line if you have a windows computer
** Makes it easier to download multiple files at once

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Review PACE Documentation
|Complete
|11/9/2020
|11/16/2020
|11/12/2020
|-
|Set up PACE on computer
|Complete
|11/9/2020
|11/13/2020
|11/12/2020
|-
|Begin researching ways to implement titanic data into ezCGP
| --
|11/9/2020
|NA
|Cancelled task to focus on MINIST Data Upload
|}

== November 16, 2020 ==

====== Class Notes ======
* Stocks subteam
** Calculated given trendscore with the stats in the paper but still not reproducible
** Going to email the people who wrote the article to see if they can reproduce the results
* ezCGP
** Started doing baseline runs, working on making a script that resubmits themselves
** Restricted to 10 hour run time
* NLP
** Only blocker is GPU system
** Uses notion for keeping track of work
* Modularity
** Going slower because they're limited by the individuals 

====== Subteam Meeting ======
* Assignment: Load in MNIST data set into numpy arrays and play around with it.
** See what you can decipher from MNIST data set
* Plans to continue running ezCGP trials and see if we can sort out bugs in the PACE-ICE system to get it to run

====== Subteam Meeting 11/19/2020 ======
* Finally got PACE-ICE working, and Rodd said he'd take care of training to rerun the simulations
** Plan to rerun with different seeds
** I'm also going to skim through and see if I can help with running a few simulations through PACE ICE
* Overview of final presentation
** Intro to ezCGP and code development progress
** More information on runs, experimental setup, and extra work done with seeding.
** Work to replicate emade on the ezCGP data to see if we get the same result
** Current benchmarks - maybe create visuals for them?
*** Also create visuals for genomes
* Need to get MNIST data in to the easy data sets for processing => talk about process for loading them in
** Creating an easy data set without downloading it through tensor flow
** Focus on block structure optimization
** Assigned to work on creating the Pareto plot for generational fitnesses to visualize how well it's doing
[[files/Overview of Process.png|thumb|An overview of the process we go through to classify handwritten digits taken from the site linked in the notebook.]]

====== Personal Exploration: ======
* Looked at information from MNIST dataset to prepare myself for downloading scripts.
** Information on loading in sourced from [http://rasbt.github.io/mlxtend/user_guide/data/loadlocal_mnist/ here]
** Overview
*** MINIST constructed from two datasets, consisting of a training set that has handwritten digits from 250 people
*** Half are high school students, half are from Census Bureau
*** Each feature vector has 784 pixels with 50000 images
*** Target variable is the respective handwritten digit from 0 to 9
* MNIST Information from Handwritten Digits classification information can be found [https://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/ here]
** Working on building a handwritten digit classifier using OpenCV
** We convert an input image into its features, then through a learning algorithm to a label assignment as seen in picture to the side
** Step 1: Deskewing or Preprocessing
*** We align the images to a reference image, normally through a facial feature detector. 
*** Can apply a similarity transformation to an image such as fixing the slant of written digits so that it's no longer skewed
*** Done through using image moments that OpenCV provides
** Step 2: Histogram of Oriented Gradients (HOG) descriptor
*** We convert he image to a feature vector using HOG descriptor that you can read about [https://www.learnopencv.com/histogram-of-oriented-gradients/ here]
*** Tweak parameters and test to see which gives you the best results
** Step 3: Model training and Learning a Classifer 
*** For images, we can use SVM as classification algorithm which you can read about [https://www.learnopencv.com/image-recognition-and-object-detection-part1/ here]
* Pareto front code block
** Found past code done by [[Notebook Aryaan Anuj Mehra|Aryaan]] when we worked together from the bootcamp which I've added into my code block
** Link to code block here: https://codeshare.io/5eAJbJ
** Edited the Minimization section to turn it to maximization by changing the <c to >c on line 9 of the code since I will be maximizing the recall and precision of the ezCGP data
** Code takes in numpy array or data frame, and returns the information for us to plot the front.
** Steps for plotting the front also done by [[Notebook Aryaan Anuj Mehra|Aryaan]] and is linked here: https://codeshare.io/G8mgxZ
** In the future I will need to preprocess and clean the data so that I can use the Pareto front function to plot fronts for the generational data

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|Look up information on MINIST
|Completed
|11/16/2020
|11/23/2020
|11/21/2020
|-
|Examine past Pareto front code from bootcamp
|Completed
|11/19/2020
|11/23/2020
|11/22/2020
|}

== November 23, 2020 ==

====== Class Meeting ======
* Stocks subteam
** Evaluated a couple of individuals using training from price data and technical indicators 
** Plan to finish technical indicator primitives in EMADE soon
** Contacted researchers of paper but did not get response
** Inverse shares correlate with price data
* ezCGP
** Working on doing their baseline runs to get data
** Gotten some individuals after 1 test run with 8 hours and visualized the individuals
** Almost done with the basic outline
* NLP
** Baseline runs for each data set with 2 runs for four different scenarios to accurately get targeting
** Worked on runs with original primatives
** Created documentations and new primatives for computer vision
*** Looked into future extraction methods and not sure how they fit in
*** How to combine primatives in three different files with any restrictions?
*** Spatial methods revolve around csv instead of numpy
* Modularity
** Planning to get 10 different runs before the final presentation, need to take in the data pair and return the data pair.
** Need to do more analysis to notice the changes in certain generations where irregularities were hpapening.
** Multiple ARLs happening when data pairs are merged
** Also working on MNIST (which I'm currently doing)
*** Might reach out to them for help
* Note: 20-25 minute presentations with 5-10 minutes of presentations

====== Subteam Meeting ======
* Daniel led the meeting since Rodd was absent
** Summary of results from last week and reiterated who is working on which slides
* Working on finishing our current slides
* Moved next meeting to Saturday at 2PM
[[files/Pareto Front for Generation 1.png|thumb|171x171px|Pareto front for the first generation with associated AUC with points]]

====== Subteam Meeting 11/28/2020 ======
* Met to update each other on final progress and finish up the final presentation
** In depth steps to PACE-ICE and setup is complete on the notion page [https://www.notion.so/PACE-ICE-GPU-for-Ez-CGP-8be7a2e57c6649229f36505d093952dd here] done by Hoa Luu.
*** Decided on PACE-ICE since it was fast and had better memory usage
** Need to finalize results from the runs so that we can analyze the data for Daniel to look at
** Need to pass in metrics for tensorflow to record as we're training the model through attributes that help the fit method.
* Our final presentation is linked [https://docs.google.com/presentation/d/1cbx_daOsFvMZIgBQvVnmiJBmmm-Lvha61Mfe68Ej7c8/edit?usp=sharing here]
** Future goal - to mix ezCGP with emade since emade uses DEAP, to see the comparisons in the results
[[files/Pareto front for generation 1 without points.png|thumb|167x167px|The pareto front for the first generation with AUC and without the points]]
====== Personal Exploration ======
* Completed the script for the MNIST data loader
** Followed the instructions from  [http://rasbt.github.io/mlxtend/user_guide/data/loadlocal_mnist/ here]
** We had to install mlxtend so that we could run the code, using "pip install mlxtend" in the command line
** Other than that we were able to directly copy and paste the code in the command line
** Worked with Angela Young to finish this script together
* Completed successful first pareto plot for generational fitness
** Complete script for generating the pareto front is seen [https://codeshare.io/50KjEN here]
** Pareto front for the first generation is seen to the side
** Notice the first AUC is 0.92 for the first generation
** To preprocess the data, first we multiply by -1 to get positive values
*** This is because ezCGP works to minimize values, so we multiplied our values by -1 so that we could minimize instead of maximize
** Need to work to just plot the line and get rid of the points on the graph and then graph multiple lines on the same plot.
*** Issue: A lot of our final AUCs and Pareto fronts past the second generation are so similar that there is no visual difference
*** Solution: I will only graph the first generation and ninth (final generation) on the plot in the final PowerPoint which is below
** Also want a chart for just the AUC or note the final AUC
*** Issue: The AUCs are similar and we cannot see a difference
*** Solution: I deleted the first generation and showed the change in AUC excluding that. Unfortunately, the AUCs for the second and third generation are the same, and the AUCs for generations 5-9 are also the same.
Completed PowerPoint slide with the three main charts is shown below
[[files/PPT slide I worked on.png|center|600x600px|Contains the three graphs that I did in matplotlib so that you can see how the AUC changes with the generations]]
* Personal Analysis of my Designed Graphs:
** Notice that there is a significant change between generations 1 and 9
** With generation 1 included, we see a large jump in AUC from 1 to 2 relative to other jumps
*** Typically, this would be a small jump, but our original model did so well it was hard to improve
*** The good performance is due to the effect of transfer learning blocks that were developed in the past
** Generations 5-9 were the same pareto front and AUC, this may be because there was nothing else to improve and it was already very close
** Without generation 1, we can see some improvement from generations 2 to 5. At the very least, our learning model ended up progressing and doing better until it was too difficult to have it do any better
* Final AUCs of all of the curves are listed below:
** Generation 1: 0.920881502437016
** Generation 2: 0.981823413391421
** Generation 3: 0.981823413391421
** Generation 4: 0.981861232839681
** Generation 5: 0.9819120285577867
** Generation 6: 0.9819120285577867
** Generation 7: 0.9819120285577867
** Generation 8: 0.9819120285577867
** Generation 9: 0.9819120285577867
* You can find all the code I used and files of generational data which I pulled from PACE-ICE here: https://github.com/ezCGP/ezCGP/tree/master/post_process/ezCGP%20pareto%20front
** I committed this to the ezCGP repository under post_process
* Presentation Notes I created for the slide:
**Results:
*** Left graph, we have the pareto front of the first generation and ninth generation for comparison, with their associated AUCâ€™s underneath the graph. As you can see, there was an improvement
*** Weâ€™re trying to maximize both recall and precision so we want higher AUCs
*** Issue: The change in the AUCs and fronts donâ€™t have a visual difference past the first generation, as we can see in the graphs on the right
*** First generation jumps up very high with AUC very quickly
**** Included second plot that doesnâ€™t have the first generation so you can see the small increases in AUC from generations 2 to 5
**** From generations 5 to 9, however, there is no increase in the AUC
** Analysis
*** Graph 1:
**** We evolved with a 3rd objective score (accuracy) which was poorly calculated so we removed it from our presentation.
**** This is why it Generation 9 isnâ€™t strictly dominating Generation 1.
*** Small change in AUC/almost no change?
**** A small population size of 20 made it hard to push the pareto front after it reached competitive scores, less diversity
**** We just started with really good performance...transfer learning block had a lot to do with that
**** The reason the auc didnâ€™t improve much after the second generation was because our original model already performed really well
**** By the fifth generation the model couldnâ€™t find a way to improve anymore

====== Action Items: ======
{| class="wikitable"
!Task
!Status
!Assigned Date
!Due Date
!Date Completed
|-
|MNIST data loader
|Complete
|11/23/2020
|11/30/2020
|11/27/2020
|-
|Plot first pareto plot for generationial fitness
|Complete
|11/23/2020
|11/30/2020
|11/26/2020
|-
|Create plot for multiple Pareto fronts
|Complete
|11/28/2020
|12/1/2020
|11/28/2020
|-
|Create plot for change in AUCs
|Complete
|11/28/2020
|12/1/2020
|11/28/2020
|-
|Run over analysis of fronts and data with Rodd
|Complete
|11/28/2020
|12/2/2020
|11/28/2020
|-
|Finish and polish notebook
|Complete
|11/23/2020
|12/2/2020
|11/30/2020
|}

== December 2, 2020 ==

====== Final Presentations ======
* '''Stocks Presentation'''
** Research paper inconsistencies
*** Trading signal calculated did not align with rules researchers specified
** Genetic labeling
*** Developed new training method to deal with inconsistencies
*** Developed new fitness function to treat time point independently to optimize profit per transactions
*** Compared two evolutions and looked at overlap for differences in results
** Technical Indicator Primatives
*** Tried simple moving average (used by paper)
*** Exponential moving average prefers recent days
*** Moving average convergence/divergence shows when and how often the direction signal line gets crossed
*** Stochastic Oscillator and relative strength is also used
*** Used EMADE features to receive more complex inputs than just prices and return an OBV value for time range
** Developing more technical indicators
*** Bollinger bands (moving average and upper/lower band)
*** CCI (momentum based and measure price relative to average time)
*** MFI (momentum and volume based to measure flow of money)
** Feature Data EMADE run
*** Truth values from paper don't align with current results, results may be a fluke
*** Second run used stream to features and worked on 10 days at a time, had issues with normalization
** Future work
*** Run Stream_to_Features on normalized data with different time ranges
**** Would smaller or larger time ranges be better?
*** Test granularity 
*** Explore with genetic labeling
*'''Neural Networks Presentation'''
**Uses an evolutionary approach to neural architecture search using EMADE
**Compared to Evolutionary Neural AutoML for Deep Learning
***Uses toxicity dataset and Chest X-ray
***They used AUROC but subteam used accuracy
***Used a combination of EMADE and NNLearner with tree based networks rather than graphs
****This increased the search space!
**Changes to Architecture
***BERT is pre-trained language model, and they used distilled version for less memory/faster processing
***In the future, want to do more with BERT
**New matting/mutations
***Single point and two point
***Adaptive mutation function
****Mutated bad individuals, decrease mutation chance for good individuals (which decreases randomness)
****In the future, use a logistic function to determine mutation probabilities depending on fitness in generations
***Work with PACE cluster and also worked with my subteam to look at EMADE on pace
****Some issues such as running out of space and user doesn't have permissions
***CV Primitives added 
****Adaptive mean threshholding
****Adaptive gaussian thresholding
****Otsu's Binarization (2D) which minimizes weighted within-class variance 
***Also began primitives documentation on Notion with 40 primitives and 17 variants
**Results and Goals
***Wikidetox dataset looked at 160k comments with classification of whether it is toxic or not
****Unbalanced since 9.5% of data was toxic
****Ran 24 generations
***Chest X-ray had 14 possible diseases looking at over 100k x-rays
****Trained with 70% of data, tested with 20%, validated with 10%
****Bounded box/YOLO architecture only looks at images once for improved speed
*****How much accuracy is sacrificed for this?
*****Unfortunately not all X-rays had this and some diseases are not localized to a single region
****Evolution didn't get to better models since they were simple. EMADE made tiny networks relative to others since it wants few parameters
*****In the future can add more mutations or modify the objective. Also possible to seed in larger models
***Amazon product reviews dataset
****Looking to determine tone from the text
****However, some of the results are in other languages
*****In future, possible to maybe build in a translator?
**Future Works
***Looking at Multi Task Learning
***Trying to make BERT layer beter
***Working on more datasets
***Trying coevolution
***Looking at more complex adaptive mutations
*'''ezCGP Presentation (Our Subteam)'''
**Notice the lack of diversity in our individuals
***Previous individuals could change brightness slightly but ezCGP liked the pass-through and transfer learning creates the best individuals
***Replacing the dense NN with the green blocks
*'''Modularity Presentation'''
**Working to abstract individuals to create blocks that can help the genetic process
***Goal is to have reusable blocks of code 
***Hopefully lead to novel solutions
***Based on Adaptive Representation through Learning (ARLs) 
**Currently:
***Search population for combination of parent and children nodes and choose combination based on cdf from frequency/fitness
***Next, abstract the combinations into single node
***This way, ARLs can wrap around each other and grow
**Experimentation on titanic dataset
***Has 40 generations with 10 trials with seeded runs (5 individuals each)
***Uses default parameters
***Differential Fitness: Different in individual and the most fit parent as a herustic
***Only find individuals with positive differential function values, modified cdf to value individuals with higher differences
**Results
***Big significance in generations 16-19 that converges with basline in future generations
***Pareto individuals used ARLs but they're mostly pass primitives
***Alternative selection method: Wants to increase probability of getting ARLs
****Individuals more likely to be picked depending on number of ARLs that they have in linear fashion 
****However, some individuals have bloat with over 40 ARLs
***Next, they restricted data pairs to only allow subtrees that return EmadeDataPair object such as filters and learners
****Unfortunately, no statistical significance
****Does not converge in the future but could be limited by titanic dataset
***Last data pair restrictions were merged with alternative selection to reduce bloat
****Most individuals only have 0,1,or 2 ARLs
****Performs worse overall but there is a small sample size
**Overall Results/Future
***Many generations have results from generations 10-20
***Overall, ARLs seem to be beneficial
***They may lower the std.dev in later generations
***In the future
****MNIST dataset
*****Looks like a lot of what we've done in our subteam
****Measuring diversity
****Heuristics, integrating ARLs and ADFs
*****Maybe compare individual's fitness with the population
*****ADFs (automatically designed functions) are more individualistic but also mroe evolable
****Modifying creation of ARLs
*****Increased targeting and isolation with abstract larger trees
****Increase genetic material