== Overview ==
Speech and signal processing have many applications in both the enterprise and consumer worlds, ranging from language detection to power your phone's personal assistant (Siri, Ok Google, Alexa, etc.), to discovering new songs (Shazam), to powering closed captioning for videos (YouTube). 

Final group presentation from the Spring 2018 semester:       [[files/AAD VIP - NLP Group Spring 2018.pdf]] 

===Goals===
----
The overall goal of this sub team project us to use genetic programming and EMADE to improve the accuracy and efficiency of speech/signal processing and unlock new applications, including video processing and natural language processing (i.e. sentiment detection).

'''1''': Explore the characteristics of speech signals to create useful features capable of being used by EMADE for classification.
* Zero crossing rate
* Signal Energy
* Entropy of Energy
* Spectral Centroid 
* Spectral Entropy
* Spectral Flux
* Spectral Roll Off
* Harmonic Ratio
'''2''': Analyze audio samples as a time series of frequencies. Use existing, and add additional signal processing methods as primitives and use EMADE to classify each time series to a spoken word.
* Hidden Markov Models
* Dynamic Time Warping
* Neural Networks
* Connectionalist Temporal Classification

'''3''': Once image processing capability is implemented into EMADE, create a spectrogram image of each audio signal, then have EMADE classify each spectrogram to a spoken word.

===Members===

----
Michael Hill <br>
Matt Lewine <br>
Joe Mosby <br>
Hrisheek Radhakrishnan <br>
Bhargava Samanthula <br>

== Background Research ==

=== What is the problem? ===
Given speech as a set of finite sound intervals (O), you need to find the model of a target instance M that maximizes P(M | O).

===What Has Worked Well?===
'''Feature Extraction'''

This is the first step in automatic speech recognition. We want to identify the components of the audio signal that are useful for identifying linguistic content and discard things like background noise and emotion.

Low-level feature extraction is actually quite similar to human hearing. One of the main steps in extracting a feature called Mel-frequency is motivated by the human cochlea (an organ in the ear) which vibrates in different areas depending on the frequency of incoming sounds. Depending on this location, different nerves fire to the brain that indicate the presence of certain frequencies. 

Logarithms are also heavily involved, which are also motivated by human hearing since we don't hear loudness on a linear scale. A compression operation makes features match more closely to what humans actually hear. 

Other main low-level feature extraction methods such as Wavelet Based Features Non-Negative Matrix Factorization also help to provide features which describe linguistic audio. The post-processing of the low-level features with a neural network can then be used to generate more robust features.

'''Hidden Markov Model'''

The most widespread and successful approach to speech recognition systems is the Hidden Markov model. At each speech interval, the HMM will calculate the probabilities that the sound sequence will follow the model of each target instance. These probabilities are computed efficiently using viterbi decoding. HMMs are great for processing temporal and sequential data. A good explanation of the process and how it is programmed can be seen in this paper: 'Implementing a Hidden Markov Model Speech Recognition System in Programmable Logic' by S.J. Melnikoff, S.F. Quigley & M.J. Russell.   

'''Perceptual Linear Predictive (PLP)'''   

'Perceptual linear predictive (PLP) analysis of speech' by Hermansky H.This technique uses three concepts from the psychophysics of hearing to derive an estimate of the auditory spectrum: (1) the critical-band spectral resolution, (2) the equal-loudness curve, and (3) the intensity-loudness power law. 

'''Neural Networks'''   

HMM's modern competitor is a system of neural network based feature extraction in combination with neural network acoustic models. These have shown to be more accurate than PLP speech analysis in a paper published in 2015: 'On Deep and Shallow Neural Networks in Speech Recognition from Speech Spectrum' 

'''Short-Time Analysis''' 

Spectral content of speech is non-stationary and changes over time. Applying Discrete Fourier Transforms over long periods of time does not reveal changes in spectral content. By applying DFT's over short time intervals, called blocks or frames, properties of the speech waveform can be assumed to remain relatively constant, or stationary. 

Two basic short-time analysis functions useful for speech signals: 
# Short-time energy- an indication of the amplitude of the signal in the given interval.  
# Zero-crossing rate- defined as the weight average of the number of times the speech signal changes within a given time window.  
Short-time energy and short-time zero-crossing rate can be the basis for an algorithm for making a decision as to whether the speech signal is voiced or unvoiced at a particular time. A complete algorithm would involve measurements of the statistical distributions of the energy and zero-crossing rate for both voiced and unvoiced speech segments. These distributions can be used to derive thresholds used in voiced/unvoiced decision. 

'''Function Words vs Content Words''' 

Function words carry only grammatical meaning such as: prepositions (in, on, at), auxiliaries (are, was, do), quantifiers (some, any, all), and pronouns (he, we, this). Content words carry real meaning such as: nouns (Michael, chair, computer), verbs (hit, swim, eat), adverbs (wrongly, frequently, generally), and adjectives (beautiful, green, fantastic). Normally content words carry stress. Function words are typically unstressed and occur much more frequently. In connected speech, frequent words have shorter durations and a variety of other lenited (shortened or weakened) characteristics such as reduced vowels, deleted codas, more tapping and palatalization, and reduced pitch range. 

This is useful when for using surrounding words to predict or interpret a particular word in a sentence. The features listed above could be used to narrow the search space and identify a word in less time. 

===Why Is It Important?===
There are numerous practical applications of speech recognition technology. In cars, home assistant systems like google home, in computers like cortana, in your phone like speech to text and siri, etc. Improving the accuracy pf speech recognition technology is particularly important in courtrooms and depositions where a scribe is still necessary to verify the accuracy of the transcription of a conversation. This technology can also be used in education to provide students with transcriptions of lectures, or for writing essays through speech. 

== Resources ==
https://www.kaggle.com/c/tensorflow-speech-recognition-challenge

https://www.kaggle.com/davids1992/speech-representation-and-data-exploration/notebook

https://arxiv.org/abs/1412.5567

https://www.kaggle.com/solomonk/pytorch-speech-recognition-challenge-wip

https://link.springer.com/content/pdf/10.1007%2F978-1-4471-5779-3.pdf

https://github.com/tyiannak/pyAudioAnalysis/blob/master/audioFeatureExtraction.py

http://research.iaun.ac.ir/pd/mahmoodian/pdfs/UploadFile_2643.pdf

https://www.sciencedirect.com/science/article/pii/S0749596X08000600

http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/

https://github.com/jameslyons/python_speech_features

http://ieeexplore.ieee.org/document/5276486/

'Perceptual linear predictive (PLP) analysis of speech' by Hermansky H.

'Implementing a Hidden Markov Model Speech Recognition System in Programmable Logic' by S.J. Melnikoff, S.F. Quigley & M.J. Russell.

'On Deep and Shallow Neural Networks in Speech Recognition from Speech Spectrum'